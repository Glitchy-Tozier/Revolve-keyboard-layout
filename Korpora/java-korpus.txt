/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.node.Node;
import freenet.support.SimpleFieldSet;

public class UnknownNodeIdentifierMessage extends FCPMessage {

	final String nodeIdentifier;
	final String identifier;
	
	public UnknownNodeIdentifierMessage(String id, String identifier) {
		this.nodeIdentifier = id;
		this.identifier = identifier;
	}

	@Override
	public SimpleFieldSet getFieldSet() {
		SimpleFieldSet sfs = new SimpleFieldSet(true);
		sfs.putSingle("NodeIdentifier", nodeIdentifier);
		if(identifier != null)
			sfs.putSingle("Identifier", identifier);
		return sfs;
	}

	@Override
	public String getName() {
		return "UnknownNodeIdentifier";
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node)
			throws MessageInvalidException {
		throw new MessageInvalidException(ProtocolErrorMessage.INVALID_MESSAGE, "UnknownNodeIdentifier goes from server to client not the other way around", nodeIdentifier, false);
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		container.delete(this);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.net.MalformedURLException;
import java.net.URL;
import java.net.URLConnection;
import java.util.Arrays;

import com.db4o.ObjectContainer;

import freenet.io.comm.PeerParseException;
import freenet.io.comm.ReferenceSignatureVerificationException;
import freenet.node.DarknetPeerNode.FRIEND_TRUST;
import freenet.node.FSParseException;
import freenet.node.Node;
import freenet.node.OpennetDisabledException;
import freenet.node.PeerNode;
import freenet.support.Fields;
import freenet.support.SimpleFieldSet;

public class AddPeer extends FCPMessage {

	public static final String NAME = "AddPeer";
	
	SimpleFieldSet fs;
	final String identifier;
	final FRIEND_TRUST trust;
	
	public AddPeer(SimpleFieldSet fs) throws MessageInvalidException {
		this.fs = fs;
		this.identifier = fs.get("Identifier");
		fs.removeValue("Identifier");
		try {
			this.trust = FRIEND_TRUST.valueOf(fs.get("Trust"));
		} catch (NullPointerException e) {
			throw new MessageInvalidException(ProtocolErrorMessage.MISSING_FIELD, "AddPeer requires Trust", identifier, false);
		} catch (IllegalArgumentException e) {
			throw new MessageInvalidException(ProtocolErrorMessage.INVALID_FIELD, "Invalid Trust value on AddPeer", identifier, false);
		}
	}

	@Override
	public SimpleFieldSet getFieldSet() {
		return new SimpleFieldSet(true);
	}

	@Override
	public String getName() {
		return NAME;
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node) throws MessageInvalidException {
		if(!handler.hasFullAccess()) {
			throw new MessageInvalidException(ProtocolErrorMessage.ACCESS_DENIED, "AddPeer requires full access", identifier, false);
		}
		String urlString = fs.get("URL");
		String fileString = fs.get("File");
		StringBuilder ref = null;
		BufferedReader in;
		if(urlString != null) {
			try {
				URL url = new URL(urlString);
				URLConnection uc = url.openConnection();
				// FIXME get charset from uc.getContentType()
				in = new BufferedReader( new InputStreamReader(uc.getInputStream()));
				ref = new StringBuilder(1024);
				String line;
				while((line = in.readLine()) != null) {
					line = line.trim();
					ref.append( line ).append('\n');
				}
				in.close();
			} catch (MalformedURLException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.URL_PARSE_ERROR, "Error parsing ref URL <"+urlString+">: "+e.getMessage(), identifier, false);
			} catch (IOException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.URL_PARSE_ERROR, "IO error while retrieving ref URL <"+urlString+">: "+e.getMessage(), identifier, false);
			}
			ref = new StringBuilder(ref.toString().trim());
			if(ref == null) {
				throw new MessageInvalidException(ProtocolErrorMessage.REF_PARSE_ERROR, "Error parsing ref from URL <"+urlString+ '>', identifier, false);
			}
			if("".equals(ref.toString())) {
				throw new MessageInvalidException(ProtocolErrorMessage.REF_PARSE_ERROR, "Error parsing ref from URL <"+urlString+ '>', identifier, false);
			}
			try {
				fs = new SimpleFieldSet(ref.toString(), false, true);
			} catch (IOException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.REF_PARSE_ERROR, "Error parsing ref from URL <"+urlString+">: "+e.getMessage(), identifier, false);
			}
		} else if(fileString != null) {
			File f = new File(fileString);
			if(!f.isFile()) {
				throw new MessageInvalidException(ProtocolErrorMessage.NOT_A_FILE_ERROR, "The given ref file path <"+fileString+"> is not a file", identifier, false);
			}
			try {
				in = new BufferedReader(new FileReader(f));
				ref = new StringBuilder(1024);
				String line;
				while((line = in.readLine()) != null) {
					line = line.trim();
					ref.append( line ).append('\n');
				}
				in.close();
			} catch (FileNotFoundException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.FILE_NOT_FOUND, "File not found when retrieving ref file <"+fileString+">: "+e.getMessage(), identifier, false);
			} catch (IOException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.FILE_PARSE_ERROR, "IO error while retrieving ref file <"+fileString+">: "+e.getMessage(), identifier, false);
			}
			ref = new StringBuilder(ref.toString().trim());
			if(ref == null) {
				throw new MessageInvalidException(ProtocolErrorMessage.REF_PARSE_ERROR, "Error parsing ref from file <"+fileString+ '>', identifier, false);
			}
			if("".equals(ref.toString())) {
				throw new MessageInvalidException(ProtocolErrorMessage.REF_PARSE_ERROR, "Error parsing ref from file <"+fileString+ '>', identifier, false);
			}
			try {
				fs = new SimpleFieldSet(ref.toString(), false, true);
			} catch (IOException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.REF_PARSE_ERROR, "Error parsing ref from file <"+fileString+">: "+e.getMessage(), identifier, false);
			}
		}
		fs.setEndMarker( "End" );
		PeerNode pn;
		boolean isOpennetRef = Fields.stringToBool(fs.get("opennet"), false);
		if(isOpennetRef) {
			try {
				pn = node.createNewOpennetNode(fs);
			} catch (FSParseException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.REF_PARSE_ERROR, "Error parsing ref: "+e.getMessage(), identifier, false);
			} catch (OpennetDisabledException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.OPENNET_DISABLED, "Error adding ref: "+e.getMessage(), identifier, false);
			} catch (PeerParseException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.REF_PARSE_ERROR, "Error parsing ref: "+e.getMessage(), identifier, false);
			} catch (ReferenceSignatureVerificationException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.REF_SIGNATURE_INVALID, "Error adding ref: "+e.getMessage(), identifier, false);
			}
			if(Arrays.equals(pn.getIdentity(), node.getOpennetIdentity()))
				throw new MessageInvalidException(ProtocolErrorMessage.CANNOT_PEER_WITH_SELF, "Node cannot peer with itself", identifier, false);
			if(!node.addPeerConnection(pn)) {
				throw new MessageInvalidException(ProtocolErrorMessage.DUPLICATE_PEER_REF, "Node already has a peer with that identity", identifier, false);
			}
			System.out.println("Added opennet peer: "+pn);
		} else {
			try {
				pn = node.createNewDarknetNode(fs, trust);
			} catch (FSParseException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.REF_PARSE_ERROR, "Error parsing ref: "+e.getMessage(), identifier, false);
			} catch (PeerParseException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.REF_PARSE_ERROR, "Error parsing ref: "+e.getMessage(), identifier, false);
			} catch (ReferenceSignatureVerificationException e) {
				throw new MessageInvalidException(ProtocolErrorMessage.REF_SIGNATURE_INVALID, "Error adding ref: "+e.getMessage(), identifier, false);
			}
			if(Arrays.equals(pn.getIdentity(), node.getDarknetIdentity()))
				throw new MessageInvalidException(ProtocolErrorMessage.CANNOT_PEER_WITH_SELF, "Node cannot peer with itself", identifier, false);
			if(!node.addPeerConnection(pn)) {
				throw new MessageInvalidException(ProtocolErrorMessage.DUPLICATE_PEER_REF, "Node already has a peer with that identity", identifier, false);
			}
			System.out.println("Added darknet peer: "+pn);
		}
		handler.outputHandler.queue(new PeerMessage(pn, true, true, identifier));
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		throw new UnsupportedOperationException();
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.plugins.helpers1;

import java.util.Vector;

import freenet.clients.http.PageMaker;
import freenet.clients.http.Toadlet;
import freenet.clients.http.ToadletContainer;
import freenet.pluginmanager.FredPluginL10n;

public class WebInterface {

	private final Vector<WebInterfaceToadlet> _toadlets;
	private final Vector<String> _categories;

	private final ToadletContainer _container;
	private final PageMaker _pageMaker;

	public WebInterface(final PluginContext context) {
		_toadlets = new Vector<WebInterfaceToadlet>();
		_categories = new Vector<String>();
		_container = context.pluginRespirator.getToadletContainer();
		_pageMaker = context.pageMaker;
	}

	public void addNavigationCategory(String uri, String category, String title, FredPluginL10n plugin) {
		_pageMaker.addNavigationCategory(uri, category, title, plugin);
		_categories.add(category);
	}

	public void kill() {
		for (WebInterfaceToadlet toadlet : _toadlets) {
			_container.unregister(toadlet);
		}
		_toadlets.clear();
		for (String category : _categories) {
			_pageMaker.removeNavigationCategory(category);
		}
		_categories.clear();
	}

	public void registerVisible(Toadlet toadlet, String category, String name, String title) {
		_container.register(toadlet, category, toadlet.path(), true, name, title, false, null);
	}

	public void registerInvisible(Toadlet toadlet) {
		_container.register(toadlet , null, toadlet.path(), true, false);
	}
}
package freenet.pluginmanager;

/** Version of a plugin in a form that is easy to compare. */
public interface FredPluginRealVersioned {
	
	/** The version of the plugin in a form that is easy to compare: a long!
	 * Version 150 will always be later than version 20. */
	public long getRealVersion();
	
	// There is no point in reporting the dependancies or the minimum node version,
	// because we have already been loaded!
	
	// SVN revisions are going away, so there is no point returning them either.
	// Git has hashes, which are strings.
	
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.client.filter;

import java.io.IOException;

/**
 * For a specific text/-based MIME type, extracts the charset if
 * possible.
 */
public interface CharsetExtractor {
	
	String getCharset(byte[] input, int length, String parseCharset) throws DataFilterException, IOException;

	/** Inspect the first few bytes of the file for any obvious but 
	 * type-specific BOM. Don't try too hard, if we don't find anything we 
	 * will call getCharset() with some specific charset families to try.
	 * @param data The data.
	 * @return The BOM-detected charset family, this is essentially a guess
	 * which will have to be fed to getCharset().
	 * (A true BOM would give an exact match, but the caller will have 
	 * already tested for true BOMs by this point; we are looking for 
	 * "@charset \"" encoded with the given format)
	 * @throws DataFilterException
	 * @throws IOException 
	 */
	BOMDetection getCharsetByBOM(byte[] input, int length) throws DataFilterException, IOException;

	/**How many bytes must be fed into the CharsetExtractor to figure
	 * out the charset
	 */
	public int getCharsetBufferSize();

	public class BOMDetection {
		/** The charset, guessed from the first few characters. */
		final String charset;
		/** If this is true, getCharset() must return a charset, if it does
		 * not, we ignore the whole stylesheet. See CSS 2.1 section 4.4, at
		 * the end, "as specified" rule. */
		final boolean mustHaveCharset;
		BOMDetection(String charset, boolean mustHaveCharset) {
			this.charset = charset;
			this.mustHaveCharset = mustHaveCharset;
		}
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

/**
 * Thrown to indicate reuse of an Identifier.
 */
public class IdentifierCollisionException extends Exception {
	private static final long serialVersionUID = -1;
}
package freenet.support.io;

import java.io.FilterOutputStream;
import java.io.IOException;
import java.io.OutputStream;

public class CountedOutputStream extends FilterOutputStream {

	private long written;
	
	public CountedOutputStream(OutputStream arg0) {
		super(arg0);
	}
	
	@Override
	public void write(int x) throws IOException {
		super.write(x);
		written++;
	}
	
	@Override
	public void write(byte[] buf) throws IOException {
		write(buf, 0, buf.length);
	}
	
	@Override
	public void write(byte[] buf, int offset, int length) throws IOException {
		out.write(buf, offset, length);
		written += length;
	}

	public long written() {
		return written;
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support;

import java.util.Calendar;
import java.util.Date;
import java.util.GregorianCalendar;
import java.util.TimeZone;


/**
 * A wrapper class around a GregorianCalendar which always returns the current time.
 * This is useful for working around the pitfall of class Calendar: It only returns the current time when you first use a get*() function,
 * in any get*() calls after the first call, the time value of the first call is returned. One would have to call Calendar.clear() before each
 * get to obtain the current time and this class takes care of that for you.
 * 
 * Further, this class is synchronized so you do not need to worry about synchronization of a Calendar anymore.
 */
public class CurrentTimeUTC {

	private static final GregorianCalendar mCalendar = new GregorianCalendar(TimeZone.getTimeZone("UTC"));

	public static Date get() {
		synchronized(mCalendar) {
			mCalendar.setTimeInMillis(System.currentTimeMillis());
			return mCalendar.getTime();
		}
	}

	/**
	 * Get the current time in milliseconds. 
	 * 
	 * In the current implementation, this just returns System.currentTimeMilis(). You should however use CurrenTimeUTC.getInMillis() instead because
	 * the JavaDoc of System.currentTimeMilis() does not explicitly state what time zone it returns. Therefore, by using this wrapper function, your code
	 * clearly states that it uses UTC time. 
	 */
	public static long getInMillis() {
		return System.currentTimeMillis();
	}

	public static int getYear() {
		synchronized(mCalendar) {
			mCalendar.setTimeInMillis(System.currentTimeMillis());
			return mCalendar.get(Calendar.YEAR);
		}
	}

	public static int getMonth() {
		synchronized(mCalendar) {
			mCalendar.setTimeInMillis(System.currentTimeMillis());
			return mCalendar.get(Calendar.MONTH);
		}
	}

	public static int getDayOfMonth() {
		synchronized(mCalendar) {
			mCalendar.setTimeInMillis(System.currentTimeMillis());
			return mCalendar.get(Calendar.DAY_OF_MONTH);
		}
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

import java.util.HashMap;

import com.db4o.ObjectContainer;

import freenet.client.async.ManifestElement;
import freenet.client.async.SimpleManifestPutter;
import freenet.keys.FreenetURI;
import freenet.node.Node;
import freenet.support.Logger;
import freenet.support.SimpleFieldSet;
import freenet.support.api.Bucket;
import freenet.support.io.DelayedFreeBucket;
import freenet.support.io.FileBucket;
import freenet.support.io.NullBucket;
import freenet.support.io.PaddedEphemerallyEncryptedBucket;
import freenet.support.io.PersistentTempFileBucket;

public class PersistentPutDir extends FCPMessage {

	static final String name = "PersistentPutDir";
	
	final String identifier;
	final FreenetURI uri;
	final int verbosity; 
	final short priorityClass;
	final short persistenceType; 
	final boolean global;
	private final HashMap<String, Object> manifestElements;
	final String defaultName;
	final String token;
	final boolean started;
	final int maxRetries;
	final boolean wasDiskPut;
	private final SimpleFieldSet cached;
	
	public PersistentPutDir(String identifier, FreenetURI uri, int verbosity, short priorityClass,
	        short persistenceType, boolean global, String defaultName, HashMap<String, Object> manifestElements,
	        String token, boolean started, int maxRetries, boolean wasDiskPut, ObjectContainer container) {
		this.identifier = identifier;
		this.uri = uri;
		this.verbosity = verbosity;
		this.priorityClass = priorityClass;
		this.persistenceType = persistenceType;
		this.global = global;
		this.defaultName = defaultName;
		this.manifestElements = manifestElements;
		this.token = token;
		this.started = started;
		this.maxRetries = maxRetries;
		this.wasDiskPut = wasDiskPut;
		cached = generateFieldSet(container);
	}

	private SimpleFieldSet generateFieldSet(ObjectContainer container) {
		SimpleFieldSet fs = new SimpleFieldSet(false); // false because this can get HUGE
		fs.putSingle("Identifier", identifier);
		fs.putSingle("URI", uri.toString(false, false));
		fs.put("Verbosity", verbosity);
		fs.putSingle("Persistence", ClientRequest.persistenceTypeString(persistenceType));
		fs.put("PriorityClass", priorityClass);
		fs.putSingle("Global", Boolean.toString(global));
		fs.putSingle("PutDirType", wasDiskPut ? "disk" : "complex");
		SimpleFieldSet files = new SimpleFieldSet(false);
		// Flatten the hierarchy, it can be reconstructed on restarting.
		// Storing it directly would be a PITA.
		// FIXME/RESOLVE: The new BaseManifestPutter's container mode does not hold the origin data,
		//                 after composing the PutHandlers (done in BaseManifestPutter), they are 'lost':
		//                 A resumed half done container put can not get the complete file list from BaseManifestPutter.
		//                 Is it really necessary to include the file list here?
		ManifestElement[] elements = SimpleManifestPutter.flatten(manifestElements);
		fs.putSingle("DefaultName", defaultName);
		for(int i=0;i<elements.length;i++) {
			String num = Integer.toString(i);
			ManifestElement e = elements[i];
			String mimeOverride = e.getMimeTypeOverride();
			SimpleFieldSet subset = new SimpleFieldSet(false);
			FreenetURI tempURI = e.getTargetURI();
			subset.putSingle("Name", e.getName());
			if(tempURI != null) {
				subset.putSingle("UploadFrom", "redirect");
				subset.putSingle("TargetURI", tempURI.toString());
			} else {
				// Deactivate the top, not the middle.
				// Deactivating the middle can cause big problems.
				Bucket origData = e.getData();
				Bucket data = origData;
				boolean deactivate = false;
				if(persistenceType == ClientRequest.PERSIST_FOREVER)
					deactivate = !container.ext().isActive(data);
				if(deactivate)
					container.activate(data, 1);
				if(data instanceof DelayedFreeBucket) {
					data = ((DelayedFreeBucket)data).getUnderlying();
				}
				subset.put("DataLength", e.getSize());
				if(mimeOverride != null)
					subset.putSingle("Metadata.ContentType", mimeOverride);
				// What to do with the bucket?
				// It is either a persistent encrypted bucket or a file bucket ...
				if(data == null) {
					Logger.error(this, "Bucket already freed: "+e.getData()+" for "+e+" for "+e.getName()+" for "+identifier);
				} else if(data instanceof FileBucket) {
					subset.putSingle("UploadFrom", "disk");
					subset.putSingle("Filename", ((FileBucket)data).getFile().getPath());
				} else if (data instanceof PaddedEphemerallyEncryptedBucket || data instanceof NullBucket || data instanceof PersistentTempFileBucket) {
					subset.putSingle("UploadFrom", "direct");
				} else {
					throw new IllegalStateException("Don't know what to do with bucket: "+data);
				}
				if(deactivate)
					container.deactivate(origData, 1);
			}
			files.put(num, subset);
		}
		files.put("Count", elements.length);
		fs.put("Files", files);
		if(token != null)
			fs.putSingle("ClientToken", token);
		fs.put("Started", started);
		fs.put("MaxRetries", maxRetries);
		return fs;
	}

	@Override
	public SimpleFieldSet getFieldSet() {
		return cached;
	}

	@Override
	public String getName() {
		return name;
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node)
			throws MessageInvalidException {
		throw new MessageInvalidException(ProtocolErrorMessage.INVALID_MESSAGE, "PersistentPut goes from server to client not the other way around", identifier, global);
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		container.activate(uri, 5);
		uri.removeFrom(container);
		// manifestElements will be removed by ClientPutDir.freeData, not our problem.
		container.activate(cached, Integer.MAX_VALUE);
		cached.removeFrom(container);
		container.delete(this);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.clients.http.bookmark;

import java.net.MalformedURLException;

import freenet.client.async.ClientGetter;
import freenet.keys.FreenetURI;
import freenet.keys.USK;
import freenet.l10n.NodeL10n;
import freenet.node.FSParseException;
import freenet.node.NodeClientCore;
import freenet.node.useralerts.AbstractUserAlert;
import freenet.node.useralerts.UserAlert;
import freenet.node.useralerts.UserAlertManager;
import freenet.support.Fields;
import freenet.support.HTMLNode;
import freenet.support.Logger;
import freenet.support.SimpleFieldSet;

public class BookmarkItem extends Bookmark {
    public static final String NAME = "Bookmark";
    private FreenetURI key;
    private boolean updated;
    private boolean hasAnActivelink = false;
    private final BookmarkUpdatedUserAlert alert;
    private final UserAlertManager alerts;
    protected String desc;

    public BookmarkItem(FreenetURI k, String n, String d, boolean hasAnActivelink, UserAlertManager uam)
            throws MalformedURLException {

        this.key = k;
        this.name = n;
        this.desc = d;
        this.hasAnActivelink = hasAnActivelink;
        this.alerts = uam;
        alert = new BookmarkUpdatedUserAlert();
        assert(name != null);
        assert(key != null);
    }

    public BookmarkItem(String line, UserAlertManager uam) throws MalformedURLException {
        String[] result = line.split("###");
        this.name = result[0];
        this.desc = result[1];
        this.hasAnActivelink = Fields.stringToBool(result[2], false);
        this.key = new FreenetURI(result[3]);
        this.alerts = uam;
        this.alert = new BookmarkUpdatedUserAlert();
        assert(name != null);
        assert(key != null);
    }
    
    public BookmarkItem(SimpleFieldSet sfs, UserAlertManager uam) throws FSParseException, MalformedURLException {
        this.name = sfs.get("Name");
        if(name == null) name = "";
        this.desc = sfs.get("Description");
        if(desc == null) desc = "";
        this.hasAnActivelink = sfs.getBoolean("hasAnActivelink");
        this.key = new FreenetURI(sfs.get("URI"));
        this.alerts = uam;
        this.alert = new BookmarkUpdatedUserAlert();
    }

	private static volatile boolean logMINOR;

	static {
		Logger.registerClass(ClientGetter.class);
	}

    private class BookmarkUpdatedUserAlert extends AbstractUserAlert {

        public BookmarkUpdatedUserAlert() {
            super(true, null, null, null, null, UserAlert.MINOR, false, null, true, null);
        }

        @Override
		public String getTitle() {
            return l10n("bookmarkUpdatedTitle", "name", name);
        }

        @Override
		public String getText() {
            return l10n("bookmarkUpdated", new String[]{"name", "edition"},
                    new String[]{name, Long.toString(key.getSuggestedEdition())});
        }

        @Override
		public HTMLNode getHTMLText() {
            HTMLNode n = new HTMLNode("div");
            NodeL10n.getBase().addL10nSubstitution(n, "BookmarkItem.bookmarkUpdatedWithLink", new String[]{"link", "name", "edition"},
            		new HTMLNode[] { HTMLNode.link("/"+key), HTMLNode.text(name), HTMLNode.text(key.getSuggestedEdition()) });
            return n;
        }

        @Override
		public boolean isValid() {
            synchronized (BookmarkItem.this) {
                return updated;
            }
        }

        @Override
		public void isValid(boolean validity) {
            if (validity) {
                return;
            }
            disableBookmark();
        }

        @Override
		public String dismissButtonText() {
            return l10n("deleteBookmarkUpdateNotification");
        }

        @Override
		public void onDismiss() {
            disableBookmark();
        }

		@Override
		public String getShortText() {
			return l10n("bookmarkUpdatedShort", "name", name);
		}

		@Override
		public boolean isEventNotification() {
			return true;
		}
    }

    private synchronized void disableBookmark() {
        updated = false;
        alerts.unregister(alert);
    }

    private String l10n(String key) {
        return NodeL10n.getBase().getString("BookmarkItem." + key);
    }

    private String l10n(String key, String pattern, String value) {
        return NodeL10n.getBase().getString("BookmarkItem." + key, new String[]{pattern}, new String[]{value});
    }

    private String l10n(String key, String[] patterns, String[] values) {
        return NodeL10n.getBase().getString("BookmarkItem." + key, patterns, values);
    }

    private synchronized void enableBookmark() {
        if (updated) {
            return;
        }
        assert(key.isUSK());
        updated = true;
        alerts.register(alert);
    }

    public String getKey() {
        return key.toString();
    }

    public synchronized FreenetURI getURI() {
        return key;
    }

    public synchronized void update(FreenetURI uri, boolean hasAnActivelink, String description) {
        this.key = uri;
        this.desc = description;
        this.hasAnActivelink = hasAnActivelink;
        if(!key.isUSK())
        	disableBookmark();
    }

    public synchronized String getKeyType() {
        return key.getKeyType();
    }

    @Override
	public String getName() {
        return ("".equals(name) ? l10n("unnamedBookmark") : name);
    }

    @Override
	public String toString() {
        return this.name + "###" + (this.desc != null ? this.desc : "") + "###" + this.hasAnActivelink + "###" + this.key.toString();
    }

    public synchronized void setEdition(long ed, NodeClientCore node) {
        if (key.getSuggestedEdition() >= ed) {
        	if(logMINOR) Logger.minor(this, "Edition "+ed+" is too old, not updating "+key);
            return;
        }
        key = key.setSuggestedEdition(ed);
        enableBookmark();
    }

    public USK getUSK() throws MalformedURLException {
        return USK.create(key);
    }

	@Override
	public int hashCode() {
		int hash = super.hashCode();
		hash = 31 * hash + this.key.setSuggestedEdition(0).hashCode();
		hash = 31 * hash + (this.hasAnActivelink ? 1 : 0);
		hash = 31 * hash + (this.desc != null ? this.desc.hashCode() : 0);
		return hash;
	}

    @Override
	public boolean equals(Object o) {
        if (o == this) {
            return true;
        }
        if (o instanceof BookmarkItem) {
            BookmarkItem b = (BookmarkItem) o;
            if (!super.equals(o)) {
                return false;
            }
            if (!b.key.equals(key)) {
				if ("USK".equals(b.key.getKeyType())) {
                    if (!b.key.setSuggestedEdition(key.getSuggestedEdition()).equals(key)) {
                        return false;
                    }
                } else {
                    return false;
                }
            }
            if (b.alerts != alerts) {
                return false;
            } // Belongs to a different node???
            if (b.hasAnActivelink != hasAnActivelink) {
                return false;
            }
			if (b.desc.equals(desc))
				return true;
			if (b.desc == null || desc == null)
				return false;
		if(!b.desc.equals(desc)) {
	                return false;
		}
            return true;
        } else {
            return false;
        }
    }

    public boolean hasAnActivelink() {
        return hasAnActivelink;
    }
    
    public String getDescription() {
        return (desc == null ? "" : desc);
    }
    
    @Override
	public SimpleFieldSet getSimpleFieldSet() {
	SimpleFieldSet sfs = new SimpleFieldSet(true);
	sfs.putSingle("Name", name);
	sfs.putSingle("Description", desc);
	sfs.put("hasAnActivelink", hasAnActivelink);
	sfs.putSingle("URI", key.toString());
	return sfs;
    }
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

/** Used to associate a port with a node database handle */
// WARNING: THIS CLASS IS STORED IN DB4O -- THINK TWICE BEFORE ADD/REMOVE/RENAME FIELDS
class HandlePortTuple {
	long handle;
	int portNumber;
}
/*
 * Created on Jul 17, 2004
 *
 */
package freenet.support.CPUInformation;

/**
 * @author Iakin
 * An interface for classes that provide lowlevel information about AMD CPU's
 *
 * free (adj.): unencumbered; not under the control of others
 * Written by Iakin in 2004 and released into the public domain 
 * with no warranty of any kind, either expressed or implied.  
 * It probably won't make your computer catch on fire, or eat 
 * your children, but it might.  Use at your own risk.
 */
public interface AMDCPUInfo extends CPUInfo {
	/**
	 * @return true iff the CPU present in the machine is at least an 'k6' CPU
	 */
	public boolean IsK6Compatible();
	/**
	 * @return true iff the CPU present in the machine is at least an 'k6-2' CPU
	 */
	public boolean IsK6_2_Compatible();
	/**
	 * @return true iff the CPU present in the machine is at least an 'k6-3' CPU
	 */
	public boolean IsK6_3_Compatible();

	/**
	 * @return true iff the CPU present in the machine is at least an 'k7' CPU (Atlhon, Duron etc. and better)
	 */
	public boolean IsAthlonCompatible();
	/**
	 * @return true iff the CPU present in the machine is at least an 'k8' CPU (Atlhon 64, Opteron etc. and better)
	 */
	public boolean IsAthlon64Compatible();
}
package freenet.clients.http;

import freenet.support.HTMLNode;

public class PageNode extends InfoboxNode {
	
	public final HTMLNode headNode;

	PageNode(HTMLNode page, HTMLNode head, HTMLNode content) {
		super(page, content);
		this.headNode = head;
	}
	
	/**
	 * Adds a custom style sheet to the header of the page.
	 *
	 * @param customStyleSheet
	 *            The URL of the custom style sheet
	 */
	public void addCustomStyleSheet(String customStyleSheet) {
		addForwardLink("stylesheet", customStyleSheet, "text/css", "screen");
	}

	/**
	 * Adds a document relationship forward link to the HTML document's HEAD
	 * node.
	 *
	 * @param linkType
	 *            The link type (e.g. "stylesheet" or "shortcut icon")
	 * @param href
	 *            The link
	 */
	public void addForwardLink(String linkType, String href) {
		addForwardLink(linkType, href, null, null);
	}

	/**
	 * Adds a document relationship forward link to the HTML document's HEAD
	 * node.
	 *
	 * @param linkType
	 *            The link type (e.g. "stylesheet" or "shortcut icon")
	 * @param href
	 *            The link
	 * @param type
	 *            The type of the referenced data
	 * @param media
	 *            The media for which this link is valid
	 */
	public void addForwardLink(String linkType, String href, String type, String media) {
		HTMLNode linkNode = headNode.addChild("link", new String[] { "rel", "href" }, new String[] { linkType, href });
		if (type != null) {
			linkNode.addAttribute("type", type);
		}
		if (media != null) {
			linkNode.addAttribute("media", media);
		}
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.simulator;

import java.io.File;
import java.io.UnsupportedEncodingException;

import freenet.client.HighLevelSimpleClient;
import freenet.crypt.DummyRandomSource;
import freenet.io.comm.PeerParseException;
import freenet.io.comm.ReferenceSignatureVerificationException;
import freenet.keys.CHKBlock;
import freenet.keys.CHKDecodeException;
import freenet.keys.CHKEncodeException;
import freenet.keys.CHKVerifyException;
import freenet.keys.ClientCHK;
import freenet.keys.ClientCHKBlock;
import freenet.node.FSParseException;
import freenet.node.Node;
import freenet.node.NodeInitException;
import freenet.node.NodeStarter;
import freenet.node.RequestStarter;
import freenet.support.Executor;
import freenet.support.Fields;
import freenet.support.HexUtil;
import freenet.support.Logger;
import freenet.support.PooledExecutor;
import freenet.support.Logger.LogLevel;
import freenet.support.LoggerHook.InvalidThresholdException;
import freenet.support.compress.InvalidCompressionCodecException;
import freenet.support.compress.Compressor.COMPRESSOR_TYPE;
import freenet.support.io.FileUtil;

/**
 * Test a busy, bandwidth limited network. Hopefully this should reveal any serious problems with
 * load limiting and block transfer.
 * @author toad
 */
public class RealNodeBusyNetworkTest extends RealNodeRoutingTest {

    static final int NUMBER_OF_NODES = 25;
    static final int DEGREE = 5;
    static final short MAX_HTL = (short)8;
    static final int INSERT_KEYS = 50;
    static final boolean START_WITH_IDEAL_LOCATIONS = true;
    static final boolean FORCE_NEIGHBOUR_CONNECTIONS = true;
    static final boolean ENABLE_SWAPPING = false;
    static final boolean ENABLE_ULPRS = false;
    static final boolean ENABLE_PER_NODE_FAILURE_TABLES = false;
    static final boolean ENABLE_SWAP_QUEUEING = false;
    static final boolean ENABLE_PACKET_COALESCING = true;
    static final boolean ENABLE_FOAF = true;
    static final boolean FORK_ON_CACHEABLE = false;
    static final boolean REAL_TIME_FLAG = false;

    static final int TARGET_SUCCESSES = 20;
    //static final int NUMBER_OF_NODES = 50;
    //static final short MAX_HTL = 10;

    static final int DARKNET_PORT_BASE = 5008;
    static final int DARKNET_PORT_END = DARKNET_PORT_BASE + NUMBER_OF_NODES;

    public static void main(String[] args) throws FSParseException, PeerParseException, CHKEncodeException, InvalidThresholdException, NodeInitException, ReferenceSignatureVerificationException, InterruptedException, UnsupportedEncodingException, CHKVerifyException, CHKDecodeException, InvalidCompressionCodecException {
        String name = "realNodeRequestInsertTest";
        File wd = new File(name);
        if(!FileUtil.removeAll(wd)) {
        	System.err.println("Mass delete failed, test may not be accurate.");
        	System.exit(EXIT_CANNOT_DELETE_OLD_DATA);
        }
        wd.mkdir();
        //NOTE: globalTestInit returns in ignored random source
        //NodeStarter.globalTestInit(name, false, LogLevel.ERROR, "freenet.node.Location:normal,freenet.node.simulator.RealNode:minor,freenet.node.Insert:MINOR,freenet.node.Request:MINOR,freenet.node.Node:MINOR");
        //NodeStarter.globalTestInit(name, false, LogLevel.ERROR, "freenet.node.Location:MINOR,freenet.io.comm:MINOR,freenet.node.NodeDispatcher:MINOR,freenet.node.simulator:MINOR,freenet.node.PeerManager:MINOR,freenet.node.RequestSender:MINOR");
        //NodeStarter.globalTestInit(name, false, LogLevel.ERROR, "freenet.node.FNP:MINOR,freenet.node.Packet:MINOR,freenet.io.comm:MINOR,freenet.node.PeerNode:MINOR,freenet.node.DarknetPeerNode:MINOR");
        NodeStarter.globalTestInit(name, false, LogLevel.ERROR, "", true);
        System.out.println("Busy network test (inserts/retrieves in quantity/stress test)");
        System.out.println();
        DummyRandomSource random = new DummyRandomSource();
        //DiffieHellman.init(random);
        Node[] nodes = new Node[NUMBER_OF_NODES];
        Logger.normal(RealNodeRoutingTest.class, "Creating nodes...");
        Executor executor = new PooledExecutor();
        for(int i=0;i<NUMBER_OF_NODES;i++) {
            nodes[i] =
            	NodeStarter.createTestNode(DARKNET_PORT_BASE+i, 0, name, false, MAX_HTL, 20 /* 5% */, random, executor, 500*NUMBER_OF_NODES, (CHKBlock.DATA_LENGTH+CHKBlock.TOTAL_HEADERS_LENGTH)*100, true, ENABLE_SWAPPING, false, ENABLE_ULPRS, ENABLE_PER_NODE_FAILURE_TABLES, ENABLE_SWAP_QUEUEING, ENABLE_PACKET_COALESCING, 8000, ENABLE_FOAF, false, true, false, null);
            Logger.normal(RealNodeRoutingTest.class, "Created node "+i);
        }

        // Now link them up
        makeKleinbergNetwork(nodes, START_WITH_IDEAL_LOCATIONS, DEGREE, FORCE_NEIGHBOUR_CONNECTIONS, random);

        Logger.normal(RealNodeRoutingTest.class, "Added random links");

        for(int i=0;i<NUMBER_OF_NODES;i++) {
            nodes[i].start(false);
            System.err.println("Started node "+i+"/"+nodes.length);
        }

        waitForAllConnected(nodes);

        waitForPingAverage(0.95, nodes, random, MAX_PINGS, 1000);

        System.out.println();
        System.out.println("Ping average > 95%, lets do some inserts/requests");
        System.out.println();

        HighLevelSimpleClient[] clients = new HighLevelSimpleClient[nodes.length];
        for(int i=0;i<clients.length;i++) {
        	clients[i] = nodes[i].clientCore.makeClient(RequestStarter.IMMEDIATE_SPLITFILE_PRIORITY_CLASS);
        }

        // Insert 100 keys into random nodes

        ClientCHK[] keys = new ClientCHK[INSERT_KEYS];

        String baseString = System.currentTimeMillis() + " ";
        for(int i=0;i<INSERT_KEYS;i++) {
        	System.err.println("Inserting "+i+" of "+INSERT_KEYS);
            int node1 = random.nextInt(NUMBER_OF_NODES);
            Node randomNode = nodes[node1];
            String dataString = baseString + i;
            byte[] data = dataString.getBytes("UTF-8");
            ClientCHKBlock block;
            block = ClientCHKBlock.encode(data, false, false, (short)-1, 0, COMPRESSOR_TYPE.DEFAULT_COMPRESSORDESCRIPTOR, false);
            ClientCHK chk = block.getClientKey();
            byte[] encData = block.getData();
            byte[] encHeaders = block.getHeaders();
            ClientCHKBlock newBlock = new ClientCHKBlock(encData, encHeaders, chk, true);
            keys[i] = chk;
            Logger.minor(RealNodeRequestInsertTest.class, "Decoded: "+new String(newBlock.memoryDecode()));
            Logger.normal(RealNodeRequestInsertTest.class,"CHK: "+chk.getURI());
            Logger.minor(RealNodeRequestInsertTest.class,"Headers: "+HexUtil.bytesToHex(block.getHeaders()));
            // Insert it.
			try {
				randomNode.clientCore.realPut(block, false, FORK_ON_CACHEABLE, false, false, REAL_TIME_FLAG);
				Logger.error(RealNodeRequestInsertTest.class, "Inserted to "+node1);
				Logger.minor(RealNodeRequestInsertTest.class, "Data: "+Fields.hashCode(encData)+", Headers: "+Fields.hashCode(encHeaders));
			} catch (freenet.node.LowLevelPutException putEx) {
				Logger.error(RealNodeRequestInsertTest.class, "Insert failed: "+ putEx);
				System.err.println("Insert failed: "+ putEx);
				System.exit(EXIT_INSERT_FAILED);
			}
        }

        // Now queue requests for each key on every node.
        for(int i=0;i<INSERT_KEYS;i++) {
        	ClientCHK key = keys[i];
        	System.err.println("Queueing requests for "+i+" of "+INSERT_KEYS);
        	for(int j=0;j<nodes.length;j++) {
        		clients[j].prefetch(key.getURI(), 24*60*60*1000, 32768, null);
        	}
        	long totalRunningRequests = 0;
        	for(int j=0;j<nodes.length;j++) {
        		totalRunningRequests += nodes[j].clientCore.countTransientQueuedRequests();
        	}
        	System.err.println("Running requests: "+totalRunningRequests);
        }

        // Now wait until finished. How???

        while(true) {
        	long totalRunningRequests = 0;
        	for(int i=0;i<nodes.length;i++) {
        		totalRunningRequests += nodes[i].clientCore.countTransientQueuedRequests();
        	}
        	System.err.println("Running requests: "+totalRunningRequests);
        	if(totalRunningRequests == 0) break;
        	Thread.sleep(1000);
        }
        System.exit(0);
    }
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
/* Freenet 0.7 node. */
package freenet.node;

import static freenet.node.stats.DataStoreKeyType.CHK;
import static freenet.node.stats.DataStoreKeyType.PUB_KEY;
import static freenet.node.stats.DataStoreKeyType.SSK;
import static freenet.node.stats.DataStoreType.CACHE;
import static freenet.node.stats.DataStoreType.CLIENT;
import static freenet.node.stats.DataStoreType.SLASHDOT;
import static freenet.node.stats.DataStoreType.STORE;

import java.io.BufferedReader;
import java.io.EOFException;
import java.io.File;
import java.io.FileFilter;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.RandomAccessFile;
import java.io.UnsupportedEncodingException;
import java.net.InetAddress;
import java.net.UnknownHostException;
import java.text.DecimalFormat;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.Locale;
import java.util.Map;
import java.util.MissingResourceException;
import java.util.Random;
import java.util.Set;
import java.util.TimeZone;
import java.util.Vector;

import freenet.support.math.MersenneTwister;
import org.tanukisoftware.wrapper.WrapperManager;

import com.db4o.Db4o;
import com.db4o.ObjectContainer;
import com.db4o.ObjectSet;
import com.db4o.config.Configuration;
import com.db4o.config.GlobalOnlyConfigException;
import com.db4o.defragment.AvailableClassFilter;
import com.db4o.defragment.BTreeIDMapping;
import com.db4o.defragment.Defragment;
import com.db4o.defragment.DefragmentConfig;
import com.db4o.diagnostic.ClassHasNoFields;
import com.db4o.diagnostic.Diagnostic;
import com.db4o.diagnostic.DiagnosticBase;
import com.db4o.diagnostic.DiagnosticListener;
import com.db4o.ext.Db4oException;
import com.db4o.io.IoAdapter;
import com.db4o.io.RandomAccessFileAdapter;
import com.sleepycat.je.DatabaseException;
import com.sleepycat.je.Environment;
import com.sleepycat.je.EnvironmentConfig;
import com.sleepycat.je.EnvironmentMutableConfig;

import freenet.client.FECQueue;
import freenet.client.FetchContext;
import freenet.client.async.ClientRequestScheduler;
import freenet.client.async.SplitFileInserterSegment;
import freenet.clients.http.SecurityLevelsToadlet;
import freenet.clients.http.SimpleToadletServer;
import freenet.config.EnumerableOptionCallback;
import freenet.config.FreenetFilePersistentConfig;
import freenet.config.InvalidConfigValueException;
import freenet.config.NodeNeedRestartException;
import freenet.config.PersistentConfig;
import freenet.config.SubConfig;
import freenet.crypt.DSAPublicKey;
import freenet.crypt.DiffieHellman;
import freenet.crypt.EncryptingIoAdapter;
import freenet.crypt.RandomSource;
import freenet.crypt.Yarrow;
import freenet.io.comm.DMT;
import freenet.io.comm.DisconnectedException;
import freenet.io.comm.FreenetInetAddress;
import freenet.io.comm.IOStatisticCollector;
import freenet.io.comm.Message;
import freenet.io.comm.MessageCore;
import freenet.io.comm.MessageFilter;
import freenet.io.comm.Peer;
import freenet.io.comm.PeerParseException;
import freenet.io.comm.ReferenceSignatureVerificationException;
import freenet.io.comm.UdpSocketHandler;
import freenet.io.xfer.PartiallyReceivedBlock;
import freenet.keys.CHKBlock;
import freenet.keys.CHKVerifyException;
import freenet.keys.ClientCHK;
import freenet.keys.ClientCHKBlock;
import freenet.keys.ClientKey;
import freenet.keys.ClientKeyBlock;
import freenet.keys.ClientSSK;
import freenet.keys.ClientSSKBlock;
import freenet.keys.Key;
import freenet.keys.KeyBlock;
import freenet.keys.KeyVerifyException;
import freenet.keys.NodeCHK;
import freenet.keys.NodeSSK;
import freenet.keys.SSKBlock;
import freenet.keys.SSKVerifyException;
import freenet.l10n.BaseL10n;
import freenet.l10n.NodeL10n;
import freenet.node.DarknetPeerNode.FRIEND_TRUST;
import freenet.node.NodeDispatcher.NodeDispatcherCallback;
import freenet.node.OpennetManager.ConnectionType;
import freenet.node.SecurityLevels.FRIENDS_THREAT_LEVEL;
import freenet.node.SecurityLevels.NETWORK_THREAT_LEVEL;
import freenet.node.SecurityLevels.PHYSICAL_THREAT_LEVEL;
import freenet.node.fcp.FCPMessage;
import freenet.node.fcp.FeedMessage;
import freenet.node.stats.DataStoreInstanceType;
import freenet.node.stats.DataStoreStats;
import freenet.node.stats.NotAvailNodeStoreStats;
import freenet.node.stats.StoreCallbackStats;
import freenet.node.updater.NodeUpdateManager;
import freenet.node.useralerts.BuildOldAgeUserAlert;
import freenet.node.useralerts.ExtOldAgeUserAlert;
import freenet.node.useralerts.MeaningfulNodeNameUserAlert;
import freenet.node.useralerts.NotEnoughNiceLevelsUserAlert;
import freenet.node.useralerts.SimpleUserAlert;
import freenet.node.useralerts.TimeSkewDetectedUserAlert;
import freenet.node.useralerts.UserAlert;
import freenet.pluginmanager.ForwardPort;
import freenet.pluginmanager.PluginManager;
import freenet.pluginmanager.PluginStore;
import freenet.store.BerkeleyDBFreenetStore;
import freenet.store.BlockMetadata;
import freenet.store.CHKStore;
import freenet.store.FreenetStore;
import freenet.store.KeyCollisionException;
import freenet.store.NullFreenetStore;
import freenet.store.PubkeyStore;
import freenet.store.RAMFreenetStore;
import freenet.store.SSKStore;
import freenet.store.SlashdotStore;
import freenet.store.StorableBlock;
import freenet.store.StoreCallback;
import freenet.store.FreenetStore.StoreType;
import freenet.store.saltedhash.SaltedHashFreenetStore;
import freenet.support.Executor;
import freenet.support.Fields;
import freenet.support.FileLoggerHook;
import freenet.support.HTMLNode;
import freenet.support.HexUtil;
import freenet.support.LRUQueue;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.OOMHandler;
import freenet.support.PooledExecutor;
import freenet.support.PrioritizedTicker;
import freenet.support.ShortBuffer;
import freenet.support.SimpleFieldSet;
import freenet.support.SizeUtil;
import freenet.support.Ticker;
import freenet.support.TokenBucket;
import freenet.support.Logger.LogLevel;
import freenet.support.api.BooleanCallback;
import freenet.support.api.IntCallback;
import freenet.support.api.LongCallback;
import freenet.support.api.ShortCallback;
import freenet.support.api.StringCallback;
import freenet.support.io.ArrayBucketFactory;
import freenet.support.io.Closer;
import freenet.support.io.FileUtil;
import freenet.support.io.NativeThread;
import freenet.support.transport.ip.HostnameSyntaxException;

/**
 * @author amphibian
 */
public class Node implements TimeSkewDetectorCallback {

	public class MigrateOldStoreData implements Runnable {

		private final boolean clientCache;

		public MigrateOldStoreData(boolean clientCache) {
			this.clientCache = clientCache;
			if(clientCache) {
				oldCHKClientCache = chkClientcache;
				oldPKClientCache = pubKeyClientcache;
				oldSSKClientCache = sskClientcache;
			} else {
				oldCHK = chkDatastore;
				oldPK = pubKeyDatastore;
				oldSSK = sskDatastore;
				oldCHKCache = chkDatastore;
				oldPKCache = pubKeyDatastore;
				oldSSKCache = sskDatastore;
			}
		}

		public void run() {
			System.err.println("Migrating old "+(clientCache ? "client cache" : "datastore"));
			if(clientCache) {
				migrateOldStore(oldCHKClientCache, chkClientcache, true);
				StoreCallback<? extends StorableBlock> old;
				synchronized(Node.this) {
					old = oldCHKClientCache;
					oldCHKClientCache = null;
				}
				closeOldStore(old);
				migrateOldStore(oldPKClientCache, pubKeyClientcache, true);
				synchronized(Node.this) {
					old = oldPKClientCache;
					oldPKClientCache = null;
				}
				closeOldStore(old);
				migrateOldStore(oldSSKClientCache, sskClientcache, true);
				synchronized(Node.this) {
					old = oldSSKClientCache;
					oldSSKClientCache = null;
				}
				closeOldStore(old);
			} else {
				migrateOldStore(oldCHK, chkDatastore, false);
				oldCHK = null;
				migrateOldStore(oldPK, pubKeyDatastore, false);
				oldPK = null;
				migrateOldStore(oldSSK, sskDatastore, false);
				oldSSK = null;
				migrateOldStore(oldCHKCache, chkDatacache, false);
				oldCHKCache = null;
				migrateOldStore(oldPKCache, pubKeyDatacache, false);
				oldPKCache = null;
				migrateOldStore(oldSSKCache, sskDatacache, false);
				oldSSKCache = null;
			}
			System.err.println("Finished migrating old "+(clientCache ? "client cache" : "datastore"));
		}

	}

	volatile CHKStore oldCHK;
	volatile PubkeyStore oldPK;
	volatile SSKStore oldSSK;

	volatile CHKStore oldCHKCache;
	volatile PubkeyStore oldPKCache;
	volatile SSKStore oldSSKCache;

	volatile CHKStore oldCHKClientCache;
	volatile PubkeyStore oldPKClientCache;
	volatile SSKStore oldSSKClientCache;

	private <T extends StorableBlock> void migrateOldStore(StoreCallback<T> old, StoreCallback<T> newStore, boolean canReadClientCache) {
		FreenetStore<T> store = old.getStore();
		if(store instanceof RAMFreenetStore) {
			RAMFreenetStore<T> ramstore = (RAMFreenetStore<T>)store;
			try {
				ramstore.migrateTo(newStore, canReadClientCache);
			} catch (IOException e) {
				Logger.error(this, "Caught migrating old store: "+e, e);
			}
			ramstore.clear();
		} else if(store instanceof SaltedHashFreenetStore) {
			SaltedHashFreenetStore<T> saltstore = (SaltedHashFreenetStore<T>) store;
			// FIXME
			Logger.error(this, "Migrating from from a saltedhashstore not fully supported yet: will not keep old keys");
		}
	}


	public <T extends StorableBlock> void closeOldStore(StoreCallback<T> old) {
		FreenetStore<T> store = old.getStore();
		if(store instanceof SaltedHashFreenetStore) {
			SaltedHashFreenetStore<T> saltstore = (SaltedHashFreenetStore<T>) store;
			saltstore.close();
			saltstore.destruct();
		}
	}


	private static volatile boolean logMINOR;
	private static volatile boolean logDEBUG;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
				logDEBUG = Logger.shouldLog(LogLevel.DEBUG, this);
			}
		});
	}
	private static MeaningfulNodeNameUserAlert nodeNameUserAlert;
	private static BuildOldAgeUserAlert buildOldAgeUserAlert;
	private static TimeSkewDetectedUserAlert timeSkewDetectedUserAlert;

	public class NodeNameCallback extends StringCallback  {
		NodeNameCallback() {
		}
		@Override
		public String get() {
			String name;
			synchronized(this) {
				name = myName;
			}
			if(name.startsWith("Node id|")|| name.equals("MyFirstFreenetNode")){
				clientCore.alerts.register(nodeNameUserAlert);
			}else{
				clientCore.alerts.unregister(nodeNameUserAlert);
			}
			return name;
		}

		@Override
		public void set(String val) throws InvalidConfigValueException {
			if(get().equals(val)) return;
			else if(val.length() > 128)
				throw new InvalidConfigValueException("The given node name is too long ("+val+')');
			else if("".equals(val))
				val = "~none~";
			synchronized(this) {
				myName = val;
			}
			// We'll broadcast the new name to our connected darknet peers via a differential node reference
			SimpleFieldSet fs = new SimpleFieldSet(true);
			fs.putSingle("myName", myName);
			peers.locallyBroadcastDiffNodeRef(fs, true, false);
			// We call the callback once again to ensure MeaningfulNodeNameUserAlert
			// has been unregistered ... see #1595
			get();
		}
	}

	private class StoreTypeCallback extends StringCallback implements EnumerableOptionCallback {

		@Override
		public String get() {
			synchronized(Node.this) {
				return storeType;
			}
		}

		@Override
		public void set(String val) throws InvalidConfigValueException, NodeNeedRestartException {
			boolean found = false;
			for (String p : getPossibleValues()) {
				if (p.equals(val)) {
					found = true;
					break;
				}
			}
			if (!found)
				throw new InvalidConfigValueException("Invalid store type");

			String type;
			synchronized(Node.this) {
				type = storeType;
			}
			if(type.equals("ram")) {
				synchronized(this) { // Serialise this part.
					makeStore(val);
				}
			} else {
				synchronized(Node.this) {
					storeType = val;
				}
				throw new NodeNeedRestartException("Store type cannot be changed on the fly");
			}
		}

		public String[] getPossibleValues() {
			return new String[] { "bdb-index", "salt-hash", "ram" };
		}
	}

	private class ClientCacheTypeCallback extends StringCallback implements EnumerableOptionCallback {

		@Override
		public String get() {
			synchronized(Node.this) {
				return clientCacheType;
			}
		}

		@Override
		public void set(String val) throws InvalidConfigValueException, NodeNeedRestartException {
			boolean found = false;
			for (String p : getPossibleValues()) {
				if (p.equals(val)) {
					found = true;
					break;
				}
			}
			if (!found)
				throw new InvalidConfigValueException("Invalid store type");

			String type;
			synchronized(Node.this) {
				type = clientCacheType;
				if(clientCacheAwaitingPassword)
					type = "ram";
			}
				synchronized(this) { // Serialise this part.
					String suffix = getStoreSuffix();
					if (val.equals("salt-hash")) {
						byte[] key;
						synchronized(Node.this) {
							key = cachedClientCacheKey;
							cachedClientCacheKey = null;
						}
						if(key == null) {
							MasterKeys keys = null;
							try {
								if(securityLevels.physicalThreatLevel == PHYSICAL_THREAT_LEVEL.MAXIMUM) {
									key = new byte[32];
									random.nextBytes(key);
								} else {
									keys = MasterKeys.read(masterKeysFile, random, "");
									key = keys.clientCacheMasterKey;
									keys.clearAllNotClientCacheKey();
								}
							} catch (MasterKeysWrongPasswordException e1) {
								setClientCacheAwaitingPassword();
								synchronized(Node.this) {
									clientCacheType = val;
								}
								throw new InvalidConfigValueException("You must enter the password");
							} catch (MasterKeysFileSizeException e1) {
								throw new InvalidConfigValueException("Master keys file corrupted (too " + e1.sizeToString() + ")");
							} catch (IOException e1) {
								throw new InvalidConfigValueException("Master keys file cannot be accessed: "+e1);
							}
						}
						try {
							initSaltHashClientCacheFS(suffix, true, key);
						} catch (NodeInitException e) {
							Logger.error(this, "Unable to create new store", e);
							System.err.println("Unable to create new store: "+e);
							e.printStackTrace();
							// FIXME l10n both on the NodeInitException and the wrapper message
							throw new InvalidConfigValueException("Unable to create new store: "+e);
						} finally {
							MasterKeys.clear(key);
						}
					} else if(val.equals("ram")) {
						initRAMClientCacheFS();
					} else /*if(val.equals("none")) */{
						initNoClientCacheFS();
					}

					synchronized(Node.this) {
						clientCacheType = val;
					}
				}
		}

		public String[] getPossibleValues() {
			return new String[] { "salt-hash", "ram", "none" };
		}
	}

	private static class L10nCallback extends StringCallback implements EnumerableOptionCallback {
		@Override
		public String get() {
			return NodeL10n.getBase().getSelectedLanguage().fullName;
		}

		@Override
		public void set(String val) throws InvalidConfigValueException {
			if(val == null || get().equalsIgnoreCase(val)) return;
			try {
				NodeL10n.getBase().setLanguage(BaseL10n.LANGUAGE.mapToLanguage(val));
			} catch (MissingResourceException e) {
				throw new InvalidConfigValueException(e.getLocalizedMessage());
			}
			PluginManager.setLanguage(NodeL10n.getBase().getSelectedLanguage());
		}

		public String[] getPossibleValues() {
			return BaseL10n.LANGUAGE.valuesWithFullNames();
		}
	}

	private final File dbFile;
	private final File dbFileCrypt;
	private boolean defragDatabaseOnStartup;
	private boolean defragOnce;
	/** db4o database for node and client layer.
	 * Other databases can be created for the datastore (since its usage
	 * patterns and content are completely different), or for plugins (for
	 * security reasons). */
	public ObjectContainer db;
	/** A fixed random number which identifies the top-level objects belonging to
	 * this node, as opposed to any others that might be stored in the same database
	 * (e.g. because of many-nodes-in-one-VM). */
	public long nodeDBHandle;

	private boolean autoChangeDatabaseEncryption = true;

	/** Stats */
	public final NodeStats nodeStats;
	public final NetworkIDManager netid;

	/** Config object for the whole node. */
	public final PersistentConfig config;

	// Static stuff related to logger

	/** Directory to log to */
	static File logDir;
	/** Maximum size of gzipped logfiles */
	static long maxLogSize;
	/** Log config handler */
	public static LoggingConfigHandler logConfigHandler;

	public static final int PACKETS_IN_BLOCK = 32;
	public static final int PACKET_SIZE = 1024;
	public static final double DECREMENT_AT_MIN_PROB = 0.25;
	public static final double DECREMENT_AT_MAX_PROB = 0.5;
	// Send keepalives every 7-14 seconds. Will be acked and if necessary resent.
	// Old behaviour was keepalives every 14-28. Even that was adequate for a 30 second
	// timeout. Most nodes don't need to send keepalives because they are constantly busy,
	// this is only an issue for disabled darknet connections, very quiet private networks
	// etc.
	public static final int KEEPALIVE_INTERVAL = 7000;
	// If no activity for 30 seconds, node is dead
	// 35 seconds allows plenty of time for resends etc even if above is 14 sec as it is on older nodes.
	public static final int MAX_PEER_INACTIVITY = 35000;
	/** Time after which a handshake is assumed to have failed. */
	public static final int HANDSHAKE_TIMEOUT = 4800; // Keep the below within the 30 second assumed timeout.
	// Inter-handshake time must be at least 2x handshake timeout
	public static final int MIN_TIME_BETWEEN_HANDSHAKE_SENDS = HANDSHAKE_TIMEOUT*2; // 10-20 secs
	public static final int RANDOMIZED_TIME_BETWEEN_HANDSHAKE_SENDS = HANDSHAKE_TIMEOUT*2; // avoid overlap when the two handshakes are at the same time
	public static final int MIN_TIME_BETWEEN_VERSION_PROBES = HANDSHAKE_TIMEOUT*4;
	public static final int RANDOMIZED_TIME_BETWEEN_VERSION_PROBES = HANDSHAKE_TIMEOUT*2; // 20-30 secs
	public static final int MIN_TIME_BETWEEN_VERSION_SENDS = HANDSHAKE_TIMEOUT*4;
	public static final int RANDOMIZED_TIME_BETWEEN_VERSION_SENDS = HANDSHAKE_TIMEOUT*2; // 20-30 secs
	public static final int MIN_TIME_BETWEEN_BURSTING_HANDSHAKE_BURSTS = HANDSHAKE_TIMEOUT*24; // 2-5 minutes
	public static final int RANDOMIZED_TIME_BETWEEN_BURSTING_HANDSHAKE_BURSTS = HANDSHAKE_TIMEOUT*36;
	public static final int MIN_BURSTING_HANDSHAKE_BURST_SIZE = 1; // 1-4 handshake sends per burst
	public static final int RANDOMIZED_BURSTING_HANDSHAKE_BURST_SIZE = 3;
	// If we don't receive any packets at all in this period, from any node, tell the user
	public static final long ALARM_TIME = 60*1000;

	// 900ms
	static final int MIN_INTERVAL_BETWEEN_INCOMING_SWAP_REQUESTS = 900;
	static final int MIN_INTERVAL_BETWEEN_INCOMING_PROBE_REQUESTS = 1000;
	public static final int SYMMETRIC_KEY_LENGTH = 32; // 256 bits - note that this isn't used everywhere to determine it

	/** Datastore directory */
	private final ProgramDirectory storeDir;

	/** Datastore properties */
	private String storeType;
	private int storeBloomFilterSize;
	private final boolean storeBloomFilterCounting;
	private boolean storeSaltHashResizeOnStart;

	/** The number of bytes per key total in all the different datastores. All the datastores
	 * are always the same size in number of keys. */
	static final int sizePerKey = CHKBlock.DATA_LENGTH + CHKBlock.TOTAL_HEADERS_LENGTH +
		DSAPublicKey.PADDED_SIZE + SSKBlock.DATA_LENGTH + SSKBlock.TOTAL_HEADERS_LENGTH;

	/** The maximum number of keys stored in each of the datastores, cache and store combined. */
	private long maxTotalKeys;
	long maxCacheKeys;
	long maxStoreKeys;
	/** The maximum size of the datastore. Kept to avoid rounding turning 5G into 5368698672 */
	private long maxTotalDatastoreSize;
	/** If true, store shrinks occur immediately even if they are over 10% of the store size. If false,
	 * we just set the storeSize and do an offline shrink on the next startup. Online shrinks do not
	 * preserve the most recently used data so are not recommended. */
	private boolean storeForceBigShrinks;

	/* These are private because must be protected by synchronized(this) */
	private Environment storeEnvironment;
	private EnvironmentMutableConfig envMutableConfig;
	private final SemiOrderedShutdownHook shutdownHook;
	private long databaseMaxMemory;
	/** The CHK datastore. Long term storage; data should only be inserted here if
	 * this node is the closest location on the chain so far, and it is on an
	 * insert (because inserts will always reach the most specialized node; if we
	 * allow requests to store here, then we get pollution by inserts for keys not
	 * close to our specialization). These conclusions derived from Oskar's simulations. */
	private CHKStore chkDatastore;
	/** The SSK datastore. See description for chkDatastore. */
	private SSKStore sskDatastore;
	/** The store of DSAPublicKeys (by hash). See description for chkDatastore. */
	private PubkeyStore pubKeyDatastore;

	/** Client cache store type */
	private String clientCacheType;
	/** Client cache could not be opened so is a RAMFS until the correct password is entered */
	private boolean clientCacheAwaitingPassword;
	private boolean databaseAwaitingPassword;
	/** Client cache maximum cached keys for each type */
	long maxClientCacheKeys;
	/** Maximum size of the client cache. Kept to avoid rounding problems. */
	private long maxTotalClientCacheSize;

	/** Cached client cache key if the user is in the first-time wizard */
	private byte[] cachedClientCacheKey;

	/** The CHK datacache. Short term cache which stores everything that passes
	 * through this node. */
	private CHKStore chkDatacache;
	/** The SSK datacache. Short term cache which stores everything that passes
	 * through this node. */
	private SSKStore sskDatacache;
	/** The public key datacache (by hash). Short term cache which stores
	 * everything that passes through this node. */
	private PubkeyStore pubKeyDatacache;

	/** The CHK client cache. Caches local requests only. */
	private CHKStore chkClientcache;
	/** The SSK client cache. Caches local requests only. */
	private SSKStore sskClientcache;
	/** The pubkey client cache. Caches local requests only. */
	private PubkeyStore pubKeyClientcache;

	// These only cache keys for 30 minutes.

	// FIXME make the first two configurable
	private long maxSlashdotCacheSize;
	private int maxSlashdotCacheKeys;
	static final long PURGE_INTERVAL = 60*1000;

	private CHKStore chkSlashdotcache;
	private SlashdotStore<CHKBlock> chkSlashdotcacheStore;
	private SSKStore sskSlashdotcache;
	private SlashdotStore<SSKBlock> sskSlashdotcacheStore;
	private PubkeyStore pubKeySlashdotcache;
	private SlashdotStore<DSAPublicKey> pubKeySlashdotcacheStore;

	/** If false, only ULPRs will use the slashdot cache. If true, everything does. */
	private boolean useSlashdotCache;
	/** If true, we write stuff to the datastore even though we shouldn't because the HTL is
	 * too high. However it is flagged as old so it won't be included in the Bloom filter for
	 * sharing purposes. */
	private boolean writeLocalToDatastore;

	final GetPubkey getPubKey;

	/** RequestSender's currently transferring, by key */
	private final HashMap<NodeCHK, RequestSender> transferringRequestSendersRT;
	private final HashMap<NodeCHK, RequestSender> transferringRequestSendersBulk;
	/** UIDs of RequestHandler's currently transferring */
	private final HashSet<Long> transferringRequestHandlers;
	/** FetchContext for ARKs */
	public final FetchContext arkFetcherContext;

	/** IP detector */
	public final NodeIPDetector ipDetector;
	/** For debugging/testing, set this to true to stop the
	 * probabilistic decrement at the edges of the HTLs. */
	boolean disableProbabilisticHTLs;

	/** HashSet of currently running request UIDs */
	private final HashMap<Long,UIDTag> runningUIDs;
	private final HashMap<Long,RequestTag> runningCHKGetUIDsBulk;
	private final HashMap<Long,RequestTag> runningLocalCHKGetUIDsBulk;
	private final HashMap<Long,RequestTag> runningSSKGetUIDsBulk;
	private final HashMap<Long,RequestTag> runningLocalSSKGetUIDsBulk;
	private final HashMap<Long,InsertTag> runningCHKPutUIDsBulk;
	private final HashMap<Long,InsertTag> runningLocalCHKPutUIDsBulk;
	private final HashMap<Long,InsertTag> runningSSKPutUIDsBulk;
	private final HashMap<Long,InsertTag> runningLocalSSKPutUIDsBulk;
	private final HashMap<Long,OfferReplyTag> runningCHKOfferReplyUIDsBulk;
	private final HashMap<Long,OfferReplyTag> runningSSKOfferReplyUIDsBulk;

	private final HashMap<Long,RequestTag> runningCHKGetUIDsRT;
	private final HashMap<Long,RequestTag> runningLocalCHKGetUIDsRT;
	private final HashMap<Long,RequestTag> runningSSKGetUIDsRT;
	private final HashMap<Long,RequestTag> runningLocalSSKGetUIDsRT;
	private final HashMap<Long,InsertTag> runningCHKPutUIDsRT;
	private final HashMap<Long,InsertTag> runningLocalCHKPutUIDsRT;
	private final HashMap<Long,InsertTag> runningSSKPutUIDsRT;
	private final HashMap<Long,InsertTag> runningLocalSSKPutUIDsRT;
	private final HashMap<Long,OfferReplyTag> runningCHKOfferReplyUIDsRT;
	private final HashMap<Long,OfferReplyTag> runningSSKOfferReplyUIDsRT;


	/** Semi-unique ID for swap requests. Used to identify us so that the
	 * topology can be reconstructed. */
	public long swapIdentifier;
	private String myName;
	public final LocationManager lm;
	/** My peers */
	public final PeerManager peers;
	/** Node-reference directory (node identity, peers, etc) */
	final ProgramDirectory nodeDir;
	/** Config directory (l10n overrides, etc) */
	final ProgramDirectory cfgDir;
	/** User data directory (bookmarks, download lists, etc) */
	final ProgramDirectory userDir;
	/** Run-time state directory (bootID, PRNG seed, etc) */
	final ProgramDirectory runDir;
	/** Plugin directory */
	final ProgramDirectory pluginDir;

	/** File to write crypto master keys into, possibly passworded */
	final File masterKeysFile;
	/** Directory to put extra peer data into */
	final File extraPeerDataDir;
	/** Strong RNG */
	public final RandomSource random;
	/** Weak but fast RNG */
	public final Random fastWeakRandom;
	/** The object which handles incoming messages and allows us to wait for them */
	final MessageCore usm;

	// Darknet stuff

	NodeCrypto darknetCrypto;

	// Opennet stuff

	private final NodeCryptoConfig opennetCryptoConfig;
	OpennetManager opennet;
	private volatile boolean isAllowedToConnectToSeednodes;
	private int maxOpennetPeers;
	private boolean acceptSeedConnections;
	private boolean passOpennetRefsThroughDarknet;

	// General stuff

	public final Executor executor;
	public final PacketSender ps;
	public final PrioritizedTicker ticker;
	final DNSRequester dnsr;
	final NodeDispatcher dispatcher;
	public final UptimeEstimator uptime;
	public final TokenBucket outputThrottle;
	public boolean throttleLocalData;
	private int outputBandwidthLimit;
	private int inputBandwidthLimit;
	boolean inputLimitDefault;
	final boolean enableARKs;
	final boolean enablePerNodeFailureTables;
	final boolean enableULPRDataPropagation;
	final boolean enableSwapping;
	private volatile boolean publishOurPeersLocation;
	private volatile boolean routeAccordingToOurPeersLocation;
	boolean enableSwapQueueing;
	boolean enablePacketCoalescing;
	public static final short DEFAULT_MAX_HTL = (short)18;
	private short maxHTL;
	private boolean skipWrapperWarning;
	private int maxPacketSize;
	/** Should inserts ignore low backoff times by default? */
	public static boolean IGNORE_LOW_BACKOFF_DEFAULT = false;
	/** Definition of "low backoff times" for above. */
	public static final int LOW_BACKOFF = 30*1000;
	/** Should inserts be fairly blatently prioritised on accept by default? */
	public static boolean PREFER_INSERT_DEFAULT = false;
	/** Should inserts fork when the HTL reaches cacheability? */
	public static boolean FORK_ON_CACHEABLE_DEFAULT = true;
	public final IOStatisticCollector collector;
	/** Type identifier for fproxy node to node messages, as sent on DMT.nodeToNodeMessage's */
	public static final int N2N_MESSAGE_TYPE_FPROXY = 1;
	/** Type identifier for differential node reference messages, as sent on DMT.nodeToNodeMessage's */
	public static final int N2N_MESSAGE_TYPE_DIFFNODEREF = 2;
	/** Identifier within fproxy messages for simple, short text messages to be displayed on the homepage as useralerts */
	public static final int N2N_TEXT_MESSAGE_TYPE_USERALERT = 1;
	/** Identifier within fproxy messages for an offer to transfer a file */
	public static final int N2N_TEXT_MESSAGE_TYPE_FILE_OFFER = 2;
	/** Identifier within fproxy messages for accepting an offer to transfer a file */
	public static final int N2N_TEXT_MESSAGE_TYPE_FILE_OFFER_ACCEPTED = 3;
	/** Identifier within fproxy messages for rejecting an offer to transfer a file */
	public static final int N2N_TEXT_MESSAGE_TYPE_FILE_OFFER_REJECTED = 4;
	/** Identified within friend feed for the recommendation of a bookmark */
	public static final int N2N_TEXT_MESSAGE_TYPE_BOOKMARK = 5;
	/** Identified within friend feed for the recommendation of a file */
	public static final int N2N_TEXT_MESSAGE_TYPE_DOWNLOAD = 6;
	public static final int EXTRA_PEER_DATA_TYPE_N2NTM = 1;
	public static final int EXTRA_PEER_DATA_TYPE_PEER_NOTE = 2;
	public static final int EXTRA_PEER_DATA_TYPE_QUEUED_TO_SEND_N2NM = 3;
	public static final int EXTRA_PEER_DATA_TYPE_BOOKMARK = 4;
	public static final int EXTRA_PEER_DATA_TYPE_DOWNLOAD = 5;
	public static final int PEER_NOTE_TYPE_PRIVATE_DARKNET_COMMENT = 1;

	/** The bootID of the last time the node booted up. Or -1 if we don't know due to
	 * permissions problems, or we suspect that the node has been booted and not
	 * written the file e.g. if we can't write it. So if we want to compare data
	 * gathered in the last session and only recorded to disk on a clean shutdown
	 * to data we have now, we just include the lastBootID. */
	public final long lastBootID;
	public final long bootID;
	public final long startupTime;

	private SimpleToadletServer toadlets;

	public final NodeClientCore clientCore;

	// ULPRs, RecentlyFailed, per node failure tables, are all managed by FailureTable.
	final FailureTable failureTable;

	// The version we were before we restarted.
	public int lastVersion;

	/** NodeUpdater **/
	public final NodeUpdateManager nodeUpdater;

	public final SecurityLevels securityLevels;

	// Things that's needed to keep track of
	public final PluginManager pluginManager;

	// Helpers
	public final InetAddress localhostAddress;
	public final FreenetInetAddress fLocalhostAddress;

	// The node starter
	private static NodeStarter nodeStarter;

	// The watchdog will be silenced until it's true
	private boolean hasStarted;
	private boolean isStopping = false;

	/**
	 * Minimum uptime for us to consider a node an acceptable place to store a key. We store a key
	 * to the datastore only if it's from an insert, and we are a sink, but when calculating whether
	 * we are a sink we ignore nodes which have less uptime (percentage) than this parameter.
	 */
	static final int MIN_UPTIME_STORE_KEY = 40;

	private volatile boolean isPRNGReady = false;

	private boolean storePreallocate;

	/**
	 * Read all storable settings (identity etc) from the node file.
	 * @param filename The name of the file to read from.
	 * @throws IOException throw when I/O error occur
	 */
	private void readNodeFile(String filename) throws IOException {
		// REDFLAG: Any way to share this code with NodePeer?
		FileInputStream fis = new FileInputStream(filename);
		InputStreamReader isr = new InputStreamReader(fis, "UTF-8");
		BufferedReader br = new BufferedReader(isr);
		SimpleFieldSet fs = new SimpleFieldSet(br, false, true);
		br.close();
		// Read contents
		String[] udp = fs.getAll("physical.udp");
		if((udp != null) && (udp.length > 0)) {
			for(String udpAddr : udp) {
				// Just keep the first one with the correct port number.
				Peer p;
				try {
					p = new Peer(udpAddr, false, true);
				} catch (HostnameSyntaxException e) {
					Logger.error(this, "Invalid hostname or IP Address syntax error while parsing our darknet node reference: "+udpAddr);
					System.err.println("Invalid hostname or IP Address syntax error while parsing our darknet node reference: "+udpAddr);
					continue;
				} catch (PeerParseException e) {
					throw (IOException)new IOException().initCause(e);
				}
				if(p.getPort() == getDarknetPortNumber()) {
					// DNSRequester doesn't deal with our own node
					ipDetector.setOldIPAddress(p.getFreenetAddress());
					break;
				}
			}
		}

		darknetCrypto.readCrypto(fs);

		swapIdentifier = Fields.bytesToLong(darknetCrypto.identityHashHash);
		String loc = fs.get("location");
		double locD = Location.getLocation(loc);
		if (locD == -1.0)
			throw new IOException("Invalid location: " + loc);
		lm.setLocation(locD);
		myName = fs.get("myName");
		if(myName == null) {
			myName = newName();
		}

		String verString = fs.get("version");
		if(verString == null) {
			Logger.error(this, "No version!");
			System.err.println("No version!");
		} else {
			lastVersion = Version.getArbitraryBuildNumber(verString, -1);
		}
	}

	public void makeStore(String val) throws InvalidConfigValueException {
		String suffix = getStoreSuffix();
		if (val.equals("salt-hash")) {
			try {
				initSaltHashFS(suffix, true, null);
			} catch (NodeInitException e) {
				Logger.error(this, "Unable to create new store", e);
				System.err.println("Unable to create new store: "+e);
				e.printStackTrace();
				// FIXME l10n both on the NodeInitException and the wrapper message
				throw new InvalidConfigValueException("Unable to create new store: "+e);
			}
		} else if (val.equals("bdb-index")) {
			try {
				initBDBFS(suffix);
			} catch (NodeInitException e) {
				Logger.error(this, "Unable to create new store", e);
				System.err.println("Unable to create new store: "+e);
				e.printStackTrace();
				// FIXME l10n both on the NodeInitException and the wrapper message
				throw new InvalidConfigValueException("Unable to create new store: "+e);
			}
		} else {
			initRAMFS();
		}

		synchronized(Node.this) {
			storeType = val;
		}
	}


	private String newName() {
		return "Node id|"+random.nextLong();
	}

	private final Object writeNodeFileSync = new Object();

	public void writeNodeFile() {
		synchronized(writeNodeFileSync) {
			writeNodeFile(nodeDir.file("node-"+getDarknetPortNumber()), nodeDir.file("node-"+getDarknetPortNumber()+".bak"));
		}
	}

	public void writeOpennetFile() {
		OpennetManager om = opennet;
		if(om != null) om.writeFile();
	}

	private void writeNodeFile(File orig, File backup) {
		SimpleFieldSet fs = darknetCrypto.exportPrivateFieldSet();

		if(orig.exists()) backup.delete();

		FileOutputStream fos = null;
		try {
			fos = new FileOutputStream(backup);
			fs.writeTo(fos);
			fos.close();
			fos = null;
			FileUtil.renameTo(backup, orig);
		} catch (IOException ioe) {
			Logger.error(this, "IOE :"+ioe.getMessage(), ioe);
			return;
		} finally {
			Closer.close(fos);
		}
	}

	private void initNodeFileSettings() {
		Logger.normal(this, "Creating new node file from scratch");
		// Don't need to set getDarknetPortNumber()
		// FIXME use a real IP!
		darknetCrypto.initCrypto();
		swapIdentifier = Fields.bytesToLong(darknetCrypto.identityHashHash);
		myName = newName();
	}

	/**
	 * Read the config file from the arguments.
	 * Then create a node.
	 * Anything that needs static init should ideally be in here.
	 * @param args
	 */
	public static void main(String[] args) throws IOException {
		NodeStarter.main(args);
	}

	public boolean isUsingWrapper(){
		if(nodeStarter!=null && WrapperManager.isControlledByNativeWrapper())
			return true;
		else
			return false;
	}

	public NodeStarter getNodeStarter(){
		return nodeStarter;
	}

	/**
	 * Create a Node from a Config object.
	 * @param config The Config object for this node.
	 * @param r The random number generator for this node. Passed in because we may want
	 * to use a non-secure RNG for e.g. one-JVM live-code simulations. Should be a Yarrow in
	 * a production node. Yarrow will be used if that parameter is null
	 * @param weakRandom The fast random number generator the node will use. If null a MT
	 * instance will be used, seeded from the secure PRNG.
	 * @param lc logging config Handler
	 * @param ns NodeStarter
	 * @param executor Executor
	 * @throws NodeInitException If the node initialization fails.
	 */
	 Node(PersistentConfig config, RandomSource r, RandomSource weakRandom, LoggingConfigHandler lc, NodeStarter ns, Executor executor) throws NodeInitException {
		this.shutdownHook = SemiOrderedShutdownHook.get();
		// Easy stuff
		String tmp = "Initializing Node using Freenet Build #"+Version.buildNumber()+" r"+Version.cvsRevision()+" and freenet-ext Build #"+NodeStarter.extBuildNumber+" r"+NodeStarter.extRevisionNumber+" with "+System.getProperty("java.vendor")+" JVM version "+System.getProperty("java.version")+" running on "+System.getProperty("os.arch")+' '+System.getProperty("os.name")+' '+System.getProperty("os.version");
		Logger.normal(this, tmp);
		System.out.println(tmp);
		collector = new IOStatisticCollector();
		this.executor = executor;
		nodeStarter=ns;
		if(logConfigHandler != lc)
			logConfigHandler=lc;
		getPubKey = new GetPubkey(this);
		startupTime = System.currentTimeMillis();
		SimpleFieldSet oldConfig = config.getSimpleFieldSet();
		// Setup node-specific configuration
		final SubConfig nodeConfig = new SubConfig("node", config);
		final SubConfig installConfig = new SubConfig("node.install", config);

		int sortOrder = 0;

		// Directory for node-related files other than store
		this.userDir = setupProgramDir(installConfig, "userDir", ".",
		  "Node.userDir", "Node.userDirLong", nodeConfig);
		this.cfgDir = setupProgramDir(installConfig, "cfgDir", getUserDir().toString(),
		  "Node.cfgDir", "Node.cfgDirLong", nodeConfig);
		this.nodeDir = setupProgramDir(installConfig, "nodeDir", getUserDir().toString(),
		  "Node.nodeDir", "Node.nodeDirLong", nodeConfig);
		this.runDir = setupProgramDir(installConfig, "runDir", getUserDir().toString(),
		  "Node.runDir", "Node.runDirLong", nodeConfig);
		this.pluginDir = setupProgramDir(installConfig, "pluginDir",  userDir().file("plugins").toString(),
		  "Node.pluginDir", "Node.pluginDirLong", nodeConfig);

		// l10n stuffs
		nodeConfig.register("l10n", Locale.getDefault().getLanguage().toLowerCase(), sortOrder++, false, true,
				"Node.l10nLanguage",
				"Node.l10nLanguageLong",
				new L10nCallback());

		try {
			new NodeL10n(BaseL10n.LANGUAGE.mapToLanguage(nodeConfig.getString("l10n")), getCfgDir());
		} catch (MissingResourceException e) {
			try {
				new NodeL10n(BaseL10n.LANGUAGE.mapToLanguage(nodeConfig.getOption("l10n").getDefault()), getCfgDir());
			} catch (MissingResourceException e1) {
				new NodeL10n(BaseL10n.LANGUAGE.mapToLanguage(BaseL10n.LANGUAGE.getDefault().shortCode), getCfgDir());
			}
		}

		// FProxy config needs to be here too
		SubConfig fproxyConfig = new SubConfig("fproxy", config);
		try {
			toadlets = new SimpleToadletServer(fproxyConfig, new ArrayBucketFactory(), executor, this);
			fproxyConfig.finishedInitialization();
			toadlets.start();
		} catch (IOException e4) {
			Logger.error(this, "Could not start web interface: "+e4, e4);
			System.err.println("Could not start web interface: "+e4);
			e4.printStackTrace();
			throw new NodeInitException(NodeInitException.EXIT_COULD_NOT_START_FPROXY, "Could not start FProxy: "+e4);
		} catch (InvalidConfigValueException e4) {
			System.err.println("Invalid config value, cannot start web interface: "+e4);
			e4.printStackTrace();
			throw new NodeInitException(NodeInitException.EXIT_COULD_NOT_START_FPROXY, "Could not start FProxy: "+e4);
		}

		// Setup RNG if needed : DO NOT USE IT BEFORE THAT POINT!
		if (r == null) {
			final NativeThread entropyGatheringThread = new NativeThread(new Runnable() {

				private void recurse(File f) {
					if(isPRNGReady)
						return;
					File[] subDirs = f.listFiles(new FileFilter() {

						public boolean accept(File pathname) {
							return pathname.exists() && pathname.canRead() && pathname.isDirectory();
						}
					});


					// @see http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=5086412
					if(subDirs != null)
						for(File currentDir : subDirs)
							recurse(currentDir);
				}

				public void run() {
					for(File root : File.listRoots()) {
						if(isPRNGReady)
							return;
						recurse(root);
					}
				}
			}, "Entropy Gathering Thread", NativeThread.MIN_PRIORITY, true);

			File seed = userDir.file("prng.seed");
			FileUtil.setOwnerRW(seed);
			entropyGatheringThread.start();
			this.random = new Yarrow(seed);
			DiffieHellman.init(random);

		} else // if it's not null it's because we are running in the simulator
			this.random = r;
		isPRNGReady = true;
		toadlets.getStartupToadlet().setIsPRNGReady();
		if(weakRandom == null) {
			byte buffer[] = new byte[16];
			random.nextBytes(buffer);
			this.fastWeakRandom = new MersenneTwister(buffer);
		}else
			this.fastWeakRandom = weakRandom;

		nodeNameUserAlert = new MeaningfulNodeNameUserAlert(this);
		recentlyCompletedIDs = new LRUQueue<Long>();
		this.config = config;
		lm = new LocationManager(random, this);

		try {
			localhostAddress = InetAddress.getByName("127.0.0.1");
		} catch (UnknownHostException e3) {
			// Does not do a reverse lookup, so this is impossible
			throw new Error(e3);
		}
		fLocalhostAddress = new FreenetInetAddress(localhostAddress);
		transferringRequestSendersRT = new HashMap<NodeCHK, RequestSender>();
		transferringRequestSendersBulk = new HashMap<NodeCHK, RequestSender>();
		transferringRequestHandlers = new HashSet<Long>();
		runningUIDs = new HashMap<Long,UIDTag>();
		runningCHKGetUIDsRT = new HashMap<Long,RequestTag>();
		runningLocalCHKGetUIDsRT = new HashMap<Long,RequestTag>();
		runningSSKGetUIDsRT = new HashMap<Long,RequestTag>();
		runningLocalSSKGetUIDsRT = new HashMap<Long,RequestTag>();
		runningCHKPutUIDsRT = new HashMap<Long,InsertTag>();
		runningLocalCHKPutUIDsRT = new HashMap<Long,InsertTag>();
		runningSSKPutUIDsRT = new HashMap<Long,InsertTag>();
		runningLocalSSKPutUIDsRT = new HashMap<Long,InsertTag>();
		runningCHKOfferReplyUIDsRT = new HashMap<Long,OfferReplyTag>();
		runningSSKOfferReplyUIDsRT = new HashMap<Long,OfferReplyTag>();

		runningCHKGetUIDsBulk = new HashMap<Long,RequestTag>();
		runningLocalCHKGetUIDsBulk = new HashMap<Long,RequestTag>();
		runningSSKGetUIDsBulk = new HashMap<Long,RequestTag>();
		runningLocalSSKGetUIDsBulk = new HashMap<Long,RequestTag>();
		runningCHKPutUIDsBulk = new HashMap<Long,InsertTag>();
		runningLocalCHKPutUIDsBulk = new HashMap<Long,InsertTag>();
		runningSSKPutUIDsBulk = new HashMap<Long,InsertTag>();
		runningLocalSSKPutUIDsBulk = new HashMap<Long,InsertTag>();
		runningCHKOfferReplyUIDsBulk = new HashMap<Long,OfferReplyTag>();
		runningSSKOfferReplyUIDsBulk = new HashMap<Long,OfferReplyTag>();

		this.securityLevels = new SecurityLevels(this, config);

		nodeConfig.register("autoChangeDatabaseEncryption", true, sortOrder++, true, false, "Node.autoChangeDatabaseEncryption", "Node.autoChangeDatabaseEncryptionLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				synchronized(Node.this) {
					return autoChangeDatabaseEncryption;
				}
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException, NodeNeedRestartException {
				synchronized(Node.this) {
					autoChangeDatabaseEncryption = val;
				}
			}

		});

		autoChangeDatabaseEncryption = nodeConfig.getBoolean("autoChangeDatabaseEncryption");

		// Location of master key
		nodeConfig.register("masterKeyFile", "master.keys", sortOrder++, true, true, "Node.masterKeyFile", "Node.masterKeyFileLong",
			new StringCallback() {

				@Override
				public String get() {
					if(masterKeysFile == null) return "none";
					else return masterKeysFile.getPath();
				}

				@Override
				public void set(String val) throws InvalidConfigValueException, NodeNeedRestartException {
					// FIXME l10n
					// FIXME wipe the old one and move
					throw new InvalidConfigValueException("Node.masterKeyFile cannot be changed on the fly, you must shutdown, wipe the old file and reconfigure");
				}

		});
		String value = nodeConfig.getString("masterKeyFile");
		File f;
		if (value.equalsIgnoreCase("none")) {
			f = null;
		} else {
			f = new File(value);
			if (!f.isAbsolute()) { f = userDir.file(value); }

			if(f.exists() && !(f.canWrite() && f.canRead()))
				throw new NodeInitException(NodeInitException.EXIT_CANT_WRITE_MASTER_KEYS, "Cannot read from and write to master keys file "+f);
		}
		masterKeysFile = f;
		FileUtil.setOwnerRW(masterKeysFile);

		shutdownHook.addEarlyJob(new NativeThread("Shutdown database", NativeThread.HIGH_PRIORITY, true) {

			public void realRun() {
				System.err.println("Stopping database jobs...");
				if(clientCore == null) return;
				clientCore.killDatabase();
			}

		});

		shutdownHook.addLateJob(new NativeThread("Close database", NativeThread.HIGH_PRIORITY, true) {

			@Override
			public void realRun() {
				if(db == null) return;
				System.err.println("Rolling back unfinished transactions...");
				db.rollback();
				System.err.println("Closing database...");
				db.close();
				if(securityLevels.getPhysicalThreatLevel() == PHYSICAL_THREAT_LEVEL.MAXIMUM) {
					try {
						FileUtil.secureDelete(dbFileCrypt, random);
					} catch (IOException e) {
						// Ignore
					} finally {
						dbFileCrypt.delete();
					}
				}
			}

		});

		nodeConfig.register("defragDatabaseOnStartup", true, sortOrder++, false, true, "Node.defragDatabaseOnStartup", "Node.defragDatabaseOnStartupLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				synchronized(Node.this) {
					return defragDatabaseOnStartup;
				}
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException, NodeNeedRestartException {
				synchronized(Node.this) {
					defragDatabaseOnStartup = val;
				}
			}

		});

		defragDatabaseOnStartup = nodeConfig.getBoolean("defragDatabaseOnStartup");

		nodeConfig.register("defragOnce", false, sortOrder++, false, true, "Node.defragOnce", "Node.defragOnceLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				synchronized(Node.this) {
					return defragOnce;
				}
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException, NodeNeedRestartException {
				synchronized(Node.this) {
					defragOnce = val;
				}
			}

		});

		defragOnce = nodeConfig.getBoolean("defragOnce");

		dbFile = userDir.file("node.db4o");
		dbFileCrypt = userDir.file("node.db4o.crypt");

		boolean dontCreate = (!dbFile.exists()) && (!dbFileCrypt.exists()) && (!toadlets.fproxyHasCompletedWizard());

		if(!dontCreate) {
			try {
				setupDatabase(null);
			} catch (MasterKeysWrongPasswordException e2) {
				System.out.println("Client database node.db4o is encrypted!");
				databaseAwaitingPassword = true;
			} catch (MasterKeysFileSizeException e2) {
				System.err.println("Unable to decrypt database: master.keys file too " + e2.sizeToString() + "!");
			} catch (IOException e2) {
				System.err.println("Unable to access master.keys file to decrypt database: "+e2);
				e2.printStackTrace();
			}
		} else
			System.out.println("Not creating node.db4o for now, waiting for config as to security level...");

		// Boot ID
		bootID = random.nextLong();
		// Fixed length file containing boot ID. Accessed with random access file. So hopefully it will always be
		// written. Note that we set lastBootID to -1 if we can't _write_ our ID as well as if we can't read it,
		// because if we can't write it then we probably couldn't write it on the last bootup either.
		File bootIDFile = runDir.file("bootID");
		int BOOT_FILE_LENGTH = 64 / 4; // A long in padded hex bytes
		long oldBootID = -1;
		RandomAccessFile raf = null;
		try {
			raf = new RandomAccessFile(bootIDFile, "rw");
			if(raf.length() < BOOT_FILE_LENGTH) {
				oldBootID = -1;
			} else {
				byte[] buf = new byte[BOOT_FILE_LENGTH];
				raf.readFully(buf);
				String s = new String(buf, "ISO-8859-1");
				try {
					oldBootID = Fields.bytesToLong(HexUtil.hexToBytes(s));
				} catch (NumberFormatException e) {
					oldBootID = -1;
				}
				raf.seek(0);
			}
			String s = HexUtil.bytesToHex(Fields.longToBytes(bootID));
			byte[] buf = s.getBytes("ISO-8859-1");
			if(buf.length != BOOT_FILE_LENGTH)
				System.err.println("Not 16 bytes for boot ID "+bootID+" - WTF??");
			raf.write(buf);
		} catch (IOException e) {
			oldBootID = -1;
			// If we have an error in reading, *or in writing*, we don't reliably know the last boot ID.
		} finally {
			Closer.close(raf);
		}
		lastBootID = oldBootID;

		buildOldAgeUserAlert = new BuildOldAgeUserAlert();

		nodeConfig.register("disableProbabilisticHTLs", false, sortOrder++, true, false, "Node.disablePHTLS", "Node.disablePHTLSLong",
				new BooleanCallback() {

					@Override
					public Boolean get() {
						return disableProbabilisticHTLs;
					}

					@Override
					public void set(Boolean val) throws InvalidConfigValueException {
						disableProbabilisticHTLs = val;
					}

		});

		disableProbabilisticHTLs = nodeConfig.getBoolean("disableProbabilisticHTLs");

		nodeConfig.register("maxHTL", DEFAULT_MAX_HTL, sortOrder++, true, false, "Node.maxHTL", "Node.maxHTLLong", new ShortCallback() {

					@Override
					public Short get() {
						return maxHTL;
					}

					@Override
					public void set(Short val) throws InvalidConfigValueException {
						if(maxHTL < 0) throw new InvalidConfigValueException("Impossible max HTL");
						maxHTL = val;
					}
		}, false);

		maxHTL = nodeConfig.getShort("maxHTL");

		// FIXME maybe these should persist? They need to be private.
		decrementAtMax = random.nextDouble() <= DECREMENT_AT_MAX_PROB;
		decrementAtMin = random.nextDouble() <= DECREMENT_AT_MIN_PROB;

		// Determine where to bind to

		usm = new MessageCore(executor);

		// FIXME maybe these configs should actually be under a node.ip subconfig?
		ipDetector = new NodeIPDetector(this);
		sortOrder = ipDetector.registerConfigs(nodeConfig, sortOrder);

		// ARKs enabled?

		nodeConfig.register("enableARKs", true, sortOrder++, true, false, "Node.enableARKs", "Node.enableARKsLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				return enableARKs;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				throw new InvalidConfigValueException("Cannot change on the fly");
			}

			@Override
			public boolean isReadOnly() {
				        return true;
			        }
		});
		enableARKs = nodeConfig.getBoolean("enableARKs");

		nodeConfig.register("enablePerNodeFailureTables", true, sortOrder++, true, false, "Node.enablePerNodeFailureTables", "Node.enablePerNodeFailureTablesLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				return enablePerNodeFailureTables;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				throw new InvalidConfigValueException("Cannot change on the fly");
			}

			@Override
			public boolean isReadOnly() {
				        return true;
			      }
		});
		enablePerNodeFailureTables = nodeConfig.getBoolean("enablePerNodeFailureTables");

		nodeConfig.register("enableULPRDataPropagation", true, sortOrder++, true, false, "Node.enableULPRDataPropagation", "Node.enableULPRDataPropagationLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				return enableULPRDataPropagation;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				throw new InvalidConfigValueException("Cannot change on the fly");
			}

			@Override
			public boolean isReadOnly() {
				        return true;
			        }
		});
		enableULPRDataPropagation = nodeConfig.getBoolean("enableULPRDataPropagation");

		nodeConfig.register("enableSwapping", true, sortOrder++, true, false, "Node.enableSwapping", "Node.enableSwappingLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				return enableSwapping;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				throw new InvalidConfigValueException("Cannot change on the fly");
			}

			@Override
			public boolean isReadOnly() {
				        return true;
			        }
		});
		enableSwapping = nodeConfig.getBoolean("enableSwapping");

		/*
		 * Publish our peers' locations is enabled, even in MAXIMUM network security and/or HIGH friends security,
		 * because a node which doesn't publish its peers' locations will get dramatically less traffic.
		 *
		 * Publishing our peers' locations does make us slightly more vulnerable to some attacks, but I don't think
		 * it's a big difference: swapping reveals the same information, it just doesn't update as quickly. This
		 * may help slightly, but probably not dramatically against a clever attacker.
		 *
		 * FIXME review this decision.
		 */
		nodeConfig.register("publishOurPeersLocation", true, sortOrder++, true, false, "Node.publishOurPeersLocation", "Node.publishOurPeersLocationLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				return publishOurPeersLocation;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				publishOurPeersLocation = val;
			}
		});
		publishOurPeersLocation = nodeConfig.getBoolean("publishOurPeersLocation");

		nodeConfig.register("routeAccordingToOurPeersLocation", true, sortOrder++, true, false, "Node.routeAccordingToOurPeersLocation", "Node.routeAccordingToOurPeersLocationLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				return routeAccordingToOurPeersLocation;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				routeAccordingToOurPeersLocation = val;
			}
		});
		routeAccordingToOurPeersLocation = nodeConfig.getBoolean("routeAccordingToOurPeersLocation");

		nodeConfig.register("enableSwapQueueing", true, sortOrder++, true, false, "Node.enableSwapQueueing", "Node.enableSwapQueueingLong", new BooleanCallback() {
			@Override
			public Boolean get() {
				return enableSwapQueueing;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				enableSwapQueueing = val;
			}

		});
		enableSwapQueueing = nodeConfig.getBoolean("enableSwapQueueing");

		nodeConfig.register("enablePacketCoalescing", true, sortOrder++, true, false, "Node.enablePacketCoalescing", "Node.enablePacketCoalescingLong", new BooleanCallback() {
			@Override
			public Boolean get() {
				return enablePacketCoalescing;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				enablePacketCoalescing = val;
			}

		});
		enablePacketCoalescing = nodeConfig.getBoolean("enablePacketCoalescing");

		// Determine the port number
		// @see #191
		if(oldConfig != null && "-1".equals(oldConfig.get("node.listenPort")))
			throw new NodeInitException(NodeInitException.EXIT_COULD_NOT_BIND_USM, "Your freenet.ini file is corrupted! 'listenPort=-1'");
		NodeCryptoConfig darknetConfig = new NodeCryptoConfig(nodeConfig, sortOrder++, false, securityLevels);
		sortOrder += NodeCryptoConfig.OPTION_COUNT;

		darknetCrypto = new NodeCrypto(this, false, darknetConfig, startupTime, enableARKs);

		nodeDBHandle = darknetCrypto.getNodeHandle(db);

		if(db != null) {
			db.commit();
			if(logMINOR) Logger.minor(this, "COMMITTED");
		}

		// Must be created after darknetCrypto
		dnsr = new DNSRequester(this);
		ps = new PacketSender(this);
		ticker = new PrioritizedTicker(executor, getDarknetPortNumber());
		if(executor instanceof PooledExecutor)
			((PooledExecutor)executor).setTicker(ticker);

		Logger.normal(Node.class, "Creating node...");

		shutdownHook.addEarlyJob(new Thread() {
			@Override
			public void run() {
				if (opennet != null)
					opennet.stop(false);
			}
		});

		shutdownHook.addEarlyJob(new Thread() {
			@Override
			public void run() {
				darknetCrypto.stop();
			}
		});

		// Bandwidth limit

		nodeConfig.register("outputBandwidthLimit", "15K", sortOrder++, false, true, "Node.outBWLimit", "Node.outBWLimitLong", new IntCallback() {
					@Override
					public Integer get() {
						//return BlockTransmitter.getHardBandwidthLimit();
						return outputBandwidthLimit;
					}
					@Override
					public void set(Integer obwLimit) throws InvalidConfigValueException {
						if(obwLimit <= 0) throw new InvalidConfigValueException(l10n("bwlimitMustBePositive"));
						synchronized(Node.this) {
							outputBandwidthLimit = obwLimit;
						}
						outputThrottle.changeNanosAndBucketSize((1000L * 1000L * 1000L) / obwLimit, obwLimit/2);
						nodeStats.setOutputLimit(obwLimit);
					}
		}, true);

		int obwLimit = nodeConfig.getInt("outputBandwidthLimit");
		if(obwLimit <= 0)
			throw new NodeInitException(NodeInitException.EXIT_BAD_BWLIMIT, "Invalid outputBandwidthLimit");
		outputBandwidthLimit = obwLimit;
		// Bucket size of 0.5 seconds' worth of bytes.
		// Add them at a rate determined by the obwLimit.
		// Maximum forced bytes 80%, in other words, 20% of the bandwidth is reserved for
		// block transfers, so we will use that 20% for block transfers even if more than 80% of the limit is used for non-limited data (resends etc).
		int bucketSize = obwLimit/2;
		// Must have at least space for ONE PACKET.
		// FIXME: make compatible with alternate transports.
		bucketSize = Math.max(bucketSize, 2048);
		outputThrottle = new TokenBucket(bucketSize, (1000L*1000L*1000L) / obwLimit, obwLimit/2);

		nodeConfig.register("inputBandwidthLimit", "-1", sortOrder++, false, true, "Node.inBWLimit", "Node.inBWLimitLong",	new IntCallback() {
					@Override
					public Integer get() {
						if(inputLimitDefault) return -1;
						return inputBandwidthLimit;
					}
					@Override
					public void set(Integer ibwLimit) throws InvalidConfigValueException {
						synchronized(Node.this) {
							if(ibwLimit == -1) {
								inputLimitDefault = true;
								ibwLimit = outputBandwidthLimit * 4;
							} else {
								if(ibwLimit <= 1) throw new InvalidConfigValueException(l10n("bandwidthLimitMustBePositiveOrMinusOne"));
								inputLimitDefault = false;
							}
							inputBandwidthLimit = ibwLimit;
						}
						nodeStats.setInputLimit(ibwLimit);
					}
		}, true);

		int ibwLimit = nodeConfig.getInt("inputBandwidthLimit");
		if(ibwLimit == -1) {
			inputLimitDefault = true;
			ibwLimit = obwLimit * 4;
		} else if(ibwLimit <= 0)
			throw new NodeInitException(NodeInitException.EXIT_BAD_BWLIMIT, "Invalid inputBandwidthLimit");
		inputBandwidthLimit = ibwLimit;

		nodeConfig.register("throttleLocalTraffic", false, sortOrder++, true, false, "Node.throttleLocalTraffic", "Node.throttleLocalTrafficLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				return throttleLocalData;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				throttleLocalData = val;
			}

		});

		throttleLocalData = nodeConfig.getBoolean("throttleLocalTraffic");

		String s = "Testnet mode DISABLED. You may have some level of anonymity. :)\n"+
		"Note that this version of Freenet is still a very early alpha, and may well have numerous bugs and design flaws.\n"+
		"In particular: YOU ARE WIDE OPEN TO YOUR IMMEDIATE PEERS! They can eavesdrop on your requests with relatively little difficulty at present (correlation attacks etc).";
		Logger.normal(this, s);
		System.err.println(s);

		File nodeFile = nodeDir.file("node-"+getDarknetPortNumber());
		File nodeFileBackup = nodeDir.file("node-"+getDarknetPortNumber()+".bak");
		// After we have set up testnet and IP address, load the node file
		try {
			// FIXME should take file directly?
			readNodeFile(nodeFile.getPath());
		} catch (IOException e) {
			try {
				System.err.println("Trying to read node file backup ...");
				readNodeFile(nodeFileBackup.getPath());
			} catch (IOException e1) {
				if(nodeFile.exists() || nodeFileBackup.exists()) {
					System.err.println("No node file or cannot read, (re)initialising crypto etc");
					System.err.println(e1.toString());
					e1.printStackTrace();
					System.err.println("After:");
					System.err.println(e.toString());
					e.printStackTrace();
				} else {
					System.err.println("Creating new cryptographic keys...");
				}
				initNodeFileSettings();
			}
		}

		usm.setDispatcher(dispatcher=new NodeDispatcher(this));

		// Then read the peers
		peers = new PeerManager(this);
		peers.tryReadPeers(nodeDir.file("peers-"+getDarknetPortNumber()).getPath(), darknetCrypto, null, false, false);
		peers.writePeers();
		peers.updatePMUserAlert();

		uptime = new UptimeEstimator(runDir, ticker, darknetCrypto.identityHash);

		// ULPRs

		failureTable = new FailureTable(this);

		nodeStats = new NodeStats(this, sortOrder, new SubConfig("node.load", config), obwLimit, ibwLimit, lastVersion);

		clientCore = new NodeClientCore(this, config, nodeConfig, installConfig, getDarknetPortNumber(), sortOrder, oldConfig, fproxyConfig, toadlets, nodeDBHandle, db);

		// Node updater support

		System.out.println("Initializing Node Updater");
		try {
			nodeUpdater = NodeUpdateManager.maybeCreate(this, config);
		} catch (InvalidConfigValueException e) {
			e.printStackTrace();
			throw new NodeInitException(NodeInitException.EXIT_COULD_NOT_START_UPDATER, "Could not create Updater: "+e);
		}

		// Opennet

		final SubConfig opennetConfig = new SubConfig("node.opennet", config);
		opennetConfig.register("connectToSeednodes", true, 0, true, false, "Node.withAnnouncement", "Node.withAnnouncementLong", new BooleanCallback() {
			@Override
			public Boolean get() {
				return isAllowedToConnectToSeednodes;
			}
			@Override
			public void set(Boolean val) throws InvalidConfigValueException, NodeNeedRestartException {
				if (get().equals(val))
					        return;
				synchronized(Node.this) {
					isAllowedToConnectToSeednodes = val;
					if(opennet != null)
						throw new NodeNeedRestartException(l10n("connectToSeednodesCannotBeChangedMustDisableOpennetOrReboot"));
				}
			}
		});
		isAllowedToConnectToSeednodes = opennetConfig.getBoolean("connectToSeednodes");

		// Can be enabled on the fly
		opennetConfig.register("enabled", false, 0, true, true, "Node.opennetEnabled", "Node.opennetEnabledLong", new BooleanCallback() {
			@Override
			public Boolean get() {
				synchronized(Node.this) {
					return opennet != null;
				}
			}
			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				OpennetManager o;
				synchronized(Node.this) {
					if(val == (opennet != null)) return;
					if(val) {
						try {
							o = opennet = new OpennetManager(Node.this, opennetCryptoConfig, System.currentTimeMillis(), isAllowedToConnectToSeednodes);
						} catch (NodeInitException e) {
							opennet = null;
							throw new InvalidConfigValueException(e.getMessage());
						}
					} else {
						o = opennet;
						opennet = null;
					}
				}
				if(val) o.start();
				else o.stop(true);
				ipDetector.ipDetectorManager.notifyPortChange(getPublicInterfacePorts());
			}
		});
		boolean opennetEnabled = opennetConfig.getBoolean("enabled");

		opennetConfig.register("maxOpennetPeers", OpennetManager.MAX_PEERS_FOR_SCALING, 1, true, false, "Node.maxOpennetPeers",
				"Node.maxOpennetPeersLong", new IntCallback() {
					@Override
					public Integer get() {
						return maxOpennetPeers;
					}
					@Override
					public void set(Integer inputMaxOpennetPeers) throws InvalidConfigValueException {
						if(inputMaxOpennetPeers < 0) throw new InvalidConfigValueException(l10n("mustBePositive"));
						if(inputMaxOpennetPeers > OpennetManager.MAX_PEERS_FOR_SCALING) throw new InvalidConfigValueException(l10n("maxOpennetPeersMustBeTwentyOrLess"));
						maxOpennetPeers = inputMaxOpennetPeers;
						}
					}
		, false);

		maxOpennetPeers = opennetConfig.getInt("maxOpennetPeers");
		if(maxOpennetPeers > OpennetManager.MAX_PEERS_FOR_SCALING) {
			Logger.error(this, "maxOpennetPeers may not be over "+OpennetManager.MAX_PEERS_FOR_SCALING);
			maxOpennetPeers = OpennetManager.MAX_PEERS_FOR_SCALING;
		}

		opennetCryptoConfig = new NodeCryptoConfig(opennetConfig, 2 /* 0 = enabled */, true, securityLevels);

		if(opennetEnabled) {
			opennet = new OpennetManager(this, opennetCryptoConfig, System.currentTimeMillis(), isAllowedToConnectToSeednodes);
			// Will be started later
		} else {
			opennet = null;
		}

		securityLevels.addNetworkThreatLevelListener(new SecurityLevelListener<NETWORK_THREAT_LEVEL>() {

			public void onChange(NETWORK_THREAT_LEVEL oldLevel, NETWORK_THREAT_LEVEL newLevel) {
				if(newLevel == NETWORK_THREAT_LEVEL.HIGH
						|| newLevel == NETWORK_THREAT_LEVEL.MAXIMUM) {
					OpennetManager om;
					synchronized(Node.this) {
						om = opennet;
						if(om != null)
							opennet = null;
					}
					if(om != null) {
						om.stop(true);
						ipDetector.ipDetectorManager.notifyPortChange(getPublicInterfacePorts());
					}
				} else if(newLevel == NETWORK_THREAT_LEVEL.NORMAL
						|| newLevel == NETWORK_THREAT_LEVEL.LOW) {
					OpennetManager o = null;
					synchronized(Node.this) {
						if(opennet == null) {
							try {
								o = opennet = new OpennetManager(Node.this, opennetCryptoConfig, System.currentTimeMillis(), isAllowedToConnectToSeednodes);
							} catch (NodeInitException e) {
								opennet = null;
								Logger.error(this, "UNABLE TO ENABLE OPENNET: "+e, e);
								clientCore.alerts.register(new SimpleUserAlert(false, l10n("enableOpennetFailedTitle"), l10n("enableOpennetFailed", "message", e.getLocalizedMessage()), l10n("enableOpennetFailed", "message", e.getLocalizedMessage()), UserAlert.ERROR));
							}
						}
					}
					if(o != null) {
						o.start();
						ipDetector.ipDetectorManager.notifyPortChange(getPublicInterfacePorts());
					}
				}
				Node.this.config.store();
			}

		});

		opennetConfig.register("acceptSeedConnections", false, 2, true, true, "Node.acceptSeedConnectionsShort", "Node.acceptSeedConnections", new BooleanCallback() {

			@Override
			public Boolean get() {
				return acceptSeedConnections;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				acceptSeedConnections = val;
			}

		});

		acceptSeedConnections = opennetConfig.getBoolean("acceptSeedConnections");

		if(acceptSeedConnections && opennet != null)
			opennet.crypto.socket.getAddressTracker().setHugeTracker();

		opennetConfig.finishedInitialization();

		nodeConfig.register("passOpennetPeersThroughDarknet", true, sortOrder++, true, false, "Node.passOpennetPeersThroughDarknet", "Node.passOpennetPeersThroughDarknetLong",
				new BooleanCallback() {

					@Override
					public Boolean get() {
						synchronized(Node.this) {
							return passOpennetRefsThroughDarknet;
						}
					}

					@Override
					public void set(Boolean val) throws InvalidConfigValueException {
						synchronized(Node.this) {
							passOpennetRefsThroughDarknet = val;
						}
					}

		});

		passOpennetRefsThroughDarknet = nodeConfig.getBoolean("passOpennetPeersThroughDarknet");

		this.extraPeerDataDir = userDir.file("extra-peer-data-"+getDarknetPortNumber());
		if (!((extraPeerDataDir.exists() && extraPeerDataDir.isDirectory()) || (extraPeerDataDir.mkdir()))) {
			String msg = "Could not find or create extra peer data directory";
			throw new NodeInitException(NodeInitException.EXIT_BAD_DIR, msg);
		}

		// Name
		nodeConfig.register("name", myName, sortOrder++, false, true, "Node.nodeName", "Node.nodeNameLong",
						new NodeNameCallback());
		myName = nodeConfig.getString("name");

		// Datastore
		nodeConfig.register("storeForceBigShrinks", false, sortOrder++, true, false, "Node.forceBigShrink", "Node.forceBigShrinkLong",
				new BooleanCallback() {

					@Override
					public Boolean get() {
						synchronized(Node.this) {
							return storeForceBigShrinks;
						}
					}

					@Override
					public void set(Boolean val) throws InvalidConfigValueException {
						synchronized(Node.this) {
							storeForceBigShrinks = val;
						}
					}

		});

		// Datastore

		nodeConfig.register("storeType", "ram", sortOrder++, true, true, "Node.storeType", "Node.storeTypeLong", new StoreTypeCallback());

		storeType = nodeConfig.getString("storeType");

		/*
		 * Very small initial store size, since the node will preallocate it when starting up for the first time,
		 * BLOCKING STARTUP, and since everyone goes through the wizard anyway...
		 */
		nodeConfig.register("storeSize", "10M", sortOrder++, false, true, "Node.storeSize", "Node.storeSizeLong",
				new LongCallback() {

					@Override
					public Long get() {
						return maxTotalDatastoreSize;
					}

					@Override
					public void set(Long storeSize) throws InvalidConfigValueException {
						if((storeSize < 0) || (storeSize < (32 * 1024 * 1024)))
							throw new InvalidConfigValueException(l10n("invalidStoreSize"));
						long newMaxStoreKeys = storeSize / sizePerKey;
						if(newMaxStoreKeys == maxTotalKeys) return;
						// Update each datastore
						synchronized(Node.this) {
							maxTotalDatastoreSize = storeSize;
							maxTotalKeys = newMaxStoreKeys;
							maxStoreKeys = maxTotalKeys / 2;
							maxCacheKeys = maxTotalKeys - maxStoreKeys;
						}
						try {
							chkDatastore.setMaxKeys(maxStoreKeys, storeForceBigShrinks);
							chkDatacache.setMaxKeys(maxCacheKeys, storeForceBigShrinks);
							pubKeyDatastore.setMaxKeys(maxStoreKeys, storeForceBigShrinks);
							pubKeyDatacache.setMaxKeys(maxCacheKeys, storeForceBigShrinks);
							sskDatastore.setMaxKeys(maxStoreKeys, storeForceBigShrinks);
							sskDatacache.setMaxKeys(maxCacheKeys, storeForceBigShrinks);
						} catch (IOException e) {
							// FIXME we need to be able to tell the user.
							Logger.error(this, "Caught "+e+" resizing the datastore", e);
							System.err.println("Caught "+e+" resizing the datastore");
							e.printStackTrace();
						} catch (DatabaseException e) {
							Logger.error(this, "Caught "+e+" resizing the datastore", e);
							System.err.println("Caught "+e+" resizing the datastore");
							e.printStackTrace();
						}
						//Perhaps a bit hackish...? Seems like this should be near it's definition in NodeStats.
						nodeStats.avgStoreCHKLocation.changeMaxReports((int)maxStoreKeys);
						nodeStats.avgCacheCHKLocation.changeMaxReports((int)maxCacheKeys);
						nodeStats.avgSlashdotCacheCHKLocation.changeMaxReports((int)maxCacheKeys);
						nodeStats.avgClientCacheCHKLocation.changeMaxReports((int)maxCacheKeys);

						nodeStats.avgStoreSSKLocation.changeMaxReports((int)maxStoreKeys);
						nodeStats.avgCacheSSKLocation.changeMaxReports((int)maxCacheKeys);
						nodeStats.avgSlashdotCacheSSKLocation.changeMaxReports((int)maxCacheKeys);
						nodeStats.avgClientCacheSSKLocation.changeMaxReports((int)maxCacheKeys);
					}
		}, true);

		maxTotalDatastoreSize = nodeConfig.getLong("storeSize");

		if(maxTotalDatastoreSize < 0 || maxTotalDatastoreSize < (32 * 1024 * 1024) && !storeType.equals("ram")) { // totally arbitrary minimum!
			throw new NodeInitException(NodeInitException.EXIT_INVALID_STORE_SIZE, "Invalid store size");
		}

		maxTotalKeys = maxTotalDatastoreSize / sizePerKey;

		nodeConfig.register("storeBloomFilterSize", -1, sortOrder++, true, false, "Node.storeBloomFilterSize",
		        "Node.storeBloomFilterSizeLong", new IntCallback() {
			        private Integer cachedBloomFilterSize;

			        @Override
					public Integer get() {
			        	if (cachedBloomFilterSize == null)
					        cachedBloomFilterSize = storeBloomFilterSize;
				        return cachedBloomFilterSize;
			        }

			        @Override
					public void set(Integer val) throws InvalidConfigValueException, NodeNeedRestartException {
				        cachedBloomFilterSize = val;
				        throw new NodeNeedRestartException("Store bloom filter size cannot be changed on the fly");
			        }

			        @Override
					public boolean isReadOnly() {
				        return !("salt-hash".equals(storeType));
			        }
		        }, true);

		storeBloomFilterSize = nodeConfig.getInt("storeBloomFilterSize");

		nodeConfig.register("storeBloomFilterCounting", true, sortOrder++, true, false,
		        "Node.storeBloomFilterCounting", "Node.storeBloomFilterCountingLong", new BooleanCallback() {
			        private Boolean cachedBloomFilterCounting;

			        @Override
					public Boolean get() {
				        if (cachedBloomFilterCounting == null)
					        cachedBloomFilterCounting = storeBloomFilterCounting;
				        return cachedBloomFilterCounting;
			        }

			        @Override
					public void set(Boolean val) throws InvalidConfigValueException, NodeNeedRestartException {
				        cachedBloomFilterCounting = val;
				        throw new NodeNeedRestartException("Store bloom filter type cannot be changed on the fly");
			        }

			        @Override
					public boolean isReadOnly() {
				        return !("salt-hash".equals(storeType));
			        }
		        });

		storeBloomFilterCounting = nodeConfig.getBoolean("storeBloomFilterCounting");

		nodeConfig.register("storeSaltHashResizeOnStart", false, sortOrder++, true, false,
				"Node.storeSaltHashResizeOnStart", "Node.storeSaltHashResizeOnStartLong", new BooleanCallback() {
			@Override
			public Boolean get() {
				return storeSaltHashResizeOnStart;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException, NodeNeedRestartException {
				storeSaltHashResizeOnStart = val;
			}
		});
		storeSaltHashResizeOnStart = nodeConfig.getBoolean("storeSaltHashResizeOnStart");

		this.storeDir = setupProgramDir(installConfig, "storeDir", userDir().file("datastore").getPath(), "Node.storeDirectory", "Node.storeDirectoryLong", nodeConfig);

		final String suffix = getStoreSuffix();

		maxStoreKeys = maxTotalKeys / 2;
		maxCacheKeys = maxTotalKeys - maxStoreKeys;

		/*
		 * On Windows, setting the file length normally involves writing lots of zeros.
		 * So it's an uninterruptible system call that takes a loooong time. On OS/X,
		 * presumably the same is true. If the RNG is fast enough, this means that
		 * setting the length and writing random data take exactly the same amount
		 * of time. On most versions of Unix, holes can be created. However on all
		 * systems, predictable disk usage is a good thing. So lets turn it on by
		 * default for now, on all systems. The datastore can be read but mostly not
		 * written while the random data is being written.
		 */
		nodeConfig.register("storePreallocate", true, sortOrder++, true, true, "Node.storePreallocate", "Node.storePreallocateLong",
				new BooleanCallback() {
					@Override
                    public Boolean get() {
	                    return storePreallocate;
                    }

					@Override
                    public void set(Boolean val) throws InvalidConfigValueException, NodeNeedRestartException {
						storePreallocate = val;
						if (storeType.equals("salt-hash")) {
							((SaltedHashFreenetStore<CHKBlock>) chkDatastore.getStore()).setPreallocate(val);
							((SaltedHashFreenetStore<CHKBlock>) chkDatacache.getStore()).setPreallocate(val);
							((SaltedHashFreenetStore<DSAPublicKey>) pubKeyDatastore.getStore()).setPreallocate(val);
							((SaltedHashFreenetStore<DSAPublicKey>) pubKeyDatacache.getStore()).setPreallocate(val);
							((SaltedHashFreenetStore<SSKBlock>) sskDatastore.getStore()).setPreallocate(val);
							((SaltedHashFreenetStore<SSKBlock>) sskDatacache.getStore()).setPreallocate(val);
						}
                    }}
		);
		storePreallocate = nodeConfig.getBoolean("storePreallocate");

		if(File.separatorChar == '/' && System.getProperty("os.name").toLowerCase().indexOf("mac os") < 0) {
			securityLevels.addPhysicalThreatLevelListener(new SecurityLevelListener<SecurityLevels.PHYSICAL_THREAT_LEVEL>() {

				public void onChange(PHYSICAL_THREAT_LEVEL oldLevel, PHYSICAL_THREAT_LEVEL newLevel) {
					try {
						if(newLevel == PHYSICAL_THREAT_LEVEL.LOW)
							nodeConfig.set("storePreallocate", false);
						else
							nodeConfig.set("storePreallocate", true);
					} catch (NodeNeedRestartException e) {
						// Ignore
					} catch (InvalidConfigValueException e) {
						// Ignore
					}
				}
			});
		}

		securityLevels.addPhysicalThreatLevelListener(new SecurityLevelListener<SecurityLevels.PHYSICAL_THREAT_LEVEL>() {

			public void onChange(PHYSICAL_THREAT_LEVEL oldLevel, PHYSICAL_THREAT_LEVEL newLevel) {
					if(newLevel == PHYSICAL_THREAT_LEVEL.MAXIMUM) {
						synchronized(this) {
							clientCacheAwaitingPassword = false;
							databaseAwaitingPassword = false;
						}
						try {
							killMasterKeysFile();
						} catch (IOException e) {
							masterKeysFile.delete();
							Logger.error(this, "Unable to securely delete "+masterKeysFile);
							System.err.println(NodeL10n.getBase().getString("SecurityLevels.cantDeletePasswordFile", "filename", masterKeysFile.getAbsolutePath()));
							clientCore.alerts.register(new SimpleUserAlert(true, NodeL10n.getBase().getString("SecurityLevels.cantDeletePasswordFileTitle"), NodeL10n.getBase().getString("SecurityLevels.cantDeletePasswordFile"), NodeL10n.getBase().getString("SecurityLevels.cantDeletePasswordFileTitle"), UserAlert.CRITICAL_ERROR));
						}
					}
				}

			});

		if(securityLevels.physicalThreatLevel == PHYSICAL_THREAT_LEVEL.MAXIMUM) {
			try {
				killMasterKeysFile();
			} catch (IOException e) {
				String msg = "Unable to securely delete old master.keys file when switching to MAXIMUM seclevel!!";
				System.err.println(msg);
				throw new NodeInitException(NodeInitException.EXIT_CANT_WRITE_MASTER_KEYS, msg);
			}
		}

		nodeConfig.register("databaseMaxMemory", "20M", sortOrder++, true, false, "Node.databaseMemory", "Node.databaseMemoryLong",
				new LongCallback() {

			@Override
			public Long get() {
				return databaseMaxMemory;
			}

			@Override
			public void set(Long val) throws InvalidConfigValueException {
				if(val < 0)
					throw new InvalidConfigValueException(l10n("mustBePositive"));
				else {
					long maxHeapMemory = Runtime.getRuntime().maxMemory();
					/* There are some JVMs (for example libgcj 4.1.1) whose Runtime.maxMemory() does not work. */
					if(maxHeapMemory < Long.MAX_VALUE && val > (80 * maxHeapMemory / 100))
						throw new InvalidConfigValueException(l10n("storeMaxMemTooHigh"));
				}

				envMutableConfig.setCacheSize(val);
				try{
					storeEnvironment.setMutableConfig(envMutableConfig);
				} catch (DatabaseException e) {
					throw new InvalidConfigValueException(l10n("errorApplyingConfig", "error", e.getLocalizedMessage()));
				}
				databaseMaxMemory = val;
			}

		}, true);

		/* There are some JVMs (for example libgcj 4.1.1) whose Runtime.maxMemory() does not work. */
		long maxHeapMemory = Runtime.getRuntime().maxMemory();
		databaseMaxMemory = nodeConfig.getLong("databaseMaxMemory");
		// see #1202
		if(maxHeapMemory < Long.MAX_VALUE && databaseMaxMemory > (80 * maxHeapMemory / 100)){
			Logger.error(this, "The databaseMemory setting is set too high " + databaseMaxMemory +
					" ... let's assume it's not what the user wants to do and restore the default.");
			databaseMaxMemory = Fields.parseLong(nodeConfig.getOption("databaseMaxMemory").getDefault());
		}

		if (storeType.equals("salt-hash")) {
			initRAMFS();
			initSaltHashFS(suffix, false, null);
		} else if (storeType.equals("bdb-index")) {
			initBDBFS(suffix);
		} else {
			initRAMFS();
		}

		if(databaseAwaitingPassword) createPasswordUserAlert();
		if(notEnoughSpaceForAutoCrypt) createAutoCryptFailedUserAlert();

		netid = new NetworkIDManager(this);

		// Client cache

		// Default is 10MB, in memory only. The wizard will change this.

		nodeConfig.register("clientCacheType", "ram", sortOrder++, true, true, "Node.clientCacheType", "Node.clientCacheTypeLong", new ClientCacheTypeCallback());

		clientCacheType = nodeConfig.getString("clientCacheType");

		nodeConfig.register("clientCacheSize", "10M", sortOrder++, false, true, "Node.clientCacheSize", "Node.clientCacheSizeLong",
				new LongCallback() {

					@Override
					public Long get() {
						return maxTotalClientCacheSize;
					}

					@Override
					public void set(Long storeSize) throws InvalidConfigValueException {
						if((storeSize < 0))
							throw new InvalidConfigValueException(l10n("invalidStoreSize"));
						long newMaxStoreKeys = storeSize / sizePerKey;
						if(newMaxStoreKeys == maxClientCacheKeys) return;
						// Update each datastore
						synchronized(Node.this) {
							maxTotalClientCacheSize = storeSize;
							maxClientCacheKeys = newMaxStoreKeys;
						}
						try {
							chkClientcache.setMaxKeys(maxClientCacheKeys, storeForceBigShrinks);
							pubKeyClientcache.setMaxKeys(maxClientCacheKeys, storeForceBigShrinks);
							sskClientcache.setMaxKeys(maxClientCacheKeys, storeForceBigShrinks);
						} catch (IOException e) {
							// FIXME we need to be able to tell the user.
							Logger.error(this, "Caught "+e+" resizing the clientcache", e);
							System.err.println("Caught "+e+" resizing the clientcache");
							e.printStackTrace();
						} catch (DatabaseException e) {
							Logger.error(this, "Caught "+e+" resizing the clientcache", e);
							System.err.println("Caught "+e+" resizing the clientcache");
							e.printStackTrace();
						}
					}
		}, true);

		maxTotalClientCacheSize = nodeConfig.getLong("clientCacheSize");

		if(maxTotalClientCacheSize < 0) {
			throw new NodeInitException(NodeInitException.EXIT_INVALID_STORE_SIZE, "Invalid client cache size");
		}

		maxClientCacheKeys = maxTotalClientCacheSize / sizePerKey;

		boolean startedClientCache = false;

		boolean shouldWriteConfig = false;

		byte[] databaseKey = null;
		MasterKeys keys = null;

		for(int i=0;i<2 && !startedClientCache; i++) {
		if (clientCacheType.equals("salt-hash")) {

			byte[] clientCacheKey = null;
			try {
				if(securityLevels.physicalThreatLevel == PHYSICAL_THREAT_LEVEL.MAXIMUM) {
					clientCacheKey = new byte[32];
					random.nextBytes(clientCacheKey);
				} else {
					keys = MasterKeys.read(masterKeysFile, random, "");
					clientCacheKey = keys.clientCacheMasterKey;
					if(securityLevels.getPhysicalThreatLevel() == PHYSICAL_THREAT_LEVEL.HIGH) {
						System.err.println("Physical threat level is set to HIGH but no password, resetting to NORMAL - probably timing glitch");
						securityLevels.resetPhysicalThreatLevel(PHYSICAL_THREAT_LEVEL.NORMAL);
						databaseKey = keys.databaseKey;
						shouldWriteConfig = true;
					} else {
						keys.clearAllNotClientCacheKey();
					}
				}
				initSaltHashClientCacheFS(suffix, false, clientCacheKey);
				startedClientCache = true;
			} catch (MasterKeysWrongPasswordException e) {
				System.err.println("Cannot open client-cache, it is passworded");
				setClientCacheAwaitingPassword();
				break;
			} catch (MasterKeysFileSizeException e) {
				System.err.println("Impossible: master keys file "+masterKeysFile+" too " + e.sizeToString() + "! Deleting to enable startup, but you will lose your client cache.");
				masterKeysFile.delete();
			} catch (IOException e) {
				break;
			} finally {
				MasterKeys.clear(clientCacheKey);
			}
		} else if(clientCacheType.equals("none")) {
			initNoClientCacheFS();
			startedClientCache = true;
			break;
		} else { // ram
			initRAMClientCacheFS();
			startedClientCache = true;
			break;
		}
		}
		if(!startedClientCache)
			initRAMClientCacheFS();

		if(db == null && databaseKey != null)  {
			try {
				lateSetupDatabase(databaseKey);
			} catch (MasterKeysWrongPasswordException e2) {
				System.err.println("Impossible: "+e2);
				e2.printStackTrace();
			} catch (MasterKeysFileSizeException e2) {
				System.err.println("Impossible: "+e2);
				e2.printStackTrace();
			} catch (IOException e2) {
				System.err.println("Unable to load database: "+e2);
				e2.printStackTrace();
			}
			keys.clearAll();
		}

		nodeConfig.register("useSlashdotCache", true, sortOrder++, true, false, "Node.useSlashdotCache", "Node.useSlashdotCacheLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				return useSlashdotCache;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException, NodeNeedRestartException {
				useSlashdotCache = val;
			}

		});
		useSlashdotCache = nodeConfig.getBoolean("useSlashdotCache");

		nodeConfig.register("writeLocalToDatastore", false, sortOrder++, true, false, "Node.writeLocalToDatastore", "Node.writeLocalToDatastoreLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				return writeLocalToDatastore;
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException, NodeNeedRestartException {
				writeLocalToDatastore = val;
			}

		});

		writeLocalToDatastore = nodeConfig.getBoolean("writeLocalToDatastore");

		// LOW network *and* physical seclevel = writeLocalToDatastore

		securityLevels.addNetworkThreatLevelListener(new SecurityLevelListener<NETWORK_THREAT_LEVEL>() {

			public void onChange(NETWORK_THREAT_LEVEL oldLevel, NETWORK_THREAT_LEVEL newLevel) {
				if(newLevel == NETWORK_THREAT_LEVEL.LOW && securityLevels.getPhysicalThreatLevel() == PHYSICAL_THREAT_LEVEL.LOW)
					writeLocalToDatastore = true;
				else
					writeLocalToDatastore = false;
			}

		});

		securityLevels.addPhysicalThreatLevelListener(new SecurityLevelListener<PHYSICAL_THREAT_LEVEL>() {

			public void onChange(PHYSICAL_THREAT_LEVEL oldLevel, PHYSICAL_THREAT_LEVEL newLevel) {
				if(newLevel == PHYSICAL_THREAT_LEVEL.LOW && securityLevels.getNetworkThreatLevel() == NETWORK_THREAT_LEVEL.LOW)
					writeLocalToDatastore = true;
				else
					writeLocalToDatastore = false;
			}

		});

		nodeConfig.register("slashdotCacheLifetime", 30*60*1000L, sortOrder++, true, false, "Node.slashdotCacheLifetime", "Node.slashdotCacheLifetimeLong", new LongCallback() {

			@Override
			public Long get() {
				return chkSlashdotcacheStore.getLifetime();
			}

			@Override
			public void set(Long val) throws InvalidConfigValueException, NodeNeedRestartException {
				if(val < 0) throw new InvalidConfigValueException("Must be positive!");
				chkSlashdotcacheStore.setLifetime(val);
				pubKeySlashdotcacheStore.setLifetime(val);
				sskSlashdotcacheStore.setLifetime(val);
			}

		}, false);

		long slashdotCacheLifetime = nodeConfig.getLong("slashdotCacheLifetime");

		nodeConfig.register("slashdotCacheSize", "10M", sortOrder++, false, true, "Node.slashdotCacheSize", "Node.slashdotCacheSizeLong",
				new LongCallback() {

					@Override
					public Long get() {
						return maxSlashdotCacheSize;
					}

					@Override
					public void set(Long storeSize) throws InvalidConfigValueException {
						if((storeSize < 0))
							throw new InvalidConfigValueException(l10n("invalidStoreSize"));
						int newMaxStoreKeys = (int) Math.min(storeSize / sizePerKey, Integer.MAX_VALUE);
						if(newMaxStoreKeys == maxSlashdotCacheKeys) return;
						// Update each datastore
						synchronized(Node.this) {
							maxSlashdotCacheSize = storeSize;
							maxSlashdotCacheKeys = newMaxStoreKeys;
						}
						try {
							chkSlashdotcache.setMaxKeys(maxSlashdotCacheKeys, storeForceBigShrinks);
							pubKeySlashdotcache.setMaxKeys(maxSlashdotCacheKeys, storeForceBigShrinks);
							sskSlashdotcache.setMaxKeys(maxSlashdotCacheKeys, storeForceBigShrinks);
						} catch (IOException e) {
							// FIXME we need to be able to tell the user.
							Logger.error(this, "Caught "+e+" resizing the slashdotcache", e);
							System.err.println("Caught "+e+" resizing the slashdotcache");
							e.printStackTrace();
						} catch (DatabaseException e) {
							Logger.error(this, "Caught "+e+" resizing the slashdotcache", e);
							System.err.println("Caught "+e+" resizing the slashdotcache");
							e.printStackTrace();
						}
					}
		}, true);

		maxSlashdotCacheSize = nodeConfig.getLong("slashdotCacheSize");

		if(maxSlashdotCacheSize < 0) {
			throw new NodeInitException(NodeInitException.EXIT_INVALID_STORE_SIZE, "Invalid client cache size");
		}

		maxSlashdotCacheKeys = (int) Math.min(maxSlashdotCacheSize / sizePerKey, Integer.MAX_VALUE);

		chkSlashdotcache = new CHKStore();
		chkSlashdotcacheStore = new SlashdotStore<CHKBlock>(chkSlashdotcache, maxSlashdotCacheKeys, slashdotCacheLifetime, PURGE_INTERVAL, ticker, this.clientCore.tempBucketFactory);
		pubKeySlashdotcache = new PubkeyStore();
		pubKeySlashdotcacheStore = new SlashdotStore<DSAPublicKey>(pubKeySlashdotcache, maxSlashdotCacheKeys, slashdotCacheLifetime, PURGE_INTERVAL, ticker, this.clientCore.tempBucketFactory);
		getPubKey.setLocalSlashdotcache(pubKeySlashdotcache);
		sskSlashdotcache = new SSKStore(getPubKey);
		sskSlashdotcacheStore = new SlashdotStore<SSKBlock>(sskSlashdotcache, maxSlashdotCacheKeys, slashdotCacheLifetime, PURGE_INTERVAL, ticker, this.clientCore.tempBucketFactory);

		// MAXIMUM seclevel = no slashdot cache.

		securityLevels.addNetworkThreatLevelListener(new SecurityLevelListener<NETWORK_THREAT_LEVEL>() {

			public void onChange(NETWORK_THREAT_LEVEL oldLevel, NETWORK_THREAT_LEVEL newLevel) {
				if(newLevel == NETWORK_THREAT_LEVEL.MAXIMUM)
					useSlashdotCache = false;
				else if(oldLevel == NETWORK_THREAT_LEVEL.MAXIMUM)
					useSlashdotCache = true;
			}

		});

		nodeConfig.register("skipWrapperWarning", false, sortOrder++, true, false, "Node.skipWrapperWarning", "Node.skipWrapperWarningLong", new BooleanCallback() {

			@Override
			public void set(Boolean value) throws InvalidConfigValueException, NodeNeedRestartException {
				skipWrapperWarning = value;
			}

			@Override
			public Boolean get() {
				return skipWrapperWarning;
			}
		});

		skipWrapperWarning = nodeConfig.getBoolean("skipWrapperWarning");

		nodeConfig.register("maxPacketSize", 1280, sortOrder++, true, true, "Node.maxPacketSize", "Node.maxPacketSizeLong", new IntCallback() {

			@Override
			public Integer get() {
				synchronized(Node.this) {
					return maxPacketSize;
				}
			}

			@Override
			public void set(Integer val) throws InvalidConfigValueException,
					NodeNeedRestartException {
				synchronized(Node.this) {
					if(val == maxPacketSize) return;
					if(val < UdpSocketHandler.MIN_MTU) throw new InvalidConfigValueException("Must be over 576");
					if(val > 1492) throw new InvalidConfigValueException("Larger than ethernet frame size unlikely to work!");
					maxPacketSize = val;
				}
				updateMTU();
			}

		}, true);

		maxPacketSize = nodeConfig.getInt("maxPacketSize");
		updateMTU();

		nodeConfig.finishedInitialization();
		if(shouldWriteConfig)
			config.store();
		writeNodeFile();

		// Initialize the plugin manager
		Logger.normal(this, "Initializing Plugin Manager");
		System.out.println("Initializing Plugin Manager");
		pluginManager = new PluginManager(this, lastVersion);

		shutdownHook.addEarlyJob(new NativeThread("Shutdown plugins", NativeThread.HIGH_PRIORITY, true) {
			public void realRun() {
				pluginManager.stop(30*1000); // FIXME make it configurable??
			}
		});

		// FIXME
		// Short timeouts and JVM timeouts with nothing more said than the above have been seen...
		// I don't know why... need a stack dump...
		// For now just give it an extra 2 minutes. If it doesn't start in that time,
		// it's likely (on reports so far) that a restart will fix it.
		// And we have to get a build out because ALL plugins are now failing to load,
		// including the absolutely essential (for most nodes) JSTUN and UPnP.
		WrapperManager.signalStarting(120*1000);

		FetchContext ctx = clientCore.makeClient((short)0, true).getFetchContext();

		ctx.allowSplitfiles = false;
		ctx.dontEnterImplicitArchives = true;
		ctx.maxArchiveRestarts = 0;
		ctx.maxMetadataSize = 256;
		ctx.maxNonSplitfileRetries = 10;
		ctx.maxOutputLength = 4096;
		ctx.maxRecursionLevel = 2;
		ctx.maxTempLength = 4096;

		this.arkFetcherContext = ctx;

		registerNodeToNodeMessageListener(N2N_MESSAGE_TYPE_FPROXY, fproxyN2NMListener);
		registerNodeToNodeMessageListener(Node.N2N_MESSAGE_TYPE_DIFFNODEREF, diffNoderefListener);

		// FIXME this is a hack
		// toadlet server should start after all initialized
		// see NodeClientCore line 437
		if (toadlets.isEnabled()) {
			toadlets.finishStart();
			toadlets.createFproxy();
			toadlets.removeStartupToadlet();
		}

		Logger.normal(this, "Node constructor completed");
		System.out.println("Node constructor completed");
	}

	/**
	** Sets up a program directory using the config value defined by the given
	** parameters.
	*/
	protected ProgramDirectory setupProgramDir(SubConfig installConfig,
	  String cfgKey, String defaultValue, String shortdesc, String longdesc, String moveErrMsg,
	  SubConfig oldConfig) throws NodeInitException {
		ProgramDirectory dir = new ProgramDirectory(moveErrMsg);
		int sortOrder = ProgramDirectory.nextOrder();
		// forceWrite=true because currently it can't be changed on the fly, also for packages
		installConfig.register(cfgKey, defaultValue, sortOrder, true, true, shortdesc, longdesc, dir.getStringCallback());
		String dirName = installConfig.getString(cfgKey);
		if (oldConfig != null) {
			// TODO HACK FIXME remove this after the next few mandatory builds. current build is 1366
			oldConfig.register(cfgKey, "nonexistent", sortOrder, true, false, shortdesc, longdesc, dir.getStringCallback());
			String oldValue = oldConfig.getString(cfgKey);
			if (!oldValue.equals("nonexistent")) {
				System.err.println("migrating node." + cfgKey + " to node.install." + cfgKey + ": " + oldValue);
				dirName = oldValue;
				try {
					installConfig.set(cfgKey, dirName);
				} catch (NodeNeedRestartException e) {
					// Ignore
				} catch (InvalidConfigValueException e) {
					// can't happen since we use the same config settings
				}
			}
			oldConfig.removeOption(cfgKey);
			// end TODO
		}
		try {
			dir.move(dirName);
		} catch (IOException e) {
			throw new NodeInitException(NodeInitException.EXIT_BAD_DIR, "could not set up directory: " + longdesc);
		}
		return dir;
	}

	protected ProgramDirectory setupProgramDir(SubConfig installConfig,
	  String cfgKey, String defaultValue, String shortdesc, String longdesc,
	  SubConfig oldConfig) throws NodeInitException {
		return setupProgramDir(installConfig, cfgKey, defaultValue, shortdesc, longdesc, null, oldConfig);
	}

	public void lateSetupDatabase(byte[] databaseKey) throws MasterKeysWrongPasswordException, MasterKeysFileSizeException, IOException {
		if(db != null) return;
		System.out.println("Starting late database initialisation");
		setupDatabase(databaseKey);
		nodeDBHandle = darknetCrypto.getNodeHandle(db);

		if(db != null) {
			db.commit();
			if(logMINOR) Logger.minor(this, "COMMITTED");
			try {
				if(!clientCore.lateInitDatabase(nodeDBHandle, db))
					failLateInitDatabase();
			} catch (NodeInitException e) {
				failLateInitDatabase();
			}
		}
	}

	private void failLateInitDatabase() {
		System.err.println("Failed late initialisation of database, closing...");
		db.close();
		db = null;
	}

	private boolean databaseEncrypted;

	/**
	 * @param databaseKey The encryption key to the database. Null if the database is not encrypted
	 * @return A new Db4o Configuration object which is fully configured to Fred's desired database settings.
	 */
	private Configuration getNewDatabaseConfiguration(byte[] databaseKey) {
		Configuration dbConfig = Db4o.newConfiguration();
		/* On my db4o test node with lots of downloads, and several days old, com.db4o.internal.freespace.FreeSlotNode
		 * used 73MB out of the 128MB limit (117MB used). This memory was not reclaimed despite constant garbage collection.
		 * This is unacceptable, hence btree freespace. */
		dbConfig.freespace().useBTreeSystem();
		dbConfig.objectClass(freenet.client.async.PersistentCooldownQueueItem.class).objectField("key").indexed(true);
		dbConfig.objectClass(freenet.client.async.PersistentCooldownQueueItem.class).objectField("keyAsBytes").indexed(true);
		dbConfig.objectClass(freenet.client.async.PersistentCooldownQueueItem.class).objectField("time").indexed(true);
		dbConfig.objectClass(freenet.client.async.RegisterMe.class).objectField("core").indexed(true);
		dbConfig.objectClass(freenet.client.async.RegisterMe.class).objectField("priority").indexed(true);
		dbConfig.objectClass(freenet.client.async.PersistentCooldownQueueItem.class).objectField("time").indexed(true);
		dbConfig.objectClass(freenet.client.FECJob.class).objectField("priority").indexed(true);
		dbConfig.objectClass(freenet.client.FECJob.class).objectField("addedTime").indexed(true);
		dbConfig.objectClass(freenet.client.FECJob.class).objectField("queue").indexed(true);
		dbConfig.objectClass(freenet.client.async.InsertCompressor.class).objectField("nodeDBHandle").indexed(true);
		dbConfig.objectClass(freenet.node.fcp.FCPClient.class).objectField("name").indexed(true);
		dbConfig.objectClass(freenet.client.async.DatastoreCheckerItem.class).objectField("prio").indexed(true);
		dbConfig.objectClass(freenet.client.async.DatastoreCheckerItem.class).objectField("getter").indexed(true);
		dbConfig.objectClass(freenet.support.io.PersistentBlobTempBucketTag.class).objectField("index").indexed(true);
		dbConfig.objectClass(freenet.support.io.PersistentBlobTempBucketTag.class).objectField("bucket").indexed(true);
		dbConfig.objectClass(freenet.support.io.PersistentBlobTempBucketTag.class).objectField("factory").indexed(true);
		dbConfig.objectClass(freenet.support.io.PersistentBlobTempBucketTag.class).objectField("isFree").indexed(true);
		dbConfig.objectClass(freenet.client.FetchException.class).cascadeOnDelete(true);
		dbConfig.objectClass(PluginStore.class).cascadeOnDelete(true);
		/*
		 * HashMap: don't enable cascade on update/delete/activate, db4o handles this
		 * internally through the TMap translator.
		 */
		// LAZY appears to cause ClassCastException's relating to db4o objects inside db4o code. :(
		// Also it causes duplicates if we activate immediately.
		// And the performance gain for e.g. RegisterMeRunner isn't that great.
//		dbConfig.queries().evaluationMode(QueryEvaluationMode.LAZY);
		dbConfig.messageLevel(1);
		dbConfig.activationDepth(1);
		/* TURN OFF SHUTDOWN HOOK.
		 * The shutdown hook does auto-commit. We do NOT want auto-commit: if a
		 * transaction hasn't commit()ed, it's not safe to commit it. For example,
		 * a splitfile is started, gets half way through, then we shut down.
		 * The shutdown hook commits the half-finished transaction. When we start
		 * back up, we assume the whole transaction has been committed, and end
		 * up only registering the proportion of segments for which a RegisterMe
		 * has already been created. Yes, this has happened, yes, it sucks.
		 * Add our own hook to rollback and close... */
		dbConfig.automaticShutDown(false);
		/* Block size 8 should have minimal impact since pointers are this
		 * long, and allows databases of up to 16GB.
		 * FIXME make configurable by user. */
		dbConfig.blockSize(8);
		dbConfig.diagnostic().addListener(new DiagnosticListener() {

			public void onDiagnostic(Diagnostic arg0) {
				if(arg0 instanceof ClassHasNoFields)
					return; // Ignore
				if(arg0 instanceof DiagnosticBase) {
					DiagnosticBase d = (DiagnosticBase) arg0;
					Logger.debug(this, "Diagnostic: "+d.getClass()+" : "+d.problem()+" : "+d.solution()+" : "+d.reason(), new Exception("debug"));
				} else
					Logger.debug(this, "Diagnostic: "+arg0+" : "+arg0.getClass(), new Exception("debug"));
			}
		});

		// Make db4o throw an exception if we call store for something for which we do not have to call it, String or Date for example.
		// This prevents us from writing code which is based on misunderstanding of db4o internals...
		dbConfig.exceptionsOnNotStorable(true);

		System.err.println("Optimise native queries: "+dbConfig.optimizeNativeQueries());
		System.err.println("Query activation depth: "+dbConfig.activationDepth());

		// The database is encrypted.
		if(databaseKey != null) {
			IoAdapter baseAdapter = dbConfig.io();
			if(logDEBUG)
				Logger.debug(this, "Encrypting database with "+HexUtil.bytesToHex(databaseKey));
			try {
				dbConfig.io(new EncryptingIoAdapter(baseAdapter, databaseKey, random));
			} catch (GlobalOnlyConfigException e) {
				// Fouled up after encrypting/decrypting.
				System.err.println("Caught "+e+" opening encrypted database.");
				e.printStackTrace();
				WrapperManager.restart();
				throw e;
			}
		}

		return dbConfig;
	}

	private void setupDatabase(byte[] databaseKey) throws MasterKeysWrongPasswordException, MasterKeysFileSizeException, IOException {
		/* FIXME: Backup the database! */

		ObjectContainer database;

		File dbFileBackup = new File(dbFile.getPath()+".tmp");
		File dbFileCryptBackup = new File(dbFileCrypt.getPath()+".tmp");

		if(dbFileBackup.exists() && !dbFile.exists()) {
			if(!dbFileBackup.renameTo(dbFile)) {
				throw new IOException("Database backup file "+dbFileBackup+" exists but cannot be renamed to "+dbFile+". Not loading database, please fix permissions problems!");
			}
		}
		if(dbFileCryptBackup.exists() && !dbFileCrypt.exists()) {
			if(!dbFileCryptBackup.renameTo(dbFileCrypt)) {
				throw new IOException("Database backup file "+dbFileCryptBackup+" exists but cannot be renamed to "+dbFileCrypt+". Not loading database, please fix permissions problems!");
			}
		}

		try {
			if(securityLevels.getPhysicalThreatLevel() == PHYSICAL_THREAT_LEVEL.MAXIMUM) {
				databaseKey = new byte[32];
				random.nextBytes(databaseKey);
				FileUtil.secureDelete(dbFileCrypt, random);
				FileUtil.secureDelete(dbFile, random);
				database = openCryptDatabase(databaseKey);
				synchronized(this) {
					databaseEncrypted = true;
				}
			} else if(dbFile.exists() && securityLevels.getPhysicalThreatLevel() == PHYSICAL_THREAT_LEVEL.LOW) {
				maybeDefragmentDatabase(dbFile, null);
				// Just open it.
				database = Db4o.openFile(getNewDatabaseConfiguration(null), dbFile.toString());
				synchronized(this) {
					databaseEncrypted = false;
				}
			} else if(dbFileCrypt.exists() && securityLevels.getPhysicalThreatLevel() == PHYSICAL_THREAT_LEVEL.LOW && autoChangeDatabaseEncryption && enoughSpaceForAutoChangeEncryption(dbFileCrypt, false)) {
				// Migrate the encrypted file to plaintext, if we have the key
				if(databaseKey == null) {
					// Try with no password
					MasterKeys keys;
					try {
						keys = MasterKeys.read(masterKeysFile, random, "");
					} catch (MasterKeysWrongPasswordException e) {
						// User probably changed it in the config file
						System.err.println("Unable to decrypt the node.db4o. Please enter the correct password and set the physical security level to LOW via the GUI.");
						securityLevels.setThreatLevel(PHYSICAL_THREAT_LEVEL.HIGH);
						throw e;
					}
					databaseKey = keys.databaseKey;
					keys.clearAllNotDatabaseKey();
				}
				System.err.println("Decrypting the old node.db4o.crypt ...");
				IoAdapter baseAdapter = new RandomAccessFileAdapter();
				EncryptingIoAdapter adapter =
					new EncryptingIoAdapter(baseAdapter, databaseKey, random);
				File tempFile = new File(dbFile.getPath()+".tmp");
				tempFile.deleteOnExit();
				FileOutputStream fos = new FileOutputStream(tempFile);
				EncryptingIoAdapter readAdapter =
					(EncryptingIoAdapter) adapter.open(dbFileCrypt.toString(), false, 0, true);
				long length = readAdapter.getLength();
				// Estimate approx 1 byte/sec.
				WrapperManager.signalStarting((int)Math.min(24*60*60*1000, 300*1000+length));
				byte[] buf = new byte[65536];
				long read = 0;
				while(read < length) {
					int bytes = (int) Math.min(buf.length, length - read);
					bytes = readAdapter.read(buf, bytes);
					if(bytes < 0) throw new EOFException();
					read += bytes;
					fos.write(buf, 0, bytes);
				}
				fos.close();
				readAdapter.close();
				if(FileUtil.renameTo(tempFile, dbFile)) {
					dbFileCrypt.delete();
					database = Db4o.openFile(getNewDatabaseConfiguration(null), dbFile.toString());
					System.err.println("Completed decrypting the old node.db4o.crypt.");
					synchronized(this) {
						databaseEncrypted = false;
					}
				} else {
					synchronized(this) {
						notEnoughSpaceIsCrypt = false;
						notEnoughSpaceForAutoCrypt = true;
						notEnoughSpaceRenameFailed = true;
						renameFailedFrom = tempFile;
						renameFailedTo = dbFile;
						databaseEncrypted = true;
					}
					// Still encrypted, but we can still open it.
					database = openCryptDatabase(databaseKey);
				}
			} else if(dbFile.exists() && securityLevels.getPhysicalThreatLevel() != PHYSICAL_THREAT_LEVEL.LOW && autoChangeDatabaseEncryption && enoughSpaceForAutoChangeEncryption(dbFile, true)) {
				// Migrate the unencrypted file to ciphertext.
				// This will always succeed short of I/O errors.
				maybeDefragmentDatabase(dbFile, null);
				if(databaseKey == null) {
					// Try with no password
					MasterKeys keys;
					keys = MasterKeys.read(masterKeysFile, random, "");
					databaseKey = keys.databaseKey;
					keys.clearAllNotDatabaseKey();
				}
				System.err.println("Encrypting the old node.db4o ...");
				IoAdapter baseAdapter = new RandomAccessFileAdapter();
				EncryptingIoAdapter adapter =
					new EncryptingIoAdapter(baseAdapter, databaseKey, random);
				File tempFile = new File(dbFileCrypt.getPath()+".tmp");
				tempFile.delete();
				tempFile.deleteOnExit();
				EncryptingIoAdapter readAdapter =
					(EncryptingIoAdapter) adapter.open(tempFile.getPath(), false, 0, false);
				FileInputStream fis = new FileInputStream(dbFile);
				long length = dbFile.length();
				// Estimate approx 1 byte/sec.
				WrapperManager.signalStarting((int)Math.min(24*60*60*1000, 300*1000+length));
				byte[] buf = new byte[65536];
				long read = 0;
				while(read < length) {
					int bytes = (int) Math.min(buf.length, length - read);
					bytes = fis.read(buf, 0, bytes);
					if(bytes < 0) throw new EOFException();
					read += bytes;
					readAdapter.write(buf, bytes);
				}
				fis.close();
				readAdapter.close();
				if(FileUtil.renameTo(tempFile, dbFileCrypt)) {
					FileUtil.secureDelete(dbFile, random);
					System.err.println("Completed encrypting the old node.db4o.");
					database = openCryptDatabase(databaseKey);
					synchronized(this) {
						databaseEncrypted = true;
					}
				} else {
					synchronized(this) {
						notEnoughSpaceIsCrypt = true;
						notEnoughSpaceForAutoCrypt = true;
						notEnoughSpaceRenameFailed = true;
						renameFailedFrom = tempFile;
						renameFailedTo = dbFileCrypt;
					}
					throw new IOException("Unable to encrypt the old node.db4o.crypt because cannot rename database file");
				}
			} else if((dbFileCrypt.exists() && !dbFile.exists()) ||
					(securityLevels.getPhysicalThreatLevel() == PHYSICAL_THREAT_LEVEL.NORMAL) ||
					(securityLevels.getPhysicalThreatLevel() == PHYSICAL_THREAT_LEVEL.HIGH && databaseKey != null)) {
				// Open encrypted, regardless of seclevel.
				if(databaseKey == null) {
					// Try with no password
					MasterKeys keys;
					keys = MasterKeys.read(masterKeysFile, random, "");
					databaseKey = keys.databaseKey;
					keys.clearAllNotDatabaseKey();
				}
				database = openCryptDatabase(databaseKey);
				synchronized(this) {
					databaseEncrypted = true;
				}
			} else {
				maybeDefragmentDatabase(dbFile, null);
				// Open unencrypted.
				database = Db4o.openFile(getNewDatabaseConfiguration(null), dbFile.toString());
				synchronized(this) {
					databaseEncrypted = false;
				}
			}
		} catch (Db4oException e) {
			database = null;
			System.err.println("Failed to open database: "+e);
			e.printStackTrace();
		}
		// DUMP DATABASE CONTENTS
		if(logDEBUG && database != null) {
		try {
		System.err.println("DUMPING DATABASE CONTENTS:");
		ObjectSet<Object> contents = database.queryByExample(new Object());
		Map<String,Integer> map = new HashMap<String, Integer>();
		Iterator<Object> i = contents.iterator();
		while(i.hasNext()) {
			Object o = i.next();
			String name = o.getClass().getName();
			if((map.get(name)) != null) {
				map.put(name, map.get(name)+1);
			} else {
				map.put(name, 1);
			}
			// Activated to depth 1
			try {
				Logger.minor(this, "DATABASE: "+o.getClass()+":"+o+":"+database.ext().getID(o));
			} catch (Throwable t) {
				Logger.minor(this, "CAUGHT "+t+" FOR CLASS "+o.getClass());
			}
			database.deactivate(o, 1);
		}
		int total = 0;
		for(Map.Entry<String,Integer> entry : map.entrySet()) {
			System.err.println(entry.getKey()+" : "+entry.getValue());
			total += entry.getValue();
		}

		// Now dump the SplitFileInserterSegment's.
		ObjectSet<SplitFileInserterSegment> segments = database.query(SplitFileInserterSegment.class);
		for(SplitFileInserterSegment seg : segments) {
			try {
			database.activate(seg, 1);
			boolean finished = seg.isFinished();
			boolean cancelled = seg.isCancelled(database);
			boolean empty = seg.isEmpty(database);
			boolean encoded = seg.isEncoded();
			boolean started = seg.isStarted();
			System.out.println("Segment "+seg+" finished="+finished+" cancelled="+cancelled+" empty="+empty+" encoded="+encoded+" size="+seg.countDataBlocks()+" data "+seg.countCheckBlocks()+" check started="+started);

			if(!finished && !encoded) {
				System.out.println("Not finished and not encoded: "+seg);
				// Basic checks...
				seg.checkHasDataBlocks(true, database);

			}

			database.deactivate(seg, 1);
			} catch (Throwable t) {
				System.out.println("Caught "+t+" processing segment");
				t.printStackTrace();
			}
		}

		FECQueue.dump(database, RequestStarter.NUMBER_OF_PRIORITY_CLASSES);

		// Some structures e.g. collections are sensitive to the activation depth.
		// If they are activated to depth 1, they are broken, and activating them to
		// depth 2 does NOT un-break them! Hence we need to deactivate (above) and
		// GC here...
		System.gc();
		System.runFinalization();
		System.gc();
		System.runFinalization();
		System.err.println("END DATABASE DUMP: "+total+" objects");
		} catch (Db4oException e) {
			System.err.println("Unable to dump database contents. Treating as corrupt database.");
			e.printStackTrace();
			try {
				database.rollback();
			} catch (Throwable t) {} // ignore, closing
			try {
				database.close();
			} catch (Throwable t) {} // ignore, closing
			database = null;
		} catch (IllegalArgumentException e) {
			// Urrrrgh!
			System.err.println("Unable to dump database contents. Treating as corrupt database.");
			e.printStackTrace();
			try {
				database.rollback();
			} catch (Throwable t) {} // ignore, closing
			try {
				database.close();
			} catch (Throwable t) {} // ignore, closing
			database = null;
		}
		}

		db = database;

	}


	private volatile boolean notEnoughSpaceForAutoCrypt = false;
	private volatile boolean notEnoughSpaceIsCrypt = false;
	private volatile boolean notEnoughSpaceRenameFailed = false;
	private volatile File renameFailedFrom;
	private volatile File renameFailedTo;
	private volatile long notEnoughSpaceMinimumSpace = 0;

	private boolean enoughSpaceForAutoChangeEncryption(File file, boolean isCrypt) {
		long freeSpace = FileUtil.getFreeSpace(file);
		if(freeSpace == -1) {
			return true; // We hope ... FIXME check the error handling ...
		}
		long minSpace = (long)(file.length() * 1.1) + 10*1024*1024;
		if(freeSpace < minSpace) {
			System.err.println(l10n(isCrypt ? "notEnoughSpaceToAutoEncrypt" : "notEnoughSpaceToAutoDecrypt", new String[] { "size", "file" }, new String[] { SizeUtil.formatSize(minSpace), dbFile.getAbsolutePath() }));
			if(this.clientCore != null && this.clientCore.alerts != null)
				createAutoCryptFailedUserAlert();
			else {
				notEnoughSpaceIsCrypt = isCrypt;
				notEnoughSpaceForAutoCrypt = true;
				notEnoughSpaceMinimumSpace = minSpace;
			}
			return false;
		}
		return true;
	}


	private ObjectContainer openCryptDatabase(byte[] databaseKey) throws IOException {
		maybeDefragmentDatabase(dbFileCrypt, databaseKey);

		ObjectContainer database = Db4o.openFile(getNewDatabaseConfiguration(databaseKey), dbFileCrypt.toString());
		synchronized(this) {
			databaseAwaitingPassword = false;
		}
		return database;
	}


	private void maybeDefragmentDatabase(File databaseFile, byte[] databaseKey) throws IOException {

		synchronized(this) {
			if(!defragDatabaseOnStartup) return;
		}

		// Open it first, because defrag will throw if it needs to upgrade the file.

		ObjectContainer database = Db4o.openFile(getNewDatabaseConfiguration(databaseKey), databaseFile.toString());
		while(!database.close());

		if(!databaseFile.exists()) return;
		long length = databaseFile.length();
		// Estimate approx 1 byte/sec.
		WrapperManager.signalStarting((int)Math.min(24*60*60*1000, 300*1000+length));
		System.err.println("Defragmenting persistent downloads database.");

		File backupFile = new File(databaseFile.getPath()+".tmp");
		FileUtil.secureDelete(backupFile, random);

		File tmpFile = new File(databaseFile.getPath()+".map");
		FileUtil.secureDelete(tmpFile, random);



		DefragmentConfig config=new DefragmentConfig(databaseFile.getPath(),backupFile.getPath(),new BTreeIDMapping(tmpFile.getPath()));
		config.storedClassFilter(new AvailableClassFilter());
		config.db4oConfig(getNewDatabaseConfiguration(databaseKey));
		try {
			Defragment.defrag(config);
		} catch (IOException e) {
			if(backupFile.exists()) {
				System.err.println("Defrag failed. Trying to preserve original database file.");
				FileUtil.secureDelete(databaseFile, random);
				if(!backupFile.renameTo(databaseFile)) {
					System.err.println("Unable to rename backup file back to database file! Restarting on the assumption that it didn't get closed...");
					WrapperManager.restart();
					throw e;
				}
			}
		} catch (Db4oException e) {
			if(backupFile.exists()) {
				System.err.println("Defrag failed. Trying to preserve original database file.");
				FileUtil.secureDelete(databaseFile, random);
				if(!backupFile.renameTo(databaseFile)) {
					System.err.println("Unable to rename backup file back to database file! Restarting on the assumption that it didn't get closed...");
					WrapperManager.restart();
					throw e;
				}
			}
		}
		System.err.println("Finalising defragmentation...");
		long oldSize = backupFile.length();
		long newSize = databaseFile.length();

		if(newSize <= 0) {
			System.err.println("DEFRAG PRODUCED AN EMPTY FILE! Trying to restore old database file...");
			databaseFile.delete();
			if(!tmpFile.renameTo(databaseFile)) {
				System.err.println("Unable to restore old database file but it should be in "+tmpFile);
				throw new IOException("Defrag produced an empty file; Unable to restore old database file but it should be in "+tmpFile);
			}
		} else {
			double change = 100.0 * (((double)(oldSize - newSize)) / ((double)oldSize));
			FileUtil.secureDelete(tmpFile, random);
			FileUtil.secureDelete(backupFile, random);
			System.err.println("Defragment completed. "+SizeUtil.formatSize(oldSize)+" ("+oldSize+") -> "+SizeUtil.formatSize(newSize)+" ("+newSize+") ("+(int)change+"% shrink)");
		}

		synchronized(this) {
			if(!defragOnce) return;
			defragDatabaseOnStartup = false;
		}
		// Store after startup
		this.executor.execute(new Runnable() {

			public void run() {
				Node.this.config.store();
			}

		}, "Store config");

	}


	public void killMasterKeysFile() throws IOException {
		MasterKeys.killMasterKeys(masterKeysFile, random);
	}


	private void setClientCacheAwaitingPassword() {
		createPasswordUserAlert();
		synchronized(this) {
			clientCacheAwaitingPassword = true;
		}
	}

	private final UserAlert masterPasswordUserAlert = new UserAlert() {

		final long creationTime = System.currentTimeMillis();

		public String anchor() {
			return "password";
		}

		public String dismissButtonText() {
			return null;
		}

		public long getUpdatedTime() {
			return creationTime;
		}

		public FCPMessage getFCPMessage() {
			return new FeedMessage(getTitle(), getShortText(), getText(), getPriorityClass(), getUpdatedTime());
		}

		public HTMLNode getHTMLText() {
			HTMLNode content = new HTMLNode("div");
			SecurityLevelsToadlet.generatePasswordFormPage(false, clientCore.getToadletContainer(), content, false, false, false, null, null);
			return content;
		}

		public short getPriorityClass() {
			return UserAlert.ERROR;
		}

		public String getShortText() {
			return NodeL10n.getBase().getString("SecurityLevels.enterPassword");
		}

		public String getText() {
			return NodeL10n.getBase().getString("SecurityLevels.enterPassword");
		}

		public String getTitle() {
			return NodeL10n.getBase().getString("SecurityLevels.enterPassword");
		}

		public Object getUserIdentifier() {
			return Node.this;
		}

		public boolean isEventNotification() {
			return false;
		}

		public boolean isValid() {
			synchronized(Node.this) {
				return clientCacheAwaitingPassword || databaseAwaitingPassword;
			}
		}

		public void isValid(boolean validity) {
			// Ignore
		}

		public void onDismiss() {
			// Ignore
		}

		public boolean shouldUnregisterOnDismiss() {
			return false;
		}

		public boolean userCanDismiss() {
			return false;
		}

	};
	private boolean xmlRemoteCodeExec;

	private void createPasswordUserAlert() {
		this.clientCore.alerts.register(masterPasswordUserAlert);
	}

	private void createAutoCryptFailedUserAlert() {
		boolean isCrypt = notEnoughSpaceIsCrypt;
		String title;
		String text;
		if(notEnoughSpaceRenameFailed) {
			title = isCrypt ? l10n("failedToRenameEncryptingTitle") : l10n("failedToRenameDecryptingTitle");
			text = l10n((isCrypt ? "failedToRenameEncrypting" : "failedToRenameDecrypting"), new String[] { "fromfile", "tofile" }, new String[] { renameFailedFrom.getAbsolutePath(), renameFailedTo.getAbsolutePath() } );
		} else {
			title = isCrypt ? l10n("notEnoughSpaceToAutoEncryptTitle") : l10n("notEnoughSpaceToAutoDecryptTitle");
			text = l10n((isCrypt ? "notEnoughSpaceToAutoEncrypt" : "notEnoughSpaceToAutoDecrypt"), new String[] { "size", "file" }, new String[] { SizeUtil.formatSize(notEnoughSpaceMinimumSpace), dbFile.getAbsolutePath() });
		}
		this.clientCore.alerts.register(new SimpleUserAlert(true,
				title,
				text,
				title,
				(!isCrypt) && this.db != null ? UserAlert.WARNING : UserAlert.ERROR));
	}

	private void initRAMClientCacheFS() {
		chkClientcache = new CHKStore();
		new RAMFreenetStore<CHKBlock>(chkClientcache, (int) Math.min(Integer.MAX_VALUE, maxClientCacheKeys));
		pubKeyClientcache = new PubkeyStore();
		new RAMFreenetStore<DSAPublicKey>(pubKeyClientcache, (int) Math.min(Integer.MAX_VALUE, maxClientCacheKeys));
		sskClientcache = new SSKStore(getPubKey);
		new RAMFreenetStore<SSKBlock>(sskClientcache, (int) Math.min(Integer.MAX_VALUE, maxClientCacheKeys));
		envMutableConfig = null;
		this.storeEnvironment = null;
	}

	private void initNoClientCacheFS() {
		chkClientcache = new CHKStore();
		new NullFreenetStore<CHKBlock>(chkClientcache);
		pubKeyClientcache = new PubkeyStore();
		new NullFreenetStore<DSAPublicKey>(pubKeyClientcache);
		sskClientcache = new SSKStore(getPubKey);
		new NullFreenetStore<SSKBlock>(sskClientcache);
		envMutableConfig = null;
		this.storeEnvironment = null;
	}

	private String getStoreSuffix() {
		return "-" + getDarknetPortNumber();
	}

	private void finishInitSaltHashFS(final String suffix, NodeClientCore clientCore) {
		if(clientCore.alerts == null) throw new NullPointerException();
		((SaltedHashFreenetStore<CHKBlock>) chkDatastore.getStore()).setUserAlertManager(clientCore.alerts);
		((SaltedHashFreenetStore<CHKBlock>) chkDatacache.getStore()).setUserAlertManager(clientCore.alerts);
		((SaltedHashFreenetStore<DSAPublicKey>) pubKeyDatastore.getStore()).setUserAlertManager(clientCore.alerts);
		((SaltedHashFreenetStore<DSAPublicKey>) pubKeyDatacache.getStore()).setUserAlertManager(clientCore.alerts);
		((SaltedHashFreenetStore<SSKBlock>) sskDatastore.getStore()).setUserAlertManager(clientCore.alerts);
		((SaltedHashFreenetStore<SSKBlock>) sskDatacache.getStore()).setUserAlertManager(clientCore.alerts);

		if (isBDBStoreExist(suffix)) {
			clientCore.alerts.register(new SimpleUserAlert(true, NodeL10n.getBase().getString("Node.storeSaltHashMigratedShort"),
			        NodeL10n.getBase().getString("Node.storeSaltHashMigratedShort"), NodeL10n.getBase()
			                .getString("Node.storeSaltHashMigratedShort"), UserAlert.MINOR) {

				@Override
				public HTMLNode getHTMLText() {
					HTMLNode div = new HTMLNode("div");
					div.addChild("#", NodeL10n.getBase().getString("Node.storeSaltHashMigrated"));
					HTMLNode ul = div.addChild("ul");

					for (String type : new String[] { "chk", "pubkey", "ssk" })
						for (String storecache : new String[] { "store", "store.keys", "store.lru", "cache",
						        "cache.keys", "cache.lru" }) {
							File f = storeDir.file(type + suffix + "." + storecache);
							if (f.exists())
								ul.addChild("li", f.getAbsolutePath());
						}

					File dbDir = storeDir.file("database" + suffix);
					if (dbDir.exists())
						ul.addChild("li", dbDir.getAbsolutePath());

					return div;
				}

				@Override
				public String getText() {
					StringBuilder sb = new StringBuilder();
					sb.append(NodeL10n.getBase().getString("Node.storeSaltHashMigrated") + " \n");

					for (String type : new String[] { "chk", "pubkey", "ssk" })
						for (String storecache : new String[] { "store", "store.keys", "store.lru", "cache",
						        "cache.keys", "cache.lru" }) {
							File f = storeDir.file(type + suffix + "." + storecache);
					if (f.exists())
								sb.append(" - ");
							sb.append(f.getAbsolutePath());
							sb.append("\n");
					}
					File dbDir = storeDir.file("database" + suffix);
					if (dbDir.exists()) {
						sb.append(" - ");
						sb.append(dbDir.getAbsolutePath());
						sb.append("\n");
					}

					return sb.toString();
				}

				@Override
				public boolean isValid() {
					return isBDBStoreExist(suffix);
				}

				@Override
				public void onDismiss() {
				}

				@Override
				public boolean userCanDismiss() {
					return true;
				}
			});
		}
	}

	private void initRAMFS() {
		chkDatastore = new CHKStore();
		new RAMFreenetStore<CHKBlock>(chkDatastore, (int) Math.min(Integer.MAX_VALUE, maxStoreKeys));
		chkDatacache = new CHKStore();
		new RAMFreenetStore<CHKBlock>(chkDatacache, (int) Math.min(Integer.MAX_VALUE, maxCacheKeys));
		pubKeyDatastore = new PubkeyStore();
		new RAMFreenetStore<DSAPublicKey>(pubKeyDatastore, (int) Math.min(Integer.MAX_VALUE, maxStoreKeys));
		pubKeyDatacache = new PubkeyStore();
		getPubKey.setDataStore(pubKeyDatastore, pubKeyDatacache);
		new RAMFreenetStore<DSAPublicKey>(pubKeyDatacache, (int) Math.min(Integer.MAX_VALUE, maxCacheKeys));
		sskDatastore = new SSKStore(getPubKey);
		new RAMFreenetStore<SSKBlock>(sskDatastore, (int) Math.min(Integer.MAX_VALUE, maxStoreKeys));
		sskDatacache = new SSKStore(getPubKey);
		new RAMFreenetStore<SSKBlock>(sskDatacache, (int) Math.min(Integer.MAX_VALUE, maxCacheKeys));
		envMutableConfig = null;
		this.storeEnvironment = null;
	}

	private boolean isBDBStoreExist(final String suffix) {
		for(String type : new String[] { "chk", "pubkey", "ssk" }) {
			for(String ver : new String[] { "store", "cache" }) {
				for(String ext : new String[] { "", ".keys", ".lru" }) {
					if(storeDir.file(type + suffix + "." + ver + ext).exists()) return true;
				}
			}
		}
		if(storeDir.file("database" + suffix).exists()) return true;
		return false;
    }

	private void initSaltHashFS(final String suffix, boolean dontResizeOnStart, byte[] masterKey) throws NodeInitException {
	    storeEnvironment = null;
		envMutableConfig = null;
		try {
			int bloomSize = storeBloomFilterSize;
			if (bloomSize == -1)
				bloomSize = (int) Math.min(maxTotalDatastoreSize / 2048, Integer.MAX_VALUE);
			int bloomFilterSizeInM = storeBloomFilterCounting ? bloomSize / 6 * 4
			        : bloomSize / 6 * 8;
			// Increase size by 10% to allow some space for removing keys. Even a 2-bit counting filter gets saturated.
			bloomFilterSizeInM *= 1.1;

			final CHKStore chkDatastore = new CHKStore();
			final SaltedHashFreenetStore<CHKBlock> chkDataFS = makeStore(bloomFilterSizeInM, "CHK", true, chkDatastore, dontResizeOnStart, masterKey);
			final CHKStore chkDatacache = new CHKStore();
			final SaltedHashFreenetStore<CHKBlock> chkCacheFS = makeStore(bloomFilterSizeInM, "CHK", false, chkDatacache, dontResizeOnStart, masterKey);
			chkCacheFS.setAltStore(chkDataFS);
			final PubkeyStore pubKeyDatastore = new PubkeyStore();
			final SaltedHashFreenetStore<DSAPublicKey> pubkeyDataFS = makeStore(bloomFilterSizeInM, "PUBKEY", true, pubKeyDatastore, dontResizeOnStart, masterKey);
			final PubkeyStore pubKeyDatacache = new PubkeyStore();
			final SaltedHashFreenetStore<DSAPublicKey> pubkeyCacheFS = makeStore(bloomFilterSizeInM, "PUBKEY", false, pubKeyDatacache, dontResizeOnStart, masterKey);
			pubkeyCacheFS.setAltStore(pubkeyDataFS);
			final SSKStore sskDatastore = new SSKStore(getPubKey);
			final SaltedHashFreenetStore<SSKBlock> sskDataFS = makeStore(bloomFilterSizeInM, "SSK", true, sskDatastore, dontResizeOnStart, masterKey);
			final SSKStore sskDatacache = new SSKStore(getPubKey);
			final SaltedHashFreenetStore<SSKBlock> sskCacheFS = makeStore(bloomFilterSizeInM, "SSK", false, sskDatacache, dontResizeOnStart, masterKey);
			sskCacheFS.setAltStore(sskDataFS);

			boolean delay =
				chkDataFS.start(ticker, false) |
				chkCacheFS.start(ticker, false) |
				pubkeyDataFS.start(ticker, false) |
				pubkeyCacheFS.start(ticker, false) |
				sskDataFS.start(ticker, false) |
				sskCacheFS.start(ticker, false);

			if(delay) {

				System.err.println("Delayed init of datastore");

				initRAMFS();

				final Runnable migrate = new MigrateOldStoreData(false);

				this.getTicker().queueTimedJob(new Runnable() {

					public void run() {
						System.err.println("Starting delayed init of datastore");
						try {
							chkDataFS.start(ticker, true);
							chkCacheFS.start(ticker, true);
							pubkeyDataFS.start(ticker, true);
							pubkeyCacheFS.start(ticker, true);
							sskDataFS.start(ticker, true);
							sskCacheFS.start(ticker, true);
						} catch (IOException e) {
							Logger.error(this, "Failed to start datastore: "+e, e);
							System.err.println("Failed to start datastore: "+e);
							e.printStackTrace();
							return;
						}

						Node.this.chkDatastore = chkDatastore;
						Node.this.chkDatacache = chkDatacache;
						Node.this.pubKeyDatastore = pubKeyDatastore;
						Node.this.pubKeyDatacache = pubKeyDatacache;
						getPubKey.setDataStore(pubKeyDatastore, pubKeyDatacache);
						Node.this.sskDatastore = sskDatastore;
						Node.this.sskDatacache = sskDatacache;

						finishInitSaltHashFS(suffix, clientCore);

						System.err.println("Finishing delayed init of datastore");
						migrate.run();
					}

				}, "Start store", 0, true, false); // Use Ticker to guarantee that this runs *after* constructors have completed.

			} else {

				Node.this.chkDatastore = chkDatastore;
				Node.this.chkDatacache = chkDatacache;
				Node.this.pubKeyDatastore = pubKeyDatastore;
				Node.this.pubKeyDatacache = pubKeyDatacache;
				getPubKey.setDataStore(pubKeyDatastore, pubKeyDatacache);
				Node.this.sskDatastore = sskDatastore;
				Node.this.sskDatacache = sskDatacache;

				this.getTicker().queueTimedJob(new Runnable() {

					public void run() {
						Node.this.chkDatastore = chkDatastore;
						Node.this.chkDatacache = chkDatacache;
						Node.this.pubKeyDatastore = pubKeyDatastore;
						Node.this.pubKeyDatacache = pubKeyDatacache;
						getPubKey.setDataStore(pubKeyDatastore, pubKeyDatacache);
						Node.this.sskDatastore = sskDatastore;
						Node.this.sskDatacache = sskDatacache;

						finishInitSaltHashFS(suffix, clientCore);
					}

				}, "Start store", 0, true, false);
			}

			File migrationFile = storeDir.file("migrated");
			if (!migrationFile.exists()) {
				tryMigrate(chkDataFS, "chk", true, suffix);
				tryMigrate(chkCacheFS, "chk", false, suffix);
				tryMigrate(pubkeyDataFS, "pubkey", true, suffix);
				tryMigrate(pubkeyCacheFS, "pubkey", false, suffix);
				tryMigrate(sskDataFS, "ssk", true, suffix);
				tryMigrate(sskCacheFS, "ssk", false, suffix);
				migrationFile.createNewFile();
			}
		} catch (IOException e) {
			System.err.println("Could not open store: " + e);
			e.printStackTrace();
			throw new NodeInitException(NodeInitException.EXIT_STORE_OTHER, e.getMessage());
		}
    }

	private void initSaltHashClientCacheFS(final String suffix, boolean dontResizeOnStart, byte[] clientCacheMasterKey) throws NodeInitException {
	    storeEnvironment = null;
		envMutableConfig = null;

		try {
			int bloomSize = (int) Math.min(maxTotalClientCacheSize / 2048, Integer.MAX_VALUE);
			int bloomFilterSizeInM = storeBloomFilterCounting ? bloomSize / 6 * 4
			        : (bloomSize) / 6 * 8;

			final CHKStore chkClientcache = new CHKStore();
			final SaltedHashFreenetStore<CHKBlock> chkDataFS = makeClientcache(bloomFilterSizeInM, "CHK", true, chkClientcache, dontResizeOnStart, clientCacheMasterKey);
			final PubkeyStore pubKeyClientcache = new PubkeyStore();
			final SaltedHashFreenetStore<DSAPublicKey> pubkeyDataFS = makeClientcache(bloomFilterSizeInM, "PUBKEY", true, pubKeyClientcache, dontResizeOnStart, clientCacheMasterKey);
			final SSKStore sskClientcache = new SSKStore(getPubKey);
			final SaltedHashFreenetStore<SSKBlock> sskDataFS = makeClientcache(bloomFilterSizeInM, "SSK", true, sskClientcache, dontResizeOnStart, clientCacheMasterKey);

			boolean delay =
				chkDataFS.start(ticker, false) |
				pubkeyDataFS.start(ticker, false) |
				sskDataFS.start(ticker, false);

			if(delay) {

				System.err.println("Delayed init of client-cache");

				initRAMClientCacheFS();

				final Runnable migrate = new MigrateOldStoreData(true);

				getTicker().queueTimedJob(new Runnable() {

					public void run() {
						System.err.println("Starting delayed init of client-cache");
						try {
							chkDataFS.start(ticker, true);
							pubkeyDataFS.start(ticker, true);
							sskDataFS.start(ticker, true);
						} catch (IOException e) {
							Logger.error(this, "Failed to start client-cache: "+e, e);
							System.err.println("Failed to start client-cache: "+e);
							e.printStackTrace();
							return;
						}
						Node.this.chkClientcache = chkClientcache;
						Node.this.pubKeyClientcache = pubKeyClientcache;
						getPubKey.setLocalDataStore(pubKeyClientcache);
						Node.this.sskClientcache = sskClientcache;

						System.err.println("Finishing delayed init of client-cache");
						migrate.run();
					}
				}, "Migrate store", 0, true, false);
			} else {
				Node.this.chkClientcache = chkClientcache;
				Node.this.pubKeyClientcache = pubKeyClientcache;
				getPubKey.setLocalDataStore(pubKeyClientcache);
				Node.this.sskClientcache = sskClientcache;
			}

		} catch (IOException e) {
			System.err.println("Could not open store: " + e);
			e.printStackTrace();
			throw new NodeInitException(NodeInitException.EXIT_STORE_OTHER, e.getMessage());
		}
    }

	private <T extends StorableBlock> void tryMigrate(SaltedHashFreenetStore<T> chkDataFS, String type, boolean isStore, String suffix) {
		String store = isStore ? "store" : "cache";
		chkDataFS.migrationFrom(//
		        storeDir.file(type + suffix + "."+store), //
		        storeDir.file(type + suffix + "."+store+".keys"));
	}

	private <T extends StorableBlock> SaltedHashFreenetStore<T> makeClientcache(int bloomFilterSizeInM, String type, boolean isStore, StoreCallback<T> cb, boolean dontResizeOnStart, byte[] clientCacheMasterKey) throws IOException {
		SaltedHashFreenetStore<T> store = makeStore(bloomFilterSizeInM, type, "clientcache", maxClientCacheKeys, cb, dontResizeOnStart, clientCacheMasterKey);
		return store;
	}

	private <T extends StorableBlock> SaltedHashFreenetStore<T> makeStore(int bloomFilterSizeInM, String type, boolean isStore, StoreCallback<T> cb, boolean dontResizeOnStart, byte[] clientCacheMasterKey) throws IOException {
		String store = isStore ? "store" : "cache";
		long maxKeys = isStore ? maxStoreKeys : maxCacheKeys;
		return makeStore(bloomFilterSizeInM, type, store, maxKeys, cb, dontResizeOnStart, clientCacheMasterKey);
	}

	private <T extends StorableBlock> SaltedHashFreenetStore<T> makeStore(int bloomFilterSizeInM, String type, String store, long maxKeys, StoreCallback<T> cb, boolean lateStart, byte[] clientCacheMasterKey) throws IOException {
		Logger.normal(this, "Initializing "+type+" Data"+store);
		System.out.println("Initializing "+type+" Data"+store+" (" + maxStoreKeys + " keys)");

		SaltedHashFreenetStore<T> fs = SaltedHashFreenetStore.<T>construct(getStoreDir(), type+"-"+store, cb,
		        random, maxKeys, bloomFilterSizeInM, storeBloomFilterCounting, shutdownHook, storePreallocate, storeSaltHashResizeOnStart && !lateStart, lateStart ? ticker : null, clientCacheMasterKey);
		cb.setStore(fs);
		return fs;
	}

	private void initBDBFS(final String suffix) throws NodeInitException {
		// Setup datastores
		final EnvironmentConfig envConfig = BerkeleyDBFreenetStore.getBDBConfig();

		final File dbDir = storeDir.file("database-"+getDarknetPortNumber());
		dbDir.mkdirs();

		final File reconstructFile = new File(dbDir, "reconstruct");

		Environment env = null;
		EnvironmentMutableConfig mutableConfig;

		// This can take some time
		System.out.println("Starting database...");
		try {
			if(reconstructFile.exists()) {
				reconstructFile.delete();
				throw new DatabaseException();
			}
			// Auto-recovery can take a long time
			WrapperManager.signalStarting(60*60*1000);
			env = new Environment(dbDir, envConfig);
			mutableConfig = env.getConfig();
		} catch (final DatabaseException e) {

			// Close the database
			if(env != null) {
				try {
					env.close();
				} catch (final Throwable t) {
					System.err.println("Error closing database: "+t+" after "+e);
					t.printStackTrace();
				}
			}

			// Delete the database logs

			System.err.println("Deleting old database log files...");

			final File[] files = dbDir.listFiles();
			for(int i=0;i<files.length;i++) {
				final String name = files[i].getName().toLowerCase();
				if(name.endsWith(".jdb") || name.equals("je.lck"))
					if(!files[i].delete())
						System.err.println("Failed to delete old database log file "+files[i]);
			}

			System.err.println("Recovering...");
			// The database is broken
			// We will have to recover from scratch
			try {
				env = new Environment(dbDir, envConfig);
				mutableConfig = env.getConfig();
			} catch (final DatabaseException e1) {
				System.err.println("Could not open store: "+e1);
				e1.printStackTrace();
				System.err.println("Previous error was (tried deleting database and retrying): "+e);
				e.printStackTrace();
				throw new NodeInitException(NodeInitException.EXIT_STORE_OTHER, e1.getMessage());
			}
		}
		storeEnvironment = env;
		envMutableConfig = mutableConfig;

		shutdownHook.addLateJob(new NativeThread("Shutdown bdbje database", NativeThread.HIGH_PRIORITY, true) {
			@Override
			public void realRun() {
				try {
					storeEnvironment.close();
					System.err.println("Successfully closed all datastores.");
				} catch (final Throwable t) {
					System.err.println("Caught "+t+" closing environment");
					t.printStackTrace();
				}
			}
		});
		envMutableConfig.setCacheSize(databaseMaxMemory);
		// http://www.oracle.com/technology/products/berkeley-db/faq/je_faq.html#35

		try {
			storeEnvironment.setMutableConfig(envMutableConfig);
		} catch (final DatabaseException e) {
			System.err.println("Could not set the database configuration: "+e);
			e.printStackTrace();
			throw new NodeInitException(NodeInitException.EXIT_STORE_OTHER, e.getMessage());
		}

		try {
			Logger.normal(this, "Initializing CHK Datastore");
			System.out.println("Initializing CHK Datastore ("+maxStoreKeys+" keys)");
			chkDatastore = new CHKStore();
			BerkeleyDBFreenetStore.construct(getStoreDir(), true, suffix, maxStoreKeys, StoreType.CHK,
					storeEnvironment, shutdownHook, reconstructFile, chkDatastore, random);
			Logger.normal(this, "Initializing CHK Datacache");
			System.out.println("Initializing CHK Datacache ("+maxCacheKeys+ ':' +maxCacheKeys+" keys)");
			chkDatacache = new CHKStore();
			BerkeleyDBFreenetStore.construct(getStoreDir(), false, suffix, maxCacheKeys, StoreType.CHK,
					storeEnvironment, shutdownHook, reconstructFile, chkDatacache, random);
			Logger.normal(this, "Initializing pubKey Datastore");
			System.out.println("Initializing pubKey Datastore");
			pubKeyDatastore = new PubkeyStore();
			BerkeleyDBFreenetStore.construct(getStoreDir(), true, suffix, maxStoreKeys, StoreType.PUBKEY,
					storeEnvironment, shutdownHook, reconstructFile, pubKeyDatastore, random);
			Logger.normal(this, "Initializing pubKey Datacache");
			System.out.println("Initializing pubKey Datacache ("+maxCacheKeys+" keys)");
			pubKeyDatacache = new PubkeyStore();
			BerkeleyDBFreenetStore.construct(getStoreDir(), false, suffix, maxCacheKeys, StoreType.PUBKEY,
					storeEnvironment, shutdownHook, reconstructFile, pubKeyDatacache, random);
			getPubKey.setDataStore(pubKeyDatastore, pubKeyDatacache);
			Logger.normal(this, "Initializing SSK Datastore");
			System.out.println("Initializing SSK Datastore");
			sskDatastore = new SSKStore(getPubKey);
			BerkeleyDBFreenetStore.construct(getStoreDir(), true, suffix, maxStoreKeys, StoreType.SSK,
					storeEnvironment, shutdownHook, reconstructFile, sskDatastore, random);
			Logger.normal(this, "Initializing SSK Datacache");
			System.out.println("Initializing SSK Datacache ("+maxCacheKeys+" keys)");
			sskDatacache = new SSKStore(getPubKey);
			BerkeleyDBFreenetStore.construct(getStoreDir(), false, suffix, maxStoreKeys, StoreType.SSK,
					storeEnvironment, shutdownHook, reconstructFile, sskDatacache, random);
		} catch (final FileNotFoundException e1) {
			final String msg = "Could not open datastore: "+e1;
			Logger.error(this, msg, e1);
			System.err.println(msg);
			throw new NodeInitException(NodeInitException.EXIT_STORE_FILE_NOT_FOUND, msg);
		} catch (final IOException e1) {
			final String msg = "Could not open datastore: "+e1;
			Logger.error(this, msg, e1);
			System.err.println(msg);
			e1.printStackTrace();
			throw new NodeInitException(NodeInitException.EXIT_STORE_IOEXCEPTION, msg);
		} catch (final DatabaseException e1) {
			try {
				reconstructFile.createNewFile();
			} catch (final IOException e) {
				System.err.println("Cannot create reconstruct file "+reconstructFile+" : "+e+" - store will not be reconstructed !!!!");
				e.printStackTrace();
			}
			final String msg = "Could not open datastore due to corruption, will attempt to reconstruct on next startup: "+e1;
			Logger.error(this, msg, e1);
			System.err.println(msg);
			e1.printStackTrace();
			throw new NodeInitException(NodeInitException.EXIT_STORE_RECONSTRUCT, msg);
		}
	}

	public void start(boolean noSwaps) throws NodeInitException {

		dispatcher.start(nodeStats); // must be before usm
		dnsr.start();
		peers.start(); // must be before usm
		nodeStats.start();
		uptime.start();
		failureTable.start();

		darknetCrypto.start();
		if(opennet != null)
			opennet.start();
		ps.start(nodeStats);
		ticker.start();
		scheduleVersionTransition();
		usm.start(ticker);

		if(isUsingWrapper()) {
			Logger.normal(this, "Using wrapper correctly: "+nodeStarter);
			System.out.println("Using wrapper correctly: "+nodeStarter);
		} else {
			Logger.error(this, "NOT using wrapper (at least not correctly).  Your freenet-ext.jar <http://downloads.freenetproject.org/alpha/freenet-ext.jar> and/or wrapper.conf <https://emu.freenetproject.org/svn/trunk/apps/installer/installclasspath/config/wrapper.conf> need to be updated.");
			System.out.println("NOT using wrapper (at least not correctly).  Your freenet-ext.jar <http://downloads.freenetproject.org/alpha/freenet-ext.jar> and/or wrapper.conf <https://emu.freenetproject.org/svn/trunk/apps/installer/installclasspath/config/wrapper.conf> need to be updated.");
		}
		Logger.normal(this, "Freenet 0.7.5 Build #"+Version.buildNumber()+" r"+Version.cvsRevision());
		System.out.println("Freenet 0.7.5 Build #"+Version.buildNumber()+" r"+Version.cvsRevision());
		Logger.normal(this, "FNP port is on "+darknetCrypto.getBindTo()+ ':' +getDarknetPortNumber());
		System.out.println("FNP port is on "+darknetCrypto.getBindTo()+ ':' +getDarknetPortNumber());
		// Start services

//		SubConfig pluginManagerConfig = new SubConfig("pluginmanager3", config);
//		pluginManager3 = new freenet.plugin_new.PluginManager(pluginManagerConfig);

		ipDetector.start();

		// Start sending swaps
		lm.start();

		// Node Updater
		try{
			Logger.normal(this, "Starting the node updater");
			nodeUpdater.start();
		}catch (Exception e) {
			e.printStackTrace();
			throw new NodeInitException(NodeInitException.EXIT_COULD_NOT_START_UPDATER, "Could not start Updater: "+e);
		}

		/* TODO: Make sure that this is called BEFORE any instances of HTTPFilter are created.
		 * HTTPFilter uses checkForGCJCharConversionBug() which returns the value of the static
		 * variable jvmHasGCJCharConversionBug - and this is initialized in the following function.
		 * If this is not possible then create a separate function to check for the GCJ bug and
		 * call this function earlier.
		 */
		checkForEvilJVMBugs();

		// TODO: implement a "required" version if needed
		if(!nodeUpdater.isEnabled() && (NodeStarter.RECOMMENDED_EXT_BUILD_NUMBER > NodeStarter.extBuildNumber))
			clientCore.alerts.register(new ExtOldAgeUserAlert());
		else if(NodeStarter.extBuildNumber == -1)
			clientCore.alerts.register(new ExtOldAgeUserAlert());

		if(!NativeThread.HAS_ENOUGH_NICE_LEVELS)
			clientCore.alerts.register(new NotEnoughNiceLevelsUserAlert());

		this.clientCore.start(config);

		startDeadUIDChecker();

		// After everything has been created, write the config file back to disk.
		if(config instanceof FreenetFilePersistentConfig) {
			FreenetFilePersistentConfig cfg = (FreenetFilePersistentConfig) config;
			cfg.finishedInit(this.ticker);
			cfg.setHasNodeStarted();
		}
		config.store();

		// Process any data in the extra peer data directory
		peers.readExtraPeerData();

		Logger.normal(this, "Started node");

		hasStarted = true;
	}

	private void scheduleVersionTransition() {
		long now = System.currentTimeMillis();
		long transition = Version.transitionTime();
		if(now < transition)
			ticker.queueTimedJob(new Runnable() {

				public void run() {
					freenet.support.Logger.OSThread.logPID(this);
					PeerNode[] nodes = peers.myPeers;
					for(int i = 0; i < nodes.length; i++) {
						PeerNode pn = nodes[i];
						pn.updateVersionRoutablity();
					}
				}
			}, transition - now);
	}


	private static boolean jvmHasGCJCharConversionBug=false;

	private void checkForEvilJVMBugs() {
		// Now check whether we are likely to get the EvilJVMBug.
		// If we are running a Sun/Oracle or Blackdown JVM, on Linux, and LD_ASSUME_KERNEL is not set, then we are.

		String jvmVendor = System.getProperty("java.vm.vendor");
		String jvmSpecVendor = System.getProperty("java.specification.vendor","");
		String javaVersion = System.getProperty("java.version");
		String jvmName = System.getProperty("java.vm.name");
		String jvmVersion = System.getProperty("java.vm.version");
		String osName = System.getProperty("os.name");
		String osVersion = System.getProperty("os.version");

		boolean isOpenJDK = false;

		if(jvmName.startsWith("OpenJDK ")) {
			isOpenJDK = true;
			if(javaVersion.startsWith("1.6.0")) {
				String subverString;
				if(jvmVersion.startsWith("14.0-b"))
					subverString = jvmVersion.substring("14.0-b".length());
				else if(jvmVersion.startsWith("1.6.0_0-b"))
					subverString = jvmVersion.substring("1.6.0_0-b".length());
				else
					subverString = null;
				if(subverString != null) {
					int subver;
					try {
						subver = Integer.parseInt(subverString);
					} catch (NumberFormatException e) {
						subver = -1;
					}
				if(subver > -1 && subver < 15) {
					File javaDir = new File(System.getProperty("java.home"));

					// Assume that if the java home dir has been updated since August 11th, we have the fix.

					final Calendar _cal = Calendar.getInstance(TimeZone.getTimeZone("GMT"));
					_cal.set(2009, Calendar.AUGUST, 11, 0, 0, 0);
					if(javaDir.exists() && javaDir.isDirectory() && javaDir.lastModified() > _cal.getTimeInMillis()) {
						System.err.println("Your Java appears to have been updated, we probably do not have the XML bug (http://www.cert.fi/en/reports/2009/vulnerability2009085.html).");
					} else {
						System.err.println("Old version of OpenJDK detected. It is possible that your Java may be vulnerable to a remote code execution vulnerability. Please update your operating system ASAP. We will not disable plugins because we cannot be sure whether there is a problem.");
						System.err.println("See here: http://www.cert.fi/en/reports/2009/vulnerability2009085.html");
						clientCore.alerts.register(new SimpleUserAlert(false, l10n("openJDKMightBeVulnerableXML"), l10n("openJDKMightBeVulnerableXML"), l10n("openJDKMightBeVulnerableXML"), UserAlert.ERROR));
					}

				}
				}
			}
		}

		if(javaVersion.startsWith("1.5.0_")) {
			clientCore.alerts.register(new SimpleUserAlert(false, l10n("java15DeprecatedTitle"), l10n("java15Deprecated"), l10n("java15DeprecatedTitle"), UserAlert.CRITICAL_ERROR));
		}

		if(logMINOR) Logger.minor(this, "JVM vendor: "+jvmVendor+", JVM name: "+jvmName+", JVM version: "+javaVersion+", OS name: "+osName+", OS version: "+osVersion);

		//Add some checks for "Oracle" to futureproof against them renaming from "Sun".
		//Should have no effect because if a user has downloaded a new enough file for Oracle to have changed the name these bugs shouldn't apply.
		//Still, one never knows and this code might be extended to cover future bugs.
		if((!isOpenJDK) && (jvmVendor.startsWith("Sun ") || jvmVendor.startsWith("Oracle ")) || (jvmVendor.startsWith("The FreeBSD Foundation") && (jvmSpecVendor.startsWith("Sun ") || jvmSpecVendor.startsWith("Oracle "))) || (jvmVendor.startsWith("Apple "))) {
			// Sun/Oracle bugs

			// Spurious OOMs
			// http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4855795
			// http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=2138757
			// http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=2138759
			// Fixed in 1.5.0_10 and 1.4.2_13

			boolean is150 = javaVersion.startsWith("1.5.0_");
			boolean is160 = javaVersion.startsWith("1.6.0_");

			if(is150 || is160) {
				String[] split = javaVersion.split("_");
				String secondPart = split[1];
				if(secondPart.indexOf("-") != -1) {
					split = secondPart.split("-");
					secondPart = split[0];
				}
				int subver = Integer.parseInt(secondPart);

				Logger.minor(this, "JVM version: "+javaVersion+" subver: "+subver+" from "+secondPart);

				if(is150 && subver < 20 || is160 && subver < 15)
					xmlRemoteCodeExec = true;
			}

			if(xmlRemoteCodeExec) {
				System.err.println("Please upgrade your Java to 1.6.0 update 15 or 1.5.0 update 20 IMMEDIATELY!");
				System.err.println("Freenet plugins using XML, including the search function, and Freenet client applications such as Thaw which use XML are vulnerable to remote code execution!");

				clientCore.alerts.register(new SimpleUserAlert(false, l10n("sunJVMxmlRemoteCodeExecTitle"), l10n("sunJVMxmlRemoteCodeExec"), l10n("sunJVMxmlRemoteCodeExecTitle"), UserAlert.CRITICAL_ERROR));
			}

		} else if (jvmVendor.startsWith("Apple ") || jvmVendor.startsWith("\"Apple ")) {
			//Note that Sun/Oracle does not produce VMs for the Macintosh operating system, dont ask the user to find one...
		} else {
			if(jvmVendor.startsWith("Free Software Foundation")) {
				try {
					javaVersion = System.getProperty("java.version").split(" ")[0].replaceAll("[.]","");
					int jvmVersionInt = Integer.parseInt(javaVersion);

					if(jvmVersionInt <= 422 && jvmVersionInt >= 100) // make sure that no bogus values cause true
						jvmHasGCJCharConversionBug=true;
				}

				catch(Throwable t) {
					Logger.error(this, "GCJ version check is broken!", t);
				}
			}

			clientCore.alerts.register(new SimpleUserAlert(true, l10n("notUsingSunVMTitle"), l10n("notUsingSunVM", new String[] { "vendor", "name", "version" }, new String[] { jvmVendor, jvmName, javaVersion }), l10n("notUsingSunVMShort"), UserAlert.WARNING));
		}

		if(!isUsingWrapper() && !skipWrapperWarning) {
			clientCore.alerts.register(new SimpleUserAlert(true, l10n("notUsingWrapperTitle"), l10n("notUsingWrapper"), l10n("notUsingWrapperShort"), UserAlert.WARNING));
		}

	}

	public boolean xmlRemoteCodeExecVuln() {
		return xmlRemoteCodeExec;
	}

	public static boolean checkForGCJCharConversionBug() {
		return jvmHasGCJCharConversionBug; // should be initialized on early startup
	}

	private String l10n(String key) {
		return NodeL10n.getBase().getString("Node."+key);
	}

	private String l10n(String key, String pattern, String value) {
		return NodeL10n.getBase().getString("Node."+key, pattern, value);
	}

	private String l10n(String key, String[] pattern, String[] value) {
		return NodeL10n.getBase().getString("Node."+key, pattern, value);
	}

	/**
	 * Export volatile data about the node as a SimpleFieldSet
	 */
	public SimpleFieldSet exportVolatileFieldSet() {
		return nodeStats.exportVolatileFieldSet();
	}

	/**
	 * Do a routed ping of another node on the network by its location.
	 * @param loc2 The location of the other node to ping. It must match
	 * exactly.
	 * @return The number of hops it took to find the node, if it was found.
	 * Otherwise -1.
	 */
	public int routedPing(double loc2, byte[] nodeIdentity) {
		long uid = random.nextLong();
		int initialX = random.nextInt();
		Message m = DMT.createFNPRoutedPing(uid, loc2, maxHTL, initialX, nodeIdentity);
		Logger.normal(this, "Message: "+m);

		dispatcher.handleRouted(m, null);
		// FIXME: might be rejected
		MessageFilter mf1 = MessageFilter.create().setField(DMT.UID, uid).setType(DMT.FNPRoutedPong).setTimeout(5000);
		try {
			//MessageFilter mf2 = MessageFilter.create().setField(DMT.UID, uid).setType(DMT.FNPRoutedRejected).setTimeout(5000);
			// Ignore Rejected - let it be retried on other peers
			m = usm.waitFor(mf1/*.or(mf2)*/, null);
		} catch (DisconnectedException e) {
			Logger.normal(this, "Disconnected in waiting for pong");
			return -1;
		}
		if(m == null) return -1;
		if(m.getSpec() == DMT.FNPRoutedRejected) return -1;
		return m.getInt(DMT.COUNTER) - initialX;
	}

	/**
	 * Look for a block in the datastore, as part of a request.
	 * @param key The key to fetch.
	 * @param uid The UID of the request (for logging only).
	 * @param promoteCache Whether to promote the key if found.
	 * @param canReadClientCache If the request is local, we can read the client cache.
	 * @param canWriteClientCache If the request is local, and the client hasn't turned off
	 * writing to the client cache, we can write to the client cache.
	 * @param canWriteDatastore If the request HTL is too high, including if it is local, we
	 * cannot write to the datastore.
	 * @return A KeyBlock for the key requested or null.
	 */
	private KeyBlock makeRequestLocal(Key key, long uid, boolean canReadClientCache, boolean canWriteClientCache, boolean canWriteDatastore, boolean offersOnly) {
		KeyBlock kb = null;

		if (key instanceof NodeCHK) {
			kb = fetch((NodeCHK) key, false, canReadClientCache, canWriteClientCache, canWriteDatastore, null);
		} else if (key instanceof NodeSSK) {
			NodeSSK sskKey = (NodeSSK) key;
			DSAPublicKey pubKey = sskKey.getPubKey();
			if (pubKey == null) {
				pubKey = getPubKey.getKey(sskKey.getPubKeyHash(), canReadClientCache, offersOnly, null);
				if (logMINOR)
					Logger.minor(this, "Fetched pubkey: " + pubKey);
				try {
					sskKey.setPubKey(pubKey);
				} catch (SSKVerifyException e) {
					Logger.error(this, "Error setting pubkey: " + e, e);
				}
			}
			if (pubKey != null) {
				if (logMINOR)
					Logger.minor(this, "Got pubkey: " + pubKey);
				kb = fetch(sskKey, canReadClientCache, canWriteClientCache, canWriteDatastore, false, null);
			} else {
				if (logMINOR)
					Logger.minor(this, "Not found because no pubkey: " + uid);
			}
		} else
			throw new IllegalStateException("Unknown key type: " + key.getClass());

		if (kb != null) {
			// Probably somebody waiting for it. Trip it.
			if (clientCore != null && clientCore.requestStarters != null) {
				if (kb instanceof CHKBlock) {
					clientCore.requestStarters.chkFetchSchedulerBulk.tripPendingKey(kb);
					clientCore.requestStarters.chkFetchSchedulerRT.tripPendingKey(kb);
				} else {
					clientCore.requestStarters.sskFetchSchedulerBulk.tripPendingKey(kb);
					clientCore.requestStarters.sskFetchSchedulerRT.tripPendingKey(kb);
				}
			}
			failureTable.onFound(kb);
			return kb;
		}

		return null;
	}

	/**
	 * Check the datastore, then if the key is not in the store,
	 * check whether another node is requesting the same key at
	 * the same HTL, and if all else fails, create a new
	 * RequestSender for the key/htl.
	 * @param closestLocation The closest location to the key so far.
	 * @param localOnly If true, only check the datastore.
	 * @return A KeyBlock if the data is in the store, otherwise
	 * a RequestSender, unless the HTL is 0, in which case NULL.
	 * RequestSender.
	 */
	public Object makeRequestSender(Key key, short htl, long uid, RequestTag tag, PeerNode source, boolean localOnly, boolean ignoreStore, boolean offersOnly, boolean canReadClientCache, boolean canWriteClientCache, boolean realTimeFlag) {
		boolean canWriteDatastore = canWriteDatastoreRequest(htl);
		if(logMINOR) Logger.minor(this, "makeRequestSender("+key+ ',' +htl+ ',' +uid+ ',' +source+") on "+getDarknetPortNumber());
		// In store?
		if(!ignoreStore) {
			KeyBlock kb = makeRequestLocal(key, uid, canReadClientCache, canWriteClientCache, canWriteDatastore, offersOnly);
			if (kb != null)
				return kb;
		}
		if(localOnly) return null;
		if(logMINOR) Logger.minor(this, "Not in store locally");

		// Transfer coalescing - match key only as HTL irrelevant
		RequestSender sender = null;
		HashMap<NodeCHK, RequestSender> transferringRequestSenders =
			realTimeFlag ? transferringRequestSendersRT : transferringRequestSendersBulk;
		synchronized(transferringRequestSenders) {
			sender = transferringRequestSenders.get(key);
		}
		if(sender != null) {
			if(logMINOR) Logger.minor(this, "Data already being transferred: "+sender);
			sender.setTransferCoalesced();
			tag.setSender(sender, true);
			return sender;
		}

		// HTL == 0 => Don't search further
		if(htl == 0) {
			if(logMINOR) Logger.minor(this, "No HTL");
			return null;
		}

		sender = new RequestSender(key, null, htl, uid, tag, this, source, offersOnly, canWriteClientCache, canWriteDatastore, realTimeFlag);
		tag.setSender(sender, false);
		sender.start();
		if(logMINOR) Logger.minor(this, "Created new sender: "+sender);
		return sender;
	}

	/** Can we write to the datastore for a given request?
	 * We do not write to the datastore until 2 hops below maximum. This is an average of 4
	 * hops from the originator. Thus, data returned from local requests is never cached,
	 * finally solving The Register's attack, Bloom filter sharing doesn't give away your local
	 * requests and inserts, and *anything starting at high HTL* is not cached, including stuff
	 * from other nodes which hasn't been decremented far enough yet, so it's not ONLY local
	 * requests that don't get cached. */
	boolean canWriteDatastoreRequest(short htl) {
		return htl <= (maxHTL - 2);
	}

	/** Can we write to the datastore for a given insert?
	 * We do not write to the datastore until 3 hops below maximum. This is an average of 5
	 * hops from the originator. Thus, data sent by local inserts is never cached,
	 * finally solving The Register's attack, Bloom filter sharing doesn't give away your local
	 * requests and inserts, and *anything starting at high HTL* is not cached, including stuff
	 * from other nodes which hasn't been decremented far enough yet, so it's not ONLY local
	 * inserts that don't get cached. */
	boolean canWriteDatastoreInsert(short htl) {
		return htl <= (maxHTL - 3);
	}

	/**
	 * Add a transferring RequestSender to our HashMap.
	 */
	public void addTransferringSender(NodeCHK key, RequestSender sender) {
		HashMap<NodeCHK, RequestSender> transferringRequestSenders =
			sender.realTimeFlag ? transferringRequestSendersRT : transferringRequestSendersBulk;
		synchronized(transferringRequestSenders) {
			transferringRequestSenders.put(key, sender);
		}
	}

	void addTransferringRequestHandler(long id) {
		synchronized(transferringRequestHandlers) {
			transferringRequestHandlers.add(id);
		}
	}

	void removeTransferringRequestHandler(long id) {
		synchronized(transferringRequestHandlers) {
			transferringRequestHandlers.remove(id);
		}
	}

	/**
	 * Fetch a block from the datastore.
	 * @param key
	 * @param canReadClientCache
	 * @param canWriteClientCache
	 * @param canWriteDatastore
	 * @param forULPR
	 * @param mustBeMarkedAsPostCachingChanges If true, the key must have the
	 * ENTRY_NEW_BLOCK flag (if saltedhash), indicating that it a) has been added
	 * since the caching changes in 1224 (since we didn't delete the stores), and b)
	 * that it wasn't added due to low network security caching everything, unless we
	 * are currently in low network security mode. Only applies to main store.
	 */
	public KeyBlock fetch(Key key, boolean canReadClientCache, boolean canWriteClientCache, boolean canWriteDatastore, boolean forULPR, BlockMetadata meta) {
		if(key instanceof NodeSSK)
			return fetch((NodeSSK)key, false, canReadClientCache, canWriteClientCache, canWriteDatastore, forULPR, meta);
		else if(key instanceof NodeCHK)
			return fetch((NodeCHK)key, false, canReadClientCache, canWriteClientCache, canWriteDatastore, forULPR, meta);
		else throw new IllegalArgumentException();
	}

	public SSKBlock fetch(NodeSSK key, boolean dontPromote, boolean canReadClientCache, boolean canWriteClientCache, boolean canWriteDatastore, boolean forULPR, BlockMetadata meta) {
		double loc=key.toNormalizedDouble();
		double dist=Location.distance(lm.getLocation(), loc);
		if(canReadClientCache) {
			try {
				SSKBlock block = sskClientcache.fetch(key, dontPromote || !canWriteClientCache, canReadClientCache, forULPR, false, meta);
				if(block != null) {
					nodeStats.avgClientCacheSSKSuccess.report(loc);
					if (dist > nodeStats.furthestClientCacheSSKSuccess)
					nodeStats.furthestClientCacheSSKSuccess=dist;
					if(logDEBUG) Logger.debug(this, "Found key "+key+" in client-cache");
					return block;
				}
			} catch (IOException e) {
				Logger.error(this, "Could not read from client cache: "+e, e);
			}
		}
		if(forULPR || useSlashdotCache || canReadClientCache) {
			try {
				SSKBlock block = sskSlashdotcache.fetch(key, dontPromote, canReadClientCache, forULPR, false, meta);
				if(block != null) {
					nodeStats.avgSlashdotCacheSSKSuccess.report(loc);
					if (dist > nodeStats.furthestSlashdotCacheSSKSuccess)
					nodeStats.furthestSlashdotCacheSSKSuccess=dist;
					if(logDEBUG) Logger.debug(this, "Found key "+key+" in slashdot-cache");
					return block;
				}
			} catch (IOException e) {
				Logger.error(this, "Could not read from slashdot/ULPR cache: "+e, e);
			}
		}
		boolean ignoreOldBlocks = !writeLocalToDatastore;
		if(canReadClientCache) ignoreOldBlocks = false;
		if(logMINOR) dumpStoreHits();
		try {

			nodeStats.avgRequestLocation.report(loc);
			SSKBlock block = sskDatastore.fetch(key, dontPromote || !canWriteDatastore, canReadClientCache, forULPR, ignoreOldBlocks, meta);
			if(block == null) {
				SSKStore store = oldSSK;
				if(store != null)
					block = store.fetch(key, dontPromote || !canWriteDatastore, canReadClientCache, forULPR, ignoreOldBlocks, meta);
			}
			if(block != null) {
			nodeStats.avgStoreSSKSuccess.report(loc);
			if (dist > nodeStats.furthestStoreSSKSuccess)
				nodeStats.furthestStoreSSKSuccess=dist;
				if(logDEBUG) Logger.debug(this, "Found key "+key+" in store");
				return block;
			}
			block=sskDatacache.fetch(key, dontPromote || !canWriteDatastore, canReadClientCache, forULPR, ignoreOldBlocks, meta);
			if(block == null) {
				SSKStore store = oldSSKCache;
				if(store != null)
					block = store.fetch(key, dontPromote || !canWriteDatastore, canReadClientCache, forULPR, ignoreOldBlocks, meta);
			}
			if (block != null) {
			nodeStats.avgCacheSSKSuccess.report(loc);
			if (dist > nodeStats.furthestCacheSSKSuccess)
				nodeStats.furthestCacheSSKSuccess=dist;
			}
			if(logDEBUG) Logger.debug(this, "Found key "+key+" in cache");
			return block;
		} catch (IOException e) {
			Logger.error(this, "Cannot fetch data: "+e, e);
			return null;
		}
	}

	public CHKBlock fetch(NodeCHK key, boolean dontPromote, boolean canReadClientCache, boolean canWriteClientCache, boolean canWriteDatastore, boolean forULPR, BlockMetadata meta) {
		double loc=key.toNormalizedDouble();
		double dist=Location.distance(lm.getLocation(), loc);
		if(canReadClientCache) {
			try {
				CHKBlock block = chkClientcache.fetch(key, dontPromote || !canWriteClientCache, false, meta);
				if(block != null) {
					nodeStats.avgClientCacheCHKSuccess.report(loc);
					if (dist > nodeStats.furthestClientCacheCHKSuccess)
					nodeStats.furthestClientCacheCHKSuccess=dist;
					return block;
				}
			} catch (IOException e) {
				Logger.error(this, "Could not read from client cache: "+e, e);
			}
		}
		if(forULPR || useSlashdotCache || canReadClientCache) {
			try {
				CHKBlock block = chkSlashdotcache.fetch(key, dontPromote, false, meta);
				if(block != null) {
					nodeStats.avgSlashdotCacheCHKSucess.report(loc);
					if (dist > nodeStats.furthestSlashdotCacheCHKSuccess)
					nodeStats.furthestSlashdotCacheCHKSuccess=dist;
					return block;
				}
			} catch (IOException e) {
				Logger.error(this, "Could not read from slashdot/ULPR cache: "+e, e);
			}
		}
		boolean ignoreOldBlocks = !writeLocalToDatastore;
		if(canReadClientCache) ignoreOldBlocks = false;
		if(logMINOR) dumpStoreHits();
		try {
			nodeStats.avgRequestLocation.report(loc);
			CHKBlock block = chkDatastore.fetch(key, dontPromote || !canWriteDatastore, ignoreOldBlocks, meta);
			if(block == null) {
				CHKStore store = oldCHK;
				if(store != null)
					block = store.fetch(key, dontPromote || !canWriteDatastore, ignoreOldBlocks, meta);
			}
			if (block != null) {
				nodeStats.avgStoreCHKSuccess.report(loc);
				if (dist > nodeStats.furthestStoreCHKSuccess)
					nodeStats.furthestStoreCHKSuccess=dist;
				return block;
			}
			block=chkDatacache.fetch(key, dontPromote || !canWriteDatastore, ignoreOldBlocks, meta);
			if(block == null) {
				CHKStore store = oldCHKCache;
				if(store != null)
					block = store.fetch(key, dontPromote || !canWriteDatastore, ignoreOldBlocks, meta);
			}
			if (block != null) {
				nodeStats.avgCacheCHKSuccess.report(loc);
				if (dist > nodeStats.furthestCacheCHKSuccess)
					nodeStats.furthestCacheCHKSuccess=dist;
			}
			return block;
		} catch (IOException e) {
			Logger.error(this, "Cannot fetch data: "+e, e);
			return null;
		}
	}

	public CHKStore getChkDatacache() {
		return chkDatacache;
	}
	public CHKStore getChkDatastore() {
		return chkDatastore;
	}
	public SSKStore getSskDatacache() {
		return sskDatacache;
	}
	public SSKStore getSskDatastore() {
		return sskDatastore;
	}


	/**
	 * This method returns all statistics info for our data store stats table
	 *
	 * @return map that has an entry for each data store instance type and corresponding stats
	 */
	public Map<DataStoreInstanceType, DataStoreStats> getDataStoreStats() {
		Map<DataStoreInstanceType, DataStoreStats> map = new LinkedHashMap<DataStoreInstanceType, DataStoreStats>();

		map.put(new DataStoreInstanceType(CHK, STORE), new StoreCallbackStats(chkDatastore, nodeStats.chkStoreStats()));
		map.put(new DataStoreInstanceType(CHK, CACHE), new StoreCallbackStats(chkDatacache, nodeStats.chkCacheStats()));
		map.put(new DataStoreInstanceType(CHK, SLASHDOT), new StoreCallbackStats(chkSlashdotcache,nodeStats.chkSlashDotCacheStats()));
		map.put(new DataStoreInstanceType(CHK, CLIENT), new StoreCallbackStats(chkClientcache, nodeStats.chkClientCacheStats()));

		map.put(new DataStoreInstanceType(SSK, STORE), new StoreCallbackStats(sskDatastore, nodeStats.sskStoreStats()));
		map.put(new DataStoreInstanceType(SSK, CACHE), new StoreCallbackStats(sskDatacache, nodeStats.sskCacheStats()));
		map.put(new DataStoreInstanceType(SSK, SLASHDOT), new StoreCallbackStats(sskSlashdotcache, nodeStats.sskSlashDotCacheStats()));
		map.put(new DataStoreInstanceType(SSK, CLIENT), new StoreCallbackStats(sskClientcache, nodeStats.sskClientCacheStats()));

		map.put(new DataStoreInstanceType(PUB_KEY, STORE), new StoreCallbackStats(pubKeyDatastore, new NotAvailNodeStoreStats()));
		map.put(new DataStoreInstanceType(PUB_KEY, CACHE), new StoreCallbackStats(pubKeyDatacache, new NotAvailNodeStoreStats()));
		map.put(new DataStoreInstanceType(PUB_KEY, SLASHDOT), new StoreCallbackStats(pubKeySlashdotcache, new NotAvailNodeStoreStats()));
		map.put(new DataStoreInstanceType(PUB_KEY, CLIENT), new StoreCallbackStats(pubKeyClientcache, new NotAvailNodeStoreStats()));

		return map;
	}

	public long getMaxTotalKeys() {
		return maxTotalKeys;
	}

	long timeLastDumpedHits;

	public void dumpStoreHits() {
		long now = System.currentTimeMillis();
		if(now - timeLastDumpedHits > 5000) {
			timeLastDumpedHits = now;
		} else return;
		Logger.minor(this, "Distribution of hits and misses over stores:\n"+
				"CHK Datastore: "+chkDatastore.hits()+ '/' +(chkDatastore.hits()+chkDatastore.misses())+ '/' +chkDatastore.keyCount()+
				"\nCHK Datacache: "+chkDatacache.hits()+ '/' +(chkDatacache.hits()+chkDatacache.misses())+ '/' +chkDatacache.keyCount()+
				"\nSSK Datastore: "+sskDatastore.hits()+ '/' +(sskDatastore.hits()+sskDatastore.misses())+ '/' +sskDatastore.keyCount()+
				"\nSSK Datacache: "+sskDatacache.hits()+ '/' +(sskDatacache.hits()+sskDatacache.misses())+ '/' +sskDatacache.keyCount());
	}

	public void storeShallow(CHKBlock block, boolean canWriteClientCache, boolean canWriteDatastore, boolean forULPR) {
		store(block, false, canWriteClientCache, canWriteDatastore, forULPR);
	}

	/**
	 * Store a datum.
	 * @param block
	 *      a KeyBlock
	 * @param deep If true, insert to the store as well as the cache. Do not set
	 * this to true unless the store results from an insert, and this node is the
	 * closest node to the target; see the description of chkDatastore.
	 */
	public void store(KeyBlock block, boolean deep, boolean canWriteClientCache, boolean canWriteDatastore, boolean forULPR) throws KeyCollisionException {
		if(block instanceof CHKBlock)
			store((CHKBlock)block, deep, canWriteClientCache, canWriteDatastore, forULPR);
		else if(block instanceof SSKBlock)
			store((SSKBlock)block, deep, false, canWriteClientCache, canWriteDatastore, forULPR);
		else throw new IllegalArgumentException("Unknown keytype ");
	}

	private void store(CHKBlock block, boolean deep, boolean canWriteClientCache, boolean canWriteDatastore, boolean forULPR) {
		try {
			double loc = block.getKey().toNormalizedDouble();
			if (canWriteClientCache) {
				chkClientcache.put(block, false);
				nodeStats.avgClientCacheCHKLocation.report(loc);
			}

			if ((forULPR || useSlashdotCache) && !(canWriteDatastore || writeLocalToDatastore)) {
				chkSlashdotcache.put(block, false);
				nodeStats.avgSlashdotCacheCHKLocation.report(loc);
			}
			if (canWriteDatastore || writeLocalToDatastore) {

				if (deep) {
					chkDatastore.put(block, !canWriteDatastore);
					nodeStats.avgStoreCHKLocation.report(loc);

				}
				chkDatacache.put(block, !canWriteDatastore);
				nodeStats.avgCacheCHKLocation.report(loc);
			}
			if (canWriteDatastore || forULPR || useSlashdotCache)
				failureTable.onFound(block);
		} catch (IOException e) {
			Logger.error(this, "Cannot store data: "+e, e);
		} catch (OutOfMemoryError e) {
			OOMHandler.handleOOM(e);
		} catch (Throwable t) {
			System.err.println(t);
			t.printStackTrace();
			Logger.error(this, "Caught "+t+" storing data", t);
		}
		if(clientCore != null && clientCore.requestStarters != null) {
			clientCore.requestStarters.chkFetchSchedulerBulk.tripPendingKey(block);
			clientCore.requestStarters.chkFetchSchedulerRT.tripPendingKey(block);
		}
	}

	/** Store the block if this is a sink. Call for inserts. */
	public void storeInsert(SSKBlock block, boolean deep, boolean overwrite, boolean canWriteClientCache, boolean canWriteDatastore) throws KeyCollisionException {
		store(block, deep, overwrite, canWriteClientCache, canWriteDatastore, false);
	}

	/** Store only to the cache, and not the store. Called by requests,
	 * as only inserts cause data to be added to the store. */
	public void storeShallow(SSKBlock block, boolean canWriteClientCache, boolean canWriteDatastore, boolean fromULPR) throws KeyCollisionException {
		store(block, false, canWriteClientCache, canWriteDatastore, fromULPR);
	}

	public void store(SSKBlock block, boolean deep, boolean overwrite, boolean canWriteClientCache, boolean canWriteDatastore, boolean forULPR) throws KeyCollisionException {
		try {
			// Store the pubkey before storing the data, otherwise we can get a race condition and
			// end up deleting the SSK data.
			double loc = block.getKey().toNormalizedDouble();
			getPubKey.cacheKey((block.getKey()).getPubKeyHash(), (block.getKey()).getPubKey(), deep, canWriteClientCache, canWriteDatastore, forULPR || useSlashdotCache, writeLocalToDatastore);
			if(canWriteClientCache) {
				sskClientcache.put(block, overwrite, false);
				nodeStats.avgClientCacheSSKLocation.report(loc);
			}
			if((forULPR || useSlashdotCache) && !(canWriteDatastore || writeLocalToDatastore)) {
				sskSlashdotcache.put(block, overwrite, false);
				nodeStats.avgSlashdotCacheSSKLocation.report(loc);
			}
			if(canWriteDatastore || writeLocalToDatastore) {
				if(deep) {
					sskDatastore.put(block, overwrite, !canWriteDatastore);
					nodeStats.avgStoreSSKLocation.report(loc);
				}
				sskDatacache.put(block, overwrite, !canWriteDatastore);
				nodeStats.avgCacheSSKLocation.report(loc);
			}
			if(canWriteDatastore || forULPR || useSlashdotCache)
				failureTable.onFound(block);
		} catch (IOException e) {
			Logger.error(this, "Cannot store data: "+e, e);
		} catch (OutOfMemoryError e) {
			OOMHandler.handleOOM(e);
		} catch (KeyCollisionException e) {
			throw e;
		} catch (Throwable t) {
			System.err.println(t);
			t.printStackTrace();
			Logger.error(this, "Caught "+t+" storing data", t);
		}
		if(clientCore != null && clientCore.requestStarters != null) {
			clientCore.requestStarters.sskFetchSchedulerBulk.tripPendingKey(block);
			clientCore.requestStarters.sskFetchSchedulerRT.tripPendingKey(block);
		}
	}

	/**
	 * Remove a sender from the set of currently transferring senders.
	 */
	public void removeTransferringSender(NodeCHK key, RequestSender sender) {
		HashMap<NodeCHK, RequestSender> transferringRequestSenders =
			sender.realTimeFlag ? transferringRequestSendersRT : transferringRequestSendersBulk;
		synchronized(transferringRequestSenders) {
//			RequestSender rs = (RequestSender) transferringRequestSenders.remove(key);
//			if(rs != sender) {
//				Logger.error(this, "Removed "+rs+" should be "+sender+" for "+key+" in removeTransferringSender");
//			}

			// Since there is no request coalescing, we only remove it if it matches,
			// and don't complain if it doesn't.
			if(transferringRequestSenders.get(key) == sender)
				transferringRequestSenders.remove(key);
		}
	}

	final boolean decrementAtMax;
	final boolean decrementAtMin;

	/**
	 * Decrement the HTL according to the policy of the given
	 * NodePeer if it is non-null, or do something else if it is
	 * null.
	 */
	public short decrementHTL(PeerNode source, short htl) {
		if(source != null)
			return source.decrementHTL(htl);
		// Otherwise...
		if(htl >= maxHTL) htl = maxHTL;
		if(htl <= 0) {
			return 0;
		}
		if(htl == maxHTL) {
			if(decrementAtMax || disableProbabilisticHTLs) htl--;
			return htl;
		}
		if(htl == 1) {
			if(decrementAtMin || disableProbabilisticHTLs) htl--;
			return htl;
		}
		return --htl;
	}

	/**
	 * Fetch or create an CHKInsertSender for a given key/htl.
	 * @param key The key to be inserted.
	 * @param htl The current HTL. We can't coalesce inserts across
	 * HTL's.
	 * @param uid The UID of the caller's request chain, or a new
	 * one. This is obviously not used if there is already an
	 * CHKInsertSender running.
	 * @param source The node that sent the InsertRequest, or null
	 * if it originated locally.
	 * @param ignoreLowBackoff
	 * @param preferInsert
	 */
	public CHKInsertSender makeInsertSender(NodeCHK key, short htl, long uid, InsertTag tag, PeerNode source,
			byte[] headers, PartiallyReceivedBlock prb, boolean fromStore, boolean canWriteClientCache, boolean forkOnCacheable, boolean preferInsert, boolean ignoreLowBackoff, boolean realTimeFlag) {
		if(logMINOR) Logger.minor(this, "makeInsertSender("+key+ ',' +htl+ ',' +uid+ ',' +source+",...,"+fromStore);
		CHKInsertSender is = null;
		is = new CHKInsertSender(key, uid, tag, headers, htl, source, this, prb, fromStore, canWriteClientCache, forkOnCacheable, preferInsert, ignoreLowBackoff,realTimeFlag);
		is.start();
		// CHKInsertSender adds itself to insertSenders
		return is;
	}

	/**
	 * Fetch or create an SSKInsertSender for a given key/htl.
	 * @param key The key to be inserted.
	 * @param htl The current HTL. We can't coalesce inserts across
	 * HTL's.
	 * @param uid The UID of the caller's request chain, or a new
	 * one. This is obviously not used if there is already an
	 * SSKInsertSender running.
	 * @param source The node that sent the InsertRequest, or null
	 * if it originated locally.
	 * @param ignoreLowBackoff
	 * @param preferInsert
	 */
	public SSKInsertSender makeInsertSender(SSKBlock block, short htl, long uid, InsertTag tag, PeerNode source,
			boolean fromStore, boolean canWriteClientCache, boolean canWriteDatastore, boolean forkOnCacheable, boolean preferInsert, boolean ignoreLowBackoff, boolean realTimeFlag) {
		NodeSSK key = block.getKey();
		if(key.getPubKey() == null) {
			throw new IllegalArgumentException("No pub key when inserting");
		}

		getPubKey.cacheKey(key.getPubKeyHash(), key.getPubKey(), false, canWriteClientCache, canWriteDatastore, false, writeLocalToDatastore);
		Logger.minor(this, "makeInsertSender("+key+ ',' +htl+ ',' +uid+ ',' +source+",...,"+fromStore);
		SSKInsertSender is = null;
		is = new SSKInsertSender(block, uid, tag, htl, source, this, fromStore, canWriteClientCache, forkOnCacheable, preferInsert, ignoreLowBackoff, realTimeFlag);
		is.start();
		return is;
	}

	public boolean lockUID(long uid, boolean ssk, boolean insert, boolean offerReply, boolean local, boolean realTimeFlag, UIDTag tag) {
		synchronized(runningUIDs) {
			if(runningUIDs.containsKey(uid)) return false; // Already present.
			runningUIDs.put(uid, tag);
		}
		// If these are switched around, we must remember to remove from both.
		if(offerReply) {
			HashMap<Long,OfferReplyTag> map = getOfferTracker(ssk, realTimeFlag);
			innerLock(map, (OfferReplyTag)tag, uid, ssk, insert, offerReply, local);
		} else if(insert) {
			HashMap<Long,InsertTag> map = getInsertTracker(ssk,local, realTimeFlag);
			innerLock(map, (InsertTag)tag, uid, ssk, insert, offerReply, local);
		} else {
			HashMap<Long,RequestTag> map = getRequestTracker(ssk,local, realTimeFlag);
			innerLock(map, (RequestTag)tag, uid, ssk, insert, offerReply, local);
		}
		return true;
	}

	private<T extends UIDTag> void innerLock(HashMap<Long, T> map, T tag, Long uid, boolean ssk, boolean insert, boolean offerReply, boolean local) {
		synchronized(map) {
			if(logMINOR) Logger.minor(this, "Locking "+uid+" ssk="+ssk+" insert="+insert+" offerReply="+offerReply+" local="+local+" size="+map.size(), new Exception("debug"));
			if(map.containsKey(uid)) {
				Logger.error(this, "Already have UID in specific map ("+ssk+","+insert+","+offerReply+","+local+") but not in general map: trying to register "+tag+" but already have "+map.get(uid));
			}
			map.put(uid, tag);
			if(logMINOR) Logger.minor(this, "Locked "+uid+" ssk="+ssk+" insert="+insert+" offerReply="+offerReply+" local="+local+" size="+map.size());
		}
	}

	/** Only used by UIDTag. */
	void unlockUID(UIDTag tag, boolean canFail, boolean noRecord) {
		unlockUID(tag.uid, tag.isSSK(), tag.isInsert(), canFail, tag.isOfferReply(), tag.wasLocal(), tag.realTimeFlag, tag, noRecord);
	}

	protected void unlockUID(long uid, boolean ssk, boolean insert, boolean canFail, boolean offerReply, boolean local, boolean realTimeFlag, UIDTag tag, boolean noRecord) {
		if(!noRecord)
			completed(uid);

		if(offerReply) {
			HashMap<Long,OfferReplyTag> map = getOfferTracker(ssk, realTimeFlag);
			innerUnlock(map, (OfferReplyTag)tag, uid, ssk, insert, offerReply, local, canFail);
		} else if(insert) {
			HashMap<Long,InsertTag> map = getInsertTracker(ssk,local, realTimeFlag);
			innerUnlock(map, (InsertTag)tag, uid, ssk, insert, offerReply, local, canFail);
		} else {
			HashMap<Long,RequestTag> map = getRequestTracker(ssk,local, realTimeFlag);
			innerUnlock(map, (RequestTag)tag, uid, ssk, insert, offerReply, local, canFail);
		}

		synchronized(runningUIDs) {
			UIDTag oldTag = runningUIDs.get(uid);
			if(oldTag == null) {
				if(canFail) return;
				throw new IllegalStateException("Could not unlock "+uid+ "! : ssk="+ssk+" insert="+insert+" canFail="+canFail+" offerReply="+offerReply+" local="+local);
			} else if(tag != oldTag) {
				if(canFail) return;
				Logger.error(this, "Removing "+tag+" for "+uid+" but "+tag+" is registered!");
				return;
			} else {
				runningUIDs.remove(uid);
			}
		}
	}

	private<T extends UIDTag> void innerUnlock(HashMap<Long, T> map, T tag, Long uid, boolean ssk, boolean insert, boolean offerReply, boolean local, boolean canFail) {
		synchronized(map) {
			if(logMINOR) Logger.minor(this, "Unlocking "+uid+" ssk="+ssk+" insert="+insert+" offerReply="+offerReply+" local="+local+" size="+map.size(), new Exception("debug"));
			if(map.get(uid) != tag) {
				if(canFail) {
					if(logMINOR) Logger.minor(this, "Can fail and did fail: removing "+tag+" got "+map.get(uid)+" for "+uid);
				} else {
					Logger.error(this, "Removing "+tag+" for "+uid+" returned "+map.get(uid));
				}
			} else
				map.remove(uid);
			if(logMINOR) Logger.minor(this, "Unlocked "+uid+" ssk="+ssk+" insert="+insert+" offerReply="+offerReply+" local="+local+" size="+map.size());
		}
	}

	public class CountedRequests {
		final int total;
		final int expectedTransfersOut;
		final int expectedTransfersIn;
		private CountedRequests(int count, int out, int in) {
			total = count;
			expectedTransfersOut = out;
			expectedTransfersIn = in;
		}
	}

	public synchronized CountedRequests countRequests(boolean local, boolean ssk, boolean insert, boolean offer, boolean realTimeFlag, int transfersPerInsert, boolean ignoreLocalVsRemote) {
		HashMap<Long, ? extends UIDTag> map = getTracker(local, ssk, insert, offer, realTimeFlag);
		synchronized(map) {
			int count = 0;
			int transfersOut = 0;
			int transfersIn = 0;
			for(Map.Entry<Long, ? extends UIDTag> entry : map.entrySet()) {
				UIDTag tag = entry.getValue();
				count++;
				transfersOut += tag.expectedTransfersOut(ignoreLocalVsRemote, transfersPerInsert);
				transfersIn += tag.expectedTransfersIn(ignoreLocalVsRemote, transfersPerInsert);
				if(logDEBUG) Logger.debug(this, "UID "+entry.getKey()+" : out "+transfersOut+" in "+transfersIn);
			}
			return new CountedRequests(count, transfersOut, transfersIn);
		}
	}

	public CountedRequests countRequests(PeerNode source, boolean requestsToNode, boolean local, boolean ssk, boolean insert, boolean offer, boolean realTimeFlag, int transfersPerInsert, boolean ignoreLocalVsRemote) {
		HashMap<Long, ? extends UIDTag> map = getTracker(local, ssk, insert, offer, realTimeFlag);
		synchronized(map) {
		int count = 0;
		int transfersOut = 0;
		int transfersIn = 0;
		if(!requestsToNode) {
			// If a request is adopted by us as a result of a timeout, it can be in the
			// remote map despite having source == null. However, if a request is in the
			// local map it will always have source == null.
			if(source != null && local) return new CountedRequests(0, 0, 0);
			for(Map.Entry<Long, ? extends UIDTag> entry : map.entrySet()) {
				UIDTag tag = entry.getValue();
				if(tag.getSource() == source) {
					if(logMINOR) Logger.minor(this, "Counting "+tag+" from "+entry.getKey()+" from "+source);
					count++;
					transfersOut += tag.expectedTransfersOut(ignoreLocalVsRemote, transfersPerInsert);
					transfersIn += tag.expectedTransfersIn(ignoreLocalVsRemote, transfersPerInsert);
				} else if(logDEBUG) Logger.debug(this, "Not counting "+entry.getKey());
			}
			return new CountedRequests(count, transfersOut, transfersIn);
		} else {
			// FIXME improve efficiency!
			for(Map.Entry<Long, ? extends UIDTag> entry : map.entrySet()) {
				UIDTag tag = entry.getValue();
				// Ordinary requests can be routed to an offered key.
				// So we *DO NOT* care whether it's an ordinary routed relayed request or a GetOfferedKey, if we are counting outgoing requests.
				if(tag.currentlyFetchingOfferedKeyFrom(source)) {
					if(logMINOR) Logger.minor(this, "Counting "+tag+" to "+entry.getKey());
					transfersOut += tag.expectedTransfersOut(ignoreLocalVsRemote, transfersPerInsert);
					transfersIn += tag.expectedTransfersIn(ignoreLocalVsRemote, transfersPerInsert);
					count++;
				} else if(tag.currentlyRoutingTo(source)) {
					if(logMINOR) Logger.minor(this, "Counting "+tag+" to "+entry.getKey());
					transfersOut += tag.expectedTransfersOut(ignoreLocalVsRemote, transfersPerInsert);
					transfersIn += tag.expectedTransfersIn(ignoreLocalVsRemote, transfersPerInsert);
					count++;
				} else if(logDEBUG) Logger.debug(this, "Not counting "+entry.getKey());
			}
			if(logMINOR) Logger.minor(this, "Counted for "+(local?"local":"remote")+" "+(ssk?"ssk":"chk")+" "+(insert?"insert":"request")+" "+(offer?"offer":"")+" : "+count+" of "+map.size()+" for "+source);
			return new CountedRequests(count, transfersOut, transfersIn);
		}
		}
	}

	void reassignTagToSelf(UIDTag tag) {
		// The tag remains remote, but we flag it as adopted.
		tag.reassignToSelf();
	}

	private synchronized HashMap<Long, ? extends UIDTag> getTracker(boolean local, boolean ssk,
			boolean insert, boolean offer, boolean realTimeFlag) {
		if(offer)
			return getOfferTracker(ssk, realTimeFlag);
		else if(insert)
			return getInsertTracker(ssk, local, realTimeFlag);
		else
			return getRequestTracker(ssk, local, realTimeFlag);
	}


	private HashMap<Long, RequestTag> getRequestTracker(boolean ssk, boolean local, boolean realTimeFlag) {
		if(realTimeFlag) {
			if(ssk) {
				return local ? runningLocalSSKGetUIDsRT : runningSSKGetUIDsRT;
			} else {
				return local ? runningLocalCHKGetUIDsRT : runningCHKGetUIDsRT;
			}
		} else {
			if(ssk) {
				return local ? runningLocalSSKGetUIDsBulk : runningSSKGetUIDsBulk;
			} else {
				return local ? runningLocalCHKGetUIDsBulk : runningCHKGetUIDsBulk;
			}
		}
	}

	private HashMap<Long, InsertTag> getInsertTracker(boolean ssk, boolean local, boolean realTimeFlag) {
		if(realTimeFlag) {
			if(ssk) {
				return local ? runningLocalSSKPutUIDsRT : runningSSKPutUIDsRT;
			} else {
				return local ? runningLocalCHKPutUIDsRT : runningCHKPutUIDsRT;
			}
		} else {
			if(ssk) {
				return local ? runningLocalSSKPutUIDsBulk : runningSSKPutUIDsBulk;
			} else {
				return local ? runningLocalCHKPutUIDsBulk : runningCHKPutUIDsBulk;
			}
		}
	}

	private HashMap<Long, OfferReplyTag> getOfferTracker(boolean ssk, boolean realTimeFlag) {
		if(realTimeFlag)
			return ssk ? runningSSKOfferReplyUIDsRT : runningCHKOfferReplyUIDsRT;
		else
			return ssk ? runningSSKOfferReplyUIDsBulk : runningCHKOfferReplyUIDsBulk;
	}

	// Must include bulk inserts so fairly long.
	static final int TIMEOUT = 16 * 60 * 1000;

	private void startDeadUIDChecker() {
		getTicker().queueTimedJob(deadUIDChecker, TIMEOUT);
	}

	private Runnable deadUIDChecker = new Runnable() {
		public void run() {
			try {
				checkUIDs(runningLocalSSKGetUIDsRT);
				checkUIDs(runningLocalCHKGetUIDsRT);
				checkUIDs(runningLocalSSKPutUIDsRT);
				checkUIDs(runningLocalCHKPutUIDsRT);
				checkUIDs(runningSSKGetUIDsRT);
				checkUIDs(runningCHKGetUIDsRT);
				checkUIDs(runningSSKPutUIDsRT);
				checkUIDs(runningCHKPutUIDsRT);
				checkUIDs(runningSSKOfferReplyUIDsRT);
				checkUIDs(runningCHKOfferReplyUIDsRT);
				checkUIDs(runningLocalSSKGetUIDsBulk);
				checkUIDs(runningLocalCHKGetUIDsBulk);
				checkUIDs(runningLocalSSKPutUIDsBulk);
				checkUIDs(runningLocalCHKPutUIDsBulk);
				checkUIDs(runningSSKGetUIDsBulk);
				checkUIDs(runningCHKGetUIDsBulk);
				checkUIDs(runningSSKPutUIDsBulk);
				checkUIDs(runningCHKPutUIDsBulk);
				checkUIDs(runningSSKOfferReplyUIDsBulk);
				checkUIDs(runningCHKOfferReplyUIDsBulk);
			} finally {
				getTicker().queueTimedJob(this, 60*1000);
			}
		}

		private void checkUIDs(HashMap<Long, ? extends UIDTag> map) {
			Long[] uids;
			UIDTag[] tags;
			synchronized(map) {
				uids = map.keySet().toArray(new Long[map.size()]);
				tags = map.values().toArray(new UIDTag[map.size()]);
			}
			long now = System.currentTimeMillis();
			for(int i=0;i<uids.length;i++) {
				if(now - tags[i].createdTime > TIMEOUT) {
					tags[i].logStillPresent(uids[i]);
					synchronized(map) {
						map.remove(uids[i]);
					}
				}
			}
		}
	};


	/**
	 * @return Some status information.
	 */
	public String getStatus() {
		StringBuilder sb = new StringBuilder();
		if (peers != null)
			sb.append(peers.getStatus());
		else
			sb.append("No peers yet");
		sb.append(getNumTransferringRequestSenders());
		sb.append('\n');
		return sb.toString();
	}

	/**
	 * @return TMCI peer list
	 */
	public String getTMCIPeerList() {
		StringBuilder sb = new StringBuilder();
		if (peers != null)
			sb.append(peers.getTMCIPeerList());
		else
			sb.append("No peers yet");
		return sb.toString();
	}

	public int getNumSSKRequests() {
		int total = 0;
		synchronized(runningSSKGetUIDsBulk) {
			total += runningSSKGetUIDsBulk.size();
		}
		synchronized(runningSSKGetUIDsRT) {
			total += runningSSKGetUIDsRT.size();
		}
		synchronized(runningLocalSSKGetUIDsBulk) {
			total += runningLocalSSKGetUIDsBulk.size();
		}
		synchronized(runningLocalSSKGetUIDsRT) {
			total += runningLocalSSKGetUIDsRT.size();
		}
		return total;
	}

	public int getNumCHKRequests() {
		int total = 0;
		synchronized(runningCHKGetUIDsBulk) {
			total += runningCHKGetUIDsBulk.size();
		}
		synchronized(runningCHKGetUIDsRT) {
			total += runningCHKGetUIDsRT.size();
		}
		synchronized(runningLocalCHKGetUIDsBulk) {
			total += runningLocalCHKGetUIDsBulk.size();
		}
		synchronized(runningLocalCHKGetUIDsRT) {
			total += runningLocalCHKGetUIDsRT.size();
		}
		return total;
	}

	public int getNumSSKInserts() {
		int total = 0;
		synchronized(runningSSKPutUIDsBulk) {
			total += runningSSKPutUIDsBulk.size();
		}
		synchronized(runningSSKPutUIDsRT) {
			total += runningSSKPutUIDsRT.size();
		}
		synchronized(runningLocalSSKPutUIDsBulk) {
			total += runningLocalSSKPutUIDsBulk.size();
		}
		synchronized(runningLocalSSKPutUIDsRT) {
			total += runningLocalSSKPutUIDsRT.size();
		}
		return total;
	}

	public int getNumCHKInserts() {
		int total = 0;
		synchronized(runningCHKPutUIDsBulk) {
			total += runningCHKPutUIDsBulk.size();
		}
		synchronized(runningCHKPutUIDsRT) {
			total += runningCHKPutUIDsRT.size();
		}
		synchronized(runningLocalCHKPutUIDsBulk) {
			total += runningLocalCHKPutUIDsBulk.size();
		}
		synchronized(runningLocalCHKPutUIDsRT) {
			total += runningLocalCHKPutUIDsRT.size();
		}
		return total;
	}

	public int getNumLocalSSKRequests() {
		int total = 0;
		synchronized(runningLocalSSKGetUIDsBulk) {
			total += runningLocalSSKGetUIDsBulk.size();
		}
		synchronized(runningLocalSSKGetUIDsRT) {
			total += runningLocalSSKGetUIDsRT.size();
		}
		return total;
	}

	public int getNumLocalCHKRequests() {
		int total = 0;
		synchronized(runningLocalCHKGetUIDsBulk) {
			total += runningLocalCHKGetUIDsBulk.size();
		}
		synchronized(runningLocalCHKGetUIDsRT) {
			total += runningLocalCHKGetUIDsRT.size();
		}
		return total;
	}

	public int getNumRemoteCHKRequests() {
		int total = 0;
		synchronized(runningCHKGetUIDsBulk) {
			total += runningCHKGetUIDsBulk.size();
		}
		synchronized(runningCHKGetUIDsRT) {
			total += runningCHKGetUIDsRT.size();
		}
		return total;
	}

	public int getNumRemoteSSKRequests() {
		int total = 0;
		synchronized(runningSSKGetUIDsBulk) {
			total += runningSSKGetUIDsBulk.size();
		}
		synchronized(runningSSKGetUIDsRT) {
			total += runningSSKGetUIDsRT.size();
		}
		return total;
	}

	public int getNumRemoteSSKRequests(boolean realTimeFlag) {
		if(realTimeFlag) {
			synchronized(runningSSKGetUIDsRT) {
				return runningSSKGetUIDsRT.size();
			}
		} else {
			synchronized(runningSSKGetUIDsBulk) {
				return runningSSKGetUIDsBulk.size();
			}
		}
	}

	public int getNumLocalCHKInserts() {
		int total = 0;
		synchronized(runningLocalCHKPutUIDsBulk) {
			total += runningLocalCHKPutUIDsBulk.size();
		}
		synchronized(runningLocalCHKPutUIDsRT) {
			total += runningLocalCHKPutUIDsRT.size();
		}
		return total;
	}

	public int getNumLocalSSKInserts() {
		int total = 0;
		synchronized(runningLocalSSKPutUIDsBulk) {
			total += runningLocalSSKPutUIDsBulk.size();
		}
		synchronized(runningLocalSSKPutUIDsRT) {
			total += runningLocalSSKPutUIDsRT.size();
		}
		return total;
	}

	public int getNumRemoteCHKInserts() {
		int total = 0;
		synchronized(runningCHKPutUIDsBulk) {
			total += runningCHKPutUIDsBulk.size();
		}
		synchronized(runningCHKPutUIDsRT) {
			total += runningCHKPutUIDsRT.size();
		}
		return total;
	}

	public int getNumRemoteSSKInserts() {
		int total = 0;
		synchronized(runningSSKPutUIDsRT) {
			total += runningSSKPutUIDsRT.size();
		}
		synchronized(runningSSKPutUIDsBulk) {
			total += runningSSKPutUIDsBulk.size();
		}
		return total;
	}

	public int getNumRemoteCHKRequests(boolean realTimeFlag) {
		return realTimeFlag ? runningCHKGetUIDsRT.size() : runningCHKGetUIDsBulk.size();
	}

	public int getNumLocalSSKInserts(boolean realTimeFlag) {
		return realTimeFlag ? runningLocalSSKPutUIDsRT.size() : runningLocalSSKPutUIDsBulk.size();
	}

	public int getNumLocalCHKInserts(boolean realTimeFlag) {
		return realTimeFlag ? runningLocalCHKPutUIDsRT.size() : runningLocalCHKPutUIDsBulk.size();
	}

	public int getNumLocalCHKRequests(boolean realTimeFlag) {
		return realTimeFlag ? runningLocalCHKGetUIDsRT.size() : runningLocalCHKGetUIDsBulk.size();
	}

	public int getNumLocalSSKRequests(boolean realTimeFlag) {
		return realTimeFlag ? runningLocalSSKGetUIDsRT.size() : runningLocalSSKGetUIDsBulk.size();
	}

	public int getNumRemoteSSKInserts(boolean realTimeFlag) {
		return realTimeFlag ? runningSSKPutUIDsRT.size() : runningSSKPutUIDsBulk.size();
	}

	public int getNumRemoteCHKInserts(boolean realTimeFlag) {
		return realTimeFlag ? runningCHKPutUIDsRT.size() : runningCHKPutUIDsBulk.size();
	}

	public int getNumSSKOfferReplies() {
		int total = 0;
		synchronized(runningSSKOfferReplyUIDsRT) {
			total += runningSSKOfferReplyUIDsRT.size();
		}
		synchronized(runningSSKOfferReplyUIDsBulk) {
			total += runningSSKOfferReplyUIDsBulk.size();
		}
		return total;
	}

	public int getNumCHKOfferReplies() {
		int total = 0;
		synchronized(runningCHKOfferReplyUIDsRT) {
			total += runningCHKOfferReplyUIDsRT.size();
		}
		synchronized(runningCHKOfferReplyUIDsBulk) {
			total += runningCHKOfferReplyUIDsBulk.size();
		}
		return total;
	}

	public int getNumSSKOfferReplies(boolean realTimeFlag) {
		return realTimeFlag ? runningSSKOfferReplyUIDsRT.size() : runningSSKOfferReplyUIDsBulk.size();
	}

	public int getNumCHKOfferReplies(boolean realTimeFlag) {
		return realTimeFlag ? runningCHKOfferReplyUIDsRT.size() : runningCHKOfferReplyUIDsBulk.size();
	}

	public int getNumTransferringRequestSenders() {
		int total = 0;
		synchronized(transferringRequestSendersRT) {
			total += transferringRequestSendersRT.size();
		}
		synchronized(transferringRequestSendersBulk) {
			total += transferringRequestSendersBulk.size();
		}
		return total;
	}

	public int getNumTransferringRequestHandlers() {
		synchronized(transferringRequestHandlers) {
			return transferringRequestHandlers.size();
		}
	}

	/**
	 * @return Data String for freeviz.
	 */
	public String getFreevizOutput() {
		StringBuilder sb = new StringBuilder();

		sb.append("\ntransferring_requests=");
		sb.append(getNumTransferringRequestSenders());

		sb.append('\n');

		if (peers != null)
			sb.append(peers.getFreevizOutput());

		return sb.toString();
	}

	final LRUQueue<Long> recentlyCompletedIDs;

	static final int MAX_RECENTLY_COMPLETED_IDS = 10*1000;
	/** Length of signature parameters R and S */
	static final int SIGNATURE_PARAMETER_LENGTH = 32;

	/**
	 * Has a request completed with this ID recently?
	 */
	public boolean recentlyCompleted(long id) {
		synchronized (recentlyCompletedIDs) {
			return recentlyCompletedIDs.contains(id);
		}
	}

	private ArrayList<Long> completedBuffer = new ArrayList<Long>();

	// Every this many slots, we tell all the PeerMessageQueue's to remove the old Items for the ID's in question.
	// This prevents memory DoS amongst other things.
	static final int COMPLETED_THRESHOLD = 128;

	/**
	 * A request completed (regardless of success).
	 */
	void completed(long id) {
		Long[] list;
		synchronized (recentlyCompletedIDs) {
			recentlyCompletedIDs.push(id);
			while(recentlyCompletedIDs.size() > MAX_RECENTLY_COMPLETED_IDS)
				recentlyCompletedIDs.pop();
			completedBuffer.add(id);
			if(completedBuffer.size() < COMPLETED_THRESHOLD) return;
			list = completedBuffer.toArray(new Long[completedBuffer.size()]);
			completedBuffer.clear();
		}
		for(PeerNode pn : peers.myPeers) {
			if(!pn.isRoutingCompatible()) continue;
			pn.removeUIDsFromMessageQueues(list);
		}
	}

	public ClientKeyBlock fetchKey(ClientKey key, boolean canReadClientCache, boolean canWriteClientCache, boolean canWriteDatastore) throws KeyVerifyException {
		if(key instanceof ClientCHK)
			return fetch((ClientCHK)key, canReadClientCache, canWriteClientCache, canWriteDatastore);
		else if(key instanceof ClientSSK)
			return fetch((ClientSSK)key, canReadClientCache, canWriteClientCache, canWriteDatastore);
		else
			throw new IllegalStateException("Don't know what to do with "+key);
	}

	public ClientKeyBlock fetch(ClientSSK clientSSK, boolean canReadClientCache, boolean canWriteClientCache, boolean canWriteDatastore) throws SSKVerifyException {
		DSAPublicKey key = clientSSK.getPubKey();
		if(key == null) {
			key = getPubKey.getKey(clientSSK.pubKeyHash, canReadClientCache, false, null);
		}
		if(key == null) return null;
		clientSSK.setPublicKey(key);
		SSKBlock block = fetch((NodeSSK)clientSSK.getNodeKey(true), false, canReadClientCache, canWriteClientCache, canWriteDatastore, false, null);
		if(block == null) {
			if(logMINOR)
				Logger.minor(this, "Could not find key for "+clientSSK);
			return null;
		}
		// Move the pubkey to the top of the LRU, and fix it if it
		// was corrupt.
		getPubKey.cacheKey(clientSSK.pubKeyHash, key, false, canWriteClientCache, canWriteDatastore, false, writeLocalToDatastore);
		return ClientSSKBlock.construct(block, clientSSK);
	}

	private ClientKeyBlock fetch(ClientCHK clientCHK, boolean canReadClientCache, boolean canWriteClientCache, boolean canWriteDatastore) throws CHKVerifyException {
		CHKBlock block = fetch(clientCHK.getNodeCHK(), false, canReadClientCache, canWriteClientCache, canWriteDatastore, false, null);
		if(block == null) return null;
		return new ClientCHKBlock(block, clientCHK);
	}

	public void exit(int reason) {
		try {
			this.park();
			System.out.println("Goodbye.");
			System.out.println(reason);
		} finally {
			System.exit(reason);
		}
	}

	public void exit(String reason){
		try {
			this.park();
			System.out.println("Goodbye. from "+this+" ("+reason+ ')');
		} finally {
			System.exit(0);
		}
	}

	/**
	 * Returns true if the node is shutting down.
	 * The packet receiver calls this for every packet, and boolean is atomic, so this method is not synchronized.
	 */
	public boolean isStopping() {
		return isStopping;
	}

	/**
	 * Get the node into a state where it can be stopped safely
	 * May be called twice - once in exit (above) and then again
	 * from the wrapper triggered by calling System.exit(). Beware!
	 */
	public void park() {
		synchronized(this) {
			if(isStopping) return;
			isStopping = true;
		}

		try {
			Message msg = DMT.createFNPDisconnect(false, false, -1, new ShortBuffer(new byte[0]));
			peers.localBroadcast(msg, true, false, peers.ctrDisconn);
		} catch (Throwable t) {
			try {
				// E.g. if we haven't finished startup
				Logger.error(this, "Failed to tell peers we are going down: "+t, t);
			} catch (Throwable t1) {
				// Ignore. We don't want to mess up the exit process!
			}
		}

		config.store();

		// TODO: find a smarter way of doing it not involving any casting
		Yarrow myRandom = (Yarrow) random;
		myRandom.write_seed(myRandom.seedfile, true);
	}

	public NodeUpdateManager getNodeUpdater(){
		return nodeUpdater;
	}

	public DarknetPeerNode[] getDarknetConnections() {
		return peers.getDarknetPeers();
	}

	public boolean addPeerConnection(PeerNode pn) {
		boolean retval = peers.addPeer(pn);
		peers.writePeers();
		return retval;
	}

	public void removePeerConnection(PeerNode pn) {
		peers.disconnect(pn, true, false, false);
	}

	public void onConnectedPeer() {
		if(logMINOR) Logger.minor(this, "onConnectedPeer()");
		ipDetector.onConnectedPeer();
	}

	public int getFNPPort(){
		return this.getDarknetPortNumber();
	}

	public synchronized boolean setNewestPeerLastGoodVersion( int version ) {
		if( version > buildOldAgeUserAlert.lastGoodVersion ) {
			if( buildOldAgeUserAlert.lastGoodVersion == 0 ) {
				clientCore.alerts.register(buildOldAgeUserAlert);
			}
			buildOldAgeUserAlert.lastGoodVersion = version;
			return true;
		}
		return false;
	}

	public synchronized boolean isOudated() {
		return (buildOldAgeUserAlert.lastGoodVersion > 0);
	}

	private Map<Integer, NodeToNodeMessageListener> n2nmListeners = new HashMap<Integer, NodeToNodeMessageListener>();

	public synchronized void registerNodeToNodeMessageListener(int type, NodeToNodeMessageListener listener) {
		n2nmListeners.put(type, listener);
	}

	/**
	 * Handle a received node to node message
	 */
	public void receivedNodeToNodeMessage(Message m, PeerNode src) {
		int type = ((Integer) m.getObject(DMT.NODE_TO_NODE_MESSAGE_TYPE)).intValue();
		ShortBuffer messageData = (ShortBuffer) m.getObject(DMT.NODE_TO_NODE_MESSAGE_DATA);
		receivedNodeToNodeMessage(src, type, messageData, false);
	}

	public void receivedNodeToNodeMessage(PeerNode src, int type, ShortBuffer messageData, boolean partingMessage) {
		boolean fromDarknet = src instanceof DarknetPeerNode;

		NodeToNodeMessageListener listener = null;
		synchronized(this) {
			listener = n2nmListeners.get(type);
		}

		if(listener == null) {
			Logger.error(this, "Unknown n2nm ID: "+type+" - discarding packet length "+messageData.getLength());
			return;
		}

		listener.handleMessage(messageData.getData(), fromDarknet, src, type);
	}

	private NodeToNodeMessageListener diffNoderefListener = new NodeToNodeMessageListener() {

		public void handleMessage(byte[] data, boolean fromDarknet, PeerNode src, int type) {
			Logger.normal(this, "Received differential node reference node to node message from "+src.getPeer());
			SimpleFieldSet fs = null;
			try {
				fs = new SimpleFieldSet(new String(data, "UTF-8"), false, true);
			} catch (IOException e) {
				Logger.error(this, "IOException while parsing node to node message data", e);
				return;
			}
			if(fs.get("n2nType") != null) {
				fs.removeValue("n2nType");
			}
			try {
				src.processDiffNoderef(fs);
			} catch (FSParseException e) {
				Logger.error(this, "FSParseException while parsing node to node message data", e);
				return;
			}
		}

	};

	private NodeToNodeMessageListener fproxyN2NMListener = new NodeToNodeMessageListener() {

		public void handleMessage(byte[] data, boolean fromDarknet, PeerNode src, int type) {
			if(!fromDarknet) {
				Logger.error(this, "Got N2NTM from non-darknet node ?!?!?!: from "+src);
				return;
			}
			DarknetPeerNode darkSource = (DarknetPeerNode) src;
			Logger.normal(this, "Received N2NTM from '"+darkSource.getPeer()+"'");
			SimpleFieldSet fs = null;
			try {
				fs = new SimpleFieldSet(new String(data, "UTF-8"), false, true);
			} catch (UnsupportedEncodingException e) {
				throw new Error("Impossible: JVM doesn't support UTF-8: " + e, e);
			} catch (IOException e) {
				Logger.error(this, "IOException while parsing node to node message data", e);
				return;
			}
			fs.putOverwrite("n2nType", Integer.toString(type));
			fs.putOverwrite("receivedTime", Long.toString(System.currentTimeMillis()));
			fs.putOverwrite("receivedAs", "nodeToNodeMessage");
			int fileNumber = darkSource.writeNewExtraPeerDataFile( fs, EXTRA_PEER_DATA_TYPE_N2NTM);
			if( fileNumber == -1 ) {
				Logger.error( this, "Failed to write N2NTM to extra peer data file for peer "+darkSource.getPeer());
			}
			// Keep track of the fileNumber so we can potentially delete the extra peer data file later, the file is authoritative
			try {
				handleNodeToNodeTextMessageSimpleFieldSet(fs, darkSource, fileNumber);
			} catch (FSParseException e) {
				// Shouldn't happen
				throw new Error(e);
			}
		}

	};

	/**
	 * Handle a node to node text message SimpleFieldSet
	 * @throws FSParseException
	 */
	public void handleNodeToNodeTextMessageSimpleFieldSet(SimpleFieldSet fs, DarknetPeerNode source, int fileNumber) throws FSParseException {
		if(logMINOR)
			Logger.minor(this, "Got node to node message: \n"+fs);
		int overallType = fs.getInt("n2nType");
		fs.removeValue("n2nType");
		if(overallType == Node.N2N_MESSAGE_TYPE_FPROXY) {
			handleFproxyNodeToNodeTextMessageSimpleFieldSet(fs, source, fileNumber);
		} else {
			Logger.error(this, "Received unknown node to node message type '"+overallType+"' from "+source.getPeer());
		}
	}

	private void handleFproxyNodeToNodeTextMessageSimpleFieldSet(SimpleFieldSet fs, DarknetPeerNode source, int fileNumber) throws FSParseException {
		int type = fs.getInt("type");
		if(type == Node.N2N_TEXT_MESSAGE_TYPE_USERALERT) {
			source.handleFproxyN2NTM(fs, fileNumber);
		} else if(type == Node.N2N_TEXT_MESSAGE_TYPE_FILE_OFFER) {
			source.handleFproxyFileOffer(fs, fileNumber);
		} else if(type == Node.N2N_TEXT_MESSAGE_TYPE_FILE_OFFER_ACCEPTED) {
			source.handleFproxyFileOfferAccepted(fs, fileNumber);
		} else if(type == Node.N2N_TEXT_MESSAGE_TYPE_FILE_OFFER_REJECTED) {
			source.handleFproxyFileOfferRejected(fs, fileNumber);
		} else if(type == Node.N2N_TEXT_MESSAGE_TYPE_BOOKMARK) {
			source.handleFproxyBookmarkFeed(fs, fileNumber);
		} else if(type == Node.N2N_TEXT_MESSAGE_TYPE_DOWNLOAD) {
			source.handleFproxyDownloadFeed(fs, fileNumber);
		} else {
			Logger.error(this, "Received unknown fproxy node to node message sub-type '"+type+"' from "+source.getPeer());
		}
	}

	public String getMyName() {
		return myName;
	}

	public MessageCore getUSM() {
		return usm;
	}

	public LocationManager getLocationManager() {
		return lm;
	}

	public int getSwaps() {
		return LocationManager.swaps;
	}

	public int getNoSwaps() {
		return LocationManager.noSwaps;
	}

	public int getStartedSwaps() {
		return LocationManager.startedSwaps;
	}

	public int getSwapsRejectedAlreadyLocked() {
		return LocationManager.swapsRejectedAlreadyLocked;
	}

	public int getSwapsRejectedNowhereToGo() {
		return LocationManager.swapsRejectedNowhereToGo;
	}

	public int getSwapsRejectedRateLimit() {
		return LocationManager.swapsRejectedRateLimit;
	}

	public int getSwapsRejectedRecognizedID() {
		return LocationManager.swapsRejectedRecognizedID;
	}

	public PeerNode[] getPeerNodes() {
		return peers.myPeers;
	}

	public PeerNode[] getConnectedPeers() {
		return peers.connectedPeers;
	}

	/**
	 * Return a peer of the node given its ip and port, name or identity, as a String
	 */
	public PeerNode getPeerNode(String nodeIdentifier) {
		PeerNode[] pn = peers.myPeers;
		for(int i=0;i<pn.length;i++)
		{
			Peer peer = pn[i].getPeer();
			String nodeIpAndPort = "";
			if(peer != null) {
				nodeIpAndPort = peer.toString();
			}
			String identity = pn[i].getIdentityString();
			if(pn[i] instanceof DarknetPeerNode) {
				DarknetPeerNode dpn = (DarknetPeerNode) pn[i];
				String name = dpn.myName;
				if(identity.equals(nodeIdentifier) || nodeIpAndPort.equals(nodeIdentifier) || name.equals(nodeIdentifier)) {
					return pn[i];
				}
			} else {
				if(identity.equals(nodeIdentifier) || nodeIpAndPort.equals(nodeIdentifier)) {
					return pn[i];
				}
			}
		}
		return null;
	}

	public boolean isHasStarted() {
		return hasStarted;
	}

	public void queueRandomReinsert(KeyBlock block) {
		clientCore.queueRandomReinsert(block);
	}

	public String getExtraPeerDataDir() {
		return extraPeerDataDir.getPath();
	}

	public boolean noConnectedPeers() {
		return !peers.anyConnectedPeers();
	}

	public double getLocation() {
		return lm.getLocation();
	}

	public double getLocationChangeSession() {
		return lm.getLocChangeSession();
	}

	public int getAverageOutgoingSwapTime() {
		return lm.getAverageSwapTime();
	}

	public int getSendSwapInterval() {
		return lm.getSendSwapInterval();
	}

	public int getNumberOfRemotePeerLocationsSeenInSwaps() {
		return lm.numberOfRemotePeerLocationsSeenInSwaps;
	}

	public boolean isAdvancedModeEnabled() {
		if(clientCore == null) return false;
		return clientCore.isAdvancedModeEnabled();
	}

	public boolean isFProxyJavascriptEnabled() {
		return clientCore.isFProxyJavascriptEnabled();
	}

	// FIXME convert these kind of threads to Checkpointed's and implement a handler
	// using the PacketSender/Ticker. Would save a few threads.

	public int getNumARKFetchers() {
		PeerNode[] p = peers.myPeers;
		int x = 0;
		for(int i=0;i<p.length;i++) {
			if(p[i].isFetchingARK()) x++;
		}
		return x;
	}

	// FIXME put this somewhere else
	private volatile Object statsSync = new Object();

	/** The total number of bytes of real data i.e.&nbsp;payload sent by the node */
	private long totalPayloadSent;

	public void sentPayload(int len) {
		synchronized(statsSync) {
			totalPayloadSent += len;
		}
	}

	/**
	 * Get the total number of bytes of payload (real data) sent by the node
	 *
	 * @return Total payload sent in bytes
	 */
	public long getTotalPayloadSent() {
		synchronized(statsSync) {
			return totalPayloadSent;
		}
	}

	public void setName(String key) throws InvalidConfigValueException, NodeNeedRestartException {
		 config.get("node").getOption("name").setValue(key);
	}

	public Ticker getTicker() {
		return ticker;
	}

	public int getUnclaimedFIFOSize() {
		return usm.getUnclaimedFIFOSize();
	}

	/**
	 * Connect this node to another node (for purposes of testing)
	 */
	public void connectToSeednode(SeedServerTestPeerNode node) throws OpennetDisabledException, FSParseException, PeerParseException, ReferenceSignatureVerificationException {
		peers.addPeer(node,false,false);
	}
	public void connect(Node node, FRIEND_TRUST trust) throws FSParseException, PeerParseException, ReferenceSignatureVerificationException {
		peers.connect(node.darknetCrypto.exportPublicFieldSet(), darknetCrypto.packetMangler, trust);
	}

	public short maxHTL() {
		return maxHTL;
	}

	public int getDarknetPortNumber() {
		return darknetCrypto.portNumber;
	}

	public int getOutputBandwidthLimit() {
		return outputBandwidthLimit;
	}

	public synchronized int getInputBandwidthLimit() {
		if(inputLimitDefault)
			return outputBandwidthLimit * 4;
		return inputBandwidthLimit;
	}

	public synchronized void setTimeSkewDetectedUserAlert() {
		if(timeSkewDetectedUserAlert == null) {
			timeSkewDetectedUserAlert = new TimeSkewDetectedUserAlert();
			clientCore.alerts.register(timeSkewDetectedUserAlert);
		}
	}

	public File getNodeDir() { return nodeDir.dir(); }
	public File getCfgDir() { return cfgDir.dir(); }
	public File getUserDir() { return userDir.dir(); }
	public File getRunDir() { return runDir.dir(); }
	public File getStoreDir() { return storeDir.dir(); }
	public File getPluginDir() { return pluginDir.dir(); }

	public ProgramDirectory nodeDir() { return nodeDir; }
	public ProgramDirectory cfgDir() { return cfgDir; }
	public ProgramDirectory userDir() { return userDir; }
	public ProgramDirectory runDir() { return runDir; }
	public ProgramDirectory storeDir() { return storeDir; }
	public ProgramDirectory pluginDir() { return pluginDir; }


	public DarknetPeerNode createNewDarknetNode(SimpleFieldSet fs, FRIEND_TRUST trust) throws FSParseException, PeerParseException, ReferenceSignatureVerificationException {
		return new DarknetPeerNode(fs, this, darknetCrypto, peers, false, darknetCrypto.packetMangler, trust);
	}

	public OpennetPeerNode createNewOpennetNode(SimpleFieldSet fs) throws FSParseException, OpennetDisabledException, PeerParseException, ReferenceSignatureVerificationException {
		if(opennet == null) throw new OpennetDisabledException("Opennet is not currently enabled");
		return new OpennetPeerNode(fs, this, opennet.crypto, opennet, peers, false, opennet.crypto.packetMangler);
	}

	public SeedServerTestPeerNode createNewSeedServerTestPeerNode(SimpleFieldSet fs) throws FSParseException, OpennetDisabledException, PeerParseException, ReferenceSignatureVerificationException {
		if(opennet == null) throw new OpennetDisabledException("Opennet is not currently enabled");
		return new SeedServerTestPeerNode(fs, this, opennet.crypto, peers, true, opennet.crypto.packetMangler);
	}

	public OpennetPeerNode addNewOpennetNode(SimpleFieldSet fs, ConnectionType connectionType) throws FSParseException, PeerParseException, ReferenceSignatureVerificationException {
		// FIXME: perhaps this should throw OpennetDisabledExcemption rather than returing false?
		if(opennet == null) return null;
		return opennet.addNewOpennetNode(fs, connectionType, false);
	}

	public byte[] getOpennetIdentity() {
		return opennet.crypto.myIdentity;
	}

	public byte[] getDarknetIdentity() {
		return darknetCrypto.myIdentity;
	}

	public int estimateFullHeadersLengthOneMessage() {
		return darknetCrypto.packetMangler.fullHeadersLengthOneMessage();
	}

	public synchronized boolean isOpennetEnabled() {
		return opennet != null;
	}

	public SimpleFieldSet exportDarknetPublicFieldSet() {
		return darknetCrypto.exportPublicFieldSet();
	}

	public SimpleFieldSet exportOpennetPublicFieldSet() {
		return opennet.crypto.exportPublicFieldSet();
	}

	public SimpleFieldSet exportDarknetPrivateFieldSet() {
		return darknetCrypto.exportPrivateFieldSet();
	}

	public SimpleFieldSet exportOpennetPrivateFieldSet() {
		return opennet.crypto.exportPrivateFieldSet();
	}

	/**
	 * Should the IP detection code only use the IP address override and the bindTo information,
	 * rather than doing a full detection?
	 */
	public synchronized boolean dontDetect() {
		// Only return true if bindTo is set on all ports which are in use
		if(!darknetCrypto.getBindTo().isRealInternetAddress(false, true, false)) return false;
		if(opennet != null) {
			if(opennet.crypto.getBindTo().isRealInternetAddress(false, true, false)) return false;
		}
		return true;
	}

	public int getOpennetFNPPort() {
		if(opennet == null) return -1;
		return opennet.crypto.portNumber;
	}

	public OpennetManager getOpennet() {
		return opennet;
	}

	public synchronized boolean passOpennetRefsThroughDarknet() {
		return passOpennetRefsThroughDarknet;
	}

	/**
	 * Get the set of public ports that need to be forwarded. These are internal
	 * ports, not necessarily external - they may be rewritten by the NAT.
	 * @return A Set of ForwardPort's to be fed to port forward plugins.
	 */
	public Set<ForwardPort> getPublicInterfacePorts() {
		HashSet<ForwardPort> set = new HashSet<ForwardPort>();
		// FIXME IPv6 support
		set.add(new ForwardPort("darknet", false, ForwardPort.PROTOCOL_UDP_IPV4, darknetCrypto.portNumber));
		if(opennet != null) {
			NodeCrypto crypto = opennet.crypto;
			if(crypto != null) {
				set.add(new ForwardPort("opennet", false, ForwardPort.PROTOCOL_UDP_IPV4, crypto.portNumber));
			}
		}
		return set;
	}

	/**
	 * Get the time since the node was started in milliseconds.
	 *
	 * @return Uptime in milliseconds
	 */
	public long getUptime() {
		return System.currentTimeMillis() - usm.getStartedTime();
	}

	public synchronized UdpSocketHandler[] getPacketSocketHandlers() {
		// FIXME better way to get these!
		if(opennet != null) {
			return new UdpSocketHandler[] { darknetCrypto.socket, opennet.crypto.socket };
			// TODO Auto-generated method stub
		} else {
			return new UdpSocketHandler[] { darknetCrypto.socket };
		}
	}

	public int getMaxOpennetPeers() {
		return maxOpennetPeers;
	}

	public void onAddedValidIP() {
		OpennetManager om;
		synchronized(this) {
			om = opennet;
		}
		if(om != null) {
			Announcer announcer = om.announcer;
			if(announcer != null) {
				announcer.maybeSendAnnouncement();
			}
		}
	}

	public boolean isSeednode() {
		return acceptSeedConnections;
	}

	/**
	 * Returns true if the packet receiver should try to decode/process packets that are not from a peer (i.e. from a seed connection)
	 * The packet receiver calls this upon receiving an unrecognized packet.
	 */
	public boolean wantAnonAuth(boolean isOpennet) {
		if(isOpennet)
			return opennet != null && acceptSeedConnections;
		else
			return false;
	}

	// FIXME make this configurable
	// Probably should wait until we have non-opennet anon auth so we can add it to NodeCrypto.
	public boolean wantAnonAuthChangeIP(boolean isOpennet) {
		return !isOpennet;
	}

	public boolean opennetDefinitelyPortForwarded() {
		OpennetManager om;
		synchronized(this) {
			om = this.opennet;
		}
		if(om == null) return false;
		NodeCrypto crypto = om.crypto;
		if(crypto == null) return false;
		return crypto.definitelyPortForwarded();
	}

	public boolean darknetDefinitelyPortForwarded() {
		if(darknetCrypto == null) return false;
		return darknetCrypto.definitelyPortForwarded();
	}

	public boolean hasKey(Key key, boolean canReadClientCache, boolean forULPR) {
		// FIXME optimise!
		if(key instanceof NodeCHK)
			return fetch((NodeCHK)key, true, canReadClientCache, false, false, forULPR, null) != null;
		else
			return fetch((NodeSSK)key, true, canReadClientCache, false, false, forULPR, null) != null;
	}

	public int getTotalRunningUIDs() {
		synchronized(runningUIDs) {
			return runningUIDs.size();
		}
	}

	public void addRunningUIDs(Vector<Long> list) {
		synchronized(runningUIDs) {
			list.addAll(runningUIDs.keySet());
		}
	}

	public int getTotalRunningUIDsAlt() {
		synchronized(runningUIDs) {
			return this.runningCHKGetUIDsRT.size() + this.runningCHKPutUIDsRT.size() + this.runningSSKGetUIDsRT.size() +
			this.runningSSKGetUIDsRT.size() + this.runningSSKOfferReplyUIDsRT.size() + this.runningCHKOfferReplyUIDsRT.size() +
			this.runningCHKGetUIDsBulk.size() + this.runningCHKPutUIDsBulk.size() + this.runningSSKGetUIDsBulk.size() +
			this.runningSSKGetUIDsBulk.size() + this.runningSSKOfferReplyUIDsBulk.size() + this.runningCHKOfferReplyUIDsBulk.size();
		}
	}

	/**
	 * Warning: does not announce change in location!
	 */
	public void setLocation(double loc) {
		lm.setLocation(loc);
	}

	public boolean peersWantKey(Key key) {
		return failureTable.peersWantKey(key, null);
	}

	private SimpleUserAlert alertMTUTooSmall;

	public final RequestClient nonPersistentClientBulk = new RequestClient() {
		public boolean persistent() {
			return false;
		}
		public void removeFrom(ObjectContainer container) {
			throw new UnsupportedOperationException();
		}
		public boolean realTimeFlag() {
			return false;
		}
	};
	public final RequestClient nonPersistentClientRT = new RequestClient() {
		public boolean persistent() {
			return false;
		}
		public void removeFrom(ObjectContainer container) {
			throw new UnsupportedOperationException();
		}
		public boolean realTimeFlag() {
			return true;
		}
	};

	public void onTooLowMTU(int minAdvertisedMTU, int minAcceptableMTU) {
		if(alertMTUTooSmall == null) {
			alertMTUTooSmall = new SimpleUserAlert(false, l10n("tooSmallMTU"), l10n("tooSmallMTULong", new String[] { "mtu", "minMTU" }, new String[] { Integer.toString(minAdvertisedMTU), Integer.toString(minAcceptableMTU) }), l10n("tooSmallMTUShort"), UserAlert.ERROR);
		} else return;
		clientCore.alerts.register(alertMTUTooSmall);
	}

	public void setDispatcherHook(NodeDispatcherCallback cb) {
		this.dispatcher.setHook(cb);
	}

	public boolean shallWePublishOurPeersLocation() {
		return publishOurPeersLocation;
	}

	public boolean shallWeRouteAccordingToOurPeersLocation() {
		return routeAccordingToOurPeersLocation;
	}

	public boolean objectCanNew(ObjectContainer container) {
		Logger.error(this, "Not storing Node in database", new Exception("error"));
		return false;
	}

	public void drawClientCacheBox(HTMLNode storeSizeInfobox) {
		HTMLNode div = storeSizeInfobox.addChild("div");
		div.addChild("p", "Client cache max size: "+this.maxClientCacheKeys+" keys");
		div.addChild("p", "Client cache size: CHK "+this.chkClientcache.keyCount()+" pubkey "+this.pubKeyClientcache.keyCount()+" SSK "+this.sskClientcache.keyCount());
		div.addChild("p", "Client cache misses: CHK "+this.chkClientcache.misses()+" pubkey "+this.pubKeyClientcache.misses()+" SSK "+this.sskClientcache.misses());
		div.addChild("p", "Client cache hits: CHK "+this.chkClientcache.hits()+" pubkey "+this.pubKeyClientcache.hits()+" SSK "+this.sskClientcache.hits());
	}

	public void drawSlashdotCacheBox(HTMLNode storeSizeInfobox) {
		HTMLNode div = storeSizeInfobox.addChild("div");
		div.addChild("p", "Slashdot/ULPR cache max size: "+maxSlashdotCacheKeys+" keys");
		div.addChild("p", "Slashdot/ULPR cache size: CHK "+this.chkSlashdotcache.keyCount()+" pubkey "+this.pubKeySlashdotcache.keyCount()+" SSK "+this.sskSlashdotcache.keyCount());
		div.addChild("p", "Slashdot/ULPR cache misses: CHK "+this.chkSlashdotcache.misses()+" pubkey "+this.pubKeySlashdotcache.misses()+" SSK "+this.sskSlashdotcache.misses());
		div.addChild("p", "Slashdot/ULPR cache hits: CHK "+this.chkSlashdotcache.hits()+" pubkey "+this.pubKeySlashdotcache.hits()+" SSK "+this.sskSlashdotcache.hits());
		div.addChild("p", "Slashdot/ULPR cache writes: CHK "+this.chkSlashdotcache.writes()+" pubkey "+this.pubKeySlashdotcache.writes()+" SSK "+this.sskSlashdotcache.writes());
	}

	private boolean enteredPassword;

	public void setMasterPassword(String password, boolean inFirstTimeWizard) throws AlreadySetPasswordException, MasterKeysWrongPasswordException, MasterKeysFileSizeException, IOException {
		synchronized(this) {
			if(enteredPassword)
				throw new AlreadySetPasswordException();
		}
		if(securityLevels.getPhysicalThreatLevel() == PHYSICAL_THREAT_LEVEL.MAXIMUM)
			Logger.error(this, "Setting password while physical threat level is at MAXIMUM???");
		MasterKeys keys = MasterKeys.read(masterKeysFile, random, password);
		try {
			setPasswordInner(keys, inFirstTimeWizard);
		} finally {
			keys.clearAll();
		}
	}

	private void setPasswordInner(MasterKeys keys, boolean inFirstTimeWizard) throws MasterKeysWrongPasswordException, MasterKeysFileSizeException, IOException {
		boolean wantClientCache = false;
		boolean wantDatabase = false;
		synchronized(this) {
			enteredPassword = true;
			if(!clientCacheAwaitingPassword) {
				if(inFirstTimeWizard) {
					byte[] copied = new byte[keys.clientCacheMasterKey.length];
					System.arraycopy(keys.clientCacheMasterKey, 0, copied, 0, copied.length);
					cachedClientCacheKey = copied;
					// Wipe it if haven't specified datastore size in 10 minutes.
					ticker.queueTimedJob(new Runnable() {
						public void run() {
							synchronized(Node.this) {
								MasterKeys.clear(cachedClientCacheKey);
								cachedClientCacheKey = null;
							}
						}

					}, 10*60*1000);
				}
			} else wantClientCache = true;
			wantDatabase = db == null;
		}
		if(wantClientCache)
			activatePasswordedClientCache(keys);
		if(wantDatabase)
			lateSetupDatabase(keys.databaseKey);
	}


	private void activatePasswordedClientCache(MasterKeys keys) {
		synchronized(this) {
			if(clientCacheType.equals("ram")) {
				System.err.println("RAM client cache cannot be passworded!");
				return;
			}
			if(!clientCacheType.equals("salt-hash")) {
				System.err.println("Unknown client cache type, cannot activate passworded store: "+clientCacheType);
				return;
			}
		}
		Runnable migrate = new MigrateOldStoreData(true);
		String suffix = getStoreSuffix();
		try {
			initSaltHashClientCacheFS(suffix, true, keys.clientCacheMasterKey);
		} catch (NodeInitException e) {
			Logger.error(this, "Unable to activate passworded client cache", e);
			System.err.println("Unable to activate passworded client cache: "+e);
			e.printStackTrace();
			return;
		}

		synchronized(this) {
			clientCacheAwaitingPassword = false;
		}

		executor.execute(migrate, "Migrate data from previous store");
	}

	public void changeMasterPassword(String oldPassword, String newPassword, boolean inFirstTimeWizard) throws MasterKeysWrongPasswordException, MasterKeysFileSizeException, IOException, AlreadySetPasswordException {
		if(securityLevels.getPhysicalThreatLevel() == PHYSICAL_THREAT_LEVEL.MAXIMUM)
			Logger.error(this, "Changing password while physical threat level is at MAXIMUM???");
		if(masterKeysFile.exists()) {
			MasterKeys keys = MasterKeys.read(masterKeysFile, random, oldPassword);
			keys.changePassword(masterKeysFile, newPassword, random);
			setPasswordInner(keys, inFirstTimeWizard);
			keys.clearAll();
		} else {
			setMasterPassword(newPassword, inFirstTimeWizard);
		}
	}

	public static class AlreadySetPasswordException extends Exception {

	   final private static long serialVersionUID = -7328456475029374032L;

	}

	public synchronized File getMasterPasswordFile() {
		return masterKeysFile;
	}

	public void panic() {
		try {
			db.close();
		} catch (Throwable t) {
			// Ignore
		}
		synchronized(this) {
			db = null;
		}
		try {
			FileUtil.secureDelete(dbFile, random);
			FileUtil.secureDelete(dbFileCrypt, random);
		} catch (IOException e) {
			// Ignore
		}
		dbFile.delete();
		dbFileCrypt.delete();
	}

	public void finishPanic() {
		WrapperManager.restart();
		System.exit(0);
	}


	public boolean awaitingPassword() {
		if(clientCacheAwaitingPassword) return true;
		if(databaseAwaitingPassword) return true;
		return false;
	}


	public boolean isDatabaseEncrypted() {
		return databaseEncrypted;
	}

	public boolean hasDatabase() {
		return db != null;
	}


	public synchronized boolean autoChangeDatabaseEncryption() {
		return autoChangeDatabaseEncryption;
	}


	private long completeInsertsStored;
	private long completeInsertsOldStore;
	private long completeInsertsTotal;
	private long completeInsertsNotStoredWouldHaveStored;	// DEBUGGING: should be 0 but can be nonzero if e.g. a request originates from a backed off node; should be very low in any case; FIXME remove eventually

	/** Should we commit the block to the store rather than the cache?
	 *
	 * <p>We used to check whether we are a sink by checking whether any peer has
	 * a closer location than we do. Then we made low-uptime nodes exempt from
	 * this calculation: if we route to a low uptime node with a closer location,
	 * we want to store it anyway since he may go offline. The problem was that
	 * if we routed to a low-uptime node, and there was another option that wasn't
	 * low-uptime but was closer to the target than we were, then we would not
	 * store in the store. Also, routing isn't always by the closest peer location:
	 * FOAF and per-node failure tables change it. So now, we consider the nodes
	 * we have actually routed to:</p>
	 *
	 * <p>Store in datastore if our location is closer to the target than:</p><ol>
	 * <li>the source location (if any, and ignoring if low-uptime)</li>
	 * <li>the locations of the nodes we just routed to (ditto)</li>
	 * </ol>
	 *
	 * @param key
	 * @param source
	 * @param routedTo
	 * @return
	 */
	public boolean shouldStoreDeep(Key key, PeerNode source, PeerNode[] routedTo) {
    	double myLoc = getLocation();
    	double target = key.toNormalizedDouble();
    	double myDist = Location.distance(myLoc, target);

    	boolean wouldHaveStored = !peers.isCloserLocation(target, MIN_UPTIME_STORE_KEY);

    	// First, calculate whether we would have stored it using the old formula.
		if(wouldHaveStored)
			completeInsertsOldStore++;

    	if(logMINOR) Logger.minor(this, "Should store for "+key+" ?");
    	// Don't sink store if any of the nodes we routed to, or our predecessor, is both high-uptime and closer to the target than we are.
    	if(source != null && !source.isLowUptime()) {
    		if(Location.distance(source, target) < myDist) {
    	    	if(logMINOR) Logger.minor(this, "Not storing because source is closer to target for "+key+" : "+source);
    	    	synchronized(this) {
    	    		completeInsertsTotal++;
    	    		if(wouldHaveStored) {
    	    			if(logMINOR) Logger.minor(this, "Would have stored but haven't stored");
    	    			completeInsertsNotStoredWouldHaveStored++;
    	    		}
    	    	}
    			return false;
    		}
    	}
    	for(PeerNode pn : routedTo) {
    		if(Location.distance(pn, target) < myDist && !pn.isLowUptime()) {
    	    	if(logMINOR) Logger.minor(this, "Not storing because peer "+pn+" is closer to target for "+key+" his loc "+pn.getLocation()+" my loc "+myLoc+" target is "+target);
    	    	synchronized(this) {
    	    		completeInsertsTotal++;
    	    		if(wouldHaveStored) {
    	    			if(logMINOR) Logger.minor(this, "Would have stored but haven't stored");
    	    			completeInsertsNotStoredWouldHaveStored++;
    	    		}
    	    	}
    			return false;
    		} else {
    			if(logMINOR) Logger.minor(this, "Should store maybe, peer "+pn+" loc = "+pn.getLocation()+" my loc is "+myLoc+" target is "+target+" low uptime is "+pn.isLowUptime());
    		}
    	}
    	synchronized(this) {
    		completeInsertsStored++;
    		completeInsertsTotal++;
    	}
    	if(logMINOR) Logger.minor(this, "Should store returning true for "+key+" target="+target+" myLoc="+myLoc+" peers: "+routedTo.length);
    	return true;
	}


	private final DecimalFormat fix3p3pct = new DecimalFormat("##0.000%");

	public synchronized void drawStoreStats(HTMLNode infobox) {
		if (completeInsertsTotal != 0) {
			infobox.addChild("p", "Stored inserts: "+completeInsertsStored+" of "+completeInsertsTotal+" ("+fix3p3pct.format((completeInsertsStored*1.0)/completeInsertsTotal)+")");
			infobox.addChild("p", "Would have stored: "+completeInsertsOldStore+" of "+completeInsertsTotal+" ("+fix3p3pct.format((completeInsertsOldStore*1.0)/completeInsertsTotal)+")");
			infobox.addChild("p", "Would have stored but wasn't stored: "+completeInsertsNotStoredWouldHaveStored+" of "+completeInsertsTotal+" ("+fix3p3pct.format((completeInsertsNotStoredWouldHaveStored*1.0)/completeInsertsTotal)+")");
		}
	}

	public boolean getWriteLocalToDatastore() {
		return writeLocalToDatastore;
	}

	public boolean getUseSlashdotCache() {
		return useSlashdotCache;
	}


	public int getMinimumMTU() {
		int mtu;
		synchronized(this) {
			mtu = maxPacketSize;
		}
		if(ipDetector != null) {
			int detected = ipDetector.getMinimumDetectedMTU();
			if(detected < mtu) return detected;
		}
		return mtu;
	}


	public void updateMTU() {
		this.darknetCrypto.socket.calculateMaxPacketSize();
		OpennetManager om = opennet;
		if(om != null) {
			om.crypto.socket.calculateMaxPacketSize();
		}
	}


	public boolean isTestnetEnabled() {
		return false;
	}


	public MersenneTwister createRandom() {
		byte[] buf = new byte[16];
		random.nextBytes(buf);
		return new MersenneTwister(buf);
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
* Public License, version 2 (or at your option any later version). See
* http://www.gnu.org/ for further details of the GPL. */
package freenet.support.compress;

import java.io.BufferedInputStream;
import java.io.BufferedOutputStream;
import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.DataInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;

import SevenZip.Compression.LZMA.Decoder;
import SevenZip.Compression.LZMA.Encoder;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;
import freenet.support.api.BucketFactory;
import freenet.support.io.Closer;
import freenet.support.io.CountedInputStream;
import freenet.support.io.CountedOutputStream;

// WARNING: THIS CLASS IS STORED IN DB4O -- THINK TWICE BEFORE ADD/REMOVE/RENAME FIELDS
public class NewLZMACompressor implements Compressor {
	
    // Dictionary size 1MB, this is equivalent to lzma -4, it uses 16MB to compress and 2MB to decompress.
    // Next one up is 2MB = -5 = 26M compress, 3M decompress.
	static final int MAX_DICTIONARY_SIZE = 1<<20;

        private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	// Copied from EncoderThread. See below re licensing.
	public Bucket compress(Bucket data, BucketFactory bf, long maxReadLength, long maxWriteLength) throws IOException, CompressionOutputSizeException {
		Bucket output;
		InputStream is = null;
		OutputStream os = null;
		try {
			output = bf.makeBucket(maxWriteLength);
			is = data.getInputStream();
			os = output.getOutputStream();
			if(logMINOR)
				Logger.minor(this, "Compressing "+data+" size "+data.size()+" to new bucket "+output);
			compress(is, os, maxReadLength, maxWriteLength);
			is.close();
			os.close();
		} finally {
			Closer.close(is);
			Closer.close(os);
		}
		return output;
	}
	
	public long compress(InputStream is, OutputStream os, long maxReadLength, long maxWriteLength) throws IOException {
		CountedInputStream cis = null;
		CountedOutputStream cos = null;
		cis = new CountedInputStream(new BufferedInputStream(is, 32768));
		cos = new CountedOutputStream(new BufferedOutputStream(os, 32768));
		Encoder encoder = new Encoder();
        encoder.SetEndMarkerMode( true );
        int dictionarySize = 1;
        if(maxReadLength == Long.MAX_VALUE || maxReadLength < 0) {
        	dictionarySize = MAX_DICTIONARY_SIZE;
        	Logger.error(this, "No indication of size, having to use maximum dictionary size", new Exception("debug"));
        } else {
        	while(dictionarySize < maxReadLength && dictionarySize < MAX_DICTIONARY_SIZE)
        		dictionarySize <<= 1;
        }
        encoder.SetDictionarySize( dictionarySize );
        encoder.WriteCoderProperties(os);
        encoder.Code( cis, cos, maxReadLength, maxWriteLength, null );
        cos.flush();
		if(logMINOR)
			Logger.minor(this, "Read "+cis.count()+" written "+cos.written());
		return cos.written();
	}

	public Bucket decompress(Bucket data, BucketFactory bf, long maxLength, long maxCheckSizeLength, Bucket preferred) throws IOException, CompressionOutputSizeException {
		Bucket output;
		if(preferred != null)
			output = preferred;
		else
			output = bf.makeBucket(maxLength);
		if(logMINOR)
			Logger.minor(this, "Decompressing "+data+" size "+data.size()+" to new bucket "+output);
		CountedInputStream is = new CountedInputStream(new BufferedInputStream(data.getInputStream(), 32768));
		BufferedOutputStream os = new BufferedOutputStream(output.getOutputStream(), 32768);
		decompress(is, os, maxLength, maxCheckSizeLength);
		os.close();
		if(logMINOR)
			Logger.minor(this, "Output: "+output+" size "+output.size()+" read "+is.count());
		is.close();
		return output;
	}

	public long decompress(InputStream is, OutputStream os, long maxLength, long maxCheckSizeBytes) throws IOException, CompressionOutputSizeException {
		byte[] props = new byte[5];
		DataInputStream dis = new DataInputStream(is);
		dis.readFully(props);
		CountedOutputStream cos = new CountedOutputStream(os);
		
		int dictionarySize = 0;
		for (int i = 0; i < 4; i++)
			dictionarySize += ((int)(props[1 + i]) & 0xFF) << (i * 8);
		
		if(dictionarySize < 0) throw new InvalidCompressedDataException("Invalid dictionary size");
		if(dictionarySize > MAX_DICTIONARY_SIZE) throw new TooBigDictionaryException();
		Decoder decoder = new Decoder();
		if(!decoder.SetDecoderProperties(props)) throw new InvalidCompressedDataException("Invalid properties");
		decoder.Code(is, cos, maxLength);
		//cos.flush();
		return cos.written();
	}

	public int decompress(byte[] dbuf, int i, int j, byte[] output) throws CompressionOutputSizeException {
		// Didn't work with Inflater.
		// FIXME fix sometimes to use Inflater - format issue?
		ByteArrayInputStream bais = new ByteArrayInputStream(dbuf, i, j);
		ByteArrayOutputStream baos = new ByteArrayOutputStream(output.length);
		int bytes = 0;
		try {
			decompress(bais, baos, output.length, -1);
			bytes = baos.size();
		} catch (IOException e) {
			// Impossible
			throw new Error("Got IOException: " + e.getMessage(), e);
		}
		byte[] buf = baos.toByteArray();
		System.arraycopy(buf, 0, output, 0, bytes);
		return bytes;
	}
}
package freenet.support;

import com.db4o.ObjectContainer;

import freenet.client.async.ClientContext;
import freenet.client.async.HasCooldownCacheItem;

public interface RemoveRandomParent extends HasCooldownCacheItem {

	/** If the specified RemoveRandom is empty, remove it.
	 * LOCKING: Must be called with no locks held, particularly no locks on the
	 * RemoveRandom, because we take locks in order!
	 * @param context 
	 */
	public void maybeRemove(RemoveRandom r, ObjectContainer container, ClientContext context);

}
package freenet.node;

import java.util.List;
import java.util.Vector;

import freenet.io.comm.Peer;

public interface PacketFormat {

	boolean handleReceivedPacket(byte[] buf, int offset, int length, long now, Peer replyTo);

	/**
	 * Maybe send something. A SINGLE PACKET. Don't send everything at once, for two reasons:
	 * <ol>
	 * <li>It is possible for a node to have a very long backlog.</li>
	 * <li>Sometimes sending a packet can take a long time.</li>
	 * <li>In the near future PacketSender will be responsible for output bandwidth throttling. So it makes sense to
	 * send a single packet and round-robin.</li>
	 * </ol>
	 * @param ackOnly 
	 */
	boolean maybeSendPacket(long now, Vector<ResendPacketItem> rpiTemp, int[] rpiIntTemp, boolean ackOnly)
	                throws BlockedTooLongException;

	/**
	 * Called when the peer has been disconnected.
	 * THE CALLER SHOULD STOP USING THE PACKET FORMAT OBJECT AFTER CALLING THIS FUNCTION!
	 */
	List<MessageItem> onDisconnect();

	/**
	 * Returns {@code false} if the packet format can't send new messages because it must wait for some internal event.
	 * For example, if a packet sequence number can not be allocated this method should return {@code false}, but if
	 * nothing can be sent because there is no (external) data to send it should not.
	 * Note that this only applies to packets being created from messages on the @see PeerMessageQueue.
	 * Note also that there may already be messages in flight, but it may return false in that
	 * case, so you need to check timeNextUrgent() as well.
	 * @return {@code false} if the packet format can't send packets
	 */
	boolean canSend(SessionKey key);

	/**
	 * @return The time at which the packet format will want to send an ack, finish sending a message,
	 * retransmit a packet, or similar. Long.MAX_VALUE if not supported or if there is nothing to ack 
	 * and nothing in flight. 
	 * @param canSend If false, canSend() has returned false. Some transports will
	 * want to send a packet anyway e.g. an ack, a resend in some cases. */
	long timeNextUrgent(boolean canSend);
	
	/**
	 * @return The time at which the packet format will want to send an ack. Resends
	 * etc don't count, only acks. The reason acks are special is they are needed
	 * for the other side to recognise that their packets have been received and
	 * thus avoid retransmission.
	 */
	long timeSendAcks();
	
	/** Is there enough data queued to justify sending a packet immediately? Ideally
	 * this should take into account transport level headers. */
	boolean fullPacketQueued(int maxPacketSize);

	void checkForLostPackets();

	long timeCheckForLostPackets();

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.io.comm;

/**
 * @author amphibian
 * 
 * Exception thrown when we try to send a message to a node that is
 * not currently connected.
 */
public class NotConnectedException extends Exception {
	private static final long serialVersionUID = -1;
    public NotConnectedException(String string) {
        super(string);
    }

    public NotConnectedException() {
        super();
    }

	public NotConnectedException(DisconnectedException e) {
		super(e.toString());
		initCause(e);
	}

    @Override
    public final synchronized Throwable fillInStackTrace() {
        return null;
    }
}
package freenet.clients.http.updateableelements;

public class UpdaterConstants {
	public static final String	FINISHED					= "Finished";

	// Updaters
	/** Replaces the element and reloads the page if the content is FINISHED */
	public static final String	PROGRESSBAR_UPDATER			= "progressBar";

	/** Replaces the element and refreshes the total image fetching message */
	public static final String	IMAGE_ELEMENT_UPDATER		= "ImageElementUpdater";

	/** Replaces the element and replaces the title with the value of a hidden input named 'pageTitle' */
	public static final String	CONNECTIONS_TABLE_UPDATER	= "ConnectionsList";

	/** Simply replaces the element */
	public static final String	REPLACER_UPDATER			= "ReplacerUpdater";

	/** Replaces the element, and updates the messages */
	public static final String	XMLALERT_UPDATER			= "XmlAlertUpdater";
	// End of Updaters

	// 10 minute timeout.
	// We get a notification on page closure unless the browser dies, and
	// CSS fetches and manual downloads can occupy many connections (e.g.
	// The Activelink Index has 8 CSS's), so don't set it too low.
	
	public static final int		KEEPALIVE_INTERVAL_SECONDS	= 600;

	public static final String	SUCCESS						= "SUCCESS";

	public static final String	FAILURE						= "FAILURE";

	public static final String	SEPARATOR					= ":";

	// Paths
	public static final String	dataPath					= "/pushdata/";

	public static final String	notificationPath			= "/pushnotifications/";

	public static final String	keepalivePath				= "/keepalive/";

	public static final String	failoverPath				= "/failover/";

	public static final String	leavingPath					= "/leaving/";

	public static final String	dismissAlertPath			= "/dismissalert/";

	public static final String	logWritebackPath			= "/logwriteback/";
	// End of Paths
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.io.xfer;

import freenet.io.comm.AsyncMessageCallback;
import freenet.io.comm.AsyncMessageFilterCallback;
import freenet.io.comm.ByteCounter;
import freenet.io.comm.DMT;
import freenet.io.comm.DisconnectedException;
import freenet.io.comm.Message;
import freenet.io.comm.MessageFilter;
import freenet.io.comm.NotConnectedException;
import freenet.io.comm.PeerContext;
import freenet.io.comm.PeerRestartedException;
import freenet.node.SyncSendWaitedTooLongException;
import freenet.support.BitArray;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.TimeUtil;
import freenet.support.Logger.LogLevel;

/**
 * Bulk data transfer (not block). Bulk transfer is designed for files which may be much bigger than a 
 * key block, and where we have the whole file at the outset. Do not persist across node restarts.
 * 
 * Used by update over mandatory, sending a file to our peers attached to an N2NTM etc.
 * @author toad
 */
public class BulkTransmitter {

	/** If no packets sent in this period, and no completion acknowledgement / cancellation, assume failure. */
	static final int TIMEOUT = 5*60*1000;
	/** Time to hang around listening for the last FNPBulkReceivedAll message */
	static final int FINAL_ACK_TIMEOUT = 10*1000;
	/** Available blocks */
	final PartiallyReceivedBulk prb;
	/** Peer who we are sending the data to */
	final PeerContext peer;
	/** Transfer UID for messages */
	final long uid;
	/** Blocks we have but haven't sent yet. 0 = block sent or not present, 1 = block present but not sent */
	final BitArray blocksNotSentButPresent;
	private boolean cancelled;
	/** Not persistent over reboots */
	final long peerBootID;
	private boolean sentCancel;
	private boolean finished;
	final int packetSize;
	/** Not expecting a response? */
	final boolean noWait;
	private long finishTime=-1;
	private String cancelReason;
	private final ByteCounter ctr;
	private final boolean realTime;
	
	private static long transfersCompleted;
	private static long transfersSucceeded;

        private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}
	/**
	 * Create a bulk data transmitter.
	 * @param prb The PartiallyReceivedBulk containing the file we want to send, or the part of it that we have so far.
	 * @param peer The peer we want to send it to.
	 * @param uid The unique identifier for this data transfer
	 * @param masterThrottle The overall output throttle
	 * @param noWait If true, don't wait for an FNPBulkReceivedAll, return as soon as we've sent everything.
	 * @throws DisconnectedException If the peer we are trying to send to becomes disconnected.
	 */
	public BulkTransmitter(PartiallyReceivedBulk prb, PeerContext peer, long uid, boolean noWait, ByteCounter ctr, boolean realTime) throws DisconnectedException {
		this.prb = prb;
		this.peer = peer;
		this.uid = uid;
		this.noWait = noWait;
		this.ctr = ctr;
		this.realTime = realTime;
		if(ctr == null) throw new NullPointerException();
		peerBootID = peer.getBootID();
		// Need to sync on prb while doing both operations, to avoid race condition.
		// Specifically, we must not get calls to blockReceived() until blocksNotSentButPresent
		// has been set, AND it must be accurate, so there must not be an unlocked period
		// between cloning and adding.
		synchronized(prb) {
			// We can just clone it.
			blocksNotSentButPresent = prb.cloneBlocksReceived();
			prb.add(this);
		}
		try {
			prb.usm.addAsyncFilter(MessageFilter.create().setNoTimeout().setSource(peer).setType(DMT.FNPBulkReceiveAborted).setField(DMT.UID, uid),
					new AsyncMessageFilterCallback() {
						public void onMatched(Message m) {
							cancel("Other side sent FNPBulkReceiveAborted");
						}
						public boolean shouldTimeout() {
							synchronized(BulkTransmitter.this) {
								if(cancelled || finished) return true;
							}
							if(BulkTransmitter.this.prb.isAborted()) return true;
							return false;
						}
						public void onTimeout() {
							// Ignore
						}
						public void onDisconnect(PeerContext ctx) {
							// Ignore
						}
						public void onRestarted(PeerContext ctx) {
							// Ignore
						}
			}, ctr);
			prb.usm.addAsyncFilter(MessageFilter.create().setNoTimeout().setSource(peer).setType(DMT.FNPBulkReceivedAll).setField(DMT.UID, uid),
					new AsyncMessageFilterCallback() {
						public void onMatched(Message m) {
							completed();
						}
						public boolean shouldTimeout() {
							synchronized(BulkTransmitter.this) {
								   if (cancelled) return true;
								   if (finished)  return (System.currentTimeMillis()-finishTime > FINAL_ACK_TIMEOUT);
							}
							if(BulkTransmitter.this.prb.isAborted()) return true;
							return false;
						}
						public void onTimeout() {
							// Ignore
						}
						public void onDisconnect(PeerContext ctx) {
							// Ignore
						}
						public void onRestarted(PeerContext ctx) {
							// Ignore
						}
			}, ctr);
		} catch (DisconnectedException e) {
			cancel("Disconnected");
			throw e;
		}
		packetSize = DMT.bulkPacketTransmitSize(prb.blockSize) +
			peer.getOutgoingMangler().fullHeadersLengthOneMessage();
	}

	/**
	 * Received a block. Set the relevant bit to 1 to indicate that we have the block but haven't sent
	 * it yet. **Only called by PartiallyReceivedBulk.**
	 * @param block The block number that has been received.
	 */
	synchronized void blockReceived(int block) {
		blocksNotSentButPresent.setBit(block, true);
		notifyAll();
	}

	/**
	 * Called when the PRB is aborted.
	 */
	public void onAborted() {
		sendAbortedMessage();
		synchronized(this) {
			notifyAll();
		}
	}
	
	private void sendAbortedMessage() {
		synchronized(this) {
			if(sentCancel) return;
			sentCancel = true;
		}
		try {
			peer.sendAsync(DMT.createFNPBulkSendAborted(uid), null, ctr);
		} catch (NotConnectedException e) {
			// Cool
		}
	}

	public void cancel(String reason) {
		if(logMINOR)
			Logger.minor(this, "Cancelling "+this);
		sendAbortedMessage();
		synchronized(this) {
			if(cancelled || finished) return;
			cancelled = true;
			cancelReason = reason;
			notifyAll();
		}
		prb.remove(this);
		synchronized(BulkTransmitter.class) {
			transfersCompleted++;
		}
	}

	/** Like cancel(), but without the negative overtones: The client says it's got everything,
	 * we believe them (even if we haven't sent everything; maybe they had a partial). */
	public void completed() {
		synchronized(this) {
			if(cancelled || finished) return;
			finished = true;
			finishTime = System.currentTimeMillis();
			notifyAll();
		}
		prb.remove(this);
		synchronized(BulkTransmitter.class) {
			transfersCompleted++;
			transfersSucceeded++;
		}
	}
	
	/**
	 * Send the file.
	 * @return True if the file was successfully sent. False otherwise.
	 */
	public boolean send() {
		long lastSentPacket = System.currentTimeMillis();
outer:	while(true) {
			int max = Math.min(Integer.MAX_VALUE, prb.blocks);
			PacketThrottle throttle = peer.getThrottle();
			if(throttle != null)
				max = Math.min(max, (int)Math.min(Integer.MAX_VALUE, throttle.getWindowSize()));
			// FIXME hardcoded limit for memory usage. We can probably get away with more for now but if we start doing lots of bulk transfers we'll need to limit this globally...
			max = Math.min(max, 100);
			if(max < 1) max = 1;
			
			if(prb.isAborted()) {
				if(logMINOR)
					Logger.minor(this, "Aborted "+this);
				return false;
			}
			int blockNo;
			if(peer.getBootID() != peerBootID) {
				synchronized(this) {
					cancelled = true;
					notifyAll();
				}
				prb.remove(BulkTransmitter.this);
				if(logMINOR)
					Logger.minor(this, "Failed to send "+uid+": peer restarted: "+peer);
				return false;
			}
			synchronized(this) {
				if(finished) return true;
				if(cancelled) return false;
				blockNo = blocksNotSentButPresent.firstOne();
			}
			if(blockNo < 0) {
				if(noWait && prb.hasWholeFile()) {
					completed();
					return true;
				}
				synchronized(this) {
					// Wait for all packets to complete
					while(true) {
						if(failedPacket) {
							cancel("Packet send failed");
							return false;
						}
						if(logMINOR)
							Logger.minor(this, "Waiting for packets: remaining: "+inFlightPackets);
						if(inFlightPackets == 0) break;
						try {
							wait();
							if(failedPacket) {
								cancel("Packet send failed");
								return false;
							}
							if(inFlightPackets == 0) break;
							continue outer; // Might be a packet...
						} catch (InterruptedException e) {
							// Ignore
						}
					}
					
					// Wait for a packet to come in, BulkReceivedAll or BulkReceiveAborted
					try {
						wait(60*1000);
					} catch (InterruptedException e) {
						// No problem
						continue;
					}
				}
				long end = System.currentTimeMillis();
				if(end - lastSentPacket > TIMEOUT) {
					Logger.error(this, "Send timed out on "+this);
					cancel("Timeout awaiting BulkReceivedAll");
					return false;
				}
				continue;
			}
			// Send a packet
			byte[] buf = prb.getBlockData(blockNo);
			if(buf == null) {
				if(logMINOR)
					Logger.minor(this, "Block "+blockNo+" is null, presumably the send is cancelled: "+this);
				// Already cancelled, quit
				return false;
			}
			
			// Congestion control and bandwidth limiting
			try {
				if(logMINOR) Logger.minor(this, "Sending packet "+blockNo);
				Message msg = DMT.createFNPBulkPacketSend(uid, blockNo, buf, realTime);
				boolean isOldFNP = peer.isOldFNP();
				UnsentPacketTag tag = new UnsentPacketTag(isOldFNP);
				if(isOldFNP) {
					peer.sendThrottledMessage(msg, buf.length, ctr, BulkReceiver.TIMEOUT, false, tag);
				} else {
					peer.sendAsync(msg, tag, ctr);
					synchronized(this) {
						while(inFlightPackets >= max && !failedPacket)
							try {
								wait(1000);
							} catch (InterruptedException e) {
								// Ignore
							}
					}
				}
				synchronized(this) {
					blocksNotSentButPresent.setBit(blockNo, false);
				}
				lastSentPacket = System.currentTimeMillis();
			} catch (NotConnectedException e) {
				cancel("Disconnected");
				if(logMINOR)
					Logger.minor(this, "Canclled: not connected "+this);
				return false;
			} catch (PeerRestartedException e) {
				cancel("PeerRestarted");
				if(logMINOR)
					Logger.minor(this, "Canclled: not connected "+this);
				return false;
			} catch (WaitedTooLongException e) {
				long rtt = peer.getThrottle().getRoundTripTime();
				Logger.error(this, "Failed to send bulk packet "+blockNo+" for "+this+" RTT is "+TimeUtil.formatTime(rtt));
				return false;
			} catch (SyncSendWaitedTooLongException e) {
				// Impossible
				Logger.error(this, "Impossible: Caught "+e, e);
				return false;
			}
		}
	}
	
	private int inFlightPackets = 0;
	private boolean failedPacket = false;
	
	private class UnsentPacketTag implements AsyncMessageCallback {

		private boolean finished;
		private final boolean isOldFNP;
		
		private UnsentPacketTag(boolean isOldFNP) {
			this.isOldFNP = isOldFNP;
			synchronized(BulkTransmitter.this) {
				inFlightPackets++;
			}
		}
		
		public synchronized void waitForCompletion() {
			while(!finished) {
				try {
					wait();
				} catch (InterruptedException e) {
					// Ignore
				}
			}
		}

		public void acknowledged() {
			complete(false);
		}

		private void complete(boolean failed) {
			synchronized(this) {
				if(finished) return;
				finished = true;
				notifyAll();
			}
			ctr.sentPayload(prb.blockSize);
			synchronized(BulkTransmitter.this) {
				if(failed) {
					failedPacket = true;
					BulkTransmitter.this.notifyAll();
					if(logMINOR) Logger.minor(this, "Packet failed for "+BulkTransmitter.this);
				} else {
					inFlightPackets--;
					BulkTransmitter.this.notifyAll();
					if(logMINOR) Logger.minor(this, "Packet sent "+BulkTransmitter.this+" remaining in flight: "+inFlightPackets);
				}
			}
		}

		public void disconnected() {
			complete(true);
		}

		public void fatalError() {
			complete(true);
		}

		public void sent() {
			// Wait for acknowledgment
		}
		
	}
	
	@Override
	public String toString() {
		return "BulkTransmitter:"+uid+":"+peer.shortToString();
	}
	
	public String getCancelReason() {
		return cancelReason;
	}
	
	public static synchronized long[] transferSuccess() {
		return new long[] { transfersCompleted, transfersSucceeded };
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */

package freenet.config;

public interface EnumerableOptionCallback {
	public String[] getPossibleValues();
	
	/** Return the current value */
	public String get();
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.simulator;

import java.io.File;

import freenet.crypt.DummyRandomSource;
import freenet.crypt.RandomSource;
import freenet.node.LocationManager;
import freenet.node.Node;
import freenet.node.NodeStarter;
import freenet.support.Executor;
import freenet.support.Logger;
import freenet.support.PooledExecutor;
import freenet.support.Logger.LogLevel;
import freenet.support.io.FileUtil;
import freenet.support.math.BootstrappingDecayingRunningAverage;
import freenet.support.math.RunningAverage;
import freenet.support.math.SimpleRunningAverage;

/**
 * @author amphibian
 * 
 * Create a mesh of nodes and let them sort out their locations.
 * 
 * Then run some node-to-node searches.
 */
public class RealNodeRoutingTest extends RealNodeTest {

	static final int NUMBER_OF_NODES = 100;
	static final int DEGREE = 5;
	static final short MAX_HTL = (short) 5;
	static final boolean START_WITH_IDEAL_LOCATIONS = true;
	static final boolean FORCE_NEIGHBOUR_CONNECTIONS = true;
	static final int MAX_PINGS = 2000;
	static final boolean ENABLE_SWAPPING = false;
	static final boolean ENABLE_SWAP_QUEUEING = false;
	static final boolean ENABLE_FOAF = true;
	
	public static int DARKNET_PORT_BASE = RealNodeRequestInsertTest.DARKNET_PORT_END;
	public static final int DARKNET_PORT_END = DARKNET_PORT_BASE + NUMBER_OF_NODES;

	public static void main(String[] args) throws Exception {
		System.out.println("Routing test using real nodes:");
		System.out.println();
		String dir = "realNodeRequestInsertTest";
		File wd = new File(dir);
		if(!FileUtil.removeAll(wd)) {
			System.err.println("Mass delete failed, test may not be accurate.");
			System.exit(EXIT_CANNOT_DELETE_OLD_DATA);
		}
		wd.mkdir();
		//NOTE: globalTestInit returns in ignored random source
		NodeStarter.globalTestInit(dir, false, LogLevel.ERROR, "", true);
		// Make the network reproducible so we can easily compare different routing options by specifying a seed.
		DummyRandomSource random = new DummyRandomSource(3142);
		//DiffieHellman.init(random);
		Node[] nodes = new Node[NUMBER_OF_NODES];
		Logger.normal(RealNodeRoutingTest.class, "Creating nodes...");
		Executor executor = new PooledExecutor();
		for(int i = 0; i < NUMBER_OF_NODES; i++) {
			System.err.println("Creating node " + i);
			nodes[i] = NodeStarter.createTestNode(DARKNET_PORT_BASE + i, 0, dir, true, MAX_HTL, 0 /* no dropped packets */, random, executor, 500 * NUMBER_OF_NODES, 65536, true, ENABLE_SWAPPING, false, false, false, ENABLE_SWAP_QUEUEING, true, 0, ENABLE_FOAF, false, true, false, null);
			Logger.normal(RealNodeRoutingTest.class, "Created node " + i);
		}
		Logger.normal(RealNodeRoutingTest.class, "Created " + NUMBER_OF_NODES + " nodes");
		// Now link them up
		makeKleinbergNetwork(nodes, START_WITH_IDEAL_LOCATIONS, DEGREE, FORCE_NEIGHBOUR_CONNECTIONS, random);

		Logger.normal(RealNodeRoutingTest.class, "Added random links");

		for(int i = 0; i < NUMBER_OF_NODES; i++) {
			System.err.println("Starting node " + i);
			nodes[i].start(false);
		}

		waitForAllConnected(nodes);

		// Make the choice of nodes to ping to and from deterministic too.
		// There is timing noise because of all the nodes, but the network
		// and the choice of nodes to start and finish are deterministic, so
		// the overall result should be more or less deterministic.
		waitForPingAverage(0.98, nodes, new DummyRandomSource(3143), MAX_PINGS, 5000);
		System.exit(0);
	}

	static void waitForPingAverage(double accuracy, Node[] nodes, RandomSource random, int maxTests, int sleepTime) throws InterruptedException {
		int totalHopsTaken = 0;
		int cycleNumber = 0;
		int lastSwaps = 0;
		int lastNoSwaps = 0;
		int failures = 0;
		int successes = 0;
		RunningAverage avg = new SimpleRunningAverage(100, 0.0);
		RunningAverage avg2 = new BootstrappingDecayingRunningAverage(0.0, 0.0, 1.0, 100, null);
		int pings = 0;
		for(int total = 0; total < maxTests; total++) {
			cycleNumber++;
			try {
				Thread.sleep(sleepTime);
			} catch(InterruptedException e) {
				// Ignore
			}
			for(int i = 0; i < nodes.length; i++) {
				System.err.println("Cycle " + cycleNumber + " node " + i + ": " + nodes[i].getLocation());
			}
			int newSwaps = LocationManager.swaps;
			int totalStarted = LocationManager.startedSwaps;
			int noSwaps = LocationManager.noSwaps;
			System.err.println("Swaps: " + (newSwaps - lastSwaps));
			System.err.println("\nTotal swaps: Started*2: " + totalStarted * 2 + ", succeeded: " + newSwaps + ", last minute failures: " + noSwaps +
				", ratio " + (double) noSwaps / (double) newSwaps + ", early failures: " + ((totalStarted * 2) - (noSwaps + newSwaps)));
			System.err.println("This cycle ratio: " + ((double) (noSwaps - lastNoSwaps)) / ((double) (newSwaps - lastSwaps)));
			lastNoSwaps = noSwaps;
			System.err.println("Swaps rejected (already locked): " + LocationManager.swapsRejectedAlreadyLocked);
			System.err.println("Swaps rejected (nowhere to go): " + LocationManager.swapsRejectedNowhereToGo);
			System.err.println("Swaps rejected (rate limit): " + LocationManager.swapsRejectedRateLimit);
			System.err.println("Swaps rejected (recognized ID):" + LocationManager.swapsRejectedRecognizedID);
			System.err.println("Swaps failed:" + LocationManager.noSwaps);
			System.err.println("Swaps succeeded:" + LocationManager.swaps);

			double totalSwapInterval = 0.0;
			double totalSwapTime = 0.0;
			for(int i = 0; i < nodes.length; i++) {
				totalSwapInterval += nodes[i].lm.getSendSwapInterval();
				totalSwapTime += nodes[i].lm.getAverageSwapTime();
			}
			System.err.println("Average swap time: " + (totalSwapTime / nodes.length));
			System.err.println("Average swap sender interval: " + (totalSwapInterval / nodes.length));

			waitForAllConnected(nodes);

			lastSwaps = newSwaps;
			// Do some (routed) test-pings
			for(int i = 0; i < 10; i++) {
				try {
					Thread.sleep(sleepTime);
				} catch(InterruptedException e1) {
				}
				try {
					Node randomNode = nodes[random.nextInt(nodes.length)];
					Node randomNode2 = randomNode;
					while(randomNode2 == randomNode) {
						randomNode2 = nodes[random.nextInt(nodes.length)];
					}
					double loc2 = randomNode2.getLocation();
					Logger.normal(RealNodeRoutingTest.class, "Pinging " + randomNode2.getDarknetPortNumber() + " @ " + loc2 + " from " + randomNode.getDarknetPortNumber() + " @ " + randomNode.getLocation());
					
					int hopsTaken = randomNode.routedPing(loc2, randomNode2.getDarknetIdentity());
					pings++;
					if(hopsTaken < 0) {
						failures++;
						avg.report(0.0);
						avg2.report(0.0);
						double ratio = (double) successes / ((double) (failures + successes));
						System.err.println("Routed ping " + pings + " FAILED from " + randomNode.getDarknetPortNumber() + " to " + randomNode2.getDarknetPortNumber() + " (long:" + ratio + ", short:" + avg.currentValue() + ", vague:" + avg2.currentValue() + ')');
					} else {
						totalHopsTaken += hopsTaken;
						successes++;
						avg.report(1.0);
						avg2.report(1.0);
						double ratio = (double) successes / ((double) (failures + successes));
						System.err.println("Routed ping " + pings + " success: " + hopsTaken + ' ' + randomNode.getDarknetPortNumber() + " to " + randomNode2.getDarknetPortNumber() + " (long:" + ratio + ", short:" + avg.currentValue() + ", vague:" + avg2.currentValue() + ')');
					}
				} catch(Throwable t) {
					Logger.error(RealNodeRoutingTest.class, "Caught " + t, t);
				}
			}
			System.err.println("Average path length for successful requests: "+totalHopsTaken/successes);
			if(pings > 10 && avg.currentValue() > accuracy && ((double) successes / ((double) (failures + successes)) > accuracy)) {
				System.err.println();
				System.err.println("Reached " + (accuracy * 100) + "% accuracy.");
				System.err.println();
				System.err.println("Network size: " + nodes.length);
				System.err.println("Maximum HTL: " + MAX_HTL);
				System.err.println("Average path length for successful requests: "+totalHopsTaken/successes);
				System.err.println("Total started swaps: " + LocationManager.startedSwaps);
				System.err.println("Total rejected swaps (already locked): " + LocationManager.swapsRejectedAlreadyLocked);
				System.err.println("Total swaps rejected (nowhere to go): " + LocationManager.swapsRejectedNowhereToGo);
				System.err.println("Total swaps rejected (rate limit): " + LocationManager.swapsRejectedRateLimit);
				System.err.println("Total swaps rejected (recognized ID):" + LocationManager.swapsRejectedRecognizedID);
				System.err.println("Total swaps failed:" + LocationManager.noSwaps);
				System.err.println("Total swaps succeeded:" + LocationManager.swaps);
				return;
			}
		}
		System.exit(EXIT_PING_TARGET_NOT_REACHED);
	}
}
package freenet.client.async;

import com.db4o.ObjectContainer;

import freenet.keys.Key;
import freenet.keys.KeyBlock;
import freenet.keys.NodeSSK;
import freenet.node.LowLevelGetException;
import freenet.node.SendableGet;
import freenet.support.Logger;

public class SingleKeyListener implements KeyListener {
	
	private final Key key;
	private final BaseSingleFileFetcher fetcher;
	private boolean done;
	private short prio;
	private final boolean persistent;
	private final boolean realTime;

	public SingleKeyListener(Key key, BaseSingleFileFetcher fetcher, short prio, boolean persistent, boolean realTime) {
		this.key = key;
		this.fetcher = fetcher;
		this.prio = prio;
		this.persistent = persistent;
		this.realTime = realTime;
	}

	public long countKeys() {
		if(done) return 0;
		else return 1;
	}

	public short definitelyWantKey(Key key, byte[] saltedKey, ObjectContainer container,
			ClientContext context) {
		if(!key.equals(this.key)) return -1;
		else return prio;
	}

	public HasKeyListener getHasKeyListener() {
		return fetcher;
	}

	public short getPriorityClass(ObjectContainer container) {
		return prio;
	}

	public SendableGet[] getRequestsForKey(Key key, byte[] saltedKey, ObjectContainer container,
			ClientContext context) {
		if(!key.equals(this.key)) return null;
		return new SendableGet[] { fetcher };
	}

	public boolean handleBlock(Key key, byte[] saltedKey, KeyBlock found,
			ObjectContainer container, ClientContext context) {
		if(!key.equals(this.key)) return false;
		if(persistent)
			container.activate(fetcher, 1);
		try {
			fetcher.onGotKey(key, found, container, context);
		} catch (Throwable t) {
			Logger.error(this, "Failed: "+t, t);
			fetcher.onFailure(new LowLevelGetException(LowLevelGetException.INTERNAL_ERROR), null, container, context);
		}
		if(persistent)
			container.deactivate(fetcher, 1);
		synchronized(this) {
			done = true;
		}
		return true;
	}

	public Key[] listKeys(ObjectContainer container) {
		return new Key[] { key };
	}

	public boolean persistent() {
		return persistent;
	}

	public boolean probablyWantKey(Key key, byte[] saltedKey) {
		if(done) return false;
		return key.equals(this.key);
	}

	public synchronized void onRemove() {
		done = true;
	}

	public boolean isEmpty() {
		return done;
	}

	public boolean isSSK() {
		return key instanceof NodeSSK;
	}

	public boolean isRealTime() {
		return realTime;
	}

}
package freenet.node.fcp;

import freenet.keys.FreenetURI;
import freenet.node.fcp.ClientPut.COMPRESS_STATE;

/** Base class for cached status of uploads */
public abstract class UploadRequestStatus extends RequestStatus {
	
	private FreenetURI finalURI;
	private final FreenetURI targetURI;
	private int failureCode;
	private String failureReasonShort;
	private String failureReasonLong;
	
	UploadRequestStatus(String identifier, short persistence, boolean started, boolean finished, 
			boolean success, int total, int min, int fetched, int fatal, int failed,
			boolean totalFinalized, long last, short prio, // all these passed to parent
			FreenetURI finalURI, FreenetURI targetURI, 
			int failureCode, String failureReasonShort, String failureReasonLong) {
		super(identifier, persistence, started, finished, success, total, min, fetched, 
				fatal, failed, totalFinalized, last, prio);
		this.finalURI = finalURI;
		this.targetURI = targetURI;
		this.failureCode = failureCode;
		this.failureReasonShort = failureReasonShort;
		this.failureReasonLong = failureReasonLong;
	}
	
	synchronized void setFinished(boolean success, FreenetURI finalURI, int failureCode, 
			String failureReasonShort, String failureReasonLong) {
		setFinished(success);
		this.finalURI = finalURI;
		this.failureCode = failureCode;
		this.failureReasonShort = failureReasonShort;
		this.failureReasonLong = failureReasonLong;
	}


	public FreenetURI getFinalURI() {
		return finalURI;
	}
	
	public FreenetURI getTargetURI() {
		return targetURI;
	}

	@Override
	public FreenetURI getURI() {
		return finalURI;
	}

	@Override
	public abstract long getDataSize();

	@Override
	public String getFailureReason(boolean longDescription) {
		return longDescription ? failureReasonLong : failureReasonShort;
	}

	public synchronized void setFinalURI(FreenetURI finalURI2) {
		this.finalURI = finalURI2;
	}

}
package freenet.clients.http;

public interface LinkEnabledCallback {

	/** Whether to show the link? 
	 * @param ctx */
	boolean isEnabled(ToadletContext ctx);

}
package freenet.client.events;

import freenet.client.InsertContext.CompatibilityMode;

public class SplitfileCompatibilityModeEvent implements ClientEvent {

	public final long minCompatibilityMode;
	public final long maxCompatibilityMode;
	public final byte[] splitfileCryptoKey;
	public final boolean dontCompress;
	public final boolean bottomLayer;
	
	public final static int CODE = 0x0D;
	
	public int getCode() {
		return CODE;
	}

	public String getDescription() {
		if(minCompatibilityMode == -1)
			return "Unknown CompatibilityMode";
		else
			return "CompatibilityMode between "+minCompatibilityMode+" and "+maxCompatibilityMode;
	}
	
	public SplitfileCompatibilityModeEvent(CompatibilityMode min, CompatibilityMode max, byte[] splitfileCryptoKey, boolean dontCompress, boolean bottomLayer) {
		this.minCompatibilityMode = min.ordinal();
		this.maxCompatibilityMode = max.ordinal();
		this.splitfileCryptoKey = splitfileCryptoKey;
		this.dontCompress = dontCompress;
		this.bottomLayer = bottomLayer;
	}
	
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

import java.util.HashSet;

import freenet.crypt.DSAPublicKey;
import freenet.crypt.SHA256;
import freenet.io.comm.AsyncMessageFilterCallback;
import freenet.io.comm.ByteCounter;
import freenet.io.comm.DMT;
import freenet.io.comm.DisconnectedException;
import freenet.io.comm.Message;
import freenet.io.comm.MessageFilter;
import freenet.io.comm.NotConnectedException;
import freenet.io.comm.PeerContext;
import freenet.io.comm.PeerRestartedException;
import freenet.io.comm.SlowAsyncMessageFilterCallback;
import freenet.io.xfer.WaitedTooLongException;
import freenet.keys.NodeSSK;
import freenet.keys.SSKBlock;
import freenet.keys.SSKVerifyException;
import freenet.support.Logger;
import freenet.support.OOMHandler;
import freenet.support.ShortBuffer;
import freenet.support.Logger.LogLevel;
import freenet.support.io.NativeThread;

/**
 * SSKs require separate logic for inserts and requests, for various reasons:
 * - SSKs can collide.
 * - SSKs have only 1kB of data, so we can pack it into the DataReply, and we don't need to
 *   wait for a long data-transfer timeout.
 * - SSKs have pubkeys, which don't always need to be sent.
 */
public class SSKInsertSender implements PrioRunnable, AnyInsertSender, ByteCounter {

    // Constants
    static final int ACCEPTED_TIMEOUT = 10000;
    static final int SEARCH_TIMEOUT_REALTIME = 30*1000;
    static final int SEARCH_TIMEOUT_BULK = 120*1000;
    
    final int searchTimeout;

    // Basics
    final NodeSSK myKey;
    final double target;
    final long origUID;
    final InsertTag origTag;
    long uid;
    short htl;
    final PeerNode source;
    final Node node;
    /** SSK's pubkey */
    final DSAPublicKey pubKey;
    /** SSK's pubkey's hash */
    final byte[] pubKeyHash;
    /** Data (we know at start of insert) - can change if we get a collision */
    byte[] data;
    /** Headers (we know at start of insert) - can change if we get a collision */
    byte[] headers;
    final boolean fromStore;
    final long startTime;
    private boolean sentRequest;
    private boolean hasCollided;
    private boolean hasRecentlyCollided;
    private SSKBlock block;
    private static boolean logMINOR;
    private HashSet<PeerNode> nodesRoutedTo = new HashSet<PeerNode>();
    private final boolean forkOnCacheable;
    private final boolean preferInsert;
    private final boolean ignoreLowBackoff;
    private final boolean realTimeFlag;
    private InsertTag forkedRequestTag;
    
    private int status = -1;
    /** Still running */
    static final int NOT_FINISHED = -1;
    /** Successful insert */
    static final int SUCCESS = 0;
    /** Route not found */
    static final int ROUTE_NOT_FOUND = 1;
    /** Internal error */
    static final int INTERNAL_ERROR = 3;
    /** Timed out waiting for response */
    static final int TIMED_OUT = 4;
    /** Locally Generated a RejectedOverload */
    static final int GENERATED_REJECTED_OVERLOAD = 5;
    /** Could not get off the node at all! */
    static final int ROUTE_REALLY_NOT_FOUND = 6;
    
    SSKInsertSender(SSKBlock block, long uid, InsertTag tag, short htl, PeerNode source, Node node, boolean fromStore, boolean canWriteClientCache, boolean forkOnCacheable, boolean preferInsert, boolean ignoreLowBackoff, boolean realTimeFlag) {
    	logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
    	this.fromStore = fromStore;
    	this.node = node;
    	this.source = source;
    	this.htl = htl;
    	this.origUID = uid;
    	this.uid = uid;
    	this.origTag = tag;
    	myKey = block.getKey();
    	data = block.getRawData();
    	headers = block.getRawHeaders();
    	target = myKey.toNormalizedDouble();
    	pubKey = myKey.getPubKey();
    	if(pubKey == null)
    		throw new IllegalArgumentException("Must have pubkey to insert data!!");
    	// pubKey.fingerprint() is not the same as hash(pubKey.asBytes())). FIXME it should be!
    	byte[] pubKeyAsBytes = pubKey.asBytes();
    	pubKeyHash = SHA256.digest(pubKeyAsBytes);
    	this.block = block;
    	startTime = System.currentTimeMillis();
    	this.forkOnCacheable = forkOnCacheable;
    	this.preferInsert = preferInsert;
    	this.ignoreLowBackoff = ignoreLowBackoff;
    	this.realTimeFlag = realTimeFlag;
    	if(realTimeFlag)
    		searchTimeout = SEARCH_TIMEOUT_REALTIME;
    	else
    		searchTimeout = SEARCH_TIMEOUT_BULK;
    }

    void start() {
    	node.executor.execute(this, "SSKInsertSender for UID "+uid+" on "+node.getDarknetPortNumber()+" at "+System.currentTimeMillis());
    }
    
	public void run() {
	    freenet.support.Logger.OSThread.logPID(this);
        short origHTL = htl;
        origTag.startedSender();
        try {
        	realRun();
		} catch (OutOfMemoryError e) {
			OOMHandler.handleOOM(e);
            if(status == NOT_FINISHED)
            	finish(INTERNAL_ERROR, null);
        } catch (Throwable t) {
            Logger.error(this, "Caught "+t, t);
            if(status == NOT_FINISHED)
            	finish(INTERNAL_ERROR, null);
        } finally {
        	if(logMINOR) Logger.minor(this, "Finishing "+this);
            if(status == NOT_FINISHED)
            	finish(INTERNAL_ERROR, null);
            origTag.finishedSender();
        	if(forkedRequestTag != null)
        		forkedRequestTag.finishedSender();
        }
	}

	static final int MAX_HIGH_HTL_FAILURES = 5;
	
    private void realRun() {
        PeerNode next = null;
        // While in no-cache mode, we don't decrement HTL on a RejectedLoop or similar, but we only allow a limited number of such failures before RNFing.
        int highHTLFailureCount = 0;
        boolean starting = true;
        while(true) {
            /*
             * If we haven't routed to any node yet, decrement according to the source.
             * If we have, decrement according to the node which just failed.
             * Because:
             * 1) If we always decrement according to source then we can be at max or min HTL
             * for a long time while we visit *every* peer node. This is BAD!
             * 2) The node which just failed can be seen as the requestor for our purposes.
             */
            // Decrement at this point so we can DNF immediately on reaching HTL 0.
            boolean canWriteStorePrev = node.canWriteDatastoreInsert(htl);
            if((!starting) && (!canWriteStorePrev)) {
            	// We always decrement on starting a sender.
            	// However, after that, if our HTL is above the no-cache threshold,
            	// we do not want to decrement the HTL for trivial rejections (e.g. RejectedLoop),
            	// because we would end up caching data too close to the originator.
            	// So allow 5 failures and then RNF.
            	if(highHTLFailureCount++ >= MAX_HIGH_HTL_FAILURES) {
            		if(logMINOR) Logger.minor(this, "Too many failures at non-cacheable HTL");
                    finish(ROUTE_NOT_FOUND, null);
                    return;
            	}
            	if(logMINOR) Logger.minor(this, "Allowing failure "+highHTLFailureCount+" htl is still "+htl);
            } else {
                htl = node.decrementHTL(sentRequest ? next : source, htl);
                if(logMINOR) Logger.minor(this, "Decremented HTL to "+htl);
            }
            starting = false;
            if(htl == 0) {
                // Send an InsertReply back
        		if(!sentRequest)
        			origTag.setNotRoutedOnwards();
                finish(SUCCESS, null);
                return;
            }
            
            if( node.canWriteDatastoreInsert(htl) && (!canWriteStorePrev) && forkOnCacheable && forkedRequestTag == null) {
            	// FORK! We are now cacheable, and it is quite possible that we have already gone over the ideal sink nodes,
            	// in which case if we don't fork we will miss them, and greatly reduce the insert's reachability.
            	// So we fork: Create a new UID so we can go over the previous hops again if they happen to be good places to store the data.
            	
            	// Existing transfers will keep their existing UIDs, since they copied the UID in the constructor.
            	
            	uid = node.clientCore.makeUID();
            	forkedRequestTag = new InsertTag(true, InsertTag.START.REMOTE, source, realTimeFlag, uid, node);
            	forkedRequestTag.reassignToSelf();
            	forkedRequestTag.startedSender();
            	forkedRequestTag.unlockHandler();
            	Logger.normal(this, "FORKING SSK INSERT "+origUID+" to "+uid);
            	nodesRoutedTo.clear();
            	node.lockUID(uid, true, true, false, false, realTimeFlag, forkedRequestTag);
            }
            
            // Route it
            next = node.peers.closerPeer(forkedRequestTag == null ? source : null, nodesRoutedTo, target, true, node.isAdvancedModeEnabled(), -1, null,
			        null, htl, ignoreLowBackoff ? Node.LOW_BACKOFF : 0, source == null, realTimeFlag);
            
            if(next == null) {
                // Backtrack
        		if(!sentRequest)
        			origTag.setNotRoutedOnwards();
                finish(ROUTE_NOT_FOUND, null);
                return;
            }
            if(logMINOR) Logger.minor(this, "Routing insert to "+next);
            nodesRoutedTo.add(next);
            
            Message request = DMT.createFNPSSKInsertRequestNew(uid, htl, myKey);
            if(forkOnCacheable != Node.FORK_ON_CACHEABLE_DEFAULT) {
            	request.addSubMessage(DMT.createFNPSubInsertForkControl(forkOnCacheable));
            }
            if(ignoreLowBackoff != Node.IGNORE_LOW_BACKOFF_DEFAULT) {
            	request.addSubMessage(DMT.createFNPSubInsertIgnoreLowBackoff(ignoreLowBackoff));
            }
            if(preferInsert != Node.PREFER_INSERT_DEFAULT) {
            	request.addSubMessage(DMT.createFNPSubInsertPreferInsert(preferInsert));
            }
        	request.addSubMessage(DMT.createFNPRealTimeFlag(realTimeFlag));
            
            // Wait for ack or reject... will come before even a locally generated DataReply
            
            InsertTag thisTag = forkedRequestTag;
            if(forkedRequestTag == null) thisTag = origTag;
            
            thisTag.addRoutedTo(next, false);
            
            // Send to next node
            
            try {
            	next.sendSync(request, this, realTimeFlag);
			} catch (NotConnectedException e1) {
				if(logMINOR) Logger.minor(this, "Not connected to "+next);
				thisTag.removeRoutingTo(next);
				continue;
			} catch (SyncSendWaitedTooLongException e) {
				Logger.warning(this, "Failed to send request to "+next);
				thisTag.removeRoutingTo(next);
				continue;
			}
            sentRequest = true;
            
            Message msg = waitForAccepted(next, thisTag);
            
            if(msg == null) continue;
            
            if(logMINOR) Logger.minor(this, "Got Accepted on "+this);
            
            // Send the headers and data
            
            Message headersMsg = DMT.createFNPSSKInsertRequestHeaders(uid, headers, realTimeFlag);
            Message dataMsg = DMT.createFNPSSKInsertRequestData(uid, data, realTimeFlag);
            
            try {
				next.sendAsync(headersMsg, null, this);
				if(next.isOldFNP()) {
					next.sendThrottledMessage(dataMsg, data.length, this, SSKInsertHandler.DATA_INSERT_TIMEOUT, false, null);
				} else {
					next.sendSync(dataMsg, this, realTimeFlag);
					sentPayload(data.length);
				}
			} catch (NotConnectedException e1) {
				if(logMINOR) Logger.minor(this, "Not connected to "+next);
				thisTag.removeRoutingTo(next);
				continue;
			} catch (WaitedTooLongException e) {
				Logger.error(this, "Waited too long to send "+dataMsg+" to "+next+" on "+this);
				thisTag.removeRoutingTo(next);
				continue;
			} catch (SyncSendWaitedTooLongException e) {
				Logger.error(this, "Waited too long to send "+dataMsg+" to "+next+" on "+this);
				thisTag.removeRoutingTo(next);
				continue;
			} catch (PeerRestartedException e) {
				if(logMINOR) Logger.minor(this, "Peer restarted: "+next);
				thisTag.removeRoutingTo(next);
				continue;
			}
            
            // Do we need to send them the pubkey?
            
            if(msg.getBoolean(DMT.NEED_PUB_KEY)) {
            	Message pkMsg = DMT.createFNPSSKPubKey(uid, pubKey, realTimeFlag);
            	try {
            		next.sendSync(pkMsg, this, realTimeFlag);
            	} catch (NotConnectedException e) {
            		if(logMINOR) Logger.minor(this, "Node disconnected while sending pubkey: "+next);
					thisTag.removeRoutingTo(next);
            		continue;
            	} catch (SyncSendWaitedTooLongException e) {
            		Logger.warning(this, "Took too long to send pubkey to "+next+" on "+this);
					thisTag.removeRoutingTo(next);
            		continue;
				}
            	
            	// Wait for the SSKPubKeyAccepted
            	
            	// FIXME doubled the timeout because handling it properly would involve forking.
            	MessageFilter mf1 = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(ACCEPTED_TIMEOUT*2).setType(DMT.FNPSSKPubKeyAccepted);
            	
            	Message newAck;
				try {
					newAck = node.usm.waitFor(mf1, this);
				} catch (DisconnectedException e) {
					if(logMINOR) Logger.minor(this, "Disconnected from "+next);
					thisTag.removeRoutingTo(next);
					continue;
				}
            	
            	if(newAck == null) {
            		handleNoPubkeyAccepted(next, thisTag);
					// Try another peer
					continue;
            	}
            }
            
            // We have sent them the pubkey, and the data.
            // Wait for the response.
            
    		MessageFilter mf = makeSearchFilter(next, searchTimeout);
            
            while (true) {
				try {
					msg = node.usm.waitFor(mf, this);
				} catch (DisconnectedException e) {
					Logger.normal(this, "Disconnected from " + next
							+ " while waiting for InsertReply on " + this);
					thisTag.removeRoutingTo(next);
					break;
				}

				if (msg == null) {
					
					// First timeout.
					Logger.error(this, "Timeout waiting for reply after Accepted in "+this+" from "+next);
					next.localRejectedOverload("AfterInsertAcceptedTimeout", realTimeFlag);
					forwardRejectedOverload();
					finish(TIMED_OUT, next);
					
					// Wait for second timeout.
					while(true) {
						
						try {
							msg = node.usm.waitFor(mf, this);
						} catch (DisconnectedException e) {
							Logger.normal(this, "Disconnected from " + next
									+ " while waiting for InsertReply on " + this);
							thisTag.removeRoutingTo(next);
							return;
						}
						
						if(msg == null) {
							// Second timeout.
							Logger.error(this, "Fatal timeout waiting for reply after Accepted on "+this+" from "+next);
							next.fatalTimeout(thisTag, false);
							return;
						}
						
						DO action = handleMessage(msg, next, thisTag);
						
						if(action == DO.FINISHED)
							return;
						else if(action == DO.NEXT_PEER) {
							thisTag.removeRoutingTo(next);
							return; // Don't try others
						}
						// else if(action == DO.WAIT) continue;
						
					}
				}
				
				DO action = handleMessage(msg, next, thisTag);
				
				if(action == DO.FINISHED)
					return;
				else if(action == DO.NEXT_PEER)
					break;
				// else if(action == DO.WAIT) continue;
            }
        }
    }
    
    private void handleNoPubkeyAccepted(PeerNode next, InsertTag thisTag) {
    	// FIXME implementing two stage timeout would likely involve forking at this point.
    	// The problem is the peer has now got everything it needs to run the insert!
    	
		// Try to propagate back to source
		Logger.error(this, "Timeout waiting for FNPSSKPubKeyAccepted on "+next);
		next.localRejectedOverload("Timeout2", realTimeFlag);
		// This is a local timeout, they should send it immediately.
		forwardRejectedOverload();
		next.fatalTimeout(thisTag, false);
	}

	private MessageFilter makeSearchFilter(PeerNode next,
			int searchTimeout) {
        /** What are we waiting for now??:
         * - FNPRouteNotFound - couldn't exhaust HTL, but send us the 
         *   data anyway please
         * - FNPInsertReply - used up all HTL, yay
         * - FNPRejectOverload - propagating an overload error :(
         * - FNPDataFound - target already has the data, and the data is
         *   an SVK/SSK/KSK, therefore could be different to what we are
         *   inserting.
         * - FNPDataInsertRejected - the insert was invalid
         */
        
        MessageFilter mfInsertReply = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPInsertReply);
        MessageFilter mfRejectedOverload = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPRejectedOverload);
        MessageFilter mfRouteNotFound = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPRouteNotFound);
        MessageFilter mfDataInsertRejected = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPDataInsertRejected);
        MessageFilter mfSSKDataFoundHeaders = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPSSKDataFoundHeaders);
        
        return mfRouteNotFound.or(mfInsertReply.or(mfRejectedOverload.or(mfDataInsertRejected.or(mfSSKDataFoundHeaders))));
	}

	private DO handleMessage(Message msg, PeerNode next, InsertTag thisTag) {
		if (msg.getSpec() == DMT.FNPRejectedOverload) {
			if(handleRejectedOverload(msg, next, thisTag)) return DO.NEXT_PEER;
			else return DO.WAIT;
		}

		if (msg.getSpec() == DMT.FNPRouteNotFound) {
			handleRouteNotFound(msg, next, thisTag);
			// Finished as far as this node is concerned
			return DO.NEXT_PEER;
		}

		if (msg.getSpec() == DMT.FNPDataInsertRejected) {
			handleDataInsertRejected(msg, next, thisTag);
			return DO.NEXT_PEER; // What else can we do?
		}
		
		if(msg.getSpec() == DMT.FNPSSKDataFoundHeaders) {
			return handleSSKDataFoundHeaders(msg, next, thisTag);
		}
		
		if (msg.getSpec() != DMT.FNPInsertReply) {
			Logger.error(this, "Unknown reply: " + msg);
			finish(INTERNAL_ERROR, next);
			return DO.FINISHED;
		}
				
		// Our task is complete
		next.successNotOverload(realTimeFlag);
		finish(SUCCESS, next);
		return DO.FINISHED;

    }

    private enum DO {
    	FINISHED,
    	WAIT,
    	NEXT_PEER
    }
    
	private final int TIMEOUT_AFTER_ACCEPTEDREJECTED_TIMEOUT = 60*1000;

	private void handleAcceptedRejectedTimeout(final PeerNode next, final InsertTag tag) {
		// It could still be running. So the timeout is fatal to the node.
		// This is a WARNING not an ERROR because it's possible that the problem is we simply haven't been able to send the message yet, because we don't use sendSync().
		// FIXME use a callback to rule this out and log an ERROR.
		Logger.warning(this, "Timeout awaiting Accepted/Rejected "+this+" to "+next);
		// The node didn't accept the request. So we don't need to send them the data.
		// However, we do need to wait a bit longer to try to postpone the fatalTimeout().
		// Somewhat intricate logic to try to avoid fatalTimeout() if at all possible.
		MessageFilter mf = makeAcceptedRejectedFilter(next, TIMEOUT_AFTER_ACCEPTEDREJECTED_TIMEOUT);
		try {
			node.usm.addAsyncFilter(mf, new SlowAsyncMessageFilterCallback() {

				public void onMatched(Message m) {
					if(m.getSpec() == DMT.FNPRejectedLoop ||
							m.getSpec() == DMT.FNPRejectedOverload) {
						// Ok.
						tag.removeRoutingTo(next);
					} else {
						assert(m.getSpec() == DMT.FNPSSKAccepted);
						if(logMINOR) Logger.minor(this, "Forked timed out insert but not going to send DataInsert on "+SSKInsertSender.this+" to "+next);
						// We are not going to send the DataInsert.
						// We have moved on, and we don't want inserts to fork unnecessarily.
			            MessageFilter mfDataInsertRejected = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPDataInsertRejected);
			            try {
							node.usm.addAsyncFilter(mfDataInsertRejected, new AsyncMessageFilterCallback() {

								public void onMatched(Message m) {
									// Cool.
									tag.removeRoutingTo(next);
								}

								public boolean shouldTimeout() {
									return false;
								}

								public void onTimeout() {
									// Grrr!
									Logger.error(this, "Fatal timeout awaiting FNPRejectedTimeout on insert to "+next+" for "+SSKInsertSender.this);
									next.fatalTimeout(tag, false);
								}

								public void onDisconnect(PeerContext ctx) {
									tag.removeRoutingTo(next);
								}

								public void onRestarted(PeerContext ctx) {
									tag.removeRoutingTo(next);
								}
								
							}, SSKInsertSender.this);
						} catch (DisconnectedException e) {
							tag.removeRoutingTo(next);
						}
					}
				}

				public boolean shouldTimeout() {
					return false;
				}

				public void onTimeout() {
					Logger.error(this, "Fatal: No Accepted/Rejected for "+SSKInsertSender.this);
					next.fatalTimeout(tag, false);
				}

				public void onDisconnect(PeerContext ctx) {
					tag.removeRoutingTo(next);
				}

				public void onRestarted(PeerContext ctx) {
					tag.removeRoutingTo(next);
				}

				public int getPriority() {
					return NativeThread.NORM_PRIORITY;
				}
				
			}, this);
		} catch (DisconnectedException e) {
			tag.removeRoutingTo(next);
		}
	}

    /** @return True if fatal and we should try another node, false if just relayed so 
     * we should wait for more responses. */
    private boolean handleRejectedOverload(Message msg, PeerNode next, InsertTag thisTag) {
		// Probably non-fatal, if so, we have time left, can try next one
		if (msg.getBoolean(DMT.IS_LOCAL)) {
			next.localRejectedOverload("ForwardRejectedOverload4", realTimeFlag);
			if(logMINOR) Logger.minor(this,
					"Local RejectedOverload, moving on to next peer");
			// Give up on this one, try another
        	next.noLongerRoutingTo(thisTag, false);
			return true;
		} else {
			forwardRejectedOverload();
		}
		return false; // Wait for any further response
	}

	private void handleRouteNotFound(Message msg, PeerNode next, InsertTag thisTag) {
		if(logMINOR) Logger.minor(this, "Rejected: RNF");
		short newHtl = msg.getShort(DMT.HTL);
		if (htl > newHtl)
			htl = newHtl;
		next.successNotOverload(realTimeFlag);
    	next.noLongerRoutingTo(thisTag, false);
	}

	private void handleDataInsertRejected(Message msg, PeerNode next, InsertTag thisTag) {
		next.successNotOverload(realTimeFlag);
		short reason = msg.getShort(DMT.DATA_INSERT_REJECTED_REASON);
		if(logMINOR) Logger.minor(this, "DataInsertRejected: " + reason);
		if (reason == DMT.DATA_INSERT_REJECTED_VERIFY_FAILED) {
			if (fromStore) {
				// That's odd...
				Logger.error(this,"Verify failed on next node "
						+ next + " for DataInsert but we were sending from the store!");
			}
		}
		Logger.error(this, "SSK insert rejected! Reason="
				+ DMT.getDataInsertRejectedReason(reason));
    	next.noLongerRoutingTo(thisTag, false);
	}

	/** @return True if we got new data and are propagating it. False if something failed
     * and we need to try the next node. */
	private DO handleSSKDataFoundHeaders(Message msg, PeerNode next, InsertTag thisTag) {
		
		/**
		 * Data was already on node, and was NOT equal to what we sent. COLLISION!
		 * 
		 * We can either accept the old data or the new data.
		 * OLD DATA:
		 * - KSK-based stuff is usable. Well, somewhat; a node could spoof KSKs on
		 * receiving an insert, (if it knows them in advance), but it cannot just 
		 * start inserts to overwrite old SSKs.
		 * - You cannot "update" an SSK.
		 * NEW DATA:
		 * - KSK-based stuff not usable. (Some people think this is a good idea!).
		 * - Illusion of updatability. (VERY BAD IMHO, because it's not really
		 * updatable... FIXME implement TUKs; would determine latest version based
		 * on version number, and propagate on request with a certain probability or
		 * according to time. However there are good arguments to do updating at a
		 * higher level (e.g. key bottleneck argument), and TUKs should probably be 
		 * distinct from SSKs.
		 * 
		 * For now, accept the "old" i.e. preexisting data.
		 */
		Logger.normal(this, "Got collision on "+myKey+" ("+uid+") sending to "+next.getPeer());
		
		headers = ((ShortBuffer) msg.getObject(DMT.BLOCK_HEADERS)).getData();
		// Wait for the data
		MessageFilter mfData = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(RequestSender.FETCH_TIMEOUT_REALTIME).setType(DMT.FNPSSKDataFoundData);
		Message dataMessage;
		try {
			dataMessage = node.usm.waitFor(mfData, this);
		} catch (DisconnectedException e) {
			if(logMINOR)
				Logger.minor(this, "Disconnected: "+next+" getting datareply for "+this);
			thisTag.removeRoutingTo(next);
			return DO.NEXT_PEER;
		}
		if(dataMessage == null) {
			Logger.error(this, "Got headers but not data for datareply for insert from "+this);
			thisTag.removeRoutingTo(next);
			return DO.NEXT_PEER;
		}
		// collided, overwrite data with remote data
		try {
			data = ((ShortBuffer) dataMessage.getObject(DMT.DATA)).getData();
			block = new SSKBlock(data, headers, block.getKey(), false);
			
			synchronized(this) {
				hasRecentlyCollided = true;
				hasCollided = true;
				notifyAll();
			}
			
			// The node will now propagate the new data. There is no need to move to the next node yet.
			return DO.WAIT;
		} catch (SSKVerifyException e) {
			Logger.error(this, "Invalid SSK from remote on collusion: " + this + ":" +block);
			finish(INTERNAL_ERROR, next);
			return DO.FINISHED;
		}
	}

	private Message waitForAccepted(PeerNode next, InsertTag thisTag) {
		Message msg;
		
		thisTag.handlingTimeout(next);
		
		MessageFilter mf = makeAcceptedRejectedFilter(next, ACCEPTED_TIMEOUT);

        while (true) {
        	
			try {
				msg = node.usm.waitFor(mf, this);
			} catch (DisconnectedException e) {
				Logger.normal(this, "Disconnected from " + next
						+ " while waiting for Accepted");
            	next.noLongerRoutingTo(thisTag, false);
				return null;
			}
			
			if (msg == null) {
				// Terminal overload
				// Try to propagate back to source
				if(logMINOR) Logger.minor(this, "Timeout");
				next.localRejectedOverload("Timeout", realTimeFlag);
				forwardRejectedOverload();
				// It could still be running. So the timeout is fatal to the node.
				handleAcceptedRejectedTimeout(next, thisTag);
				return null;
			}
			
			if (msg.getSpec() == DMT.FNPRejectedOverload) {
				// Non-fatal - probably still have time left
				if (msg.getBoolean(DMT.IS_LOCAL)) {
					next.localRejectedOverload("ForwardRejectedOverload3", realTimeFlag);
					if(logMINOR) Logger.minor(this, "Local RejectedOverload, moving on to next peer");
					// Give up on this one, try another
	            	next.noLongerRoutingTo(thisTag, false);
					return null;
				} else {
					if(logMINOR) Logger.minor(this, "Relaying non-local rejected overload on "+this);
					forwardRejectedOverload();
				}
				continue;
			}
			
			if (msg.getSpec() == DMT.FNPRejectedLoop) {
				if(logMINOR) Logger.minor(this, "Rejected loop on "+this+" from "+next);
				next.successNotOverload(realTimeFlag);
				// Loop - we don't want to send the data to this one
            	next.noLongerRoutingTo(thisTag, false);
				return null;
			}
			
			if (msg.getSpec() != DMT.FNPSSKAccepted) {
				Logger.error(this,
						"Unexpected message waiting for SSKAccepted: "
								+ msg);
            	next.noLongerRoutingTo(thisTag, false);
				return null;
			}
			// Otherwise is an FNPSSKAccepted
			return msg;
        }
        
	}

	private MessageFilter makeAcceptedRejectedFilter(PeerNode next,
			int acceptedTimeout) {
        /*
         * Because messages may be re-ordered, it is
         * entirely possible that we get a non-local RejectedOverload,
         * followed by an Accepted. So we must loop here.
         */
        
        MessageFilter mfAccepted = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(acceptedTimeout).setType(DMT.FNPSSKAccepted);
        MessageFilter mfRejectedLoop = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(acceptedTimeout).setType(DMT.FNPRejectedLoop);
        MessageFilter mfRejectedOverload = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(acceptedTimeout).setType(DMT.FNPRejectedOverload);
        return mfAccepted.or(mfRejectedLoop.or(mfRejectedOverload));
	}

	private boolean hasForwardedRejectedOverload;
    
    synchronized boolean receivedRejectedOverload() {
    	return hasForwardedRejectedOverload;
    }
    
    /** Forward RejectedOverload to the request originator.
     * DO NOT CALL if have a *local* RejectedOverload.
     */
    private synchronized void forwardRejectedOverload() {
    	if(hasForwardedRejectedOverload) return;
    	hasForwardedRejectedOverload = true;
   		notifyAll();
	}
    
    private void finish(int code, PeerNode next) {
    	if(logMINOR) Logger.minor(this, "Finished: "+getStatusString(code)+" on "+this+" from "+(next == null ? "(null)" : next.shortToString()), new Exception("debug"));
    	
    	if(next != null) {
    		if(origTag != null) next.noLongerRoutingTo(origTag, false);
    		if(forkedRequestTag != null) next.noLongerRoutingTo(forkedRequestTag, false);
    	}
    	
    	synchronized(this) {
    		if(status != NOT_FINISHED && status != TIMED_OUT)
    			throw new IllegalStateException("finish() called with "+code+" when was already "+status);
    		
    		if((code == ROUTE_NOT_FOUND) && !sentRequest)
    			code = ROUTE_REALLY_NOT_FOUND;
    		
    		if(status != TIMED_OUT) {
    			status = code;
    			notifyAll();
    		}
        }

        if(code == SUCCESS && next != null)
        	next.onSuccess(true, true);
        
        if(logMINOR) Logger.minor(this, "Set status code: "+getStatusString());
        // Nothing to wait for, no downstream transfers, just exit.
    }

    public synchronized int getStatus() {
        return status;
    }
    
    public synchronized short getHTL() {
        return htl;
    }

    public synchronized String getStatusString() {
    	return getStatusString(status);
    }
    
    /**
     * @return The current status as a string
     */
    public static String getStatusString(int status) {
        if(status == SUCCESS)
            return "SUCCESS";
        if(status == ROUTE_NOT_FOUND)
            return "ROUTE NOT FOUND";
        if(status == NOT_FINISHED)
            return "NOT FINISHED";
        if(status == INTERNAL_ERROR)
        	return "INTERNAL ERROR";
        if(status == TIMED_OUT)
        	return "TIMED OUT";
        if(status == GENERATED_REJECTED_OVERLOAD)
        	return "GENERATED REJECTED OVERLOAD";
        if(status == ROUTE_REALLY_NOT_FOUND)
        	return "ROUTE REALLY NOT FOUND";
        return "UNKNOWN STATUS CODE: "+status;
    }

	public boolean sentRequest() {
		return sentRequest;
	}
	
	public synchronized boolean hasRecentlyCollided() {
		boolean status = hasRecentlyCollided;
		hasRecentlyCollided = false;
		return status;
	}
	
	public boolean hasCollided() {
		return hasCollided;
	}
	
	public byte[] getPubkeyHash() {
		return headers;
	}

	public byte[] getHeaders() {
		return headers;
	}
	
	public byte[] getData() {
		return data;
	}

	public SSKBlock getBlock() {
		return block;
	}

	public long getUID() {
		return uid;
	}

	private final Object totalBytesSync = new Object();
	private int totalBytesSent;
	
	public void sentBytes(int x) {
		synchronized(totalBytesSync) {
			totalBytesSent += x;
		}
		node.nodeStats.insertSentBytes(true, x);
	}
	
	public int getTotalSentBytes() {
		synchronized(totalBytesSync) {
			return totalBytesSent;
		}
	}
	
	private int totalBytesReceived;
	
	public void receivedBytes(int x) {
		synchronized(totalBytesSync) {
			totalBytesReceived += x;
		}
		node.nodeStats.insertReceivedBytes(true, x);
	}
	
	public int getTotalReceivedBytes() {
		synchronized(totalBytesSync) {
			return totalBytesReceived;
		}
	}

	public void sentPayload(int x) {
		node.sentPayload(x);
		node.nodeStats.insertSentBytes(true, -x);
	}

	public int getPriority() {
		return NativeThread.HIGH_PRIORITY;
	}

	@Override
	public String toString() {
		return "SSKInsertSender:" + myKey+":"+uid;
	}

	public PeerNode[] getRoutedTo() {
		return this.nodesRoutedTo.toArray(new PeerNode[nodesRoutedTo.size()]);
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.config;

import java.io.IOException;
import java.util.LinkedHashMap;

/** Global configuration object for a node. SubConfig's register here.
 * Handles writing to a file etc.
 */
public class Config {
	public static enum RequestType {
		CURRENT_SETTINGS, DEFAULT_SETTINGS, SORT_ORDER, EXPERT_FLAG, FORCE_WRITE_FLAG, SHORT_DESCRIPTION, LONG_DESCRIPTION, DATA_TYPE
	};

	protected final LinkedHashMap<String, SubConfig> configsByPrefix;
	
	public Config() {
		configsByPrefix = new LinkedHashMap<String, SubConfig>();
	}
	
	public void register(SubConfig sc) {
		synchronized(this) {
			if(configsByPrefix.containsKey(sc.prefix))
				throw new IllegalArgumentException("Already registered "+sc.prefix+": "+sc);
			configsByPrefix.put(sc.prefix, sc);
		}
	}
	
	/** Write current config to disk 
	 * @throws IOException */
	public void store() {
		// Do nothing
	}

	/** Finished initialization */
	public void finishedInit() {
		// Do nothing
	}

	public void onRegister(SubConfig config, Option<?> o) {
		// Do nothing
	}

	/** Fetch all the SubConfig's. Used by user-facing config thingies. */
	public synchronized SubConfig[] getConfigs() {
		return configsByPrefix.values().toArray(new SubConfig[configsByPrefix.size()]);
	}
	
	public synchronized SubConfig get(String subConfig){
		return configsByPrefix.get(subConfig);
	}
}
package freenet.node.simulator;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.OutputStream;
import java.io.PrintStream;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.List;
import java.util.Locale;
import java.util.TimeZone;

import freenet.client.ClientMetadata;
import freenet.client.FetchException;
import freenet.client.HighLevelSimpleClient;
import freenet.client.InsertBlock;
import freenet.client.InsertException;
import freenet.crypt.RandomSource;
import freenet.keys.FreenetURI;
import freenet.node.Node;
import freenet.node.NodeStarter;
import freenet.node.Version;
import freenet.support.Fields;
import freenet.support.Logger;
import freenet.support.PooledExecutor;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;
import freenet.support.io.FileUtil;

/**
 * Push / Pull test over long period of time
 * 
 * Unlike LongTermPushPullTest, this only inserts one key per day. That key
 * is then re-pulled at increasing intervals.
 */
public class LongTermPushRepullTest {
	private static final int TEST_SIZE = 64 * 1024;

	private static final int EXIT_NO_SEEDNODES = 257;
	private static final int EXIT_FAILED_TARGET = 258;
	private static final int EXIT_THREW_SOMETHING = 261;

	private static final int DARKNET_PORT1 = 5010;
	private static final int OPENNET_PORT1 = 5011;
	private static final int DARKNET_PORT2 = 5012;
	private static final int OPENNET_PORT2 = 5013;

	private static final int MAX_N = 8;

	private static final DateFormat dateFormat = new SimpleDateFormat("yyyy.MM.dd", Locale.US);
	static {
		dateFormat.setTimeZone(TimeZone.getTimeZone("GMT"));
	}
	private static final Calendar today = Calendar.getInstance(TimeZone.getTimeZone("GMT"));

	public static void main(String[] args) {
		if (args.length != 1) {
			System.err.println("Usage: java freenet.node.simulator.LongTermPushPullTest <unique identifier>");
			System.exit(1);
		}
		String uid = args[0];

		List<String> csvLine = new ArrayList<String>(3 + 2 * MAX_N);
		System.out.println("DATE:" + dateFormat.format(today.getTime()));
		csvLine.add(dateFormat.format(today.getTime()));

		System.out.println("Version:" + Version.buildNumber());
		csvLine.add(String.valueOf(Version.buildNumber()));

		int exitCode = 0;
		Node node = null;
		Node node2 = null;
		try {
			final File dir = new File("longterm-push-pull-test-" + uid);
			FileUtil.removeAll(dir);
			RandomSource random = NodeStarter.globalTestInit(dir.getPath(), false, LogLevel.ERROR, "", false);
			File seednodes = new File("seednodes.fref");
			if (!seednodes.exists() || seednodes.length() == 0 || !seednodes.canRead()) {
				System.err.println("Unable to read seednodes.fref, it doesn't exist, or is empty");
				System.exit(EXIT_NO_SEEDNODES);
			}

			final File innerDir = new File(dir, Integer.toString(DARKNET_PORT1));
			innerDir.mkdir();
			FileInputStream fis = new FileInputStream(seednodes);
			FileUtil.writeTo(fis, new File(innerDir, "seednodes.fref"));
			fis.close();

			// Create one node
			node = NodeStarter.createTestNode(DARKNET_PORT1, OPENNET_PORT1, dir.getPath(), false, Node.DEFAULT_MAX_HTL,
			        0, random, new PooledExecutor(), 1000, 4 * 1024 * 1024, true, true, true, true, true, true, true,
			        12 * 1024, true, true, false, false, null);
			Logger.getChain().setThreshold(LogLevel.ERROR);

			// Start it
			node.start(true);
			long t1 = System.currentTimeMillis();
			if (!TestUtil.waitForNodes(node)) {
				exitCode = EXIT_FAILED_TARGET;
				return;
			}
				
			long t2 = System.currentTimeMillis();
			System.out.println("SEED-TIME:" + (t2 - t1));
			csvLine.add(String.valueOf(t2 - t1));

			// Push one block only.
			
			Bucket data = randomData(node);
			HighLevelSimpleClient client = node.clientCore.makeClient((short) 0);
			FreenetURI uri = new FreenetURI("KSK@" + uid + "-" + dateFormat.format(today.getTime()));
			System.out.println("PUSHING " + uri);
			
			try {
				InsertBlock block = new InsertBlock(data, new ClientMetadata(), uri);
				t1 = System.currentTimeMillis();
				client.insert(block, false, null);
				t2 = System.currentTimeMillis();
				
				System.out.println("PUSH-TIME-" + ":" + (t2 - t1));
				csvLine.add(String.valueOf(t2 - t1));
			} catch (InsertException e) {
				e.printStackTrace();
				csvLine.add("N/A");
			}

			data.free();

			node.park();

			// Node 2
			File innerDir2 = new File(dir, Integer.toString(DARKNET_PORT2));
			innerDir2.mkdir();
			fis = new FileInputStream(seednodes);
			FileUtil.writeTo(fis, new File(innerDir2, "seednodes.fref"));
			fis.close();
			node2 = NodeStarter.createTestNode(DARKNET_PORT2, OPENNET_PORT2, dir.getPath(), false,
			        Node.DEFAULT_MAX_HTL, 0, random, new PooledExecutor(), 1000, 5 * 1024 * 1024, true, true, true,
			        true, true, true, true, 12 * 1024, false, true, false, false, null);
			node2.start(true);

			t1 = System.currentTimeMillis();
			if (!TestUtil.waitForNodes(node2)) {
				exitCode = EXIT_FAILED_TARGET;
				return;
			}
			t2 = System.currentTimeMillis();
			System.out.println("SEED-TIME:" + (t2 - t1));
			csvLine.add(String.valueOf(t2 - t1));

			// PULL N+1 BLOCKS
			for (int i = 0; i <= MAX_N; i++) {
				client = node2.clientCore.makeClient((short) 0);
				Calendar targetDate = (Calendar) today.clone();
				targetDate.add(Calendar.DAY_OF_MONTH, -((1 << i) - 1));

				uri = new FreenetURI("KSK@" + uid + "-" + dateFormat.format(targetDate.getTime()));
				System.out.println("PULLING " + uri);

				try {
					t1 = System.currentTimeMillis();
					client.fetch(uri);
					t2 = System.currentTimeMillis();

					System.out.println("PULL-TIME-" + i + ":" + (t2 - t1));
					csvLine.add(String.valueOf(t2 - t1));
				} catch (FetchException e) {
					if (e.getMode() != FetchException.ALL_DATA_NOT_FOUND
					        && e.getMode() != FetchException.DATA_NOT_FOUND)
						e.printStackTrace();
					csvLine.add(FetchException.getShortMessage(e.getMode()));
				}
			}
		} catch (Throwable t) {
			t.printStackTrace();
			exitCode = EXIT_THREW_SOMETHING;
		} finally {
			try {
				if (node != null)
					node.park();
			} catch (Throwable t1) {
			}
			try {
				if (node2 != null)
					node2.park();
			} catch (Throwable t1) {
			}

			try {
				File file = new File(uid + ".csv");
				FileOutputStream fos = new FileOutputStream(file, true);
				PrintStream ps = new PrintStream(fos);

				ps.println(Fields.commaList(csvLine.toArray()));

				ps.close();
				fos.close();
			} catch (IOException e) {
				e.printStackTrace();
				exitCode = EXIT_THREW_SOMETHING;
			}
			
			System.exit(exitCode);
		}
	}

	private static Bucket randomData(Node node) throws IOException {
		Bucket data = node.clientCore.tempBucketFactory.makeBucket(TEST_SIZE);
		OutputStream os = data.getOutputStream();
		byte[] buf = new byte[4096];
		for (long written = 0; written < TEST_SIZE;) {
			node.fastWeakRandom.nextBytes(buf);
			int toWrite = (int) Math.min(TEST_SIZE - written, buf.length);
			os.write(buf, 0, toWrite);
			written += toWrite;
		}
		os.close();
		return data;
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.keys;

/**
 * @author amphibian
 * 
 * Exception thrown when a CHK doesn't verify.
 */
public class CHKVerifyException extends KeyVerifyException {
	private static final long serialVersionUID = -1;

    public CHKVerifyException() {
        super();
    }

    public CHKVerifyException(String message) {
        super(message);
    }

    public CHKVerifyException(String message, Throwable cause) {
        super(message, cause);
    }

    public CHKVerifyException(Throwable cause) {
        super(cause);
    }

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support;

/**
 * Contains uptime statistics.
 *
 * @author Artefact2
 */
public class UptimeContainer {
	public long creationTime = 0;
	public long totalUptime = 0;

	@Override
	public boolean equals(Object o) {
	if(o.getClass() == UptimeContainer.class) {
		UptimeContainer oB = (UptimeContainer) o;
		return (oB.creationTime == this.creationTime) &&
			(oB.totalUptime == this.totalUptime);
		} else return false;
	}

	@Override
	public int hashCode() {
		int hash = 7;
		hash = 29 * hash + (int) (this.creationTime ^ (this.creationTime >>> 32));
		hash = 29 * hash + (int) (this.totalUptime ^ (this.totalUptime >>> 32));
		return hash;
	}
}
package freenet.support;

import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Set;
import java.util.regex.Pattern;

public class HTMLNode implements XMLCharacterClasses {
	
	private static final Pattern namePattern = Pattern.compile("^[" + NAME + "]*$");
	private static final Pattern simpleNamePattern = Pattern.compile("^[A-Za-z][A-Za-z0-9]*$");
	public static HTMLNode STRONG = new HTMLNode("strong").setReadOnly();

	protected final String name;
	
	private boolean readOnly;
	
	public HTMLNode setReadOnly() {
		readOnly = true;
		return this;
	}

	/** Text to be inserted between tags, or possibly raw HTML. Only non-null if name
	 * is "#" (= text) or "%" (= raw HTML). Otherwise the constructor will allocate a
	 * separate child node to contain it. */
	private String content;

	private final Map<String, String> attributes = new HashMap<String, String>();

	protected final List<HTMLNode> children = new ArrayList<HTMLNode>();

	public HTMLNode(String name) {
		this(name, null);
	}

	public HTMLNode(String name, String content) {
		this(name, (String[]) null, (String[]) null, content);
	}

	public HTMLNode(String name, String attributeName, String attributeValue) {
		this(name, attributeName, attributeValue, null);
	}

	public HTMLNode(String name, String attributeName, String attributeValue, String content) {
		this(name, new String[] { attributeName }, new String[] { attributeValue }, content);
	}

	public HTMLNode(String name, String[] attributeNames, String[] attributeValues) {
		this(name, attributeNames, attributeValues, null);
	}

	protected HTMLNode(HTMLNode node, boolean clearReadOnly) {
		attributes.putAll(node.attributes);
		children.addAll(node.children);
		content = node.content;
		name = node.name;
		if(clearReadOnly)
			readOnly = false;
		else
			readOnly = node.readOnly;
	}
	
	public HTMLNode clone() {
		return new HTMLNode(this, true);
	}
	
	protected boolean checkNamePattern(String str) {		
		return simpleNamePattern.matcher(str).matches() || namePattern.matcher(str).matches();
	}
	
	public HTMLNode(String name, String[] attributeNames, String[] attributeValues, String content) {
		if ((name == null) || (!"#".equals(name) && !"%".equals(name) && !checkNamePattern(name))) {
			throw new IllegalArgumentException("element name is not legal");
		}
		if ((attributeNames != null) && (attributeValues != null)) {
			if (attributeNames.length != attributeValues.length) {
				throw new IllegalArgumentException("attribute names and values differ in length");
			}
			for (int attributeIndex = 0, attributeCount = attributeNames.length; attributeIndex < attributeCount; attributeIndex++) {
				if ((attributeNames[attributeIndex] == null) || !checkNamePattern(attributeNames[attributeIndex])) {
					throw new IllegalArgumentException("attributeName is not legal");
				}
				addAttribute(attributeNames[attributeIndex], attributeValues[attributeIndex]);
			}
		}
		this.name = name.toLowerCase(Locale.ENGLISH);
		if (content != null && !("#").equals(name)&& !("%").equals(name)) {
			addChild(new HTMLNode("#", content));
			this.content = null;
		} else
			this.content = content;
	}

	/**
	 * @return the content
	 */
	public String getContent() {
		return content;
	}

	public void addAttribute(String attributeName, String attributeValue) {
		if(readOnly)
			throw new IllegalArgumentException("Read only");
		if (attributeName == null)
			throw new IllegalArgumentException("Cannot add an attribute with a null name");
		if (attributeValue == null)
			throw new IllegalArgumentException("Cannot add an attribute with a null value");
		attributes.put(attributeName, attributeValue);
	}

	public Map<String, String> getAttributes() {
		return Collections.unmodifiableMap(attributes);
	}

	public String getAttribute(String attributeName) {
		return attributes.get(attributeName);
	}

	public HTMLNode addChild(HTMLNode childNode) {
		if(readOnly)
			throw new IllegalArgumentException("Read only");
		if (childNode == null) throw new NullPointerException();
		//since an efficient algorithm to check the loop presence 
		//is not present, at least it checks if we are trying to
		//addChild the node itself as a child
		if (childNode == this)	
			throw new IllegalArgumentException("A HTMLNode cannot be child of himself");
		if (children.contains(childNode))
			throw new IllegalArgumentException("Cannot add twice the same HTMLNode as child");
		children.add(childNode);
		return childNode;
	}
	
	public void addChildren(HTMLNode[] childNodes) {
		if(readOnly)
			throw new IllegalArgumentException("Read only");
		for (int i = 0, c = childNodes.length; i < c; i++) {
			addChild(childNodes[i]);
		}
	}

	public HTMLNode addChild(String nodeName) {
		return addChild(nodeName, null);
	}

	public HTMLNode addChild(String nodeName, String content) {
		return addChild(nodeName, (String[]) null, (String[]) null, content);
	}

	public HTMLNode addChild(String nodeName, String attributeName, String attributeValue) {
		return addChild(nodeName, attributeName, attributeValue, null);
	}

	public HTMLNode addChild(String nodeName, String attributeName, String attributeValue, String content) {
		return addChild(nodeName, new String[] { attributeName }, new String[] { attributeValue }, content);
	}

	public HTMLNode addChild(String nodeName, String[] attributeNames, String[] attributeValues) {
		return addChild(nodeName, attributeNames, attributeValues, null);
	}

	public HTMLNode addChild(String nodeName, String[] attributeNames, String[] attributeValues, String content) {
		return addChild(new HTMLNode(nodeName, attributeNames, attributeValues, content));
	}

	/**
	 * Returns the name of the first "real" tag found in the hierarchy below
	 * this node.
	 * 
	 * @return The name of the first "real" tag, or <code>null</code> if no
	 *         "real" tag could be found
	 */
	public String getFirstTag() {
		if (!"#".equals(name)) {
			return name;
		}
		for (int childIndex = 0, childCount = children.size(); childIndex < childCount; childIndex++) {
			HTMLNode childNode = children.get(childIndex);
			String tag = childNode.getFirstTag();
			if (tag != null) {
				return tag;
			}
		}
		return null;
	}

	public String generate() {
		StringBuilder tagBuffer = new StringBuilder();
		return generate(tagBuffer).toString();
	}

	public StringBuilder generate(StringBuilder tagBuffer) {
		if("#".equals(name)) {
			if(content != null) {
				HTMLEncoder.encodeToBuffer(content, tagBuffer);
				return tagBuffer;
			}
			
			for(int childIndex = 0, childCount = children.size(); childIndex < childCount; childIndex++) {
				HTMLNode childNode = children.get(childIndex);
				childNode.generate(tagBuffer);
			}
			return tagBuffer;
		}
		// Perhaps this should be something else, but since I don't know if '#' was not just arbitrary chosen, I'll just pick '%'
		// This allows non-encoded text to be appended to the tag buffer
		if ("%".equals(name)) {
			tagBuffer.append(content);
			return tagBuffer;
		}
		tagBuffer.append('<').append(name);
		Set<Map.Entry<String, String>> attributeSet = attributes.entrySet();
		for (Map.Entry<String, String> attributeEntry : attributeSet) {
			String attributeName = attributeEntry.getKey();
			String attributeValue = attributeEntry.getValue();
			tagBuffer.append(' ');
			HTMLEncoder.encodeToBuffer(attributeName, tagBuffer);
			tagBuffer.append("=\"");
			HTMLEncoder.encodeToBuffer(attributeValue, tagBuffer);
			tagBuffer.append('"');
		}
		if (children.size() == 0) {
			if(content==null){
				if ("textarea".equals(name) || ("div").equals(name) || ("a").equals(name) || ("script").equals(name)) {
					tagBuffer.append("></");
					tagBuffer.append(name);
					tagBuffer.append('>');
				} else {
					tagBuffer.append(" />");
				}
			}else{
				tagBuffer.append(">"+content+"</"+name+">");
			}
			
		} else {
			if(("div").equals(name) || ("form").equals(name) || ("input").equals(name) || ("script").equals(name) || ("table").equals(name) || ("tr").equals(name) || ("td").equals(name)) {
				tagBuffer.append('\n');
			}
			tagBuffer.append('>');
			for (int childIndex = 0, childCount = children.size(); childIndex < childCount; childIndex++) {
				HTMLNode childNode = children.get(childIndex);
				childNode.generate(tagBuffer);
			}
			tagBuffer.append("</");
			tagBuffer.append(name);
			if(("div").equals(name)|| ("form").equals(name)|| ("input").equals(name)|| ("li").equals(name)|| ("option").equals(name)|| ("script").equals(name)|| ("table").equals(name)|| ("tr").equals(name)|| ("td").equals(name)) {
				tagBuffer.append('\n');
			}
			tagBuffer.append('>');
		}
		return tagBuffer;
	}
	
	public String generateChildren(){
		if(content!=null){
			return content;
		}
		StringBuilder tagBuffer=new StringBuilder();
		for(int childIndex = 0, childCount = children.size(); childIndex < childCount; childIndex++) {
			HTMLNode childNode = children.get(childIndex);
			childNode.generate(tagBuffer);
		}
		return tagBuffer.toString();
	}
	
	public void setContent(String newContent){
		if(readOnly)
			throw new IllegalArgumentException("Read only");
		content=newContent;
	}
	
	public List<HTMLNode> getChildren(){
		return children;
	}

	/**
	 * Special HTML node for the DOCTYPE declaration. This node differs from a
	 * normal HTML node in that it's child (and it should only have exactly one
	 * child, the "html" node) is rendered <em>after</em> this node.
	 * 
	 * @author David 'Bombe' Roden &lt;bombe@freenetproject.org&gt;
	 * @version $Id$
	 */
	public static class HTMLDoctype extends HTMLNode {

		private final String systemUri;

		/**
		 * 
		 */
		public HTMLDoctype(String doctype, String systemUri) {
			super(doctype);
			this.systemUri = systemUri;
		}

		/**
		 * @see freenet.support.HTMLNode#generate(java.lang.StringBuilder)
		 */
		@Override
		public StringBuilder generate(StringBuilder tagBuffer) {
			tagBuffer.append("<!DOCTYPE ").append(name).append(" PUBLIC \"").append(systemUri).append("\">\n");
			//TODO A meaningful exception should be raised 
			// when trying to call the method for a HTMLDoctype 
			// with number of child != 1 
			return children.get(0).generate(tagBuffer);
		}

	}

	public static HTMLNode link(String path) {
		return new HTMLNode("a", "href", path);
	}

	public static HTMLNode linkInNewWindow(String path) {
		return new HTMLNode("a", new String[] { "href", "target" }, new String[] { path, "_blank" });
	}

	public static HTMLNode text(String text) {
		return new HTMLNode("#", text);
	}
	
	public static HTMLNode text(int count) {
		return new HTMLNode("#", Integer.toString(count));
	}

	public static HTMLNode text(long count) {
		return new HTMLNode("#", Long.toString(count));
	}

	public static HTMLNode text(short count) {
		return new HTMLNode("#", Short.toString(count));
	}

}
package freenet.clients.http;

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.net.MalformedURLException;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

import com.db4o.ObjectContainer;

import freenet.client.ClientMetadata;
import freenet.client.DefaultMIMETypes;
import freenet.client.FetchContext;
import freenet.client.FetchException;
import freenet.client.FetchResult;
import freenet.client.async.CacheFetchResult;
import freenet.client.async.ClientContext;
import freenet.client.async.ClientGetCallback;
import freenet.client.async.ClientGetter;
import freenet.client.async.DatabaseDisabledException;
import freenet.client.events.ClientEvent;
import freenet.client.events.ClientEventListener;
import freenet.client.events.ExpectedFileSizeEvent;
import freenet.client.events.ExpectedMIMEEvent;
import freenet.client.events.SendingToNetworkEvent;
import freenet.client.events.SplitfileProgressEvent;
import freenet.client.filter.ContentFilter;
import freenet.client.filter.MIMEType;
import freenet.client.filter.UnknownContentTypeException;
import freenet.keys.FreenetURI;
import freenet.keys.USK;
import freenet.node.RequestClient;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;
import freenet.support.io.Closer;

/** 
 * Fetching a page for a browser.
 * 
 * LOCKING: The lock on this object is always taken last.
 */
public class FProxyFetchInProgress implements ClientEventListener, ClientGetCallback {
	
	/** What to do when we find data which matches the request but it has already been 
	 * filtered, assuming we want a filtered copy. */
	public enum REFILTER_POLICY {
		RE_FILTER, // Re-run the filter over data that has already been filtered. Probably requires allocating a new temp file.
		ACCEPT_OLD, // Accept the old filtered data. Only as safe as the filter when the data was originally downloaded.
		RE_FETCH // Fetch the data again. Unnecessary in most cases, avoids any possibility of filter artefacts.
	}
	
	private final REFILTER_POLICY refilterPolicy;
	
	private static volatile boolean logMINOR;
	
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback() {
			
			@Override
			public void shouldUpdate() {
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}
	
	/** The key we are fetching */
	final FreenetURI uri;
	/** The maximum size specified by the client */
	final long maxSize;
	/** Unique ID for the fetch */
	private final long identifier;
	/** Fetcher */
	private final ClientGetter getter;
	/** Any request which is waiting for a progress screen or data.
	 * We may want to wake requests separately in future. */
	private final ArrayList<FProxyFetchWaiter> waiters;
	private final ArrayList<FProxyFetchResult> results;
	/** Gets notified with every change*/
	private final List<FProxyFetchListener> listener=Collections.synchronizedList(new ArrayList<FProxyFetchListener>());
	/** The data, if we have it */
	private Bucket data;
	/** Creation time */
	private final long timeStarted;
	/** Finished? */
	private boolean finished;
	/** Size, if known */
	private long size;
	/** MIME type, if known */
	private String mimeType;
	/** Gone to network? */
	private boolean goneToNetwork;
	/** Total blocks */
	private int totalBlocks;
	/** Required blocks */
	private int requiredBlocks;
	/** Fetched blocks */
	private int fetchedBlocks;
	/** Failed blocks */
	private int failedBlocks;
	/** Fatally failed blocks */
	private int fatallyFailedBlocks;
	private int fetchedBlocksPreNetwork;
	/** Finalized the block set? */
	private boolean finalizedBlocks;
	/** Fetch failed */
	private FetchException failed;
	private boolean hasWaited;
	private boolean hasNotifiedFailure;
	/** Last time the fetch was accessed from the fproxy end */
	private long lastTouched;
	final FProxyFetchTracker tracker;
	/** Show even non-fatal failures for 5 seconds. Necessary for javascript to work,
	 * because it fetches the page and then reloads it if it isn't a progress update. */
	private long timeFailed;
	/** If this is set, then it can be removed instantly, doesn't need to wait for 30sec*/
	private boolean requestImmediateCancel=false;
	private int fetched = 0;
	/** Stores the fetch context this class was created with*/
	private FetchContext fctx;
	
	public FProxyFetchInProgress(FProxyFetchTracker tracker, FreenetURI key, long maxSize2, long identifier, ClientContext context, FetchContext fctx, RequestClient rc, REFILTER_POLICY refilter) {
		this.refilterPolicy = refilter;
		this.tracker = tracker;
		this.uri = key;
		this.maxSize = maxSize2;
		this.timeStarted = System.currentTimeMillis();
		this.identifier = identifier;
		this.fctx = fctx;
		FetchContext alteredFctx = new FetchContext(fctx, FetchContext.IDENTICAL_MASK, false, null);
		alteredFctx.maxOutputLength = fctx.maxTempLength = maxSize;
		alteredFctx.eventProducer.addEventListener(this);
		waiters = new ArrayList<FProxyFetchWaiter>();
		results = new ArrayList<FProxyFetchResult>();
		getter = new ClientGetter(this, uri, alteredFctx, FProxyToadlet.PRIORITY, rc, null, null);
	}
	
	public synchronized FProxyFetchWaiter getWaiter() {
		lastTouched = System.currentTimeMillis();
		FProxyFetchWaiter waiter = new FProxyFetchWaiter(this);
		waiters.add(waiter);
		return waiter;
	}
	
	public void addCustomWaiter(FProxyFetchWaiter waiter){
		waiters.add(waiter);
	}

	synchronized FProxyFetchResult innerGetResult(boolean hasWaited) {
		lastTouched = System.currentTimeMillis();
		FProxyFetchResult res;
		if(data != null)
			res = new FProxyFetchResult(this, data, mimeType, timeStarted, goneToNetwork, getETA(), hasWaited);
		else {
			res = new FProxyFetchResult(this, mimeType, size, timeStarted, goneToNetwork,
					totalBlocks, requiredBlocks, fetchedBlocks, failedBlocks, fatallyFailedBlocks, finalizedBlocks, failed, getETA(), hasWaited);
		}
		results.add(res);
		if(data != null || failed != null) {
			res.setFetchCount(fetched);
			fetched++;
		}
		return res;
	}

	public void start(ClientContext context) throws FetchException {
		try {
			if(!checkCache(context))
				context.start(getter);
		} catch (FetchException e) {
			synchronized(this) {
				this.failed = e;
				this.finished = true;
			}
		} catch (DatabaseDisabledException e) {
			// Impossible
			Logger.error(this, "Failed to start: "+e);
			synchronized(this) {
				this.failed = new FetchException(FetchException.INTERNAL_ERROR, e);
				this.finished = true;
			}
		}
	}

	/** Look up the key in the downloads queue.
	 * @return True if it was found and we don't need to start the request. */
	private boolean checkCache(ClientContext context) {
		// Fproxy uses lookupInstant() with mustCopy = false. I.e. it can reuse stuff unsafely. If the user frees it it's their fault.
		if(bogusUSK(context)) return false;
		CacheFetchResult result = context.downloadCache == null ? null : context.downloadCache.lookupInstant(uri, !fctx.filterData, false, null);
		if(result == null) return false;
		Bucket data = null;
		String mimeType = null;
		if((!fctx.filterData) && (!result.alreadyFiltered)) {
			if(fctx.overrideMIME == null || fctx.overrideMIME.equals(result.getMimeType())) {
				// Works as-is.
				// Any time we re-use old content we need to remove the tracker because it may not remain available.
				tracker.removeFetcher(this);
				onSuccess(result, null, null);
				return true;
			} else if(fctx.overrideMIME != null && !fctx.overrideMIME.equals(result.getMimeType())) {
				// Change the MIME type.
				tracker.removeFetcher(this);
				onSuccess(new FetchResult(new ClientMetadata(fctx.overrideMIME), result.asBucket()), null, null);
				return true;
			} 
		} else if(result.alreadyFiltered) {
			if(refilterPolicy == REFILTER_POLICY.RE_FETCH || !fctx.filterData) {
				// Can't use it.
				result = null;
			} else if(fctx.filterData) {
				if(shouldAcceptCachedFilteredData(fctx, result)) {
					if(refilterPolicy == REFILTER_POLICY.ACCEPT_OLD) {
						tracker.removeFetcher(this);
						onSuccess(result, null, null);
						return true;
					} // else re-filter
				} else
					return false;
			} else {
				return false;
			}
		}
		data = result.asBucket();
		mimeType = result.getMimeType();
		if(mimeType == null || mimeType.equals("")) mimeType = DefaultMIMETypes.DEFAULT_MIME_TYPE;
		if(fctx.overrideMIME != null && !result.alreadyFiltered)
			mimeType = fctx.overrideMIME;
		else if(fctx.overrideMIME != null && !mimeType.equals(fctx.overrideMIME)) {
			// Doesn't work.
			return false;
		}
		String fullMimeType = mimeType;
		mimeType = ContentFilter.stripMIMEType(mimeType);
		MIMEType type = ContentFilter.getMIMEType(mimeType);
		if(type == null || ((!type.safeToRead) && type.readFilter == null)) {
			UnknownContentTypeException e = new UnknownContentTypeException(mimeType);
			data.free();
			onFailure(new FetchException(e.getFetchErrorCode(), data.size(), e, mimeType), null, null);
			return true;
		} else if(type.safeToRead) {
			tracker.removeFetcher(this);
			onSuccess(new FetchResult(new ClientMetadata(mimeType), data), null, null);
			return true;
		} else {
			// Try to filter it.
			Bucket output = null;
			InputStream is = null;
			OutputStream os = null;
			try {
				output = context.tempBucketFactory.makeBucket(-1);
				is = data.getInputStream();
				os = output.getOutputStream();
				ContentFilter.filter(is, os, fullMimeType, uri.toURI("/"), null, null, fctx.charset);
				is.close();
				is = null;
				os.close();
				os = null;
				// Since we are not re-using the data bucket, we can happily stay in the FProxyFetchTracker.
				this.onSuccess(new FetchResult(new ClientMetadata(fullMimeType), output), null, null);
				output = null;
				return true;
			} catch (IOException e) {
				Logger.normal(this, "Failed filtering coalesced data in fproxy");
				// Failed. :|
				// Let it run normally.
				return false;
			} catch (URISyntaxException e) {
				Logger.error(this, "Impossible: "+e, e);
				return false;
			} finally {
				Closer.close(is);
				Closer.close(os);
				Closer.close(output);
				Closer.close(data);
			}
		}
	}

	/** If the key is a USK and a) we are requested to do an exhaustive search, or b) 
	 * there is a later version, then we can't use the download queue as a cache.
	 * @return True if we can't use the download queue, false if we can. */
	private boolean bogusUSK(ClientContext context) {
		if(!uri.isUSK()) return false;
		long edition = uri.getSuggestedEdition();
		if(edition < 0) 
			return true; // Need to do the fetch.
		USK usk;
		try {
			usk = USK.create(uri);
		} catch (MalformedURLException e) {
			return false; // Will fail later.
		}
		long ret = context.uskManager.lookupKnownGood(usk);
		if(ret == -1) return false;
		return ret > edition;
	}

	private boolean shouldAcceptCachedFilteredData(FetchContext fctx,
			CacheFetchResult result) {
		// FIXME allow the charset if it's the same
		if(fctx.charset != null) return false;
		boolean okay = false;
		if(fctx.overrideMIME == null) {
			return true;
		} else {
			String finalMIME = result.getMimeType();
			if(fctx.overrideMIME.equals(finalMIME))
				return true;
			else if(ContentFilter.stripMIMEType(finalMIME).equals(fctx.overrideMIME) && fctx.charset == null)
				return true;
			// FIXME we could make this work in a few more cases... it doesn't matter much though as usually people don't override the MIME type!
		}
		return false;
	}

	public void onRemoveEventProducer(ObjectContainer container) {
		// Impossible
	}

	public void receive(ClientEvent ce, ObjectContainer maybeContainer, ClientContext context) {
		try{
			if(ce instanceof SplitfileProgressEvent) {
				SplitfileProgressEvent split = (SplitfileProgressEvent) ce;
				synchronized(this) {
					int oldReq = requiredBlocks - (fetchedBlocks + failedBlocks + fatallyFailedBlocks);
					totalBlocks = split.totalBlocks;
					fetchedBlocks = split.succeedBlocks;
					requiredBlocks = split.minSuccessfulBlocks;
					failedBlocks = split.failedBlocks;
					fatallyFailedBlocks = split.fatallyFailedBlocks;
					finalizedBlocks = split.finalizedTotal;
					int req = requiredBlocks - (fetchedBlocks + failedBlocks + fatallyFailedBlocks);
					if(!(req > 1024 && oldReq <= 1024)) return;
				}
			} else if(ce instanceof SendingToNetworkEvent) {
				synchronized(this) {
					if(goneToNetwork) return;
					goneToNetwork = true;
					fetchedBlocksPreNetwork = fetchedBlocks;
				}
			} else if(ce instanceof ExpectedMIMEEvent) {
				synchronized(this) {
					this.mimeType = ((ExpectedMIMEEvent)ce).expectedMIMEType;
				}
				if(!goneToNetwork) return;
			} else if(ce instanceof ExpectedFileSizeEvent) {
				synchronized(this) {
					this.size = ((ExpectedFileSizeEvent)ce).expectedSize;
				}
				if(!goneToNetwork) return;
			} else return;
			wakeWaiters(false);
		}finally{
			for(FProxyFetchListener l:new ArrayList<FProxyFetchListener>(listener)){
				l.onEvent();
			}
		}
	}

	private void wakeWaiters(boolean finished) {
		FProxyFetchWaiter[] waiting;
		synchronized(this) {
			waiting = waiters.toArray(new FProxyFetchWaiter[waiters.size()]);
		}
		for(FProxyFetchWaiter w : waiting) {
			w.wakeUp(finished);
		}
		if(finished==true){
			for(FProxyFetchListener l:new ArrayList<FProxyFetchListener>(listener)){
				l.onEvent();
			}
		}
	}

	public void onFailure(FetchException e, ClientGetter state, ObjectContainer container) {
		synchronized(this) {
			this.failed = e;
			this.finished = true;
			this.timeFailed = System.currentTimeMillis();
		}
		wakeWaiters(true);
	}

	public void onMajorProgress(ObjectContainer container) {
		// Ignore
	}

	public void onSuccess(FetchResult result, ClientGetter state, ObjectContainer container) {
		synchronized(this) {
			this.data = result.asBucket();
			this.mimeType = result.getMimeType();
			this.finished = true;
		}
		wakeWaiters(true);
	}

	public boolean hasData() {
		return data != null;
	}

	public boolean finished() {
		return finished;
	}

	public void close(FProxyFetchWaiter waiter) {
		synchronized(this) {
			waiters.remove(waiter);
			if(!results.isEmpty()) return;
			if(!waiters.isEmpty()) return;
		}
		tracker.queueCancel(this);
	}

	/** Keep for 30 seconds after last access */
	static final int LIFETIME = 30 * 1000;
	
	/** Caller should take the lock on FProxyToadlet.fetchers, then call this 
	 * function, if it returns true then finish the cancel outside the lock.
	 */
	public synchronized boolean canCancel() {
		if(!waiters.isEmpty()) return false;
		if(!results.isEmpty()) return false;
		if(!listener.isEmpty()) return false;
		if(lastTouched + LIFETIME >= System.currentTimeMillis() && !requestImmediateCancel) {
			if(logMINOR) Logger.minor(this, "Not able to cancel for "+this+" : "+uri+" : "+maxSize);
			return false;
		}
		if(logMINOR) Logger.minor(this, "Can cancel for "+this+" : "+uri+" : "+maxSize);
		return true;
	}
	
	public void finishCancel() {
		if(logMINOR) Logger.minor(this, "Finishing cancel for "+this+" : "+uri+" : "+maxSize);
		if(data != null) {
			try {
				data.free();
			} catch (Throwable t) {
				// Ensure we get to the next bit
				Logger.error(this, "Failed to free: "+t, t);
			}
		}
		try {
			getter.cancel(null, tracker.context);
		} catch (Throwable t) {
			// Ensure we get to the next bit
			Logger.error(this, "Failed to cancel: "+t, t);
		}
	}

	public void close(FProxyFetchResult result) {
		synchronized(this) {
			results.remove(result);
			if(!results.isEmpty()) return;
			if(!waiters.isEmpty()) return;
		}
		tracker.queueCancel(this);
	}
	
	public synchronized long getETA() {
		if(!goneToNetwork) return -1;
		if(requiredBlocks <= 0) return -1;
		if(fetchedBlocks >= requiredBlocks) return -1;
		if(fetchedBlocks - fetchedBlocksPreNetwork < 5) return -1;
		return ((System.currentTimeMillis() - timeStarted) * (requiredBlocks - fetchedBlocksPreNetwork)) / (fetchedBlocks - fetchedBlocksPreNetwork);
	}

	public synchronized boolean notFinishedOrFatallyFinished() {
		if(data == null && failed == null) return true;
		if(failed != null && failed.isFatal()) return true;
		if(failed != null && !hasNotifiedFailure) {
			hasNotifiedFailure = true;
			return true;
		}
		if(failed != null && (System.currentTimeMillis() - timeFailed < 1000 || fetched < 2)) // Once for javascript and once for the user when it re-pulls.
			return true;
		return false;
	}
	
	public synchronized boolean hasNotifiedFailure() {
		return true;
	}

	public synchronized boolean hasWaited() {
		return hasWaited;
	}

	public synchronized void setHasWaited() {
		hasWaited = true;
	}
	
	/** Adds a listener that will be notified when a change occurs to this fetch
	 * @param listener - The listener to be added*/
	public void addListener(FProxyFetchListener listener){
		if(logMINOR){
			Logger.minor(this,"Registered listener:"+listener);
		}
		this.listener.add(listener);
	}
	
	/** Removes a listener
	 * @param listener - The listener to be removed*/
	public void removeListener(FProxyFetchListener listener){
		if(logMINOR){
			Logger.minor(this,"Removed listener:"+listener);
		}
		this.listener.remove(listener);
		if(logMINOR){
			Logger.minor(this,"can cancel now?:"+canCancel());
		}
	}
	
	/** Allows the fetch to be removed immediately*/
	public void requestImmediateCancel(){
		requestImmediateCancel=true;
	}

	public long lastTouched() {
		return lastTouched;
	}
	
	public boolean fetchContextEquivalent(FetchContext context) {
		if(this.fctx.filterData != context.filterData) return false;
		if(this.fctx.maxOutputLength != context.maxOutputLength) return false;
		if(this.fctx.maxTempLength != context.maxTempLength) return false;
		if(this.fctx.charset == null && context.charset != null) return false;
		if(this.fctx.charset != null && !this.fctx.charset.equals(context.charset)) return false;
		if(this.fctx.overrideMIME == null && context.overrideMIME != null) return false;
		if(this.fctx.overrideMIME != null && !this.fctx.overrideMIME.equals(context.overrideMIME)) return false;
		return true;
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.client;

import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;

import com.db4o.ObjectContainer;

import freenet.support.Logger;
import freenet.support.SimpleFieldSet;

/**
 * Essentially a map of integer to incrementible integer.
 * FIXME maybe move this to support, give it a better name?
 */
public class FailureCodeTracker {

	public final boolean insert;
	private int total;
	
	public FailureCodeTracker(boolean insert) {
		this.insert = insert;
	}
	
	/**
	 * Create a FailureCodeTracker from a SimpleFieldSet.
	 * @param isInsert Whether this is an insert.
	 * @param fs The SimpleFieldSet containing the FieldSet (non-verbose) form of 
	 * the tracker.
	 */
	public FailureCodeTracker(boolean isInsert, SimpleFieldSet fs) {
		this.insert = isInsert;
		Iterator<String> i = fs.directSubsetNameIterator();
		while(i.hasNext()) {
			String name = i.next();
			SimpleFieldSet f = fs.subset(name);
			// We ignore the Description, if there is one; we just want the count
			int num = Integer.parseInt(name);
			int count = Integer.parseInt(f.get("Count"));
			if(count < 0) throw new IllegalArgumentException("Count < 0");
			map.put(Integer.valueOf(num), new Item(count));
			total += count;
		}
	}
	
	private static class Item {
		Item(int count) {
			this.x = count;
		}

		Item() {
			this.x = 0;
		}

		int x;
	}

	private HashMap<Integer, Item> map;

	public synchronized void inc(int k) {
		if(k == 0) {
			Logger.error(this, "Can't increment 0, not a valid failure mode", new Exception("error"));
		}
		if(map == null) map = new HashMap<Integer, Item>();
		Integer key = k;
		Item i = map.get(key);
		if(i == null)
			map.put(key, i = new Item());
		i.x++;
		total++;
	}

	public synchronized void inc(Integer k, int val) {
		if(k == 0) {
			Logger.error(this, "Can't increment 0, not a valid failure mode", new Exception("error"));
		}
		if(map == null) map = new HashMap<Integer, Item>();
		Integer key = k;
		Item i = map.get(key);
		if(i == null)
			map.put(key, i = new Item());
		i.x+=val;
		total += val;
	}
	
	public synchronized String toVerboseString() {
		if(map == null) return super.toString()+":empty";
		StringBuilder sb = new StringBuilder();
		for (Map.Entry<Integer, Item> e : map.entrySet()) {
			Integer x = e.getKey();
			Item val = e.getValue();
			String s = insert ? InsertException.getMessage(x.intValue()) : FetchException.getMessage(x.intValue());
			sb.append(val.x);
			sb.append('\t');
			sb.append(s);
			sb.append('\n');
		}
		return sb.toString();
	}

	@Override
	public synchronized String toString() {
		if(map == null) return super.toString()+":empty";
		StringBuilder sb = new StringBuilder(super.toString());
		sb.append(':');
		if(map.size() == 0) sb.append("empty");
		else if(map.size() == 1) {
			sb.append("one:");
			Integer code = (Integer) (map.keySet().toArray())[0];
			sb.append(code);
			sb.append('=');
			sb.append((map.get(code)).x);
		} else {
			sb.append(map.size());
		}
		return sb.toString();
	}
	
	/**
	 * Merge codes from another tracker into this one.
	 */
	public synchronized FailureCodeTracker merge(FailureCodeTracker source) {
		if(source.map == null) return this;
		if(map == null) map = new HashMap<Integer, Item>();
		for (Map.Entry<Integer, Item> e : source.map.entrySet()) {
			Integer k = e.getKey();
			Item item = e.getValue();
			inc(k, item.x);
		}
		return this;
	}

	public void merge(FetchException e) {
		if(insert) throw new IllegalStateException("Merging a FetchException in an insert!");
		if(e.errorCodes != null) {
			merge(e.errorCodes);
		}
		// Increment mode anyway, so we get the splitfile error as well.
		inc(e.mode);
	}

	public synchronized int totalCount() {
		return total;
	}

	/** Copy verbosely to a SimpleFieldSet */
	public synchronized SimpleFieldSet toFieldSet(boolean verbose) {
		SimpleFieldSet sfs = new SimpleFieldSet(false);
		if(map != null) {
		for (Map.Entry<Integer, Item> e : map.entrySet()) {
			Integer k = e.getKey();
			Item item = e.getValue();
			int code = k.intValue();
			// prefix.num.Description=<code description>
			// prefix.num.Count=<count>
			if(verbose)
				sfs.putSingle(Integer.toString(code)+".Description", 
						insert ? InsertException.getMessage(code) : FetchException.getMessage(code));
			sfs.put(Integer.toString(code)+".Count", item.x);
		}
		}
		return sfs;
	}

	public synchronized boolean isOneCodeOnly() {
		return map.size() == 1;
	}

	public synchronized int getFirstCode() {
		return ((Integer) map.keySet().toArray()[0]).intValue();
	}

	public synchronized boolean isFatal(boolean insert) {
		if(map == null) return false;
		for (Map.Entry<Integer, Item> e : map.entrySet()) {
			Integer code = e.getKey();
			if(e.getValue().x == 0) continue;
			if(insert) {
				if(InsertException.isFatal(code.intValue())) return true;
			} else {
				if(FetchException.isFatal(code.intValue())) return true;
			}
		}
		return false;
	}

	public void merge(InsertException e) {
		if(!insert) throw new IllegalArgumentException("This is not an insert yet merge("+e+") called!");
		if(e.errorCodes != null)
			merge(e.errorCodes);
		inc(e.getMode());
	}

	public synchronized boolean isEmpty() {
		return map == null || map.isEmpty();
	}

	public void removeFrom(ObjectContainer container) {
		Item[] items;
		Integer[] ints;
		synchronized(this) {
			items = map == null ? null : map.values().toArray(new Item[map.size()]);
			ints = map == null ? null : map.keySet().toArray(new Integer[map.size()]);
			if(map != null) map.clear();
		}
		if(items != null)
			for(int i=0;i<items.length;i++) {
				container.delete(items[i]);
				container.delete(ints[i]);
			}
		if(map != null) {
			container.activate(map, 5);
			container.delete(map);
		}
		container.delete(this);
	}
	
	public void objectOnActivate(ObjectContainer container) {
		if(map != null) container.activate(map, 5);
	}
	
	@Override
	public FailureCodeTracker clone() {
		FailureCodeTracker tracker = new FailureCodeTracker(insert);
		tracker.merge(this);
		return tracker;
	}

	public void storeTo(ObjectContainer container) {
		// Must store to at least depth 2 because of map.
		container.ext().store(this, 5);
	}

	public synchronized boolean isDataFound() {
		for(Map.Entry<Integer, Item> entry : map.entrySet()) {
			if(entry.getValue().x <= 0) continue;
			if(FetchException.isDataFound(entry.getKey(), null)) return true;
		}
		return false;
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

import freenet.io.comm.ByteCounter;
import freenet.io.comm.DMT;
import freenet.io.comm.DisconnectedException;
import freenet.io.comm.Message;
import freenet.io.comm.MessageFilter;
import freenet.io.comm.NotConnectedException;
import freenet.io.comm.PeerContext;
import freenet.io.comm.RetrievalException;
import freenet.io.comm.SlowAsyncMessageFilterCallback;
import freenet.io.xfer.AbortedException;
import freenet.io.xfer.BlockReceiver;
import freenet.io.xfer.BlockReceiver.BlockReceiverCompletion;
import freenet.io.xfer.BlockReceiver.BlockReceiverTimeoutHandler;
import freenet.io.xfer.PartiallyReceivedBlock;
import freenet.keys.CHKBlock;
import freenet.keys.CHKVerifyException;
import freenet.keys.NodeCHK;
import freenet.store.KeyCollisionException;
import freenet.support.HexUtil;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.OOMHandler;
import freenet.support.ShortBuffer;
import freenet.support.Logger.LogLevel;
import freenet.support.io.NativeThread;

/**
 * @author amphibian
 * 
 * Handle an incoming insert request.
 * This corresponds to RequestHandler.
 */
public class CHKInsertHandler implements PrioRunnable, ByteCounter {
	private static volatile boolean logMINOR;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

    static final int DATA_INSERT_TIMEOUT = 10000;
    
    final Message req;
    final Node node;
    final long uid;
    final PeerNode source;
    final NodeCHK key;
    final long startTime;
    private short htl;
    private CHKInsertSender sender;
    private byte[] headers;
    private BlockReceiver br;
    private Thread runThread;
    PartiallyReceivedBlock prb;
    final InsertTag tag;
    private boolean canWriteDatastore;
	private final boolean forkOnCacheable;
	private final boolean preferInsert;
	private final boolean ignoreLowBackoff;
	private final boolean realTimeFlag;

    CHKInsertHandler(Message req, PeerNode source, long id, Node node, long startTime, InsertTag tag, boolean forkOnCacheable, boolean preferInsert, boolean ignoreLowBackoff, boolean realTimeFlag) {
        this.req = req;
        this.node = node;
        this.uid = id;
        this.source = source;
        this.startTime = startTime;
        this.tag = tag;
        key = (NodeCHK) req.getObject(DMT.FREENET_ROUTING_KEY);
        htl = req.getShort(DMT.HTL);
        if(htl <= 0) htl = 1;
        canWriteDatastore = node.canWriteDatastoreInsert(htl);
        receivedBytes(req.receivedByteCount());
        this.forkOnCacheable = forkOnCacheable;
        this.preferInsert = preferInsert;
        this.ignoreLowBackoff = ignoreLowBackoff;
        this.realTimeFlag = realTimeFlag;
    }
    
    @Override
	public String toString() {
        return super.toString()+" for "+uid;
    }
    
    public void run() {
	    freenet.support.Logger.OSThread.logPID(this);
        try {
        	realRun();
		} catch (OutOfMemoryError e) {
			OOMHandler.handleOOM(e);
			tag.handlerThrew(e);
        } catch (Throwable t) {
            Logger.error(this, "Caught in run() "+t, t);
            tag.handlerThrew(t);
        } finally {
        	if(logMINOR) Logger.minor(this, "Exiting CHKInsertHandler.run() for "+uid);
        	tag.unlockHandler();
        }
    }

    private void realRun() {
        runThread = Thread.currentThread();
        
        // FIXME implement rate limiting or something!
        // Send Accepted
        Message accepted = DMT.createFNPAccepted(uid);
        try {
			//Using sendSync here will help the next message filter not timeout... wait here or at the message filter.
			source.sendSync(accepted, this, realTimeFlag);
		} catch (NotConnectedException e1) {
			if(logMINOR) Logger.minor(this, "Lost connection to source");
			return;
		} catch (SyncSendWaitedTooLongException e) {
			Logger.error(this, "Unable to send "+accepted+" in a reasonable time to "+source);
			return;
		}
        
        // Source will send us a DataInsert
        
        MessageFilter mf;
        mf = makeDataInsertFilter(DATA_INSERT_TIMEOUT);
        
        Message msg;
        try {
            msg = node.usm.waitFor(mf, this);
        } catch (DisconnectedException e) {
            Logger.normal(this, "Disconnected while waiting for DataInsert on "+uid);
            return;
        }
        
        if(logMINOR) Logger.minor(this, "Received "+msg);
        
        if(msg == null) {
        	handleNoDataInsert();
        	return;
        }
        
        if(msg.getSpec() == DMT.FNPDataInsertRejected) {
        	// Not caused by the immediately upstream node.
        	return;
        }
        
        // We have a DataInsert
        headers = ((ShortBuffer)msg.getObject(DMT.BLOCK_HEADERS)).getData();
        // FIXME check the headers
        
        // Now create an CHKInsertSender, or use an existing one, or
        // discover that the data is in the store.

        // From this point onwards, if we return cleanly we must go through finish().
        
        prb = new PartiallyReceivedBlock(Node.PACKETS_IN_BLOCK, Node.PACKET_SIZE);
        if(htl > 0)
            sender = node.makeInsertSender(key, htl, uid, tag, source, headers, prb, false, false, forkOnCacheable, preferInsert, ignoreLowBackoff, realTimeFlag);
        br = new BlockReceiver(node.usm, source, uid, prb, this, node.getTicker(), false, realTimeFlag, myTimeoutHandler);
        
        // Receive the data, off thread
        Runnable dataReceiver = new DataReceiver();
		receiveStarted = true;
        node.executor.execute(dataReceiver, "CHKInsertHandler$DataReceiver for UID "+uid);

        // Wait...
        // What do we want to wait for?
        // If the data receive completes, that's very nice,
        // but doesn't really matter. What matters is what
        // happens to the CHKInsertSender. If the data receive
        // fails, that does matter...
        
        // We are waiting for a terminal status on the CHKInsertSender,
        // including SUCCESS.
        // If we get transfer failed, we can check whether the receive
        // failed first. If it did it's not our fault.
        // If the receive failed, and we haven't started transferring
        // yet, we probably want to kill the sender.
        // So we call the wait method on the CHKInsertSender, but we
        // also have a flag locally to indicate the receive failed.
        // And if it does, we interrupt.
        
        boolean receivedRejectedOverload = false;
        
        while(true) {
            synchronized(sender) {
                try {
                	if(sender.getStatus() == CHKInsertSender.NOT_FINISHED)
                		sender.wait(5000);
                } catch (InterruptedException e) {
                    // Cool, probably this is because the receive failed...
                }
            }
            if(receiveFailed()) {
                // Nothing else we can do
                finish(CHKInsertSender.RECEIVE_FAILED);
                return;
            }
            
            if((!receivedRejectedOverload) && sender.receivedRejectedOverload()) {
            	receivedRejectedOverload = true;
            	// Forward it
            	// Does not need to be sent synchronously since is non-terminal.
            	Message m = DMT.createFNPRejectedOverload(uid, false, true, realTimeFlag);
            	try {
					source.sendAsync(m, null, this);
				} catch (NotConnectedException e) {
					if(logMINOR) Logger.minor(this, "Lost connection to source");
					return;
				}
            }
            
            int status = sender.getStatus();
            
            if(status == CHKInsertSender.NOT_FINISHED) {
                continue;
            }

            // Local RejectedOverload's (fatal).
            // Internal error counts as overload. It'd only create a timeout otherwise, which is the same thing anyway.
            if((status == CHKInsertSender.TIMED_OUT) ||
            		(status == CHKInsertSender.GENERATED_REJECTED_OVERLOAD) ||
            		(status == CHKInsertSender.INTERNAL_ERROR)) {
                msg = DMT.createFNPRejectedOverload(uid, true, true, realTimeFlag);
                try {
					source.sendSync(msg, this, realTimeFlag);
				} catch (NotConnectedException e) {
					if(logMINOR) Logger.minor(this, "Lost connection to source");
					return;
				} catch (SyncSendWaitedTooLongException e) {
					Logger.error(this, "Took too long to send "+msg+" to "+source);
					return;
				}
                // Might as well store it anyway.
                if((status == CHKInsertSender.TIMED_OUT) ||
                		(status == CHKInsertSender.GENERATED_REJECTED_OVERLOAD))
                	canCommit = true;
                finish(status);
                return;
            }
            
            if((status == CHKInsertSender.ROUTE_NOT_FOUND) || (status == CHKInsertSender.ROUTE_REALLY_NOT_FOUND)) {
                msg = DMT.createFNPRouteNotFound(uid, sender.getHTL());
                try {
					source.sendSync(msg, this, realTimeFlag);
				} catch (NotConnectedException e) {
					if(logMINOR) Logger.minor(this, "Lost connection to source");
					return;
				} catch (SyncSendWaitedTooLongException e) {
					Logger.error(this, "Took too long to send "+msg+" to "+source);
					return;
				}
                canCommit = true;
                finish(status);
                return;
            }
            
            if(status == CHKInsertSender.RECEIVE_FAILED) {
            	// Probably source's fault.
            	finish(status);
            	return;
            }
            
            if(status == CHKInsertSender.SUCCESS) {
            	msg = DMT.createFNPInsertReply(uid);
            	try {
					source.sendSync(msg, this, realTimeFlag);
				} catch (NotConnectedException e) {
					Logger.minor(this, "Lost connection to source");
					return;
				} catch (SyncSendWaitedTooLongException e) {
					Logger.error(this, "Took too long to send "+msg+" to "+source);
					return;
				}
                canCommit = true;
                finish(status);
                return;
            }
            
            // Otherwise...?
            Logger.error(this, "Unknown status code: "+sender.getStatusString());
            msg = DMT.createFNPRejectedOverload(uid, true, true, realTimeFlag);
            try {
				source.sendSync(msg, this, realTimeFlag);
			} catch (NotConnectedException e) {
				// Ignore
			} catch (SyncSendWaitedTooLongException e) {
				// Ignore
			}
            finish(CHKInsertSender.INTERNAL_ERROR);
            return;
        }
	}

	private MessageFilter makeDataInsertFilter(int timeout) {
    	MessageFilter mfDataInsert = MessageFilter.create().setType(DMT.FNPDataInsert).setField(DMT.UID, uid).setSource(source).setTimeout(timeout);
    	// DataInsertRejected means the transfer failed upstream so a DataInsert will not be sent.
    	MessageFilter mfDataInsertRejected = MessageFilter.create().setType(DMT.FNPDataInsertRejected).setField(DMT.UID, uid).setSource(source).setTimeout(timeout);
    	return mfDataInsert.or(mfDataInsertRejected);
	}

	private void handleNoDataInsert() {
    	try {
    		// Nodes wait until they have the DataInsert before forwarding, so there is absolutely no excuse: There is a local problem here!
    		if(source.isConnected() && (startTime > (source.timeLastConnectionCompleted()+Node.HANDSHAKE_TIMEOUT*4)))
    			Logger.warning(this, "Did not receive DataInsert on "+uid+" from "+source+" !");
    		Message tooSlow = DMT.createFNPRejectedTimeout(uid);
    		source.sendAsync(tooSlow, null, this);
    		Message m = DMT.createFNPInsertTransfersCompleted(uid, true);
    		source.sendAsync(m, null, this);
    		prb = new PartiallyReceivedBlock(Node.PACKETS_IN_BLOCK, Node.PACKET_SIZE);
    		br = new BlockReceiver(node.usm, source, uid, prb, this, node.getTicker(), false, realTimeFlag, null);
    		prb.abort(RetrievalException.NO_DATAINSERT, "No DataInsert", true);
    		source.localRejectedOverload("TimedOutAwaitingDataInsert", realTimeFlag);
    		
    		// Two stage timeout. Don't go fatal unless no response in 60 seconds.
    		// Yes it's ugly everywhere but since we have a longish connection timeout it's necessary everywhere. :|
    		// FIXME review two stage timeout everywhere with some low level networking guru.
    		MessageFilter mf = makeDataInsertFilter(60*1000);
    		node.usm.addAsyncFilter(mf, new SlowAsyncMessageFilterCallback() {

    			public void onMatched(Message m) {
    				// Okay, great.
    				// Either we got a DataInsert, in which case the transfer was aborted above, or we got a DataInsertRejected, which means it never started.
    				// FIXME arguably we should wait until we have the message before sending the transfer cancel in case the message gets lost? Or maybe not?
    			}

    			public boolean shouldTimeout() {
    				return false;
    			}

    			public void onTimeout() {
    				Logger.error(this, "No DataInsert for "+CHKInsertHandler.this+" from "+source+" ("+source.getVersionNumber()+")");
    				// Fatal timeout. Something is seriously busted.
    				// We've waited long enough that we know it's not just a connectivity problem - if it was we'd have disconnected by now.
    	    		source.fatalTimeout();
    			}

    			public void onDisconnect(PeerContext ctx) {
    				// Okay. Somewhat expected, it was having problems.
    			}

    			public void onRestarted(PeerContext ctx) {
    				// Okay.
    			}

    			public int getPriority() {
    				return NativeThread.NORM_PRIORITY;
    			}
    			
    		}, this);
    		return;
    	} catch (NotConnectedException e) {
    		if(logMINOR) Logger.minor(this, "Lost connection to source");
			return;
    	} catch (DisconnectedException e) {
    		if(logMINOR) Logger.minor(this, "Lost connection to source");
			return;
		}
	}

	private boolean canCommit = false;
    private boolean sentCompletion = false;
    private Object sentCompletionLock = new Object();
    
    /**
     * If canCommit, and we have received all the data, and it
     * verifies, then commit it.
     */
    private void finish(int code) {
    	if(logMINOR) Logger.minor(this, "Waiting for receive");
		synchronized(this) {
			while(receiveStarted && !receiveCompleted) {
				try {
					wait(100*1000);
				} catch (InterruptedException e) {
					// Ignore
				}
			}
    	}
		
		CHKBlock block = verify();
		// If we wanted to reduce latency at the cost of security (bug 3338), we'd commit here, or even on the receiver thread.
		
        // Wait for completion
        boolean sentCompletionWasSet;
        synchronized(sentCompletionLock) {
        	sentCompletionWasSet = sentCompletion;
        	sentCompletion = true;
        }
        
		Message m=null;
		
        if((sender != null) && (!sentCompletionWasSet)) {
            if(logMINOR) Logger.minor(this, "Waiting for completion");
			//If there are downstream senders, our final success report depends on there being no timeouts in the chain.
        	while(true) {
        		synchronized(sender) {
        			if(sender.completed()) {
        				break;
        			}
        			try {
        				sender.wait(10*1000);
        			} catch (InterruptedException e) {
        				// Loop
        			}
        		}
        	}
        	boolean failed = sender.anyTransfersFailed();
        	m = DMT.createFNPInsertTransfersCompleted(uid, failed);
		}
		
		if((sender == null) && (!sentCompletionWasSet) && (canCommit)) {
			//There are no downstream senders, but we stored the data locally, report successful transfer.
			//Note that this is done even if the verify fails.
			m = DMT.createFNPInsertTransfersCompleted(uid, false /* no timeouts */);
		}		
		
		// Don't commit until after we have received all the downstream transfer completion notifications.
		// We don't want an attacker to see a ULPR notice from the inserter before he sees it from the end of the chain (bug 3338).
		if(block != null) {
			commit(block);
			block = null;
		}
        
        	try {
        		source.sendSync(m, this, realTimeFlag);
        		if(logMINOR) Logger.minor(this, "Sent completion: "+m+" for "+this);
        	} catch (NotConnectedException e1) {
        		if(logMINOR) Logger.minor(this, "Not connected: "+source+" for "+this);
        		// May need to commit anyway...
        	} catch (SyncSendWaitedTooLongException e) {
        		Logger.error(this, "Took too long to send "+m+" to "+source);
        		// May need to commit anyway...
			}
		        
        if(code != CHKInsertSender.TIMED_OUT && code != CHKInsertSender.GENERATED_REJECTED_OVERLOAD && 
        		code != CHKInsertSender.INTERNAL_ERROR && code != CHKInsertSender.ROUTE_REALLY_NOT_FOUND &&
        		code != CHKInsertSender.RECEIVE_FAILED && !receiveFailed()) {
        	int totalSent = getTotalSentBytes();
        	int totalReceived = getTotalReceivedBytes();
        	if(sender != null) {
        		totalSent += sender.getTotalSentBytes();
        		totalReceived += sender.getTotalReceivedBytes();
        	}
        	if(logMINOR) Logger.minor(this, "Remote CHK insert cost "+totalSent+ '/' +totalReceived+" bytes ("+code+ ") receive failed = "+receiveFailed());
        	node.nodeStats.remoteChkInsertBytesSentAverage.report(totalSent);
        	node.nodeStats.remoteChkInsertBytesReceivedAverage.report(totalReceived);
        	if(code == CHKInsertSender.SUCCESS) {
        		// Report both sent and received because we have both a Handler and a Sender
        		if(sender != null && sender.startedSendingData())
        			node.nodeStats.successfulChkInsertBytesSentAverage.report(totalSent);
        		node.nodeStats.successfulChkInsertBytesReceivedAverage.report(totalReceived);
        	}
        }
    }
    
    /**
     * Verify data, or send DataInsertRejected.
     */
    private CHKBlock verify() {
        Message toSend = null;
        
        CHKBlock block = null;
        
        synchronized(this) {
        	if((prb == null) || prb.isAborted()) return null;
            try {
                if(!canCommit) return null;
                if(!prb.allReceived()) return null;
                block = new CHKBlock(prb.getBlock(), headers, key);
            } catch (CHKVerifyException e) {
            	Logger.error(this, "Verify failed in CHKInsertHandler: "+e+" - headers: "+HexUtil.bytesToHex(headers), e);
                toSend = DMT.createFNPDataInsertRejected(uid, DMT.DATA_INSERT_REJECTED_VERIFY_FAILED);
            } catch (AbortedException e) {
            	Logger.error(this, "Receive failed: "+e);
            	// Receiver thread (below) will handle sending the failure notice
            }
        }
        if(toSend != null) {
            try {
                source.sendAsync(toSend, null, this);
            } catch (NotConnectedException e) {
                // :(
            	if(logMINOR) Logger.minor(this, "Lost connection in "+this+" when sending FNPDataInsertRejected");
            }
        }
        return block;
	}
    
    private void commit(CHKBlock block) {
        try {
			node.store(block, node.shouldStoreDeep(key, source, sender == null ? new PeerNode[0] : sender.getRoutedTo()), false, canWriteDatastore, false);
		} catch (KeyCollisionException e) {
			// Impossible with CHKs.
		}
        if(logMINOR) Logger.minor(this, "Committed");
    }

	/** Has the receive failed? If so, there's not much more that can be done... */
    private boolean receiveFailed;
    
    private boolean receiveStarted;
    private boolean receiveCompleted;

    public class DataReceiver implements PrioRunnable {

        public void run() {
		    freenet.support.Logger.OSThread.logPID(this);
        	if(logMINOR) Logger.minor(this, "Receiving data for "+CHKInsertHandler.this);
        	// Don't log whether the transfer succeeded or failed as the transfer was initiated by the source therefore could be unreliable evidence.
        	br.receive(new BlockReceiverCompletion() {
        		
        		public void blockReceived(byte[] buf) {
        			if(logMINOR) Logger.minor(this, "Received data for "+CHKInsertHandler.this);
        			synchronized(CHKInsertHandler.this) {
        				receiveCompleted = true;
        				CHKInsertHandler.this.notifyAll();
        			}
   					node.nodeStats.successfulBlockReceive(realTimeFlag, false);
        		}

        		public void blockReceiveFailed(RetrievalException e) {
        			synchronized(CHKInsertHandler.this) {
        				receiveCompleted = true;
        				receiveFailed = true;
        				CHKInsertHandler.this.notifyAll();
        			}
        			// Cancel the sender
        			if(sender != null)
        				sender.onReceiveFailed(); // tell it to stop if it hasn't already failed... unless it's sending from store
        			runThread.interrupt();
        			tag.reassignToSelf(); // sender is finished, or will be very soon; we may however be waiting for the sendAborted downstream.
        			Message msg = DMT.createFNPDataInsertRejected(uid, DMT.DATA_INSERT_REJECTED_RECEIVE_FAILED);
        			try {
        				source.sendSync(msg, CHKInsertHandler.this, realTimeFlag);
        			} catch (NotConnectedException ex) {
        				//If they are not connected, that's probably why the receive failed!
        				if (logMINOR) Logger.minor(this, "Can't send "+msg+" to "+source+": "+ex);
        			} catch (SyncSendWaitedTooLongException ex) {
        				Logger.error(this, "Took too long to send "+msg+" to "+source);
					}
        			if (e.getReason()==RetrievalException.SENDER_DISCONNECTED)
        				Logger.normal(this, "Failed to retrieve (disconnect): "+e+" for "+CHKInsertHandler.this, e);
        			else
        				// Annoying, but we have stats for this; no need to call attention to it, it's unlikely to be a bug.
        				Logger.normal(this, "Failed to retrieve ("+e.getReason()+"/"+RetrievalException.getErrString(e.getReason())+"): "+e+" for "+CHKInsertHandler.this, e);
        			
        			if(!prb.abortedLocally())
        				node.nodeStats.failedBlockReceive(false, false, realTimeFlag, false);
        			return;
        		}
        		
        	});
        }

        @Override
		public String toString() {
        	return super.toString()+" for "+uid;
        }

		public int getPriority() {
			return NativeThread.HIGH_PRIORITY;
		}
        
    }

    private synchronized boolean receiveFailed() {
    	return receiveFailed;
    }
    
    private final Object totalSync = new Object();
    private int totalSentBytes;
    private int totalReceivedBytes;
    
	public void sentBytes(int x) {
		synchronized(totalSync) {
			totalSentBytes += x;
		}
		node.nodeStats.insertSentBytes(false, x);
	}

	public void receivedBytes(int x) {
		synchronized(totalSync) {
			totalReceivedBytes += x;
		}
		node.nodeStats.insertReceivedBytes(false, x);
	}

	public int getTotalSentBytes() {
		return totalSentBytes;
	}
	
	public int getTotalReceivedBytes() {
		return totalReceivedBytes;
	}

	public void sentPayload(int x) {
		node.sentPayload(x);
		node.nodeStats.insertSentBytes(false, -x);
	}

	public int getPriority() {
		return NativeThread.HIGH_PRIORITY;
	}
	
	private BlockReceiverTimeoutHandler myTimeoutHandler = new BlockReceiverTimeoutHandler() {

		/** We timed out waiting for a block from the request sender. We do not know 
		 * whether it is the fault of the request sender or that of some previous node.
		 * The PRB will be cancelled, resulting in all outgoing transfers for this insert
		 * being cancelled quickly. If the problem occurred on a previous node, we will
		 * receive a cancel. So we are consistent with the nodes we routed to, and it is
		 * safe to wait for the node that routed to us to send an explicit cancel. We do
		 * not need to do anything yet. */
		public void onFirstTimeout() {
			// Do nothing.
		}

		/** We timed out, and the sender did not send us a timeout message, even after we
		 * told it we were cancelling. Hence, we know that it was at fault. We need to 
		 * take action against it.
		 */
		public void onFatalTimeout(PeerContext receivingFrom) {
			Logger.error(this, "Fatal timeout receiving insert "+CHKInsertHandler.this+" from "+receivingFrom);
			((PeerNode)receivingFrom).fatalTimeout();
		}
		
	};
    
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.clients.http.bookmark;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.net.MalformedURLException;
import java.util.Date;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;

import com.db4o.ObjectContainer;

import freenet.client.async.ClientContext;
import freenet.client.async.USKCallback;
import freenet.clients.http.FProxyToadlet;
import freenet.keys.FreenetURI;
import freenet.keys.USK;
import freenet.l10n.NodeL10n;
import freenet.node.FSParseException;
import freenet.node.NodeClientCore;
import freenet.node.RequestClient;
import freenet.node.RequestStarter;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.SimpleFieldSet;
import freenet.support.io.Closer;
import freenet.support.io.FileUtil;

public class BookmarkManager implements RequestClient {

	public static final SimpleFieldSet DEFAULT_BOOKMARKS;
	private final NodeClientCore node;
	private final USKUpdatedCallback uskCB = new USKUpdatedCallback();
	public static final BookmarkCategory MAIN_CATEGORY = new BookmarkCategory("/");
	private final HashMap<String, Bookmark> bookmarks = new HashMap<String, Bookmark>();
	private final File bookmarksFile;
	private final File backupBookmarksFile;
	private boolean isSavingBookmarks = false;
	static {
		String name = "freenet/clients/http/staticfiles/defaultbookmarks.dat";
		SimpleFieldSet defaultBookmarks = null;
		InputStream in = null;
		try {
			ClassLoader loader = BookmarkManager.class.getClassLoader();

			// Returns null on lookup failures:
			in = loader.getResourceAsStream(name);
			if(in != null)
				defaultBookmarks = SimpleFieldSet.readFrom(in, false, false);
		} catch(Exception e) {
			Logger.error(BookmarkManager.class, "Error while loading the default bookmark file from " + name + " :" + e.getMessage(), e);
		} finally {
			Closer.close(in);
			DEFAULT_BOOKMARKS = defaultBookmarks;
		}
	}

        private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	public BookmarkManager(NodeClientCore n) {
		putPaths("/", MAIN_CATEGORY);
		this.node = n;
		this.bookmarksFile = n.node.userDir().file("bookmarks.dat");
		this.backupBookmarksFile = n.node.userDir().file("bookmarks.dat.bak");

		try {
			// Read the backup file if necessary
			if(!bookmarksFile.exists() || bookmarksFile.length() == 0)
				throw new IOException();
			Logger.normal(this, "Attempting to read the bookmark file from " + bookmarksFile.toString());
			SimpleFieldSet sfs = SimpleFieldSet.readFrom(bookmarksFile, false, true);
			readBookmarks(MAIN_CATEGORY, sfs);
		} catch(MalformedURLException mue) {
		} catch(IOException ioe) {
			Logger.error(this, "Error reading the bookmark file (" + bookmarksFile.toString() + "):" + ioe.getMessage(), ioe);

			try {
				if(backupBookmarksFile.exists() && backupBookmarksFile.canRead() && backupBookmarksFile.length() > 0) {
					Logger.normal(this, "Attempting to read the backup bookmark file from " + backupBookmarksFile.toString());
					SimpleFieldSet sfs = SimpleFieldSet.readFrom(backupBookmarksFile, false, true);
					readBookmarks(MAIN_CATEGORY, sfs);
				} else {
					Logger.normal(this, "We couldn't find the backup either! - " + FileUtil.getCanonicalFile(backupBookmarksFile));
					// restore the default bookmark set
					readBookmarks(MAIN_CATEGORY, DEFAULT_BOOKMARKS);
				}
			} catch(IOException e) {
				Logger.error(this, "Error reading the backup bookmark file !" + e.getMessage(), e);
			}
		}
	}

	public void reAddDefaultBookmarks() {
		BookmarkCategory bc = new BookmarkCategory(l10n("defaultBookmarks") + " - " + new Date());
		addBookmark("/", bc);
		_innerReadBookmarks("/", bc, DEFAULT_BOOKMARKS);
	}

	private class USKUpdatedCallback implements USKCallback {

		public void onFoundEdition(long edition, USK key, ObjectContainer container, ClientContext context, boolean wasMetadata, short codec, byte[] data, boolean newKnownGood, boolean newSlotToo) {
			if(!newKnownGood) {
				FreenetURI uri = key.copy(edition).getURI();
				node.makeClient(PRIORITY_PROGRESS, false, false).prefetch(uri, 60*60*1000, FProxyToadlet.MAX_LENGTH, null, PRIORITY_PROGRESS);
				return;
			}
			List<BookmarkItem> items = MAIN_CATEGORY.getAllItems();
			boolean matched = false;
			for(int i = 0; i < items.size(); i++) {
				if(!"USK".equals(items.get(i).getKeyType()))
					continue;

				try {
					FreenetURI furi = new FreenetURI(items.get(i).getKey());
					USK usk = USK.create(furi);

					if(usk.equals(key, false)) {
						if(logMINOR) Logger.minor(this, "Updating bookmark for "+furi+" to edition "+edition);
						matched = true;
						items.get(i).setEdition(edition, node);
						// We may have bookmarked the same site twice, so continue the search.
					}
				} catch(MalformedURLException mue) {
				}
			}
			if(matched) {
				storeBookmarks();
			} else {
				Logger.error(this, "No match for bookmark "+key+" edition "+edition);
			}
		}

		public short getPollingPriorityNormal() {
			return PRIORITY;
		}

		public short getPollingPriorityProgress() {
			return PRIORITY_PROGRESS;
		}
	}

	public String l10n(String key) {
		return NodeL10n.getBase().getString("BookmarkManager." + key);
	}

	public String parentPath(String path) {
		if(path.equals("/"))
			return "/";

		return path.substring(0, path.substring(0, path.length() - 1).lastIndexOf("/")) + "/";
	}

	public Bookmark getBookmarkByPath(String path) {
		synchronized(bookmarks) {
			return bookmarks.get(path);
		}
	}

	public BookmarkCategory getCategoryByPath(String path) {
		Bookmark cat = getBookmarkByPath(path);
		if(cat instanceof BookmarkCategory)
			return (BookmarkCategory) cat;

		return null;
	}

	public BookmarkItem getItemByPath(String path) {
		if(getBookmarkByPath(path) instanceof BookmarkItem)
			return (BookmarkItem) getBookmarkByPath(path);

		return null;
	}

	public void addBookmark(String parentPath, Bookmark bookmark) {
		if(logMINOR)
			Logger.minor(this, "Adding bookmark " + bookmark + " to " + parentPath);
		BookmarkCategory parent = getCategoryByPath(parentPath);
		parent.addBookmark(bookmark);
		putPaths(parentPath + bookmark.getName() + ((bookmark instanceof BookmarkCategory) ? "/" : ""),
			bookmark);

		if(bookmark instanceof BookmarkItem)
			subscribeToUSK((BookmarkItem)bookmark);
	}

	public void renameBookmark(String path, String newName) {
		Bookmark bookmark = getBookmarkByPath(path);
		String oldName = bookmark.getName();
		String oldPath = '/' + oldName;
		String newPath = path.substring(0, path.indexOf(oldPath)) + '/' + newName + (bookmark instanceof BookmarkCategory ? "/" : "");

		bookmark.setName(newName);
		synchronized(bookmarks) {
			Iterator<String> it = bookmarks.keySet().iterator();
			while(it.hasNext()) {
				String s = it.next();
				if(s.startsWith(path)) {
					it.remove();
				}
			}
			putPaths(newPath, bookmark);
		}
		storeBookmarks();
	}

	public void moveBookmark(String bookmarkPath, String newParentPath) {
		Bookmark b = getBookmarkByPath(bookmarkPath);
		addBookmark(newParentPath, b);

		getCategoryByPath(parentPath(bookmarkPath)).removeBookmark(b);
		removePaths(bookmarkPath);
	}

	public void removeBookmark(String path) {
		Bookmark bookmark = getBookmarkByPath(path);
		if(bookmark == null)
			return;

		if(bookmark instanceof BookmarkCategory) {
			BookmarkCategory cat = (BookmarkCategory) bookmark;
			for(int i = 0; i < cat.size(); i++)
				removeBookmark(path + cat.get(i).getName() + ((cat.get(i) instanceof BookmarkCategory) ? "/"
					: ""));
		} else
			if(((BookmarkItem) bookmark).getKeyType().equals("USK"))
				try {
					USK u = ((BookmarkItem) bookmark).getUSK();
					if(!wantUSK(u, (BookmarkItem)bookmark)) {
						this.node.uskManager.unsubscribe(u, this.uskCB);
					}
				} catch(MalformedURLException mue) {
				}

		getCategoryByPath(parentPath(path)).removeBookmark(bookmark);
		synchronized(bookmarks) {
			bookmarks.remove(path);
		}
	}

	private boolean wantUSK(USK u, BookmarkItem ignore) {
		List<BookmarkItem> items = MAIN_CATEGORY.getAllItems();
		for(BookmarkItem item : items) {
			if(item == ignore)
				continue;
			if(!"USK".equals(item.getKeyType()))
				continue;

			try {
				FreenetURI furi = new FreenetURI(item.getKey());
				USK usk = USK.create(furi);

				if(usk.equals(u, false)) return true;
			} catch(MalformedURLException mue) {
			}
		}
		return false;
	}

	public void moveBookmarkUp(String path, boolean store) {
		BookmarkCategory parent = getCategoryByPath(parentPath(path));
		parent.moveBookmarkUp(getBookmarkByPath(path));

		if(store)
			storeBookmarks();
	}

	public void moveBookmarkDown(String path, boolean store) {
		BookmarkCategory parent = getCategoryByPath(parentPath(path));
		parent.moveBookmarkDown(getBookmarkByPath(path));

		if(store)
			storeBookmarks();
	}

	private void putPaths(String path, Bookmark b) {
		synchronized(bookmarks) {
			bookmarks.put(path, b);
		}
		if(b instanceof BookmarkCategory)
			for(int i = 0; i < ((BookmarkCategory) b).size(); i++) {
				Bookmark child = ((BookmarkCategory) b).get(i);
				putPaths(path + child.getName() + (child instanceof BookmarkItem ? "" : "/"), child);
			}

	}

	private void removePaths(String path) {
		if(getBookmarkByPath(path) instanceof BookmarkCategory) {
			BookmarkCategory cat = getCategoryByPath(path);
			for(int i = 0; i < cat.size(); i++)
				removePaths(path + cat.get(i).getName() + (cat.get(i) instanceof BookmarkCategory ? "/" : ""));
		}
		bookmarks.remove(path);
	}

	public FreenetURI[] getBookmarkURIs() {
		List<BookmarkItem> items = MAIN_CATEGORY.getAllItems();
		FreenetURI[] uris = new FreenetURI[items.size()];
		for(int i = 0; i < items.size(); i++)
			uris[i] = items.get(i).getURI();

		return uris;
	}

	public void storeBookmarks() {
		Logger.normal(this, "Attempting to save bookmarks to " + bookmarksFile.toString());
		SimpleFieldSet sfs = null;
		synchronized(bookmarks) {
			if(isSavingBookmarks)
				return;
			isSavingBookmarks = true;

			sfs = toSimpleFieldSet();
		}
		FileOutputStream fos = null;
		try {
			fos = new FileOutputStream(backupBookmarksFile);
			sfs.writeTo(fos);
			fos.close();
			fos = null;
			if(!FileUtil.renameTo(backupBookmarksFile, bookmarksFile))
				Logger.error(this, "Unable to rename " + backupBookmarksFile.toString() + " to " + bookmarksFile.toString());
		} catch(IOException ioe) {
			Logger.error(this, "An error has occured saving the bookmark file :" + ioe.getMessage(), ioe);
		} finally {
			Closer.close(fos);

			synchronized(bookmarks) {
				isSavingBookmarks = false;
			}
		}
	}

	private void readBookmarks(BookmarkCategory category, SimpleFieldSet sfs) {
		_innerReadBookmarks("", category, sfs);
	}

	static final short PRIORITY = RequestStarter.BULK_SPLITFILE_PRIORITY_CLASS;
	static final short PRIORITY_PROGRESS = RequestStarter.UPDATE_PRIORITY_CLASS;

	private void subscribeToUSK(BookmarkItem item) {
		if("USK".equals(item.getKeyType()))
			try {
				USK u = item.getUSK();
				this.node.uskManager.subscribe(u, this.uskCB, true, this);
			} catch(MalformedURLException mue) {}
	}

	private synchronized void _innerReadBookmarks(String prefix, BookmarkCategory category, SimpleFieldSet sfs) {
		boolean hasBeenParsedWithoutAnyProblem = true;
		boolean isRoot = ("".equals(prefix) && MAIN_CATEGORY.equals(category));
		synchronized(bookmarks) {
			if(!isRoot)
				putPaths(prefix + category.name + '/', category);

			try {
				int nbBookmarks = sfs.getInt(BookmarkItem.NAME);
				int nbCategories = sfs.getInt(BookmarkCategory.NAME);

				for(int i = 0; i < nbBookmarks; i++) {
					SimpleFieldSet subset = sfs.getSubset(BookmarkItem.NAME + i);
					try {
						BookmarkItem item = new BookmarkItem(subset, node.alerts);
						String name = (isRoot ? "" : prefix + category.name) + '/' + item.name;
						putPaths(name, item);
						category.addBookmark(item);
						subscribeToUSK(item);
					} catch(MalformedURLException e) {
						throw new FSParseException(e);
					}
				}

				for(int i = 0; i < nbCategories; i++) {
					SimpleFieldSet subset = sfs.getSubset(BookmarkCategory.NAME + i);
					BookmarkCategory currentCategory = new BookmarkCategory(subset);
					category.addBookmark(currentCategory);
					String name = (isRoot ? "/" : (prefix + category.name + '/'));
					_innerReadBookmarks(name, currentCategory, subset.getSubset("Content"));
				}

			} catch(FSParseException e) {
				Logger.error(this, "Error parsing the bookmarks file!", e);
				hasBeenParsedWithoutAnyProblem = false;
			}

		}
		if(hasBeenParsedWithoutAnyProblem)
			storeBookmarks();
	}


	public SimpleFieldSet toSimpleFieldSet() {
		SimpleFieldSet sfs = new SimpleFieldSet(true);

		sfs.put("Version", 1);
		synchronized(bookmarks) {
			sfs.putAllOverwrite(BookmarkManager.toSimpleFieldSet(MAIN_CATEGORY));
		}

		return sfs;
	}

	public static SimpleFieldSet toSimpleFieldSet(BookmarkCategory cat) {
		SimpleFieldSet sfs = new SimpleFieldSet(true);
		List<BookmarkCategory> bc = cat.getSubCategories();

		for(int i = 0; i < bc.size(); i++) {
			BookmarkCategory currentCat = bc.get(i);
			sfs.put(BookmarkCategory.NAME + i, currentCat.getSimpleFieldSet());
		}
		sfs.put(BookmarkCategory.NAME, bc.size());


		List<BookmarkItem> bi = cat.getItems();
		for(int i = 0; i < bi.size(); i++)
			sfs.put(BookmarkItem.NAME + i, bi.get(i).getSimpleFieldSet());
		sfs.put(BookmarkItem.NAME, bi.size());

		return sfs;
	}

	public boolean persistent() {
		return false;
	}

	public void removeFrom(ObjectContainer container) {
		throw new UnsupportedOperationException();
	}

	public boolean realTimeFlag() {
		return false;
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.io;

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.util.ArrayList;

import com.db4o.ObjectContainer;
import freenet.support.LogThresholdCallback;

import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;

/**
 * A wrapper for a read-only bucket providing for multiple readers. The data is 
 * only freed when all of the readers have freed it.
 * @author toad
 */
public class MultiReaderBucket {
	
	private final Bucket bucket;
	
	// Assume there will be relatively few readers
	private ArrayList<Bucket> readers;
	
	private boolean closed;
        private static volatile boolean logMINOR;

        static {
            Logger.registerLogThresholdCallback(new LogThresholdCallback() {

                @Override
                public void shouldUpdate() {
                    logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
                }
            });
        }
	
	public MultiReaderBucket(Bucket underlying) {
		bucket = underlying;
	}

	/** Get a reader bucket */
	public Bucket getReaderBucket() {
		synchronized(this) {
			if(closed) return null;
			Bucket d = new ReaderBucket();
			if (readers == null)
				readers = new ArrayList<Bucket>(1);
			readers.add(d);
			if(logMINOR)
				Logger.minor(this, "getReaderBucket() returning "+d+" for "+this+" for "+bucket);
			return d;
		}
	}

	class ReaderBucket implements Bucket {
		
		private boolean freed;

		public void free() {
			if(logMINOR)
				Logger.minor(this, "ReaderBucket "+this+" for "+MultiReaderBucket.this+" free()ing for "+bucket);
			synchronized(MultiReaderBucket.this) {
				if(freed) return;
				freed = true;
				readers.remove(this);
				if(!readers.isEmpty()) return;
				readers = null;
				if(closed) return;
				closed = true;
			}
			bucket.free();
		}

		public InputStream getInputStream() throws IOException {
			synchronized(MultiReaderBucket.this) {
				if(freed || closed) {
					throw new IOException("Already freed");
				}
			}
			return new ReaderBucketInputStream();
		}
		
		private class ReaderBucketInputStream extends InputStream {
			
			InputStream is;
			
			ReaderBucketInputStream() throws IOException {
				is = bucket.getInputStream();
			}
			
			@Override
			public final int read() throws IOException {
				synchronized(MultiReaderBucket.this) {
					if(freed || closed) throw new IOException("Already closed");
				}
				return is.read();
			}
			
			@Override
			public final int read(byte[] data, int offset, int length) throws IOException {
				synchronized(MultiReaderBucket.this) {
					if(freed || closed) throw new IOException("Already closed");
				}
				return is.read(data, offset, length);
			}
			
			@Override
			public final int read(byte[] data) throws IOException {
				synchronized(MultiReaderBucket.this) {
					if(freed || closed) throw new IOException("Already closed");
				}
				return is.read(data);
			}
			
			@Override
			public final void close() throws IOException {
				is.close();
			}

			@Override
			public final int available() throws IOException {
				return is.available();
			}
		}
		
		public String getName() {
			return bucket.getName();
		}

		public OutputStream getOutputStream() throws IOException {
			throw new IOException("Read only");
		}

		public boolean isReadOnly() {
			return true;
		}

		public void setReadOnly() {
			// Already read only
		}

		public long size() {
			return bucket.size();
		}
		
		@Override
		protected void finalize() throws Throwable {
			free();
                        super.finalize();
		}

		public void storeTo(ObjectContainer container) {
			container.store(this);
		}

		public void removeFrom(ObjectContainer container) {
			container.delete(this);
			synchronized(MultiReaderBucket.this) {
				if(!closed) return;
			}
			bucket.removeFrom(container);
			container.delete(readers);
			container.delete(MultiReaderBucket.this);
		}

		public Bucket createShadow() {
			return null;
		}
		
	}
	
}
package freenet.client.async;

import com.db4o.ObjectContainer;

import freenet.node.SendableRequest;

public interface SendableRequestSet {
	
	public SendableRequest[] listRequests(ObjectContainer container);
	
	public boolean addRequest(SendableRequest req, ObjectContainer container);
	
	public boolean removeRequest(SendableRequest req, ObjectContainer container);

	public void removeFrom(ObjectContainer container);

}
package freenet.support.io;

import java.io.FilterInputStream;
import java.io.IOException;
import java.io.InputStream;

public class CountedInputStream extends FilterInputStream {

    protected long count = 0;

    public CountedInputStream(InputStream in) {
        super(in);
	if(in == null) throw new IllegalStateException("null fed to CountedInputStream");
    }

    public final long count() {
        return count;
    }

    @Override
	public int read() throws IOException {
        int ret = super.read();
        if (ret != -1)
            ++count;
        return ret;
    }

    @Override
	public int read(byte[] buf, int off, int len) throws IOException {
        int ret = in.read(buf, off, len);
        if (ret != -1)
            count += ret;
        return ret;
    }
    
    @Override
	public int read(byte[] buf) throws IOException {
        int ret = in.read(buf);
        if (ret != -1)
            count += ret;
        return ret;
    }
    
    @Override
	public long skip(long n) throws IOException {
    	long l = in.skip(n);
    	if(l > 0) count += l;
    	return l;
    }
}

package freenet.node.updater;

import java.io.BufferedReader;
import java.io.ByteArrayInputStream;
import java.io.DataInputStream;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.net.MalformedURLException;
import java.util.zip.ZipEntry;
import java.util.zip.ZipInputStream;

import com.db4o.ObjectContainer;

import freenet.client.FetchContext;
import freenet.client.FetchException;
import freenet.client.FetchResult;
import freenet.client.async.ClientContext;
import freenet.client.async.ClientGetCallback;
import freenet.client.async.ClientGetter;
import freenet.client.async.DatabaseDisabledException;
import freenet.client.async.USKCallback;
import freenet.keys.FreenetURI;
import freenet.keys.USK;
import freenet.node.Node;
import freenet.node.NodeClientCore;
import freenet.node.RequestClient;
import freenet.node.RequestStarter;
import freenet.node.Version;
import freenet.support.Logger;
import freenet.support.Ticker;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;
import freenet.support.io.BucketTools;
import freenet.support.io.Closer;
import freenet.support.io.FileBucket;

public abstract class NodeUpdater implements ClientGetCallback, USKCallback, RequestClient {

	static private boolean logMINOR;
	private FetchContext ctx;
	private FetchResult result;
	private ClientGetter cg;
	private FreenetURI URI;
	private final Ticker ticker;
	public final NodeClientCore core;
	protected final Node node;
	public final NodeUpdateManager manager;
	private final int currentVersion;
	private int realAvailableVersion;
	private int availableVersion;
	private int fetchingVersion;
	protected int fetchedVersion;
	private int writtenVersion;
	private int maxDeployVersion;
	private int minDeployVersion;
	private boolean isRunning;
	private boolean isFetching;
	private final String blobFilenamePrefix;
	protected File tempBlobFile;
	
	public abstract String jarName();

	NodeUpdater(NodeUpdateManager manager, FreenetURI URI, int current, int min, int max, String blobFilenamePrefix) {
		logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
		this.manager = manager;
		this.node = manager.node;
		this.URI = URI.setSuggestedEdition(Version.buildNumber() + 1);
		this.ticker = node.ticker;
		this.core = node.clientCore;
		this.currentVersion = current;
		this.availableVersion = -1;
		this.isRunning = true;
		this.cg = null;
		this.isFetching = false;
		this.blobFilenamePrefix = blobFilenamePrefix;
		this.maxDeployVersion = max;
		this.minDeployVersion = min;

		FetchContext tempContext = core.makeClient((short) 0, true).getFetchContext();
		tempContext.allowSplitfiles = true;
		tempContext.dontEnterImplicitArchives = false;
		this.ctx = tempContext;

	}

	void start() {
		try {
			// because of UoM, this version is actually worth having as well
			USK myUsk = USK.create(URI.setSuggestedEdition(currentVersion));
			core.uskManager.subscribe(myUsk, this, true, getRequestClient());
		} catch(MalformedURLException e) {
			Logger.error(this, "The auto-update URI isn't valid and can't be used");
			manager.blow("The auto-update URI isn't valid and can't be used", true);
		}
	}
	
	protected RequestClient getRequestClient() {
		return this;
	}
	
	public void onFoundEdition(long l, USK key, ObjectContainer container, ClientContext context, boolean wasMetadata, short codec, byte[] data, boolean newKnownGood, boolean newSlotToo) {
		if(newKnownGood && !newSlotToo) return;
		logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
		if(logMINOR)
			Logger.minor(this, "Found edition " + l);
		int found;
		synchronized(this) {
			if(!isRunning)
				return;
			found = (int) key.suggestedEdition;

			realAvailableVersion = found;
			if(found > maxDeployVersion) {
				System.err.println("Ignoring "+jarName() + " update edition "+l+": version too new (min "+minDeployVersion+" max "+maxDeployVersion+")");
				found = maxDeployVersion;
			}
			
			if(found <= availableVersion)
				return;
			System.err.println("Found " + jarName() + " update edition " + found);
			Logger.minor(this, "Updating availableVersion from " + availableVersion + " to " + found + " and queueing an update");
			this.availableVersion = found;
		}
		finishOnFoundEdition(found);
	}

	private void finishOnFoundEdition(int found) {
		ticker.queueTimedJob(new Runnable() {
			public void run() {
				maybeUpdate();
			}
		}, 60 * 1000); // leave some time in case we get later editions
		// LOCKING: Always take the NodeUpdater lock *BEFORE* the NodeUpdateManager lock
		if(found <= currentVersion) {
			System.err.println("Cancelling fetch for "+found+": not newer than current version "+currentVersion);
			return;
		}
		onStartFetching();
		Logger.minor(this, "Fetching " + jarName() + " update edition " + found);
	}

	protected abstract void onStartFetching();

	public void maybeUpdate() {
		ClientGetter toStart = null;
		if(!manager.isEnabled())
			return;
		if(manager.isBlown())
			return;
		ClientGetter cancelled = null;
		synchronized(this) {
			if(logMINOR)
				Logger.minor(this, "maybeUpdate: isFetching=" + isFetching + ", isRunning=" + isRunning + ", availableVersion=" + availableVersion);
			if(!isRunning) 
				return;
			if(isFetching && availableVersion == fetchingVersion) 
				return;
			if(availableVersion <= fetchedVersion)
				return;
			if(fetchingVersion < minDeployVersion || fetchingVersion == currentVersion) {
				Logger.normal(this, "Cancelling previous fetch");
				cancelled = cg;
				cg = null;
			}
			fetchingVersion = availableVersion;

			if(availableVersion > currentVersion) {
				Logger.normal(this, "Starting the update process (" + availableVersion + ')');
				System.err.println("Starting the update process: found the update (" + availableVersion + "), now fetching it.");
			}
			if(logMINOR)
				Logger.minor(this, "Starting the update process (" + availableVersion + ')');
			// We fetch it
			try {
				if((cg == null) || cg.isCancelled()) {
					if(logMINOR)
						Logger.minor(this, "Scheduling request for " + URI.setSuggestedEdition(availableVersion));
					if(availableVersion > currentVersion)
						System.err.println("Starting " + jarName() + " fetch for " + availableVersion);
					tempBlobFile =
						File.createTempFile(blobFilenamePrefix + availableVersion + "-", ".fblob.tmp", manager.node.clientCore.getPersistentTempDir());
					FreenetURI uri = URI.setSuggestedEdition(availableVersion);
					uri = uri.sskForUSK();
					cg = new ClientGetter(this,  
						uri, ctx, RequestStarter.IMMEDIATE_SPLITFILE_PRIORITY_CLASS,
						this, null, new FileBucket(tempBlobFile, false, false, false, false, false));
					toStart = cg;
				} else {
					System.err.println("Already fetching "+jarName() + " fetch for " + fetchingVersion + " want "+availableVersion);
				}
				isFetching = true;
			} catch(Exception e) {
				Logger.error(this, "Error while starting the fetching: " + e, e);
				isFetching = false;
			}
		}
		if(toStart != null)
			try {
				node.clientCore.clientContext.start(toStart);
			} catch(FetchException e) {
				Logger.error(this, "Error while starting the fetching: " + e, e);
				synchronized(this) {
					isFetching = false;
				}
			} catch (DatabaseDisabledException e) {
				// Impossible
			}
		if(cancelled != null)
			cancelled.cancel(null, core.clientContext);
	}

	File getBlobFile(int availableVersion) {
		return new File(node.clientCore.getPersistentTempDir(), blobFilenamePrefix + availableVersion + ".fblob");
	}
	private final Object writeJarSync = new Object();

	public void writeJarTo(File fNew) throws IOException {
		int fetched;
		synchronized(this) {
			fetched = fetchedVersion;
		}
		synchronized(writeJarSync) {
			if (!fNew.delete() && fNew.exists()) {
				System.err.println("Can't delete " + fNew + "!");
			}

			FileOutputStream fos;
			fos = new FileOutputStream(fNew);

			BucketTools.copyTo(result.asBucket(), fos, -1);

			fos.flush();
			fos.close();
		}
		synchronized(this) {
			writtenVersion = fetched;
		}
		System.err.println("Written " + jarName() + " to " + fNew);
	}

	public void onSuccess(FetchResult result, ClientGetter state, ObjectContainer container) {
		onSuccess(result, state, tempBlobFile, fetchingVersion);
	}

	void onSuccess(FetchResult result, ClientGetter state, File tempBlobFile, int fetchedVersion) {
		logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
		synchronized(this) {
			if(fetchedVersion <= this.fetchedVersion) {
				tempBlobFile.delete();
				if(result != null) {
					Bucket toFree = result.asBucket();
					if(toFree != null)
						toFree.free();
				}
				return;
			}
			if(result == null || result.asBucket() == null || result.asBucket().size() == 0) {
				tempBlobFile.delete();
				Logger.error(this, "Cannot update: result either null or empty for " + availableVersion);
				System.err.println("Cannot update: result either null or empty for " + availableVersion);
				// Try again
				if(result == null || result.asBucket() == null || availableVersion > fetchedVersion)
					node.ticker.queueTimedJob(new Runnable() {

						public void run() {
							maybeUpdate();
						}
					}, 0);
				return;
			}
			File blobFile = getBlobFile(fetchedVersion);
			if(!tempBlobFile.renameTo(blobFile)) {
				blobFile.delete();
				if(!tempBlobFile.renameTo(blobFile))
					if(blobFile.exists() && tempBlobFile.exists() &&
						blobFile.length() == tempBlobFile.length())
						Logger.minor(this, "Can't rename " + tempBlobFile + " over " + blobFile + " for " + fetchedVersion + " - probably not a big deal though as the files are the same size");
					else
						Logger.error(this, "Not able to rename binary blob for node updater: " + tempBlobFile + " -> " + blobFile + " - may not be able to tell other peers about this build");
			}
			this.fetchedVersion = fetchedVersion;
			System.out.println("Found " + jarName() + " version " + fetchedVersion);
			if(fetchedVersion > currentVersion)
				Logger.normal(this, "Found version " + fetchedVersion + ", setting up a new UpdatedVersionAvailableUserAlert");
			maybeParseManifest(result);
			this.cg = null;
			if(this.result != null)
				this.result.asBucket().free();
			this.result = result;
		}
		processSuccess();
	}
	
	/** We have fetched the jar! Do something after onSuccess(). Called unlocked. */
	protected abstract void processSuccess();

	/** Called with locks held 
	 * @param result */
	protected abstract void maybeParseManifest(FetchResult result);

	protected void parseManifest(FetchResult result) {
		InputStream is = null;
		try {
			is = result.asBucket().getInputStream();
			ZipInputStream zis = new ZipInputStream(is);
			try {
				ZipEntry ze;
				while(true) {
					ze = zis.getNextEntry();
					if(ze == null) break;
					if(ze.isDirectory()) continue;
					String name = ze.getName();
					
					if(name.equals("META-INF/MANIFEST.MF")) {
						if(logMINOR) Logger.minor(this, "Found manifest");
						long size = ze.getSize();
						if(logMINOR) Logger.minor(this, "Manifest size: "+size);
						if(size > MAX_MANIFEST_SIZE) {
							Logger.error(this, "Manifest is too big: "+size+" bytes, limit is "+MAX_MANIFEST_SIZE);
							break;
						}
						byte[] buf = new byte[(int) size];
						DataInputStream dis = new DataInputStream(zis);
						dis.readFully(buf);
						ByteArrayInputStream bais = new ByteArrayInputStream(buf);
						InputStreamReader isr = new InputStreamReader(bais, "UTF-8");
						BufferedReader br = new BufferedReader(isr);
						String line;
						while((line = br.readLine()) != null) {
							parseManifestLine(line);
						}
					} else {
						zis.closeEntry();
					}
				}
			} finally {
				Closer.close(zis);
			}
		} catch (IOException e) {
			Logger.error(this, "IOException trying to read manifest on update");
		} catch (Throwable t) {
			Logger.error(this, "Failed to parse update manifest: "+t, t);
		} finally {
			Closer.close(is);
		}
	}

	protected void parseManifestLine(String line) {
		// Do nothing by default, only some NodeUpdater's will use this, those that don't won't call parseManifest().
	}
	
	private static final int MAX_MANIFEST_SIZE = 1024*1024;

	public void onFailure(FetchException e, ClientGetter state, ObjectContainer container) {
		logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
		if(!isRunning)
			return;
		int errorCode = e.getMode();
		tempBlobFile.delete();

		if(logMINOR)
			Logger.minor(this, "onFailure(" + e + ',' + state + ')');
		synchronized(this) {
			this.cg = null;
			isFetching = false;
		}
		if(errorCode == FetchException.CANCELLED ||
			!e.isFatal()) {
			Logger.normal(this, "Rescheduling new request");
			ticker.queueTimedJob(new Runnable() {

				public void run() {
					maybeUpdate();
				}
			}, 0);
		} else {
			Logger.error(this, "Canceling fetch : " + e.getMessage());
			System.err.println("Unexpected error fetching update: " + e.getMessage());
			if(e.isFatal()) {
				// Wait for the next version
			} else
				ticker.queueTimedJob(new Runnable() {

					public void run() {
						maybeUpdate();
					}
				}, 60 * 60 * 1000);
		}
	}

	/** Called before kill(). Don't do anything that will involve taking locks. */
	public void preKill() {
		isRunning = false;
	}

	void kill() {
		try {
			ClientGetter c;
			synchronized(this) {
				isRunning = false;
				USK myUsk = USK.create(URI.setSuggestedEdition(currentVersion));
				core.uskManager.unsubscribe(myUsk, this);
				c = cg;
				cg = null;
			}
			c.cancel(null, core.clientContext);
		} catch(Exception e) {
			Logger.minor(this, "Cannot kill NodeUpdater", e);
		}
	}

	public FreenetURI getUpdateKey() {
		return URI;
	}

	public void onMajorProgress(ObjectContainer container) {
		// Ignore
	}

	public synchronized boolean canUpdateNow() {
		return fetchedVersion > currentVersion;
	}

	/** Called when the fetch URI has changed. No major locks are held by caller. 
	 * @param uri The new URI. */
	public void onChangeURI(FreenetURI uri) {
		kill();
		this.URI = uri;
		maybeUpdate();
	}

	public int getWrittenVersion() {
		return writtenVersion;
	}

	public int getFetchedVersion() {
		return fetchedVersion;
	}

	public boolean isFetching() {
		return availableVersion > fetchedVersion && availableVersion > currentVersion;
	}

	public int fetchingVersion() {
		// We will not deploy currentVersion...
		if(fetchingVersion <= currentVersion)
			return availableVersion;
		else
			return fetchingVersion;
	}

	public long getBlobSize() {
		return getBlobFile(getFetchedVersion()).length();
	}

	public File getBlobFile() {
		return getBlobFile(getFetchedVersion());
	}

	public short getPollingPriorityNormal() {
		return RequestStarter.IMMEDIATE_SPLITFILE_PRIORITY_CLASS;
	}

	public short getPollingPriorityProgress() {
		return RequestStarter.INTERACTIVE_PRIORITY_CLASS;
	}

	public boolean persistent() {
		return false;
	}

	/**
	** Called by NodeUpdateManager to re-set the min/max versions for ext when
	** a new freenet.jar has been downloaded. This is to try to avoid the node
	** installing incompatible versions of main and ext.
	*/
	public void setMinMax(int requiredExt, int recommendedExt) {
		int callFinishedFound = -1;
		synchronized(this) {
			if(recommendedExt > -1) {
				maxDeployVersion = recommendedExt;
			}
			if(requiredExt > -1) {
				minDeployVersion = requiredExt;
				if(realAvailableVersion != availableVersion && availableVersion < requiredExt && realAvailableVersion >= requiredExt) {
					// We found a revision but didn't fetch it because it wasn't within the range for the old jar.
					// The new one requires it, however.
					System.err.println("Previously out-of-range edition "+realAvailableVersion+" is now needed by the new jar; scheduling fetch.");
					callFinishedFound = availableVersion = realAvailableVersion;
				} else if(availableVersion < requiredExt) {
					// Including if it hasn't been found at all
					// Just try it ...
					callFinishedFound = availableVersion = requiredExt;
					System.err.println("Need minimum edition "+requiredExt+" for new jar, found "+availableVersion+"; scheduling fetch.");
				}
			}
		}
		if(callFinishedFound > -1)
			finishOnFoundEdition(callFinishedFound);
	}
	
	public boolean objectCanNew(ObjectContainer container) {
		Logger.error(this, "Not storing NodeUpdater in database", new Exception("error"));
		return false;
	}

	public void removeFrom(ObjectContainer container) {
		throw new UnsupportedOperationException();
	}
	
	public boolean realTimeFlag() {
		return false;
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.node.Node;
import freenet.support.Fields;
import freenet.support.SimpleFieldSet;

public class GetConfig extends FCPMessage {

	final boolean withCurrent;
	final boolean withDefaults;
	final boolean withSortOrder;
	final boolean withExpertFlag;
	final boolean withForceWriteFlag;
	final boolean withShortDescription;
	final boolean withLongDescription;
	final boolean withDataTypes;
	static final String NAME = "GetConfig";
	final String identifier;
	
	public GetConfig(SimpleFieldSet fs) {
		withCurrent = Fields.stringToBool(fs.get("WithCurrent"), false);
		withDefaults = Fields.stringToBool(fs.get("WithDefaults"), false);
		withSortOrder = Fields.stringToBool(fs.get("WithSortOrder"), false);
		withExpertFlag = Fields.stringToBool(fs.get("WithExpertFlag"), false);
		withForceWriteFlag = Fields.stringToBool(fs.get("WithForceWriteFlag"), false);
		withShortDescription = Fields.stringToBool(fs.get("WithShortDescription"), false);
		withLongDescription = Fields.stringToBool(fs.get("WithLongDescription"), false);
		withDataTypes = Fields.stringToBool(fs.get("WithDataTypes"), false);
		this.identifier = fs.get("Identifier");
		fs.removeValue("Identifier");
	}
	
	@Override
	public SimpleFieldSet getFieldSet() {
		return new SimpleFieldSet(true);
	}
	
	@Override
	public String getName() {
		return NAME;
	}
	
	@Override
	public void run(FCPConnectionHandler handler, Node node)
			throws MessageInvalidException {
		if(!handler.hasFullAccess()) {
			throw new MessageInvalidException(ProtocolErrorMessage.ACCESS_DENIED, "GetConfig requires full access", identifier, false);
		}
		handler.outputHandler.queue(new ConfigData(node, withCurrent, withDefaults, withSortOrder, withExpertFlag, withForceWriteFlag, withShortDescription, withLongDescription, withDataTypes, identifier));
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		container.delete(this);
	}
	
}
package freenet.node.fcp;

import java.io.BufferedOutputStream;
import java.io.Closeable;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.net.MalformedURLException;
import java.net.Socket;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Random;

import com.db4o.ObjectContainer;

import freenet.client.async.ClientContext;
import freenet.client.async.DBJob;
import freenet.client.async.DatabaseDisabledException;
import freenet.node.RequestClient;
import freenet.support.HexUtil;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.api.BucketFactory;
import freenet.support.io.Closer;
import freenet.support.io.FileUtil;
import freenet.support.io.NativeThread;

public class FCPConnectionHandler implements Closeable {
	private static final class DirectoryAccess {
		final boolean canWrite;
		final boolean canRead;
		
		public DirectoryAccess(boolean canRead, boolean canWrite) {
			this.canRead = canRead;
			this.canWrite = canWrite;
		}
	}
	
	public static class DDACheckJob {
		final File directory, readFilename, writeFilename;
		final String readContent, writeContent; 
		
		/**
		 * null if not requested.
		 */
		DDACheckJob(Random r, File directory, File readFilename, File writeFilename) {
			this.directory = directory;
			this.readFilename = readFilename;
			this.writeFilename = writeFilename;
			
			byte[] random = new byte[128];
			
			r.nextBytes(random);
			this.readContent = HexUtil.bytesToHex(random);

			r.nextBytes(random);
			this.writeContent = HexUtil.bytesToHex(random);
		}
	}

	final FCPServer server;
	final Socket sock;
	final FCPConnectionInputHandler inputHandler;
	final Map<String, SubscribeUSK> uskSubscriptions;
	public final FCPConnectionOutputHandler outputHandler;
	private boolean isClosed;
	private boolean inputClosed;
	private boolean outputClosed;
	private String clientName;
	private FCPClient rebootClient;
	private FCPClient foreverClient;
	private boolean failedGetForever;
	final BucketFactory bf;
	final HashMap<String, ClientRequest> requestsByIdentifier;
	protected final String connectionIdentifier;
	private static volatile boolean logMINOR;
	private boolean killedDupe;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}
	// We are confident that the given client can access those
	private final HashMap<String, DirectoryAccess> checkedDirectories = new HashMap<String, DirectoryAccess>();
	// DDACheckJobs in flight
	private final HashMap<File, DDACheckJob> inTestDirectories = new HashMap<File, DDACheckJob>();
	public final RequestClient connectionRequestClientBulk = new RequestClient() {
		
		public boolean persistent() {
			return false;
		}
		
		public void removeFrom(ObjectContainer container) {
			throw new UnsupportedOperationException();
		}

		public boolean realTimeFlag() {
			return false;
		}
		
	};
	public final RequestClient connectionRequestClientRT = new RequestClient() {
		
		public boolean persistent() {
			return false;
		}
		
		public void removeFrom(ObjectContainer container) {
			throw new UnsupportedOperationException();
		}

		public boolean realTimeFlag() {
			return true;
		}
		
	};
	
	public FCPConnectionHandler(Socket s, FCPServer server) {
		this.sock = s;
		this.server = server;
		isClosed = false;
		this.bf = server.core.tempBucketFactory;
		requestsByIdentifier = new HashMap<String, ClientRequest>();
		uskSubscriptions = new HashMap<String, SubscribeUSK>();
		this.inputHandler = new FCPConnectionInputHandler(this);
		this.outputHandler = new FCPConnectionOutputHandler(this);
		
		byte[] identifier = new byte[16];
		server.node.random.nextBytes(identifier);
		this.connectionIdentifier = HexUtil.bytesToHex(identifier);
	}
	
	void start() {
		inputHandler.start();
		outputHandler.start();
	}

	public void close() {
		ClientRequest[] requests;
		if(rebootClient != null)
			rebootClient.onLostConnection(this);
		if(foreverClient != null)
			foreverClient.onLostConnection(this);
		boolean dupe;
		SubscribeUSK[] uskSubscriptions2;
		synchronized(this) {
			if(isClosed) {
				Logger.error(this, "Already closed: "+this, new Exception("debug"));
				return;
			}
			isClosed = true;
			requests = new ClientRequest[requestsByIdentifier.size()];
			requests = requestsByIdentifier.values().toArray(requests);
			requestsByIdentifier.clear();
			uskSubscriptions2 = uskSubscriptions.values().toArray(new SubscribeUSK[uskSubscriptions.size()]);
			dupe = killedDupe;
		}
		for(ClientRequest req : requests)
			req.onLostConnection(null, server.core.clientContext);
		for(SubscribeUSK sub : uskSubscriptions2)
			sub.unsubscribe();
		if(!dupe) {
		try {
			server.core.clientContext.jobRunner.queue(new DBJob() {

				public boolean run(ObjectContainer container, ClientContext context) {
					if((rebootClient != null) && !rebootClient.hasPersistentRequests(null))
						server.unregisterClient(rebootClient, null);
					if(foreverClient != null) {
						if(!container.ext().isStored(foreverClient)) {
							Logger.normal(this, "foreverClient is not stored in the database in lost connection non-dupe callback; not deleting it");
							return false;
						}
						container.activate(foreverClient, 1);
						if(!foreverClient.hasPersistentRequests(container))
							server.unregisterClient(foreverClient, container);
						container.deactivate(foreverClient, 1);
					}
					return false;
				}
				
			}, NativeThread.NORM_PRIORITY, false);
		} catch (DatabaseDisabledException e) {
			// Ignore
		}
		}
		
		outputHandler.onClosed();
	}
	
	synchronized void setKilledDupe() {
		killedDupe = true;
	}
	
	public synchronized boolean isClosed() {
		return isClosed;
	}
	
	public void closedInput() {
		try {
			sock.shutdownInput();
		} catch (IOException e) {
			// Ignore
		}
		synchronized(this) {
			inputClosed = true;
			if(!outputClosed) return;
		}
		try {
			sock.close();
		} catch (IOException e) {
			// Ignore
		}
	}
	
	public void closedOutput() {
		try {
			sock.shutdownOutput();
		} catch (IOException e) {
			// Ignore
		}
		synchronized(this) {
			outputClosed = true;
			if(!inputClosed) return;
		}
		try {
			sock.close();
		} catch (IOException e) {
			// Ignore
		}
	}

	public void setClientName(final String name) {
		this.clientName = name;
		rebootClient = server.registerRebootClient(name, server.core, this);
		rebootClient.queuePendingMessagesOnConnectionRestartAsync(outputHandler, null, server.core.clientContext);
		// Create foreverClient lazily. Everything that needs it (especially creating ClientGet's etc) runs on a database job.
		if(logMINOR)
			Logger.minor(this, "Set client name: "+name);
	}
	
	protected FCPClient createForeverClient(String name, ObjectContainer container) {
		synchronized(FCPConnectionHandler.this) {
			if(foreverClient != null) return foreverClient;
		}
		FCPClient client = server.registerForeverClient(name, server.core, FCPConnectionHandler.this, container);
		synchronized(FCPConnectionHandler.this) {
			foreverClient = client;
			FCPConnectionHandler.this.notifyAll();
		}
		client.queuePendingMessagesOnConnectionRestartAsync(outputHandler, container, server.core.clientContext);
		return foreverClient;
	}

	public String getClientName() {
		return clientName;
	}

	// FIXME next 3 methods are in need of refactoring!
	
	/**
	 * Start a ClientGet. If there is an identifier collision, queue an IdentifierCollisionMessage.
	 * Hence, we can run stuff on other threads if we need to, as long as we send the right messages.
	 */
	public void startClientGet(final ClientGetMessage message) {
		final String id = message.identifier;
		final boolean global = message.global;
		ClientGet cg = null;
		boolean success;
		boolean persistent = message.persistenceType != ClientRequest.PERSIST_CONNECTION;
		synchronized(this) {
			if(isClosed) return;
			// We need to track non-persistent requests anyway, so we may as well check
			if(persistent)
				success = true;
			else
				success = !requestsByIdentifier.containsKey(id);
			if(success) {
				try {
					
					if(!persistent) {
						cg = new ClientGet(this, message, server, null);
						requestsByIdentifier.put(id, cg);
					} else if(message.persistenceType == ClientRequest.PERSIST_FOREVER) {
						try {
							server.core.clientContext.jobRunner.queue(new DBJob() {

								public boolean run(ObjectContainer container, ClientContext context) {
									ClientGet getter;
									try {
										getter = new ClientGet(FCPConnectionHandler.this, message, server, container);
									} catch (IdentifierCollisionException e1) {
										Logger.normal(this, "Identifier collision on "+this);
										FCPMessage msg = new IdentifierCollisionMessage(id, message.global);
										outputHandler.queue(msg);
										return false;
									} catch (MessageInvalidException e1) {
										outputHandler.queue(new ProtocolErrorMessage(e1.protocolCode, false, e1.getMessage(), e1.ident, e1.global));
										return false;
									}
									try {
										getter.register(container, false);
										container.store(getter);
									} catch (IdentifierCollisionException e) {
										Logger.normal(this, "Identifier collision on "+this);
										FCPMessage msg = new IdentifierCollisionMessage(id, global);
										outputHandler.queue(msg);
										return false;
									}
									getter.start(container, context);
									container.deactivate(getter, 1);
									return true;
								}
								
							}, NativeThread.HIGH_PRIORITY-1, false);
						} catch (DatabaseDisabledException e) {
							outputHandler.queue(new ProtocolErrorMessage(ProtocolErrorMessage.PERSISTENCE_DISABLED, false, "Persistence is disabled", id, global));
							return;
						} // user wants a response soon... but doesn't want it to block the queue page etc
						return; // Don't run the start() below
					} else {
						cg = new ClientGet(this, message, server, null);
					}
				} catch (IdentifierCollisionException e) {
					success = false;
				} catch (MessageInvalidException e) {
					outputHandler.queue(new ProtocolErrorMessage(e.protocolCode, false, e.getMessage(), e.ident, e.global));
					return;
				}
			}
		}
		if(message.persistenceType == ClientRequest.PERSIST_REBOOT)
			try {
				cg.register(null, false);
			} catch (IdentifierCollisionException e) {
				success = false;
			}
		if(!success) {
			Logger.normal(this, "Identifier collision on "+this);
			FCPMessage msg = new IdentifierCollisionMessage(id, message.global);
			outputHandler.queue(msg);
			return;
		} else {
			cg.start(null, server.core.clientContext);
		}
	}

	public void startClientPut(final ClientPutMessage message) {
		if(logMINOR)
			Logger.minor(this, "Starting insert ID=\""+message.identifier+ '"');
		final String id = message.identifier;
		final boolean global = message.global;
		ClientPut cp = null;
		boolean persistent = message.persistenceType != ClientRequest.PERSIST_CONNECTION;
		FCPMessage failedMessage = null;
		synchronized(this) {
			boolean success;
			if(isClosed) return;
			// We need to track non-persistent requests anyway, so we may as well check
			if(persistent)
				success = true;
			else
				success = !requestsByIdentifier.containsKey(id);
			if(success) {
				if(!persistent) {
					try {
						cp = new ClientPut(this, message, server, null);
						requestsByIdentifier.put(id, cp);
					} catch (IdentifierCollisionException e) {
						success = false;
					} catch (MessageInvalidException e) {
						outputHandler.queue(new ProtocolErrorMessage(e.protocolCode, false, e.getMessage(), e.ident, e.global));
						return;
					} catch (MalformedURLException e) {
						failedMessage = new ProtocolErrorMessage(ProtocolErrorMessage.FREENET_URI_PARSE_ERROR, true, null, id, message.global);
					}
				} else if(message.persistenceType == ClientRequest.PERSIST_FOREVER) {
					try {
						server.core.clientContext.jobRunner.queue(new DBJob() {

							public boolean run(ObjectContainer container, ClientContext context) {
								ClientPut putter;
								try {
									putter = new ClientPut(FCPConnectionHandler.this, message, server, container);
								} catch (IdentifierCollisionException e) {
									Logger.normal(this, "Identifier collision on "+this);
									FCPMessage msg = new IdentifierCollisionMessage(id, message.global);
									outputHandler.queue(msg);
									return false;
								} catch (MessageInvalidException e) {
									outputHandler.queue(new ProtocolErrorMessage(e.protocolCode, false, e.getMessage(), e.ident, e.global));
									return false;
								} catch (MalformedURLException e) {
									outputHandler.queue(new ProtocolErrorMessage(ProtocolErrorMessage.FREENET_URI_PARSE_ERROR, true, null, id, message.global));
									return false;
								}
								try {
									putter.register(container, false);
									container.store(putter);
								} catch (IdentifierCollisionException e) {
									Logger.normal(this, "Identifier collision on "+this);
									FCPMessage msg = new IdentifierCollisionMessage(id, global);
									outputHandler.queue(msg);
									return false;
								}
								putter.start(container, context);
								container.deactivate(putter, 1);
								return true;
							}
							
						}, NativeThread.HIGH_PRIORITY-1, false);
					} catch (DatabaseDisabledException e) {
						outputHandler.queue(new ProtocolErrorMessage(ProtocolErrorMessage.PERSISTENCE_DISABLED, false, "Persistence is disabled", id, global));
					} // user wants a response soon... but doesn't want it to block the queue page etc
					return; // Don't run the start() below
				} else {
					try {
						cp = new ClientPut(this, message, server, null);
					} catch (IdentifierCollisionException e) {
						success = false;
					} catch (MessageInvalidException e) {
						outputHandler.queue(new ProtocolErrorMessage(e.protocolCode, false, e.getMessage(), e.ident, e.global));
						return;
					} catch (MalformedURLException e) {
						failedMessage = new ProtocolErrorMessage(ProtocolErrorMessage.FREENET_URI_PARSE_ERROR, true, null, id, message.global);
					}
				}
			}
			if(!success) {
				Logger.normal(this, "Identifier collision on "+this);
				failedMessage = new IdentifierCollisionMessage(id, message.global);
			}
		}
		if(message.persistenceType == ClientRequest.PERSIST_REBOOT)
			try {
				cp.register(null, false);
			} catch (IdentifierCollisionException e) {
				failedMessage = new IdentifierCollisionMessage(id, message.global);
			}
		if(failedMessage != null) {
			outputHandler.queue(failedMessage);
			if(persistent && message.persistenceType == ClientRequest.PERSIST_FOREVER) {
				final ClientPut c = cp;
				// Run on the database thread if persistent because it will try to activate stuff...
				try {
					server.core.clientContext.jobRunner.queue(new DBJob() {

						public boolean run(ObjectContainer container, ClientContext context) {
							if(c != null)
								c.freeData(container);
							else
								message.freeData(container);
							return true;
						}
						
					}, NativeThread.HIGH_PRIORITY-1, false);
				} catch (DatabaseDisabledException e) {
					Logger.error(this, "Unable to free data for insert because database disabled: "+e, e);
				}
			} else {
				if(cp != null)
					cp.freeData(null);
				else
					message.freeData(null);
			}
			return;
		} else {
			Logger.minor(this, "Starting "+cp);
			cp.start(null, server.core.clientContext);
		}
	}

	public void startClientPutDir(final ClientPutDirMessage message, final HashMap<String, Object> buckets, final boolean wasDiskPut) {
		if(logMINOR)
			Logger.minor(this, "Start ClientPutDir");
		final String id = message.identifier;
		final boolean global = message.global;
		ClientPutDir cp = null;
		FCPMessage failedMessage = null;
		boolean persistent = message.persistenceType != ClientRequest.PERSIST_CONNECTION;
		// We need to track non-persistent requests anyway, so we may as well check
		boolean success;
		synchronized(this) {
			if(isClosed) return;
			if(!persistent)
				success = true;
			else
				success = !requestsByIdentifier.containsKey(id);
		}
		if(success) {
			if(!persistent) {
				try {
					cp = new ClientPutDir(this, message, buckets, wasDiskPut, server, null);
					synchronized(this) {
						requestsByIdentifier.put(id, cp);
					}
				} catch (IdentifierCollisionException e) {
					success = false;
				} catch (MalformedURLException e) {
					failedMessage = new ProtocolErrorMessage(ProtocolErrorMessage.FREENET_URI_PARSE_ERROR, true, null, id, message.global);
				}
				// FIXME register non-persistent requests in the constructors also, we already register persistent ones...
			} else if(message.persistenceType == ClientRequest.PERSIST_FOREVER) {
				try {
					server.core.clientContext.jobRunner.queue(new DBJob() {

						public boolean run(ObjectContainer container, ClientContext context) {
							ClientPutDir putter;
							try {
								putter = new ClientPutDir(FCPConnectionHandler.this, message, buckets, wasDiskPut, server, container);
							} catch (IdentifierCollisionException e) {
								Logger.normal(this, "Identifier collision on "+this);
								FCPMessage msg = new IdentifierCollisionMessage(id, message.global);
								outputHandler.queue(msg);
								return false;
							} catch (MalformedURLException e) {
								outputHandler.queue(new ProtocolErrorMessage(ProtocolErrorMessage.FREENET_URI_PARSE_ERROR, true, null, id, message.global));
								return false;
							}
							try {
								putter.register(container, false);
								container.store(putter);
							} catch (IdentifierCollisionException e) {
								Logger.normal(this, "Identifier collision on "+this);
								FCPMessage msg = new IdentifierCollisionMessage(id, global);
								outputHandler.queue(msg);
								return false;
							}
							putter.start(container, context);
							container.deactivate(putter, 1);
							return true;
						}
						
					}, NativeThread.HIGH_PRIORITY-1, false);
				} catch (DatabaseDisabledException e) {
					outputHandler.queue(new ProtocolErrorMessage(ProtocolErrorMessage.PERSISTENCE_DISABLED, false, "Persistence is disabled", id, global));
				} // user wants a response soon... but doesn't want it to block the queue page etc
				return; // Don't run the start() below
				
			} else {
				try {
					cp = new ClientPutDir(this, message, buckets, wasDiskPut, server, null);
				} catch (IdentifierCollisionException e) {
					success = false;
				} catch (MalformedURLException e) {
					failedMessage = new ProtocolErrorMessage(ProtocolErrorMessage.FREENET_URI_PARSE_ERROR, true, null, id, message.global);
				}
			}
			if(!success) {
				Logger.normal(this, "Identifier collision on "+this);
				failedMessage = new IdentifierCollisionMessage(id, message.global);
			}
		}
		
		if(message.persistenceType == ClientRequest.PERSIST_REBOOT)
			try {
				cp.register(null, false);
			} catch (IdentifierCollisionException e) {
				failedMessage = new IdentifierCollisionMessage(id, message.global);
			}
		if(failedMessage != null) {
			// FIXME do we need to freeData???
			outputHandler.queue(failedMessage);
			if(cp != null)
				cp.cancel(null, server.core.clientContext);
			return;
		} else {
			if(logMINOR)
				Logger.minor(this, "Starting "+cp);
			cp.start(null, server.core.clientContext);
		}
	}
	
	public FCPClient getRebootClient() {
		return rebootClient;
	}

	public FCPClient getForeverClient(ObjectContainer container) {
		synchronized(this) {
			if(foreverClient == null) {
				foreverClient = createForeverClient(clientName, container);
			}
			container.activate(foreverClient, 1);
			foreverClient.init(container);
			return foreverClient;
		}
	}

	public void finishedClientRequest(ClientRequest get) {
		synchronized(this) {
			requestsByIdentifier.remove(get.getIdentifier());
		}
	}

	public boolean isGlobalSubscribed() {
		return rebootClient.watchGlobal;
	}

	public boolean hasFullAccess() {
		return server.allowedHostsFullAccess.allowed(sock.getInetAddress());
	}

	/**
	 * That method ought to be called before any DirectDiskAccess operation is performed by the node
	 * @param filename
	 * @param writeRequest : Are willing to write or to read ?
	 * @return boolean : allowed or not
	 */
	protected boolean allowDDAFrom(File filename, boolean writeRequest) {
		String parentDirectory = FileUtil.getCanonicalFile(filename).getParent();
		DirectoryAccess da = null;
		
		synchronized (checkedDirectories) {
				da = checkedDirectories.get(parentDirectory);
		}
		
		if(logMINOR)
			Logger.minor(this, "Checking DDA: "+da+" for "+parentDirectory);
		
		if(writeRequest)
			return (da == null ? server.isDownloadDDAAlwaysAllowed() : da.canWrite);
		else
			return (da == null ? server.isUploadDDAAlwaysAllowed() : da.canRead);
	}
	
	/**
	 * SHOULD BE CALLED ONLY FROM TestDDAComplete!
	 * @param path
	 * @param read
	 * @param write
	 */
	protected void registerTestDDAResult(String path, boolean read, boolean write) {
		DirectoryAccess da = new DirectoryAccess(read, write);
		
		synchronized (checkedDirectories) {
				checkedDirectories.put(path, da);
		}
		
		if(logMINOR)
			Logger.minor(this, "DDA: read="+read+" write="+write+" for "+path);
	}
	
	/**
	 * Return a DDACheckJob : the one we created and have enqueued
	 * @param path
	 * @param read : is Read access requested ?
	 * @param write : is Write access requested ?
	 * @return
	 * @throws IllegalArgumentException
	 * 
	 * FIXME: Maybe we need to enqueue a PS job to delete the created file after something like ... 5 mins ?
	 */
	protected DDACheckJob enqueueDDACheck(String path, boolean read, boolean write) throws IllegalArgumentException {
		File directory = FileUtil.getCanonicalFile(new File(path));
		if(!directory.exists() || !directory.isDirectory())
			throw new IllegalArgumentException("The specified path isn't a directory! or doesn't exist or the node doesn't have access to it!");
		
		// See #1856
		DDACheckJob job = null;
		synchronized (inTestDirectories) {
			job = inTestDirectories.get(directory);
		}
		if(job != null)
			throw new IllegalArgumentException("There is already a TestDDA going on for that directory!");
		
		File writeFile = (write ? new File(path, "DDACheck-" + server.node.fastWeakRandom.nextInt() + ".tmp") : null);
		File readFile = null;
		if(read) {
			try {
				readFile = File.createTempFile("DDACheck-", ".tmp", directory);
				readFile.deleteOnExit();
			} catch (IOException e) {
				// Now we know it: we can't write there ;)
				readFile = null;
			}
		}

		DDACheckJob result = new DDACheckJob(server.node.fastWeakRandom, directory, readFile, writeFile);
		synchronized (inTestDirectories) {
			inTestDirectories.put(directory, result);
		}
		
		if(read && (readFile != null) && readFile.canWrite()){ 
			// We don't want to attempt to write before: in case an IOException is raised, we want to inform the
			// client somehow that the node can't write there... And setting readFile to null means we won't inform
			// it on the status (as if it hadn't requested us to do the test).
			FileOutputStream fos = null;
			BufferedOutputStream bos = null;
			try {
				fos = new FileOutputStream(result.readFilename);
				bos = new BufferedOutputStream(fos);
				bos.write(result.readContent.getBytes("UTF-8"));
				bos.flush();
			} catch (IOException e) {
				Logger.error(this, "Got a IOE while creating the file (" + readFile.toString() + " ! " + e.getMessage());
			} finally {
				Closer.close(bos);
				Closer.close(fos);
			}
		}
		
		return result;
	}
	
	/**
	 * Return a DDACheckJob or null if not found
	 * @param path
	 * @return the DDACheckJob
	 * @throws IllegalArgumentException
	 */
	protected DDACheckJob popDDACheck(String path) throws IllegalArgumentException {
		File directory = FileUtil.getCanonicalFile(new File(path));
		if(!directory.exists() || !directory.isDirectory())
			throw new IllegalArgumentException("The specified path isn't a directory! or doesn't exist or the node doesn't have access to it!");
		
		synchronized (inTestDirectories) {
			return inTestDirectories.remove(directory);
		}
	}
	
	/**
	 * Delete the files we have created using DDATest
	 * called by FCPClient.onDisconnect(handler)
	 */
	protected void freeDDAJobs(){
		synchronized (inTestDirectories) {
			Iterator<File> it = inTestDirectories.keySet().iterator();
			while(it.hasNext()) {
				DDACheckJob job = inTestDirectories.get(it.next());
				if (job.readFilename != null)
					job.readFilename.delete();
			}
		}
	}

	public ClientRequest removeRequestByIdentifier(String identifier, boolean kill) {
		ClientRequest req;
		synchronized(this) {
			req = requestsByIdentifier.remove(identifier);
		}
		if(req != null) {
			if(kill)
				req.cancel(null, server.core.clientContext);
			req.requestWasRemoved(null, server.core.clientContext);
		}
		return req;
	}
	
	ClientRequest getRebootRequest(boolean global, FCPConnectionHandler handler, String identifier) {
		if(global)
			return handler.server.globalRebootClient.getRequest(identifier, null);
		else
			return handler.getRebootClient().getRequest(identifier, null);
	}
	
	ClientRequest getForeverRequest(boolean global, FCPConnectionHandler handler, String identifier, ObjectContainer container) {
		if(global)
			return handler.server.globalForeverClient.getRequest(identifier, container);
		else
			return handler.getForeverClient(container).getRequest(identifier, container);
	}
	
	ClientRequest removePersistentRebootRequest(boolean global, String identifier) throws MessageInvalidException {
		FCPClient client =
			global ? server.globalRebootClient :
			getRebootClient();
		ClientRequest req = client.getRequest(identifier, null);
		if(req != null) {
			client.removeByIdentifier(identifier, true, server, null, server.core.clientContext);
		}
		return req;
	}
	
	ClientRequest removePersistentForeverRequest(boolean global, String identifier, ObjectContainer container) throws MessageInvalidException {
		FCPClient client =
			global ? server.globalForeverClient :
			getForeverClient(container);
		container.activate(client, 1);
		ClientRequest req = client.getRequest(identifier, container);
		if(req != null) {
			client.removeByIdentifier(identifier, true, server, container, server.core.clientContext);
		}
		if(!global)
			container.deactivate(client, 1);
		return req;
	}
	
	public boolean objectCanNew(ObjectContainer container) {
		Logger.error(this, "Not storing FCPConnectionHandler in database", new Exception("error"));
		return false;
	}

	public synchronized void addUSKSubscription(String identifier, SubscribeUSK subscribeUSK) throws IdentifierCollisionException {
		if(uskSubscriptions.containsKey(identifier)) throw new IdentifierCollisionException();
			uskSubscriptions.put(identifier, subscribeUSK);
	}

	public void unsubscribeUSK(String identifier) throws MessageInvalidException {
		SubscribeUSK sub;
		synchronized(this) {
			if(!uskSubscriptions.containsKey(identifier)) throw new MessageInvalidException(ProtocolErrorMessage.NO_SUCH_IDENTIFIER, "No such identifier unsubscribing", identifier, false);
			sub = uskSubscriptions.remove(identifier);
		}
		sub.unsubscribe();
	}

	public RequestClient connectionRequestClient(boolean realTime) {
		if(realTime)
			return connectionRequestClientRT;
		else
			return connectionRequestClientBulk;
	}

}
package freenet.support;

import java.util.Arrays;
import java.util.HashSet;

public final class StringValidityChecker {
	
	/**
	 * Taken from http://kb.mozillazine.org/Network.IDN.blacklist_chars
	 */
	private static final HashSet<Character> idnBlacklist = new HashSet<Character>(Arrays.asList(
			new Character[] {
					0x0020, /* SPACE */
					0x00A0, /* NO-BREAK SPACE */
					0x00BC, /* VULGAR FRACTION ONE QUARTER */
					0x00BD, /* VULGAR FRACTION ONE HALF */
					0x01C3, /* LATIN LETTER RETROFLEX CLICK */
					0x0337, /* COMBINING SHORT SOLIDUS OVERLAY */
					0x0338, /* COMBINING LONG SOLIDUS OVERLAY */
					0x05C3, /* HEBREW PUNCTUATION SOF PASUQ */
					0x05F4, /* HEBREW PUNCTUATION GERSHAYIM */
					0x06D4, /* ARABIC FULL STOP */
					0x0702, /* SYRIAC SUBLINEAR FULL STOP */
					0x115F, /* HANGUL CHOSEONG FILLER */
					0x1160, /* HANGUL JUNGSEONG FILLER */
					0x2000, /* EN QUAD */
					0x2001, /* EM QUAD */
					0x2002, /* EN SPACE */
					0x2003, /* EM SPACE */
					0x2004, /* THREE-PER-EM SPACE */
					0x2005, /* FOUR-PER-EM SPACE */
					0x2006, /* SIX-PER-EM-SPACE */
					0x2007, /* FIGURE SPACE */
					0x2008, /* PUNCTUATION SPACE */
					0x2009, /* THIN SPACE */
					0x200A, /* HAIR SPACE */
					0x200B, /* ZERO WIDTH SPACE */
					0x2024, /* ONE DOT LEADER */
					0x2027, /* HYPHENATION POINT */
					0x2028, /* LINE SEPARATOR */
					0x2029, /* PARAGRAPH SEPARATOR */
					0x202F, /* NARROW NO-BREAK SPACE */
					0x2039, /* SINGLE LEFT-POINTING ANGLE QUOTATION MARK */
					0x203A, /* SINGLE RIGHT-POINTING ANGLE QUOTATION MARK */
					0x2044, /* FRACTION SLASH */
					0x205F, /* MEDIUM MATHEMATICAL SPACE */
					0x2154, /* VULGAR FRACTION TWO THIRDS */
					0x2155, /* VULGAR FRACTION ONE FIFTH */
					0x2156, /* VULGAR FRACTION TWO FIFTHS */
					0x2159, /* VULGAR FRACTION ONE SIXTH */
					0x215A, /* VULGAR FRACTION FIVE SIXTHS */
					0x215B, /* VULGAR FRACTION ONE EIGTH */
					0x215F, /* FRACTION NUMERATOR ONE */
					0x2215, /* DIVISION SLASH */
					0x23AE, /* INTEGRAL EXTENSION */
					0x29F6, /* SOLIDUS WITH OVERBAR */
					0x29F8, /* BIG SOLIDUS */
					0x2AFB, /* TRIPLE SOLIDUS BINARY RELATION */
					0x2AFD, /* DOUBLE SOLIDUS OPERATOR */
					0x2FF0, /* IDEOGRAPHIC DESCRIPTION CHARACTER LEFT TO RIGHT */
					0x2FF1, /* IDEOGRAPHIC DESCRIPTION CHARACTER ABOVE TO BELOW */
					0x2FF2, /* IDEOGRAPHIC DESCRIPTION CHARACTER LEFT TO MIDDLE AND RIGHT */
					0x2FF3, /* IDEOGRAPHIC DESCRIPTION CHARACTER ABOVE TO MIDDLE AND BELOW */
					0x2FF4, /* IDEOGRAPHIC DESCRIPTION CHARACTER FULL SURROUND */
					0x2FF5, /* IDEOGRAPHIC DESCRIPTION CHARACTER SURROUND FROM ABOVE */
					0x2FF6, /* IDEOGRAPHIC DESCRIPTION CHARACTER SURROUND FROM BELOW */
					0x2FF7, /* IDEOGRAPHIC DESCRIPTION CHARACTER SURROUND FROM LEFT */
					0x2FF8, /* IDEOGRAPHIC DESCRIPTION CHARACTER SURROUND FROM UPPER LEFT */
					0x2FF9, /* IDEOGRAPHIC DESCRIPTION CHARACTER SURROUND FROM UPPER RIGHT */
					0x2FFA, /* IDEOGRAPHIC DESCRIPTION CHARACTER SURROUND FROM LOWER LEFT */
					0x2FFB, /* IDEOGRAPHIC DESCRIPTION CHARACTER OVERLAID */
					0x3000, /* IDEOGRAPHIC SPACE */
					0x3002, /* IDEOGRAPHIC FULL STOP */
					0x3014, /* LEFT TORTOISE SHELL BRACKET */
					0x3015, /* RIGHT TORTOISE SHELL BRACKET */
					0x3033, /* VERTICAL KANA REPEAT MARK UPPER HALF */
					0x3164, /* HANGUL FILLER */
					0x321D, /* PARENTHESIZED KOREAN CHARACTER OJEON */
					0x321E, /* PARENTHESIZED KOREAN CHARACTER O HU */
					0x33AE, /* SQUARE RAD OVER S */
					0x33AF, /* SQUARE RAD OVER S SQUARED */
					0x33C6, /* SQUARE C OVER KG */
					0x33DF, /* SQUARE A OVER M */
					0xFE14, /* PRESENTATION FORM FOR VERTICAL SEMICOLON */
					0xFE15, /* PRESENTATION FORM FOR VERTICAL EXCLAMATION MARK */
					0xFE3F, /* PRESENTATION FORM FOR VERTICAL LEFT ANGLE BRACKET */
					0xFE5D, /* SMALL LEFT TORTOISE SHELL BRACKET */
					0xFE5E, /* SMALL RIGHT TORTOISE SHELL BRACKET */
					0xFEFF, /* ZERO-WIDTH NO-BREAK SPACE */
					0xFF0E, /* FULLWIDTH FULL STOP */
					0xFF0F, /* FULL WIDTH SOLIDUS */
					0xFF61, /* HALFWIDTH IDEOGRAPHIC FULL STOP */
					0xFFA0, /* HALFWIDTH HANGUL FILLER */
					0xFFF9, /* INTERLINEAR ANNOTATION ANCHOR */
					0xFFFA, /* INTERLINEAR ANNOTATION SEPARATOR */
					0xFFFB, /* INTERLINEAR ANNOTATION TERMINATOR */
					0xFFFC, /* OBJECT REPLACEMENT CHARACTER */
					0xFFFD, /* REPLACEMENT CHARACTER */
			}));
	
	/**
	 * Taken from http://en.wikipedia.org/w/index.php?title=Filename&oldid=344618757
	 */
	private static final HashSet<Character> windowsReservedPrintableFilenameCharacters = new HashSet<Character>(Arrays.asList(
			new Character[] { '/', '\\', '?', '*', ':', '|', '\"', '<', '>'}));

	/**
	 * Taken from http://en.wikipedia.org/w/index.php?title=Filename&oldid=344618757
	 */
	private static final HashSet<String> windowsReservedFilenames = new HashSet<String>(Arrays.asList(
			new String[] { "aux", "clock$", "com1", "com2", "com3", "com4", "com5", "com6", "com7", "com8", "com9", "con",
					"lpt1", "lpt2", "lpt3", "lpt4", "lpt5", "lpt6", "lpt7", "lpt8", "lpt9", "nul", "prn"}));
	
	/**
	 * Taken from http://en.wikipedia.org/w/index.php?title=Filename&oldid=344618757
	 */
	private static final HashSet<Character> macOSReservedPrintableFilenameCharacters = new HashSet<Character>(Arrays.asList(
			new Character[] { ':', '/'}));

	
	/**
	 * Returns true if the given character is one of the reserved printable character in filenames on Windows.
	 * ATTENTION: This function does NOT check whether the given character is a control character, those are also forbidden!
	 * (Control characters are usually disallowed for all operating systems in filenames by our validity checker so it checks them separately)   
	 */
	public static boolean isWindowsReservedPrintableFilenameCharacter(Character c) {
		return windowsReservedPrintableFilenameCharacters.contains(c);
	}
	
	public static boolean isWindowsReservedFilename(String filename) {
		filename = filename.toLowerCase();
		int nameEnd = filename.indexOf('.'); // For files with multiple dots, the part before the first dot counts as the filename. E.g. "con.blah.txt" is reserved.
		if(nameEnd == -1)
			nameEnd = filename.length();
		
		return windowsReservedFilenames.contains(filename.substring(0, nameEnd));
	}

	/**
	 * Returns true if the given character is one of the reserved printable character in filenames on Mac OS.
	 * ATTENTION: This function does NOT check whether the given character is a control character, those are also forbidden!
	 * (Control characters are usually disallowed for all operating systems in filenames by our validity checker so it checks them separately)
	 */
	public static boolean isMacOSReservedPrintableFilenameCharacter(Character c) {
		return macOSReservedPrintableFilenameCharacters.contains(c);
	}
	
	public static boolean isUnixReservedPrintableFilenameCharacter(char c) {
		return c == '/';
	}
	
	public static boolean containsNoIDNBlacklistCharacters(String text) {
		for(Character c : text.toCharArray()) {
			if(idnBlacklist.contains(c))
				return false;
		}
		
		return true;
	}
	
	public static boolean containsNoLinebreaks(String text) {
		for(Character c : text.toCharArray()) {
			if(Character.getType(c) == Character.LINE_SEPARATOR
			   || Character.getType(c) == Character.PARAGRAPH_SEPARATOR
			   || c == '\n' || c == '\r')
				return false;
		}
		
		return true;
	}

	/**
	 * Check for any values in the string that are not valid Unicode
	 * characters.
	 */
	public static boolean containsNoInvalidCharacters(String text) {
		for (int i = 0; i < text.length(); ) {
			int c = text.codePointAt(i);
			i += Character.charCount(c);

			if ((c & 0xFFFE) == 0xFFFE
				|| Character.getType(c) == Character.SURROGATE)
				return false;
		}

		return true;
	}

	/**
	 * Check for any control characters (including tab, LF, and CR) in
	 * the string.
	 */
	public static boolean containsNoControlCharacters(String text) {
		for(Character c : text.toCharArray()) {
			if(Character.getType(c) == Character.CONTROL)
				return false;
		}

		return true;
	}

	/**
	 * Check for any unpaired directional or annotation characters in
	 * the string, or any nested annotations.
	 */
	public static boolean containsNoInvalidFormatting(String text) {
		int dirCount = 0;
		boolean inAnnotatedText = false;
		boolean inAnnotation = false;

		for (Character c : text.toCharArray()) {
			if (c == 0x202A			// LEFT-TO-RIGHT EMBEDDING
				|| c == 0x202B		// RIGHT-TO-LEFT EMBEDDING
				|| c == 0x202D		// LEFT-TO-RIGHT OVERRIDE
				|| c == 0x202E) {	// RIGHT-TO-LEFT OVERRIDE
				dirCount++;
			}
			else if (c == 0x202C) {	// POP DIRECTIONAL FORMATTING
				dirCount--;
				if (dirCount < 0)
					return false;
			}
			else if (c == 0xFFF9) {	// INTERLINEAR ANNOTATION ANCHOR
				if (inAnnotatedText || inAnnotation)
					return false;
				inAnnotatedText = true;
			}
			else if (c == 0xFFFA) {	// INTERLINEAR ANNOTATION SEPARATOR
				if (!inAnnotatedText)
					return false;
				inAnnotatedText = false;
				inAnnotation = true;
			}
			else if (c == 0xFFFB) { // INTERLINEAR ANNOTATION TERMINATOR
				if (!inAnnotation)
					return false;
				inAnnotation = false;
			}
		}

		return (dirCount == 0 && !inAnnotatedText && !inAnnotation);
	}
	
	public static boolean isLatinLettersAndNumbersOnly(String text) {
		for(char c : text.toCharArray()) {
			if((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z') || c >= '0' && c <= '9')
				continue;
			else
				return false;
		}
		
		return true;
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.math;

import freenet.node.Location;
import freenet.support.SimpleFieldSet;

/**
 * @author robert
 *
 * A filter on BootstrappingDecayingRunningAverage which makes it aware of the circular keyspace.
 */
public class DecayingKeyspaceAverage implements RunningAverage {

	private static final long serialVersionUID = 5129429614949179428L;
	/**
	'avg' is the normalized average location, note that the the reporting bounds are (-2.0, 2.0) however.
	 */
	BootstrappingDecayingRunningAverage avg;

        /**
         *
         * @param defaultValue
         * @param maxReports
         * @param fs
         */
        public DecayingKeyspaceAverage(double defaultValue, int maxReports, SimpleFieldSet fs) {
		avg = new BootstrappingDecayingRunningAverage(defaultValue, -2.0, 2.0, maxReports, fs);
	}

        /**
         *
         * @param a
         */
        public DecayingKeyspaceAverage(BootstrappingDecayingRunningAverage a) {
		//check the max/min values? ignore them?
		avg = (BootstrappingDecayingRunningAverage) a.clone();
	}

	@Override
	public synchronized Object clone() {
		return new DecayingKeyspaceAverage(avg);
	}

        /**
         *
         * @return
         */
        public synchronized double currentValue() {
		return avg.currentValue();
	}

        /**
         *
         * @param d
         */
        public synchronized void report(double d) {
		if((d < 0.0) || (d > 1.0))
			//Just because we use non-normalized locations doesn't mean we can accept them.
			throw new IllegalArgumentException("Not a valid normalized key: " + d);
		double superValue = avg.currentValue();
		double thisValue = Location.normalize(superValue);
		double diff = Location.change(thisValue, d);
		double toAverage = (superValue + diff);
		/*
		To gracefully wrap around the 1.0/0.0 threshold we average over (or under) it, and simply normalize the result when reporting a currentValue
		---example---
		d=0.2;          //being reported
		superValue=1.9; //already wrapped once, but at 0.9
		thisValue=0.9;  //the normalized value of where we are in the keyspace
		diff = +0.3;    //the diff from the normalized values; Location.change(0.9, 0.2);
		avg.report(2.2);//to successfully move the average towards the closest route to the given value.
		 */
		avg.report(toAverage);
		double newValue = avg.currentValue();
		if(newValue < 0.0 || newValue > 1.0)
			avg.setCurrentValue(Location.normalize(newValue));
	}

	public synchronized double valueIfReported(double d) {
		if((d < 0.0) || (d > 1.0))
			throw new IllegalArgumentException("Not a valid normalized key: " + d);
		double superValue = avg.currentValue();
		double thisValue = Location.normalize(superValue);
		double diff = Location.change(thisValue, d);
		return Location.normalize(avg.valueIfReported(superValue + diff));
	}

	public synchronized long countReports() {
		return avg.countReports();
	}

        /**
         *
         * @param d
         */
        public void report(long d) {
		throw new IllegalArgumentException("KeyspaceAverage does not like longs");
	}

        /**
         *
         * @param maxReports
         */
        public synchronized void changeMaxReports(int maxReports) {
		avg.changeMaxReports(maxReports);
	}

        /**
         *
         * @param shortLived
         * @return
         */
        public synchronized SimpleFieldSet exportFieldSet(boolean shortLived) {
		return avg.exportFieldSet(shortLived);
	}

	///@todo: make this a junit test
        /**
         * 
         * @param args
         */
        public static void main(String[] args) {
		DecayingKeyspaceAverage a = new DecayingKeyspaceAverage(0.9, 10, null);
		a.report(0.9);
		for(int i = 10; i != 0; i--) {
			a.report(0.2);
			System.out.println("<-0.2-- current=" + a.currentValue());
		}
		for(int i = 10; i != 0; i--) {
			a.report(0.8);
			System.out.println("--0.8-> current=" + a.currentValue());
		}
		System.out.println("--- positive wrap test ---");
		for(int wrap = 3; wrap != 0; wrap--) {
			System.out.println("wrap test #" + wrap);
			for(int i = 10; i != 0; i--) {
				a.report(0.25);
				System.out.println("<-0.25- current=" + a.currentValue());
			}
			for(int i = 10; i != 0; i--) {
				a.report(0.5);
				System.out.println("--0.5-> current=" + a.currentValue());
			}
			for(int i = 10; i != 0; i--) {
				a.report(0.75);
				System.out.println("-0.75-> current=" + a.currentValue());
			}
			for(int i = 10; i != 0; i--) {
				a.report(1.0);
				System.out.println("<-1.0-- current=" + a.currentValue());
			}
		}
		System.out.println("--- negative wrap test ---");
		a = new DecayingKeyspaceAverage(0.2, 10, null);
		a.report(0.2);
		for(int wrap = 3; wrap != 0; wrap--) {
			System.out.println("negwrap test #" + wrap);
			for(int i = 10; i != 0; i--) {
				a.report(0.75);
				System.out.println("-0.75-> current=" + a.currentValue());
			}
			for(int i = 10; i != 0; i--) {
				a.report(0.5);
				System.out.println("<-0.5-- current=" + a.currentValue());
			}
			for(int i = 10; i != 0; i--) {
				a.report(0.25);
				System.out.println("<-0.25- current=" + a.currentValue());
			}
			for(int i = 10; i != 0; i--) {
				a.report(1.0);
				System.out.println("--1.0-> current=" + a.currentValue());
			}
		}
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.keys;

import java.net.MalformedURLException;

import com.db4o.ObjectContainer;

import freenet.crypt.DSAGroup;
import freenet.crypt.DSAPrivateKey;
import freenet.crypt.DSAPublicKey;
import freenet.support.Logger;

/**
 * An insertable USK.
 * 
 * Changes from an ordinary USK:
 * - It has a private key
 * - getURI() doesn't include ,extra
 * - constructor from URI doesn't need or want ,extra
 * - It has a getUSK() method which gets the public USK
 */
public class InsertableUSK extends USK {
	
	public final DSAPrivateKey privKey;
	public final DSAGroup group;
	
	public static InsertableUSK createInsertable(FreenetURI uri, boolean persistent) throws MalformedURLException {
		if(!uri.getKeyType().equalsIgnoreCase("USK"))
			throw new MalformedURLException();
		InsertableClientSSK ssk =
			InsertableClientSSK.create(uri.setKeyType("SSK"));
		return new InsertableUSK(ssk.docName, ssk.pubKeyHash, ssk.cryptoKey, ssk.privKey, persistent ? ssk.getCryptoGroup().cloneKey() : ssk.getCryptoGroup(), uri.getSuggestedEdition(), ssk.cryptoAlgorithm);
	}
	
	InsertableUSK(String docName, byte[] pubKeyHash, byte[] cryptoKey, DSAPrivateKey key, DSAGroup group, long suggestedEdition, byte cryptoAlgorithm) throws MalformedURLException {
		super(pubKeyHash, cryptoKey, docName, suggestedEdition, cryptoAlgorithm);
		if(cryptoKey.length != ClientSSK.CRYPTO_KEY_LENGTH)
			throw new MalformedURLException("Decryption key wrong length: "+cryptoKey.length+" should be "+ClientSSK.CRYPTO_KEY_LENGTH);
		this.privKey = key;
		this.group = group;
	}

	public USK getUSK() {
		return new USK(pubKeyHash, cryptoKey, siteName, suggestedEdition, cryptoAlgorithm);
	}

	public InsertableClientSSK getInsertableSSK(long ver) {
		return getInsertableSSK(siteName + SEPARATOR + ver);
	}
	
	public InsertableClientSSK getInsertableSSK(String string) {
		try {
			return new InsertableClientSSK(string, pubKeyHash, 
					new DSAPublicKey(group, privKey), privKey, cryptoKey, cryptoAlgorithm);
		} catch (MalformedURLException e) {
			Logger.error(this, "Caught "+e+" should not be possible in USK.getSSK", e);
			throw new Error(e);
		}
	}

	public InsertableUSK privCopy(long edition) {
		if(edition == suggestedEdition) return this;
		try {
			return new InsertableUSK(siteName, pubKeyHash, cryptoKey, privKey, group, edition, cryptoAlgorithm);
		} catch (MalformedURLException e) {
			throw new Error(e);
		}
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		container.activate(privKey, 5);
		privKey.removeFrom(container);
		container.activate(group, 5);
		group.removeFrom(container);
		super.removeFrom(container);
	}
}
/*
 * Dijjer - A Peer to Peer HTTP Cache
 * Copyright (C) 2004,2005 Change.Tv, Inc
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */

package freenet.io;

import java.io.DataOutputStream;
import java.io.IOException;

/**
 * @author ian
 *
 * To change the template for this generated type comment go to Window - Preferences - Java - Code Generation - Code and
 * Comments
 */
public interface WritableToDataOutputStream {

    public static final String VERSION = "$Id: WritableToDataOutputStream.java,v 1.1 2005/01/29 19:12:10 amphibian Exp $";

	public void writeToDataOutputStream(DataOutputStream stream) throws IOException;
}/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.keys.USK;
import freenet.node.Node;
import freenet.support.SimpleFieldSet;

public class SubscribedUSKUpdate extends FCPMessage {

	final String identifier;
	final long edition;
	final USK key;
	final boolean newKnownGood;
	final boolean newSlotToo;
	
	static final String name = "SubscribedUSKUpdate";
	
	public SubscribedUSKUpdate(String identifier, long l, USK key, boolean newKnownGood, boolean newSlotToo) {
		this.identifier = identifier;
		this.edition = l;
		this.key = key;
		this.newKnownGood = newKnownGood;
		this.newSlotToo = newSlotToo;
	}

	@Override
	public SimpleFieldSet getFieldSet() {
		SimpleFieldSet fs = new SimpleFieldSet(true);
		fs.putSingle("Identifier", identifier);
		fs.put("Edition", edition);
		fs.putSingle("URI", key.getURI().toString());
		fs.put("NewKnownGood", newKnownGood);
		fs.put("NewSlotToo", newSlotToo);
		return fs;
	}

	@Override
	public String getName() {
		return name;
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node)
			throws MessageInvalidException {
		throw new MessageInvalidException(ProtocolErrorMessage.INVALID_MESSAGE, "SubscribedUSKUpdate goes from server to client not the other way around", identifier, false);
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		throw new UnsupportedOperationException();
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

public interface BaseRequestThrottle {

	public static final long DEFAULT_DELAY = 200;
	static final long MAX_DELAY = 5*60*1000;
	static final long MIN_DELAY = 20;

	/**
	 * Get the current inter-request delay.
	 */
	public abstract long getDelay();

}package freenet.node.simulator;

import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;

import freenet.crypt.RandomSource;
import freenet.node.Node;
import freenet.node.NodeInitException;
import freenet.node.NodeStarter;
import freenet.support.Executor;
import freenet.support.Logger;
import freenet.support.PooledExecutor;
import freenet.support.TimeUtil;
import freenet.support.Logger.LogLevel;
import freenet.support.LoggerHook.InvalidThresholdException;
import freenet.support.io.FileUtil;

public class BootstrapSeedTest {

	public static int EXIT_NO_SEEDNODES = 257;
	public static int EXIT_FAILED_TARGET = 258;
	public static int EXIT_THREW_SOMETHING = 259;
	
	public static int DARKNET_PORT = 5006;
	public static int OPENNET_PORT = 5007;
	
	/**
	 * @param args
	 * @throws InvalidThresholdException 
	 * @throws NodeInitException 
	 * @throws InterruptedException 
	 * @throws IOException 
	 */
	public static void main(String[] args) throws InvalidThresholdException, NodeInitException, InterruptedException, IOException {
		Node node = null;
		try {
		String ipOverride = null;
		if(args.length > 0)
			ipOverride = args[0];
        File dir = new File("bootstrap-test");
        FileUtil.removeAll(dir);
        RandomSource random = NodeStarter.globalTestInit(dir.getPath(), false, LogLevel.ERROR, "", false);
        File seednodes = new File("seednodes.fref");
        if(!seednodes.exists() || seednodes.length() == 0 || !seednodes.canRead()) {
        	System.err.println("Unable to read seednodes.fref, it doesn't exist, or is empty");
        	System.exit(EXIT_NO_SEEDNODES);
        }
        File innerDir = new File(dir, Integer.toString(DARKNET_PORT));
        innerDir.mkdir();
        FileInputStream fis = new FileInputStream(seednodes);
        FileUtil.writeTo(fis, new File(innerDir, "seednodes.fref"));
        fis.close();
        // Create one node
        Executor executor = new PooledExecutor();
        node = NodeStarter.createTestNode(DARKNET_PORT, OPENNET_PORT, "bootstrap-test", false, Node.DEFAULT_MAX_HTL, 0, random, executor, 1000, 5*1024*1024, true, true, true, true, true, true, true, 12*1024, false, true, false, false, ipOverride);
        //NodeCrypto.DISABLE_GROUP_STRIP = true;
    	//Logger.setupStdoutLogging(LogLevel.MINOR, "freenet:NORMAL,freenet.node.NodeDispatcher:MINOR,freenet.node.FNPPacketMangler:MINOR");
    	Logger.getChain().setThreshold(LogLevel.ERROR); // kill logging
    	long startTime = System.currentTimeMillis();
    	// Start it
        node.start(true);
        // Wait until we have 10 connected nodes...
        int seconds = 0;
		int targetPeers = node.getOpennet().getAnnouncementThreshold();
        while(seconds < 600) {
        	Thread.sleep(1000);
        	int seeds = node.peers.countSeednodes();
        	int seedConns = node.peers.getConnectedSeedServerPeersVector(null).size();
        	int opennetPeers = node.peers.countValidPeers();
        	int opennetConns = node.peers.countConnectedOpennetPeers();
        	System.err.println(""+seconds+" : seeds: "+seeds+", connected: "+seedConns
        			+" opennet: peers: "+opennetPeers+", connected: "+opennetConns);
        	seconds++;
        	if(opennetConns >= targetPeers) {
        		long timeTaken = System.currentTimeMillis()-startTime;
        		System.out.println("Completed bootstrap ("+targetPeers+" peers) in "+timeTaken+"ms ("+TimeUtil.formatTime(timeTaken)+")");
        		node.park();
        		System.exit(0);
        	}
        }
        System.err.println("Failed to reach target peers count "+targetPeers+" in 5 minutes.");
		node.park();
        System.exit(EXIT_FAILED_TARGET);
	    } catch (Throwable t) {
	    	System.err.println("CAUGHT: "+t);
	    	t.printStackTrace();
	    	try {
	    		if(node != null)
	    			node.park();
	    	} catch (Throwable t1) {};
	    	System.exit(EXIT_THREW_SOMETHING);
	    }
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.clients.http;

import java.net.URI;
import java.net.URISyntaxException;
import java.text.ParseException;
import java.util.ArrayList;
import java.util.Hashtable;

import freenet.support.Logger;

/**
 * A cookie which the server has received from the client.
 * 
 * This class is not thread-safe!
 * 
 * @author xor (xor@freenetproject.org)
 */
public final class ReceivedCookie extends Cookie {
	
	private static volatile boolean logMINOR;
	private static volatile boolean logDEBUG;

	static {
		Logger.registerClass(ReceivedCookie.class);
	}
	
	private String notValidatedName;
	
	private Hashtable<String, String> content;
	

	/**
	 * Constructor for creating cookies from parsed key-value pairs.
	 *
	 * Does not validate the names or values of the keys, each attribute is validated at the first call to it's getter method.
	 * Therefore, no CPU time is wasted if the client sends cookies which we do not use.
	 */
	private ReceivedCookie(String myName, Hashtable<String, String> myContent) {
		// We do not validate the input here, we only parse it if someone actually tries to access this cookie.
		notValidatedName = myName;
		content = myContent;
		
		// We do NOT parse the version even though RFC2965 requires it because Firefox (3.0.14) does not give us a version.
		
		version = 1;
		//version = Integer.parseInt(content.get("$version"));
		
		//if(version != 1)
		//	throw new IllegalArgumentException("Invalid version: " + version);
	}
	
	/**
	 * Parses the value of a "Cookie:" HTTP header and returns a list of received cookies which it contained.
	 * - A single "Cookie:" header is allowed to contain multiple cookies. Further, a HTTP request can contain multiple "Cookie" keys!.
	 * 
	 * @param httpHeader The value of a "Cookie:" header (i.e. the prefix "Cookie:" must not be contained in this parameter!) 
	 * @return A list of {@link ReceivedCookie} objects. The validity of their name/value pairs is not deeply checked, their getName() / getValue() might throw!
	 * @throws ParseException If the general formatting of the cookie is wrong.
	 */
	protected static ArrayList<ReceivedCookie> parseHeader(String httpHeader) throws ParseException {

		if(logMINOR)
			Logger.minor(ReceivedCookie.class, "Received HTTP cookie header:" + httpHeader);

		char[] header = httpHeader.toCharArray();
		
		String currentCookieName = null;
		Hashtable<String,String> currentCookieContent = new Hashtable<String, String>(16);
		
		ArrayList<ReceivedCookie> cookies = new ArrayList<ReceivedCookie>(4); // TODO: Adjust to the usual amount of cookies which fred uses + 1
		
		// We do manual parsing instead of using regular expressions for two reasons:
		// 1. The value of cookies can be quoted, therefore it is a context-free language and not a regular language - we cannot express it with a regexp!
		// 2. Its very fast :)
		
		// Set to true if a broken browser (Konqueror) specifies a cookie where the name is NOT the first attribute.
		boolean singleCookie = false;

		try {
		for(int i = 0; i < header.length;) {
			// Skip leading whitespace of key, we must do a header.length check because there might be no more key, so we continue;
			if(Character.isWhitespace(header[i])) {
				++i;
				continue;
			}
			
			String key;
			String value = null;
			
			// Parse key
			{
				int keyBeginIndex = i;
	
				while(i < header.length && header[i] != '=' && header[i] != ';')
					++i;
				
				int keyEndIndex = i;
				
				if(keyEndIndex >= header.length || header[keyEndIndex] == ';')
					value = "";
				
				while(Character.isWhitespace(header[keyEndIndex-1])) // Remove trailing whitespace
					--keyEndIndex;
				
				key = new String(header, keyBeginIndex, keyEndIndex - keyBeginIndex).toLowerCase();
				
				if(key.length() == 0)
					throw new ParseException("Invalid cookie: Contains an empty key: " + httpHeader, i);
				
				// We're done parsing the key, continue to the next character.
				++i;
			}
			
			// Parse value (empty values are allowed).
			if(value == null && i < header.length) {
				while(Character.isWhitespace(header[i])) // Skip leading whitespace
					++i;
				
				int valueBeginIndex;
				char valueEndChar;
				
				if(header[i] == '\"') { // Value is quoted
					valueEndChar = '\"';
					valueBeginIndex = ++i;
					
					while(header[i] != valueEndChar)
						++i;
					
				} else {
					valueEndChar = ';';
					valueBeginIndex = i;
					
					while(i < header.length && header[i] != valueEndChar)
						++i;
				}
				

				int valueEndIndex = i;
				
				while(valueEndIndex > valueBeginIndex && Character.isWhitespace(header[valueEndIndex-1])) // Remove trailing whitespace
					--valueEndIndex;

				value = new String(header, valueBeginIndex, valueEndIndex - valueBeginIndex);
				
				// We're done parsing the value, continue to the next character
				++i;
				
				// Skip whitespace between end of quotation and the semicolon following the quotation.
				if(valueEndChar == '\"') {
					while(i < header.length && header[i] != ';') {
						if(!Character.isWhitespace(header[i]))
							throw new ParseException("Invalid cookie: Missing terminating semicolon after value quotation: " + httpHeader, i);
						
						++i;
					}
					
					// We found the semicolon, skip it
					++i;
				}
					
			}
			else
				value = "";
			
			// RFC2965: Name MUST be first. Anything key besides the name of the cookie begins with $. The next cookie begins if a key occurs which is not
			// prefixed with $.
			
			if(currentCookieName == null) { // We have not found the name yet, the first key/value pair must be the name and the value of the cookie.
				if(key.charAt(0) == '$') {
					// We cannot throw because Konqueror (4.2.2) is broken and specifies $version as the first attribute.
					//throw new IllegalArgumentException("Invalid cookie: Name is not the first attribute: " + httpHeader);
					
					singleCookie = true;
					currentCookieContent.put(key, value);
				} else {
					currentCookieName = key;
					currentCookieContent.put(currentCookieName, value);
				}
			} else {
				if(key.charAt(0) == '$')
					currentCookieContent.put(key, value);
				else {// We finished parsing of the current cookie, a new one starts here.
					//if(singleCookie)
					//	throw new ParseException("Invalid cookie header: Multiple cookies specified but "
					//			+ " the name of the first cookie was not the first attribute: " + httpHeader, i);
					
					cookies.add(new ReceivedCookie(currentCookieName, currentCookieContent)); // Store the previous cookie.
					
					currentCookieName = key;
					currentCookieContent = new Hashtable<String, String>(16);
					currentCookieContent.put(currentCookieName, value);
				}
			}
		}
		}
		catch(ArrayIndexOutOfBoundsException e) {
			ParseException p = new ParseException("Index out of bounds (" + e.getMessage() + ") for cookie " + httpHeader, 0);
			p.setStackTrace(e.getStackTrace());
			throw p;
		}
		
		// Store the last cookie (the loop only stores the current cookie when a new one starts).
		if(currentCookieName != null)
			cookies.add(new ReceivedCookie(currentCookieName, currentCookieContent));
		
		return cookies;
	}

	
	/**
	 * @throws IllegalArgumentException If the validation of the name fails.
	 */
	public String getName() {
		if(name == null) {
			name = validateName(notValidatedName);
			notValidatedName = null;
		}
		
		return name;
	}
	
	/**
	 * @throws IllegalArgumentException If the validation of the domain fails.
	 */
	public URI getDomain() {
		if(domain == null) {
			try {
				String domainString = content.get("$domain");
				if(domainString == null)
					return null;
				
				domain = validateDomain(domainString);
			} catch (URISyntaxException e) {
				throw new IllegalArgumentException(e);
			}
		}
		
		return domain;
	}

	/**
	 * @throws IllegalArgumentException If the validation of the path fails.
	 */
	public URI getPath() {
		if(path == null) {
			try {
				path = validatePath(content.get("$path"));
			} catch (URISyntaxException e) {
				throw new IllegalArgumentException(e);
			}
		}
		
		return path;
	}

	/**
	 * @throws IllegalArgumentException If the validation of the name fails.
	 */
	public String getValue() {
		if(value == null) 
			value = validateValue(content.get(getName()));
		
		return value;
	}

// TODO: This is broken because TimeUtil.parseHTTPDate() does not work.
//	public Date getExpirationDate() {
//		if(expirationDate == null) {
//			try {
//				expirationDate = validateExpirationDate(TimeUtil.parseHTTPDate(content.get("$expires")));
//			} catch (ParseException e) {
//				throw new IllegalArgumentException(e);
//			}
//		}
//		
//		return expirationDate;
//	}

	protected String encodeToHeaderValue() {
		throw new UnsupportedOperationException("ReceivedCookie objects cannot be encoded to a HTTP header value, use Cookie objects!");
	}
	
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.crypt;

import java.io.BufferedInputStream;
import java.io.BufferedOutputStream;
import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.EOFException;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.UnsupportedEncodingException;
import java.net.InetAddress;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;
import java.security.SecureRandom;
import java.util.Arrays;
import java.util.Enumeration;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;

import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.io.Closer;

/**
 * An implementation of the Yarrow PRNG in Java.
 * <p>
 * This class implements Yarrow-160, a cryptraphically secure PRNG developed by
 * John Kelsey, Bruce Schneier, and Neils Ferguson. It was designed to follow
 * the specification (www.counterpane.com/labs) given in the paper by the same
 * authors, with the following exceptions:
 * </p>
 * <ul>
 * <li>Instead of 3DES as the output cipher, Rijndael was chosen. It was my
 * belief that an AES candidate should be selected. Twofish was an alternate
 * choice, but the AES implementation does not allow easy selection of a faster
 * key-schedule, so twofish's severely impaired performance.</li>
 * <li>h prime, described as a 'size adaptor' was not used, since its function
 * is only to constrain the size of a byte array, our own key generation
 * routine was used instead (See
 * {@link freenet.crypt.Util#makeKey freenet.crypt.Util.makeKey})</li>
 * <li>Our own entropy estimation routines are used, as they use a third-order
 * delta calculation that is quite conservative. Still, its used along side the
 * global multiplier and program- supplied guesses, as suggested.</li>
 * </ul>
 *
 * @author Scott G. Miller <scgmille@indiana.edu>
 */
public class Yarrow extends RandomSource {

	private static final long serialVersionUID = -1;
	private static volatile boolean logMINOR;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}
	/**
	 * Security parameters
	 */
	private static final boolean DEBUG = false;
	private static final int Pg = 10;
	private final SecureRandom sr;
	public final File seedfile; //A file to which seed data should be dumped periodically

	public Yarrow() {
		this("prng.seed", "SHA1", "Rijndael", true, true);
	}

	public Yarrow(boolean canBlock) {
		this("prng.seed", "SHA1", "Rijndael", true, canBlock);
	}

	public Yarrow(File seed) {
		this(seed, "SHA1", "Rijndael", true, true);
	}

	public Yarrow(String seed, String digest, String cipher, boolean updateSeed, boolean canBlock) {
		this(new File(seed), digest, cipher, updateSeed, canBlock);
	}

	public Yarrow(File seed, String digest, String cipher, boolean updateSeed, boolean canBlock) {
		this(seed, digest, cipher, updateSeed, canBlock, true);
	}

	// unset reseedOnStartup only in unit test
	Yarrow(File seed, String digest, String cipher, boolean updateSeed, boolean canBlock, boolean reseedOnStartup) {
		SecureRandom s;
		try {
			s = SecureRandom.getInstance("SHA1PRNG");
		} catch(NoSuchAlgorithmException e) {
			s = null;
		}
		sr = s;
		try {
			accumulator_init(digest);
			reseed_init(digest);
			generator_init(cipher);
		} catch(NoSuchAlgorithmException e) {
			Logger.error(this, "Could not init pools trying to getInstance(" + digest + "): " + e, e);
			throw new RuntimeException("Cannot initialize Yarrow!: " + e, e);
		}

		if(updateSeed && !(seed.toString()).equals("/dev/urandom")) //Dont try to update the seedfile if we know that it wont be possible anyways
			seedfile = seed;
		else
			seedfile = null;
		if(reseedOnStartup) {
			entropy_init(seed, reseedOnStartup);
			seedFromExternalStuff(canBlock);
			/**
			 * If we don't reseed at this point, we will be predictable,
			 * because the startup entropy won't cause a reseed.
			 */
			fast_pool_reseed();
			slow_pool_reseed();
		} else {
			read_seed(seed);
		}
	}

	private void seedFromExternalStuff(boolean canBlock) {
		byte[] buf = new byte[32];
		if(File.separatorChar == '/') {
			DataInputStream dis = null;
			FileInputStream fis = null;
			File hwrng = new File("/dev/hwrng");
			if(hwrng.exists() && hwrng.canRead())
				try {
					fis = new FileInputStream(hwrng);
					dis = new DataInputStream(fis);
					dis.readFully(buf);
					consumeBytes(buf);
					dis.readFully(buf);
					consumeBytes(buf);
					dis.close();
				} catch(Throwable t) {
					Logger.normal(this, "Can't read /dev/hwrng even though exists and is readable: " + t, t);
				} finally {
					Closer.close(dis);
					Closer.close(fis);
				}

			boolean isSystemEntropyAvailable = true;
			// Read some bits from /dev/urandom
			try {
				fis = new FileInputStream("/dev/urandom");
				dis = new DataInputStream(fis);
				dis.readFully(buf);
				consumeBytes(buf);
				dis.readFully(buf);
				consumeBytes(buf);
			} catch(Throwable t) {
				Logger.normal(this, "Can't read /dev/urandom: " + t, t);
				// We can't read it; let's skip /dev/random and seed from SecureRandom.generateSeed()
				canBlock = true;
				isSystemEntropyAvailable = false;
			} finally {
				Closer.close(dis);
				Closer.close(fis);
			}
			if(canBlock && isSystemEntropyAvailable)
				// Read some bits from /dev/random
				try {
					fis = new FileInputStream("/dev/random");
					dis = new DataInputStream(fis);
					dis.readFully(buf);
					consumeBytes(buf);
					dis.readFully(buf);
					consumeBytes(buf);
				} catch(Throwable t) {
					Logger.normal(this, "Can't read /dev/random: " + t, t);
				} finally {
					Closer.close(dis);
					Closer.close(fis);
				}
			fis = null;
		} else
			// Force generateSeed(), since we can't read random data from anywhere else.
			// Anyway, Windows's CAPI won't block.
			canBlock = true;
		if(canBlock) {
			// SecureRandom hopefully acts as a proxy for CAPI on Windows
			buf = sr.generateSeed(32);
			consumeBytes(buf);
			buf = sr.generateSeed(32);
			consumeBytes(buf);
		}
		// A few more bits
		consumeString(Long.toHexString(Runtime.getRuntime().freeMemory()));
		consumeString(Long.toHexString(Runtime.getRuntime().totalMemory()));
	}

	private void entropy_init(File seed, boolean reseedOnStartup) {
		if(reseedOnStartup) {
			Properties sys = System.getProperties();
			EntropySource startupEntropy = new EntropySource();

			// Consume the system properties list
			for(Enumeration<?> enu = sys.propertyNames(); enu.hasMoreElements();) {
				String key = (String) enu.nextElement();
				consumeString(key);
				consumeString(sys.getProperty(key));
			}

			// Consume the local IP address
			try {
				consumeString(InetAddress.getLocalHost().toString());
			} catch(Exception e) {
				// Ignore
			}
			readStartupEntropy(startupEntropy);
		}

		read_seed(seed);
	}

	protected void readStartupEntropy(EntropySource startupEntropy) {
		// Consume the current time
		acceptEntropy(startupEntropy, System.currentTimeMillis(), 0);
		acceptEntropy(startupEntropy, System.nanoTime(), 0);
		// Free memory
		acceptEntropy(startupEntropy, Runtime.getRuntime().freeMemory(), 0);
		// Total memory
		acceptEntropy(startupEntropy, Runtime.getRuntime().totalMemory(), 0);
	}

	/**
	 * Seed handling
	 */
	private void read_seed(File filename) {
		FileInputStream fis = null;
		BufferedInputStream bis = null;
		DataInputStream dis = null;

		try {
			fis = new FileInputStream(filename);
			bis = new BufferedInputStream(fis);
			dis = new DataInputStream(bis);

			EntropySource seedFile = new EntropySource();
			try {
				for(int i = 0; i < 32; i++)
					acceptEntropy(seedFile, dis.readLong(), 64);
			} catch(EOFException f) {
			}
			dis.close();
		} catch(IOException e) {
			Logger.error(this, "IOE trying to read the seedfile from disk : " + e.getMessage());
		} finally {
			Closer.close(dis);
			Closer.close(bis);
			Closer.close(fis);
		}
		fast_pool_reseed();
	}
	private long timeLastWroteSeed = -1;

	private void write_seed(File filename) {
		write_seed(filename, false);
	}

	public void write_seed(File filename, boolean force) {
		if(!force)
			synchronized(this) {
				long now = System.currentTimeMillis();
				if(now - timeLastWroteSeed <= 60 * 60 * 1000 /* once per hour */)
					return;
				else
					timeLastWroteSeed = now;
			}

		FileOutputStream fos = null;
		BufferedOutputStream bos = null;
		DataOutputStream dos = null;
		try {
			fos = new FileOutputStream(filename);
			bos = new BufferedOutputStream(fos);
			dos = new DataOutputStream(bos);

			for(int i = 0; i < 32; i++)
				dos.writeLong(nextLong());

			dos.flush();
			dos.close();
		} catch(IOException e) {
			Logger.error(this, "IOE while saving the seed file! : " + e.getMessage());
		} finally {
			Closer.close(dos);
			Closer.close(bos);
			Closer.close(fos);
		}
	}
	/**
	 * 5.1 Generation Mechanism
	 */
	private BlockCipher cipher_ctx;
	private byte[] output_buffer,  counter,  allZeroString,  tmp;
	private int output_count,  fetch_counter;

	private void generator_init(String cipher) {
		cipher_ctx = Util.getCipherByName(cipher);
		output_buffer = new byte[cipher_ctx.getBlockSize() / 8];
		counter = new byte[cipher_ctx.getBlockSize() / 8];
		allZeroString = new byte[cipher_ctx.getBlockSize() / 8];
		tmp = new byte[cipher_ctx.getKeySize() / 8];

		fetch_counter = output_buffer.length;
	}

	private final void counterInc() {
		for(int i = counter.length - 1; i >= 0; i--)
			if(++counter[i] != 0)
				break;
	}

	private final void generateOutput() {
		counterInc();

		output_buffer = new byte[counter.length];
		cipher_ctx.encipher(counter, output_buffer);

		if(output_count++ > Pg) {
			output_count = 0;
			nextBytes(tmp);
			rekey(tmp);
		}
	}

	private void rekey(byte[] key) {
		cipher_ctx.initialize(key);
		counter = new byte[allZeroString.length];
		cipher_ctx.encipher(allZeroString, counter);
		Arrays.fill(key, (byte) 0);
	}

	// Fetches count bytes of randomness into the shared buffer, returning
	// an offset to the bytes
	private synchronized int getBytes(int count) {

		if(fetch_counter + count > output_buffer.length) {
			fetch_counter = 0;
			generateOutput();
			return getBytes(count);
		}

		int rv = fetch_counter;
		fetch_counter += count;
		return rv;
	}
	static final int bitTable[][] = {{0, 0x0}, {
			1, 0x1
		}, {
			1, 0x3
		}, {
			1, 0x7
		}, {
			1, 0xf
		}, {
			1, 0x1f
		}, {
			1, 0x3f
		}, {
			1, 0x7f
		}, {
			1, 0xff
		}, {
			2, 0x1ff
		}, {
			2, 0x3ff
		}, {
			2, 0x7ff
		}, {
			2, 0xfff
		}, {
			2, 0x1fff
		}, {
			2, 0x3fff
		}, {
			2, 0x7fff
		}, {
			2, 0xffff
		}, {
			3, 0x1ffff
		}, {
			3, 0x3ffff
		}, {
			3, 0x7ffff
		}, {
			3, 0xfffff
		}, {
			3, 0x1fffff
		}, {
			3, 0x3fffff
		}, {
			3, 0x7fffff
		}, {
			3, 0xffffff
		}, {
			4, 0x1ffffff
		}, {
			4, 0x3ffffff
		}, {
			4, 0x7ffffff
		}, {
			4, 0xfffffff
		}, {
			4, 0x1fffffff
		}, {
			4, 0x3fffffff
		}, {
			4, 0x7fffffff
		}, {
			4, 0xffffffff
		}};

	// This may *look* more complicated than in is, but in fact it is
	// loop unrolled, cache and operation optimized.
	// So don't try to simplify it... Thanks. :)
	// When this was not synchronized, we were getting repeats...
	@Override
	protected synchronized int next(int bits) {
		int[] parameters = bitTable[bits];
		int offset = getBytes(parameters[0]);

		int val = output_buffer[offset];

		if(parameters[0] == 4)
			val += (output_buffer[offset + 1] << 24) + (output_buffer[offset + 2] << 16) + (output_buffer[offset + 3] << 8);
		else if(parameters[0] == 3)
			val += (output_buffer[offset + 1] << 16) + (output_buffer[offset + 2] << 8);
		else if(parameters[0] == 2)
			val += output_buffer[offset + 2] << 8;

		return val & parameters[1];
	}
	/**
	 * 5.2 Entropy Accumulator
	 */
	private MessageDigest fast_pool,  slow_pool;
	private int fast_entropy,  slow_entropy;
	private boolean fast_select;
	private Map<EntropySource, int[]> entropySeen;

	private void accumulator_init(String digest) throws NoSuchAlgorithmException {
		fast_pool = MessageDigest.getInstance(digest);
		slow_pool = MessageDigest.getInstance(digest);
		entropySeen = new HashMap<EntropySource, int[]>();
	}

	@Override
	public int acceptEntropy(EntropySource source, long data, int entropyGuess) {
		return acceptEntropy(source, data, entropyGuess, 1.0);
	}

	@Override
	public int acceptEntropyBytes(EntropySource source, byte[] buf, int offset,
		int length, double bias) {
		int totalRealEntropy = 0;
		for(int i = 0; i < length; i += 8) {
			long thingy = 0;
			int bytes = 0;
			for(int j = 0; j < Math.min(length, i + 8); j++) {
				thingy = (thingy << 8) + buf[j];
				bytes++;
			}
			totalRealEntropy += acceptEntropy(source, thingy, bytes * 8, bias);
		}
		return totalRealEntropy;
	}

	private int acceptEntropy(
		EntropySource source,
		long data,
		int entropyGuess,
		double bias) {
		return accept_entropy(
			data,
			source,
			(int) (bias * Math.min(
			32,
			Math.min(estimateEntropy(source, data), entropyGuess))));
	}

	private int accept_entropy(long data, EntropySource source, int actualEntropy) {

		boolean performedPoolReseed = false;
		byte[] b = new byte[] {
				(byte) data,
				(byte) (data >> 8),
				(byte) (data >> 16),
				(byte) (data >> 24),
				(byte) (data >> 32),
				(byte) (data >> 40),
				(byte) (data >> 48),
				(byte) (data >> 56)
		};

		synchronized(this) {
			fast_select = !fast_select;
			MessageDigest pool = (fast_select ? fast_pool : slow_pool);
			pool.update(b);

			if(fast_select) {
				fast_entropy += actualEntropy;
				if(fast_entropy > FAST_THRESHOLD) {
					fast_pool_reseed();
					performedPoolReseed = true;
				}
			} else {
				slow_entropy += actualEntropy;

				if(source != null) {
					int[] contributedEntropy = entropySeen.get(source);
					if(contributedEntropy == null) {
						contributedEntropy = new int[] { actualEntropy };
						entropySeen.put(source, contributedEntropy);
					} else
						contributedEntropy[0]+=actualEntropy;

					if(slow_entropy >= (SLOW_THRESHOLD * 2)) {
						int kc = 0;
						for(Map.Entry<EntropySource, int[]> e : entropySeen.entrySet()) {
							EntropySource key = e.getKey();
							int[] v = e.getValue();
							if(DEBUG)
								Logger.normal(this, "Key: <" + key + "> " + v);
							if(v[0] > SLOW_THRESHOLD) {
								kc++;
								if(kc >= SLOW_K) {
									slow_pool_reseed();
									performedPoolReseed = true;
									break;
								}
							}
						}
					}
				}
			}
			if(DEBUG)
				//	    Core.logger.log(this,"Fast pool: "+fast_entropy+"\tSlow pool:
				// "+slow_entropy, LogLevel.NORMAL);
				System.err.println("Fast pool: " + fast_entropy + "\tSlow pool: " + slow_entropy);
		}
		if(performedPoolReseed && (seedfile != null)) {
			//Dont do this while synchronized on 'this' since
			//opening a file seems to be suprisingly slow on windows
			if(logMINOR)
				Logger.minor(this, "Writing seedfile");
			write_seed(seedfile);
			if(logMINOR)
				Logger.minor(this, "Written seedfile");
		}

		return actualEntropy;
	}

	private int estimateEntropy(EntropySource source, long newVal) {
		int delta = (int) (newVal - source.lastVal);
		int delta2 = delta - source.lastDelta;
		source.lastDelta = delta;

		int delta3 = delta2 - source.lastDelta2;
		source.lastDelta2 = delta2;

		if(delta < 0)
			delta = -delta;
		if(delta2 < 0)
			delta2 = -delta2;
		if(delta3 < 0)
			delta3 = -delta3;
		if(delta > delta2)
			delta = delta2;
		if(delta > delta3)
			delta = delta3;

		/*
		 * delta is now minimum absolute delta. Round down by 1 bit on general
		 * principles, and limit entropy entimate to 12 bits.
		 */
		delta >>= 1;
		delta &= (1 << 12) - 1;

		/* Smear msbit right to make an n-bit mask */
		delta |= delta >> 8;
		delta |= delta >> 4;
		delta |= delta >> 2;
		delta |= delta >> 1;
		/* Remove one bit to make this a logarithm */
		delta >>= 1;
		/* Count the bits set in the word */
		delta -= (delta >> 1) & 0x555;
		delta = (delta & 0x333) + ((delta >> 2) & 0x333);
		delta += (delta >> 4);
		delta += (delta >> 8);

		source.lastVal = newVal;

		return delta & 15;
	}

	@Override
	public int acceptTimerEntropy(EntropySource timer) {
		return acceptTimerEntropy(timer, 1.0);
	}

	@Override
	public int acceptTimerEntropy(EntropySource timer, double bias) {
		long now = System.currentTimeMillis();
		return acceptEntropy(timer, now - timer.lastVal, 32, bias);
	}

	/**
	 * If entropy estimation is supported, this method will block until the
	 * specified number of bits of entropy are available. If estimation isn't
	 * supported, this method will return immediately.
	 */
	@Override
	public void waitForEntropy(int bits) {
	}
	/**
	 * 5.3 Reseed mechanism
	 */
	private static final int Pt = 5;
	private MessageDigest reseed_ctx;

	private void reseed_init(String digest) throws NoSuchAlgorithmException {
		reseed_ctx = MessageDigest.getInstance(digest);
	}

	private void fast_pool_reseed() {
		long startTime = System.currentTimeMillis();
		byte[] v0 = fast_pool.digest();
		byte[] vi = v0;

		for(byte i = 0; i < Pt; i++) {
			reseed_ctx.update(vi, 0, vi.length);
			reseed_ctx.update(v0, 0, v0.length);
			reseed_ctx.update(i);
			vi = reseed_ctx.digest();
		}

		// vPt=vi
		Util.makeKey(vi, tmp, 0, tmp.length);
		rekey(tmp);
		Arrays.fill(v0, (byte) 0); // blank out for security
		fast_entropy = 0;
		if (DEBUG) {
			long endTime = System.currentTimeMillis();
			if(endTime - startTime > 5000)
				Logger.normal(this, "Fast pool reseed took " + (endTime - startTime) + "ms");
		}
	}

	private void slow_pool_reseed() {
		byte[] slow_hash = slow_pool.digest();
		fast_pool.update(slow_hash, 0, slow_hash.length);

		fast_pool_reseed();
		slow_entropy = 0;

		entropySeen.clear();
	}
	/**
	 * 5.4 Reseed Control parameters
	 */
	private static final int FAST_THRESHOLD = 100,  SLOW_THRESHOLD = 160,  SLOW_K = 2;

	/**
	 * If the RandomSource has any resources it wants to close, it can do so
	 * when this method is called
	 */
	@Override
	public void close() {
	}

	/**
	 * Test routine
	 */
	public static void main(String[] args) throws Exception {
		Yarrow r = new Yarrow(new File("/dev/urandom"), "SHA1", "Rijndael", true, false);

		byte[] b = new byte[1024];

		if((args.length == 0) || args[0].equalsIgnoreCase("latency")) {
			if(args.length == 2)
				b = new byte[Integer.parseInt(args[1])];
			long start = System.currentTimeMillis();
			for(int i = 0; i < 100; i++)
				r.nextBytes(b);
			System.out.println(
				(double) (System.currentTimeMillis() - start) / (100 * b.length) * 1024 + " ms/k");
			start = System.currentTimeMillis();
			for(int i = 0; i < 1000; i++)
				r.nextInt();
			System.out.println(
				(double) (System.currentTimeMillis() - start) / 1000 + " ms/int");
			start = System.currentTimeMillis();
			for(int i = 0; i < 1000; i++)
				r.nextLong();
			System.out.println(
				(double) (System.currentTimeMillis() - start) / 1000 + " ms/long");
		} else if(args[0].equalsIgnoreCase("randomness")) {
			int kb = Integer.parseInt(args[1]);
			for(int i = 0; i < kb; i++) {
				r.nextBytes(b);
				System.out.write(b);
			}
		} else if(args[0].equalsIgnoreCase("gathering")) {
			System.gc();
			EntropySource t = new EntropySource();
			long start = System.currentTimeMillis();
			for(int i = 0; i < 100000; i++)
				r.acceptEntropy(t, System.currentTimeMillis(), 32);
			System.err.println(
				(double) (System.currentTimeMillis() - start) / 100000);
			System.gc();
			start = System.currentTimeMillis();
			for(int i = 0; i < 100000; i++)
				r.acceptTimerEntropy(t);
			System.err.println(
				(double) (System.currentTimeMillis() - start) / 100000);
		} else if(args[0].equalsIgnoreCase("volume")) {
			b = new byte[1020];
			long duration =
				System.currentTimeMillis() + Integer.parseInt(args[1]);
			while(System.currentTimeMillis() < duration) {
				r.nextBytes(b);
				System.out.write(b);
			}
//		} else if (args[0].equals("stream")) {
//			RandFile f = new RandFile(args[1]);
//			EntropySource rf = new EntropySource();
//			byte[] buffer = new byte[131072];
//			while (true) {
//				r.acceptEntropy(rf, f.nextLong(), 32);
//				r.nextBytes(buffer);
//				System.out.write(buffer);
//			}
		} else if(args[0].equalsIgnoreCase("bitstream"))
			while(true) {
				int v = r.nextInt();
				for(int i = 0; i < 32; i++) {
					if(((v >> i) & 1) == 1)
						System.out.print('1');
					else
						System.out.print('0');
				}
			}
		else if(args[0].equalsIgnoreCase("sample"))
			if((args.length == 1) || args[1].equals("general")) {
				System.out.println("nextInt(): ");
				for(int i = 0; i < 3; i++)
					System.out.println(r.nextInt());
				System.out.println("nextLong(): ");
				for(int i = 0; i < 3; i++)
					System.out.println(r.nextLong());
				System.out.println("nextFloat(): ");
				for(int i = 0; i < 3; i++)
					System.out.println(r.nextFloat());
				System.out.println("nextDouble(): ");
				for(int i = 0; i < 3; i++)
					System.out.println(r.nextDouble());
				System.out.println("nextFullFloat(): ");
				for(int i = 0; i < 3; i++)
					System.out.println(r.nextFullFloat());
				System.out.println("nextFullDouble(): ");
				for(int i = 0; i < 3; i++)
					System.out.println(r.nextFullDouble());
			} else if(args[1].equals("normalized"))
				for(int i = 0; i < 20; i++)
					System.out.println(r.nextDouble());
	}

	private void consumeString(String str) {
		byte[] b;
		try {
			b = str.getBytes("UTF-8");
		} catch(UnsupportedEncodingException e) {
			throw new Error("Impossible: JVM doesn't support UTF-8: " + e, e);
		}
		consumeBytes(b);
	}

	private void consumeBytes(byte[] bytes) {
		if(fast_select)
			fast_pool.update(bytes, 0, bytes.length);
		else
			slow_pool.update(bytes, 0, bytes.length);
		fast_select = !fast_select;
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.crypt;

public interface Digest {

    /**
     * retrieve the value of a hash, by filling the provided int[] with
     * n elements of the hash (where n is the bitlength of the hash/32)
     * @param digest int[] into which to place n elements
     * @param offset index of first of the n elements
     */
    public void extract(int [] digest, int offset);

     /**
     * Add one byte to the digest. When this is implemented
     * all of the abstract class methods end up calling
     * this method for types other than bytes.
     * @param b byte to add
     */
    public void update(byte b);

    /**
     * Add many bytes to the digest.
     * @param data byte data to add
     * @param offset start byte
     * @param length number of bytes to hash
     */
    public void update(byte[] data, int offset, int length);

    /**
     * Adds the entire contents of the byte array to the digest.
     */
    public void update(byte[] data);
     
    /**
     * Returns the completed digest, reinitializing the hash function.
     * @return the byte array result
     */
    public byte[] digest();

    /**
     * Write completed digest into the given buffer.
     * @param buffer the buffer to write into
     * @param offset the byte offset at which to start writing
     * @param reset If true, the hash function is reinitialized
     * after writing to the buffer.
     */
    public void digest(boolean reset, byte[] buffer, int offset);

    /**
     * Return the hash size of this digest in bits
     */
    public int digestSize();
}




/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.pluginmanager;

/**
 * 403 error code.
 * 
 * @author Florent Daigni&egrave;re &lt;nextgens@freenetproject.org&gt;
 */
public class AccessDeniedPluginHTTPException extends PluginHTTPException {
	private static final long serialVersionUID = -1;
	
	public static final short code = 403; // Access Denied

	public AccessDeniedPluginHTTPException(String errorMessage, String location) {
		super(errorMessage, location);
	}
}
package freenet.node.simulator;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.io.PrintStream;
import java.net.MalformedURLException;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.Date;
import java.util.GregorianCalendar;
import java.util.List;
import java.util.Locale;
import java.util.TimeZone;

import freenet.client.ClientMetadata;
import freenet.client.FetchException;
import freenet.client.HighLevelSimpleClient;
import freenet.client.InsertBlock;
import freenet.client.InsertException;
import freenet.crypt.RandomSource;
import freenet.keys.FreenetURI;
import freenet.node.Node;
import freenet.node.NodeStarter;
import freenet.node.Version;
import freenet.support.Fields;
import freenet.support.Logger;
import freenet.support.PooledExecutor;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;
import freenet.support.io.Closer;
import freenet.support.io.FileUtil;

/** Simulates MHKs. Creates 4 CHKs, inserts the first one 3 times, and inserts the
 * others 1 time each. Pulls them all after 1, 3, 7, 15, 31 etc days and computes 
 * success rates for the 1 versus the 3 combined. I am convinced the 3 combined will
 * be much more successful, but evanbd isn't.
 * @author Matthew Toseland <toad@amphibian.dyndns.org> (0xE43DA450)
 *
 */
public class LongTermMHKTest {
	
	private static final int TEST_SIZE = 64 * 1024;

	private static final int EXIT_NO_SEEDNODES = 257;
	private static final int EXIT_FAILED_TARGET = 258;
	private static final int EXIT_THREW_SOMETHING = 261;
	private static final int EXIT_DIFFERENT_URI = 262;

	private static final int DARKNET_PORT1 = 5010;
	private static final int OPENNET_PORT1 = 5011;
	private static final int DARKNET_PORT2 = 5012;
	private static final int OPENNET_PORT2 = 5013;
	
	/** Delta - the number of days we wait before fetching. */
	private static final int DELTA = 7;

	private static final DateFormat dateFormat = new SimpleDateFormat("yyyy.MM.dd", Locale.US);
	static {
		dateFormat.setTimeZone(TimeZone.getTimeZone("GMT"));
	}
	private static final GregorianCalendar today = (GregorianCalendar) Calendar.getInstance(TimeZone.getTimeZone("GMT"));

	public static void main(String[] args) {
		if (args.length < 1 || args.length > 2) {
			System.err.println("Usage: java freenet.node.simulator.LongTermPushPullTest <unique identifier>");
			System.exit(1);
		}
		String uid = args[0];
		
		boolean dumpOnly = args.length == 2 && "--dump".equalsIgnoreCase(args[1]);
		
		List<String> csvLine = new ArrayList<String>();
		System.out.println("DATE:" + dateFormat.format(today.getTime()));
		csvLine.add(dateFormat.format(today.getTime()));

		System.out.println("Version:" + Version.buildNumber());
		csvLine.add(String.valueOf(Version.buildNumber()));

		int exitCode = 0;
		Node node = null;
		Node node2 = null;
		FileInputStream fis = null;
		File file = new File("mhk-test-"+uid + ".csv");
		long t1, t2;

		HighLevelSimpleClient client = null;
		
		try {
			
			// INSERT STUFF
			
			final File dir = new File("longterm-mhk-test-" + uid);
			if(!dumpOnly) {
			FileUtil.removeAll(dir);
			RandomSource random = NodeStarter.globalTestInit(dir.getPath(), false, LogLevel.ERROR, "", false);
			File seednodes = new File("seednodes.fref");
			if (!seednodes.exists() || seednodes.length() == 0 || !seednodes.canRead()) {
				System.err.println("Unable to read seednodes.fref, it doesn't exist, or is empty");
				System.exit(EXIT_NO_SEEDNODES);
			}

			final File innerDir = new File(dir, Integer.toString(DARKNET_PORT1));
			innerDir.mkdir();
			fis = new FileInputStream(seednodes);
			FileUtil.writeTo(fis, new File(innerDir, "seednodes.fref"));
			fis.close();

			// Create one node
			node = NodeStarter.createTestNode(DARKNET_PORT1, OPENNET_PORT1, dir.getPath(), false, Node.DEFAULT_MAX_HTL,
			        0, random, new PooledExecutor(), 1000, 4 * 1024 * 1024, true, true, true, true, true, true, true,
			        12 * 1024, true, true, false, false, null);
			Logger.getChain().setThreshold(LogLevel.ERROR);

			// Start it
			node.start(true);
			t1 = System.currentTimeMillis();
			if (!TestUtil.waitForNodes(node)) {
				exitCode = EXIT_FAILED_TARGET;
				return;
			}
				
			t2 = System.currentTimeMillis();
			System.out.println("SEED-TIME:" + (t2 - t1));
			csvLine.add(String.valueOf(t2 - t1));

			// Create four CHKs
			
			Bucket single = randomData(node);
			Bucket[] mhks = new Bucket[3];
			
			for(int i=0;i<mhks.length;i++) mhks[i] = randomData(node);
			
			client = node.clientCore.makeClient((short) 0);

			System.err.println("Inserting single block 3 times");
			
			InsertBlock block = new InsertBlock(single, new ClientMetadata(), FreenetURI.EMPTY_CHK_URI);
			
			FreenetURI uri = null;
			
			int successes = 0;
			
			for(int i=0;i<3;i++) {
				System.err.println("Inserting single block, try #"+i);
				try {
					t1 = System.currentTimeMillis();
					FreenetURI thisURI = client.insert(block, false, null);
					if(uri != null && !thisURI.equals(uri)) {
						System.err.println("URI "+i+" is "+thisURI+" but previous is "+uri);
						System.exit(EXIT_DIFFERENT_URI);
					}
					uri = thisURI;
					t2 = System.currentTimeMillis();
					
					System.out.println("PUSH-TIME-" + i + ":" + (t2 - t1)+" for "+uri+" for single block");
					csvLine.add(String.valueOf(t2 - t1));
					csvLine.add(uri.toASCIIString());
					successes++;
				} catch (InsertException e) {
					e.printStackTrace();
					csvLine.add(FetchException.getShortMessage(e.getMode()));
					csvLine.add("N/A");
					System.out.println("INSERT FAILED: "+e+" for insert "+i+" for single block");
				}
			}
			
			if(successes == 3)
				System.err.println("All inserts succeeded for single block: "+successes);
			else if(successes != 0)
				System.err.println("Some inserts succeeded for single block: "+successes);
			else
				System.err.println("NO INSERTS SUCCEEDED FOR SINGLE BLOCK: "+successes);
			
			uri = null;
			
			// Insert 3 blocks
			
			for(int i=0;i<3;i++) {
				System.err.println("Inserting MHK #"+i);
				uri = null;
				block = new InsertBlock(mhks[i], new ClientMetadata(), FreenetURI.EMPTY_CHK_URI);
				try {
					t1 = System.currentTimeMillis();
					FreenetURI thisURI = client.insert(block, false, null);
					uri = thisURI;
					t2 = System.currentTimeMillis();
					
					System.out.println("PUSH-TIME-" + i + ":" + (t2 - t1)+" for "+uri+" for MHK #"+i);
					csvLine.add(String.valueOf(t2 - t1));
					csvLine.add(uri.toASCIIString());
					successes++;
				} catch (InsertException e) {
					e.printStackTrace();
					csvLine.add(FetchException.getShortMessage(e.getMode()));
					csvLine.add("N/A");
					System.out.println("INSERT FAILED: "+e+" for MHK #"+i);
				}
			}
			
			if(successes == 3)
				System.err.println("All inserts succeeded for MHK: "+successes);
			else if(successes != 0)
				System.err.println("Some inserts succeeded for MHK: "+successes);
			else
				System.err.println("NO INSERTS SUCCEEDED FOR MHK: "+successes);
			
			uri = null;
			}
			
			// PARSE FILE AND FETCH OLD STUFF IF APPROPRIATE
			
			boolean match = false;
			
			FreenetURI singleURI = null;
			FreenetURI[] mhkURIs = new FreenetURI[3];
			fis = new FileInputStream(file);
			BufferedReader br = new BufferedReader(new InputStreamReader(fis));
			String line = null;
			int linesTooShort = 0, linesBroken = 0, linesNoNumber = 0, linesNoURL = 0, linesNoFetch = 0;
			int total = 0, singleKeysSucceeded = 0, mhkSucceeded = 0;
			int totalSingleKeyFetches = 0, totalSingleKeySuccesses = 0;
			while((line = br.readLine()) != null) {
				
				singleURI = null;
				for(int i=0;i<mhkURIs.length;i++) mhkURIs[i] = null;
				//System.out.println("LINE: "+line);
				String[] split = line.split("!");
				Date date = dateFormat.parse(split[0]);
				GregorianCalendar calendar = new GregorianCalendar(TimeZone.getTimeZone("GMT"));
				calendar.setTime(date);
				System.out.println("Date: "+dateFormat.format(calendar.getTime()));
				GregorianCalendar target = (GregorianCalendar) today.clone();
				target.set(Calendar.HOUR_OF_DAY, 0);
				target.set(Calendar.MINUTE, 0);
				target.set(Calendar.MILLISECOND, 0);
				target.set(Calendar.SECOND, 0);
				target.add(Calendar.DAY_OF_MONTH, -DELTA);
				calendar.set(Calendar.HOUR_OF_DAY, 0);
				calendar.set(Calendar.MINUTE, 0);
				calendar.set(Calendar.MILLISECOND, 0);
				calendar.set(Calendar.SECOND, 0);
				calendar.getTime();
				target.getTime();
				try {
					if(split.length < 3) {
						linesTooShort++;
						continue;
					}
					int seedTime = Integer.parseInt(split[2]);
					System.out.println("Seed time: "+seedTime);
					
					int token = 3;
					if(split.length < 4) {
						linesTooShort++;
						continue;
					}
					
					for(int i=0;i<3;i++) {
						int insertTime = Integer.parseInt(split[token]);
						System.out.println("Single key insert "+i+" : "+insertTime);
						token++;
						FreenetURI thisURI = new FreenetURI(split[token]);
						if(singleURI == null)
							singleURI = thisURI;
						else {
							if(!singleURI.equals(thisURI)) {
								System.err.println("URI is not the same for all 3 inserts: was "+singleURI+" but "+i+" is "+thisURI);
								linesBroken++;
								continue;
							}
						}
						token++;
					}
					System.out.println("Single key URI: "+singleURI);
					
					for(int i=0;i<3;i++) {
						int insertTime = Integer.parseInt(split[token]);
						token++;
						mhkURIs[i] = new FreenetURI(split[token]);
						token++;
						System.out.println("MHK #"+i+" URI: "+mhkURIs[i]+" insert time "+insertTime);
					}
					
				} catch (NumberFormatException e) {
					System.err.println("Failed to parse row: "+e);
					linesNoNumber++;
					continue;
				} catch (MalformedURLException e) {
					System.err.println("Failed to parse row: "+e);
					linesNoURL++;
					continue;
				}
				if(Math.abs(target.getTimeInMillis() - calendar.getTimeInMillis()) < 12*60*60*1000) {
					System.out.println("Found row for target date "+dateFormat.format(target.getTime())+" : "+dateFormat.format(calendar.getTime()));
					System.out.println("Version: "+split[1]);
					match = true;
					break;
				} else if(split.length > 3+6+6) {
					int token = 3 + 6 + 6;
					int singleKeyFetchTime = -1;
					boolean singleKeySuccess = false;
					for(int i=0;i<3;i++) {
						// Fetched 3 times
						if(!singleKeySuccess) {
							try {
								singleKeyFetchTime = Integer.parseInt(split[token]);
								singleKeySuccess = true;
								System.out.println("Fetched single key on try "+i+" on "+date+" in "+singleKeyFetchTime+"ms");
							} catch (NumberFormatException e) {
								System.out.println("Failed fetch single key on "+date+" try "+i+" : "+split[token]);
								singleKeyFetchTime = -1;
							}
						} // Else will be empty.
						token++;
					}
					boolean mhkSuccess = false;
					for(int i=0;i<3;i++) {
						totalSingleKeyFetches++;
						int mhkFetchTime = -1;
						try {
							mhkFetchTime = Integer.parseInt(split[token]);
							mhkSuccess = true;
							totalSingleKeySuccesses++;
							System.out.println("Fetched MHK #"+i+" on "+date+" in "+mhkFetchTime+"ms");
						} catch (NumberFormatException e) {
							System.out.println("Failed fetch MHK #"+i+" on "+date+" : "+split[token]);
						}
						token++;
					}
					total++;
					if(singleKeySuccess)
						singleKeysSucceeded++;
					if(mhkSuccess)
						mhkSucceeded++;
				} else linesNoFetch++;
			}
			System.out.println("Lines where insert failed or no fetch: too short: "+linesTooShort+" broken: "+linesBroken+" no number: "+linesNoNumber+" no url: "+linesNoURL+" no fetch "+linesNoFetch);
			System.out.println("Total attempts where insert succeeded and fetch executed: "+total);
			System.out.println("Single keys succeeded: "+singleKeysSucceeded);
			System.out.println("MHKs succeeded: "+mhkSucceeded);
			System.out.println("Single key individual fetches: "+totalSingleKeyFetches);
			System.out.println("Single key individual fetches succeeded: "+totalSingleKeySuccesses);
			System.out.println("Success rate for individual keys (from MHK inserts): "+((double)totalSingleKeySuccesses)/((double)totalSingleKeyFetches));
			System.out.println("Success rate for the single key triple inserted: "+((double)singleKeysSucceeded)/((double)total));
			System.out.println("Success rate for the MHK (success = any of the 3 different keys worked): "+((double)mhkSucceeded)/((double)total));
			fis.close();
			fis = null;
			
			// FETCH STUFF
			
			
			if((!dumpOnly) && match) {
				
				// FETCH SINGLE URI
				
				// Fetch the first one 3 times, since the MHK is 3 fetches also.
				// Technically this is 9 fetches because we multiply by 3 fetches per high-level fetch by default.
				
				boolean fetched = false;
				for(int i=0;i<3;i++) {
					if(fetched) {
						csvLine.add("");
						continue;
					}
					try {
						t1 = System.currentTimeMillis();
						client.fetch(singleURI);
						t2 = System.currentTimeMillis();
						
						System.out.println("PULL-TIME FOR SINGLE URI:" + (t2 - t1));
						csvLine.add(String.valueOf(t2 - t1));
						fetched = true;
					} catch (FetchException e) {
						if (e.getMode() != FetchException.ALL_DATA_NOT_FOUND
								&& e.getMode() != FetchException.DATA_NOT_FOUND)
							e.printStackTrace();
						csvLine.add(FetchException.getShortMessage(e.getMode()));
						System.err.println("FAILED PULL FOR SINGLE URI: "+e);
					}
				}
				
				for(int i=0;i<mhkURIs.length;i++) {
					try {
						t1 = System.currentTimeMillis();
						client.fetch(mhkURIs[i]);
						t2 = System.currentTimeMillis();
						
						System.out.println("PULL-TIME FOR MHK #"+i+":" + (t2 - t1));
						csvLine.add(String.valueOf(t2 - t1));
					} catch (FetchException e) {
						if (e.getMode() != FetchException.ALL_DATA_NOT_FOUND
								&& e.getMode() != FetchException.DATA_NOT_FOUND)
							e.printStackTrace();
						csvLine.add(FetchException.getShortMessage(e.getMode()));
						System.err.println("FAILED PULL FOR MHK #"+i+": "+e);
					}
				}
			}
			
		} catch (Throwable t) {
			t.printStackTrace();
			exitCode = EXIT_THREW_SOMETHING;
		} finally {
			try {
				if (node != null)
					node.park();
			} catch (Throwable tt) {
			}
			try {
				if (node2 != null)
					node2.park();
			} catch (Throwable tt) {
			}
			Closer.close(fis);

			if(!dumpOnly) {
				try {
					FileOutputStream fos = new FileOutputStream(file, true);
					PrintStream ps = new PrintStream(fos);
					
					ps.println(Fields.commaList(csvLine.toArray(), '!'));
					
					ps.close();
					fos.close();
				} catch (IOException e) {
					e.printStackTrace();
					exitCode = EXIT_THREW_SOMETHING;
				}
			}
			System.exit(exitCode);
		}
	}	
	
	private static Bucket randomData(Node node) throws IOException {
		Bucket data = node.clientCore.tempBucketFactory.makeBucket(TEST_SIZE);
		OutputStream os = data.getOutputStream();
		byte[] buf = new byte[4096];
		for (long written = 0; written < TEST_SIZE;) {
			node.fastWeakRandom.nextBytes(buf);
			int toWrite = (int) Math.min(TEST_SIZE - written, buf.length);
			os.write(buf, 0, toWrite);
			written += toWrite;
		}
		os.close();
		return data;
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */

package freenet.client.async;

import java.io.IOException;
import java.io.OutputStream;

import com.db4o.ObjectContainer;

/** Writes an underlying data structure to an output stream.*/
public interface StreamGenerator {

	/** Writes the data.
	 * @param os Stream to which the data will be written
	 * @param container
	 * @param context
	 * @throws IOException
	 */
	public void writeTo(OutputStream os, ObjectContainer container, ClientContext context) throws IOException;

	/**
	 * @return The size of the underlying structure
	 */
	public long size();
}
/*
 * Dijjer - A Peer to Peer HTTP Cache
 * Copyright (C) 2004,2005 Change.Tv, Inc
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */
package freenet.io.xfer;

import freenet.io.comm.AsyncMessageCallback;
import freenet.io.comm.ByteCounter;
import freenet.io.comm.Message;
import freenet.io.comm.NotConnectedException;
import freenet.io.comm.Peer;
import freenet.io.comm.PeerContext;
import freenet.io.comm.PeerRestartedException;
import freenet.node.MessageItem;
import freenet.node.PeerNode;
import freenet.node.SyncSendWaitedTooLongException;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;

public class PacketThrottle {

	private static volatile boolean logMINOR;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	protected static final double PACKET_DROP_DECREASE_MULTIPLE = 0.875;
	protected static final double PACKET_TRANSMIT_INCREMENT = (4 * (1 - (PACKET_DROP_DECREASE_MULTIPLE * PACKET_DROP_DECREASE_MULTIPLE))) / 3;
	protected static final double SLOW_START_DIVISOR = 3.0;
	protected static final long MAX_DELAY = 1000;
	protected static final long MIN_DELAY = 1;
	public static final String VERSION = "$Id: PacketThrottle.java,v 1.3 2005/08/25 17:28:19 amphibian Exp $";
	public static final long DEFAULT_DELAY = 200;
	private long _roundTripTime = 500, _totalPackets, _droppedPackets;
	/** The size of the window, in packets.
	 * Window size must not drop below 1.0. Partly this is because we need to be able to send one packet, so it is a logical lower bound.
	 * But mostly it is because of the non-slow-start division by _windowSize! */
	private float _windowSize = 2;
	private final int PACKET_SIZE;
	private boolean slowStart = true;
	/** Total packets in flight, including waiting for bandwidth from the central throttle. */
	private int _packetsInFlight;
	/** Incremented on each send; the sequence number of the packet last added to the window/sent */
	private long _packetSeq;
	/** Last time (seqno) the window was full */
	private long _packetSeqWindowFull;
	/** Last time (seqno) we checked whether the window was full, or dropped a packet. */
	private long _packetSeqWindowFullChecked;
	/** Holds the next number to be used for fifo packet pre-sequence numbers */
	private long _packetTicketGenerator;
	/** The number of would-be packets which are no longer waiting in line for the transmition window */
	private long _abandonedTickets;
	
	public PacketThrottle(int packetSize) {
		PACKET_SIZE = packetSize;
	}

	public synchronized void setRoundTripTime(long rtt) {
		_roundTripTime = Math.max(rtt, 10);
		if(logMINOR) Logger.minor(this, "Set round trip time to "+rtt+" on "+this);
	}

    public synchronized void notifyOfPacketLost() {
		_droppedPackets++;
		_totalPackets++;
		_windowSize *= PACKET_DROP_DECREASE_MULTIPLE;
		if(_windowSize < 1.0F) _windowSize = 1.0F;
		slowStart = false;
		if(logMINOR)
			Logger.minor(this, "notifyOfPacketLost(): "+this);
		_packetSeqWindowFullChecked = _packetSeq;
    }

    /**
     * Notify the throttle that a packet was transmitted successfully. We will increase the window size.
     * @param maxWindowSize The maximum window size. This should be at least twice the largest window
     * size actually seen in flight at any time so far. We will ensure that the throttle's window size
     * does not get bigger than this. This works even for new packet format, and solves some of the 
     * problems that RFC 2861 does.
     */
    public synchronized void notifyOfPacketAcknowledged(double maxWindowSize) {
        _totalPackets++;
		// If we didn't use the whole window, shrink the window a bit.
		// This is similar but not identical to RFC2861
		// See [freenet-dev] Major weakness in our current link-level congestion control
        int windowSize = (int)getWindowSize();
        if(_packetSeqWindowFullChecked + windowSize < _packetSeq) {
        	// FIXME this is only relevant for old packet format, which uses sendThrottledMessage(), get rid of it when we get rid of old packet format.
        	if(_packetSeqWindowFull < _packetSeqWindowFullChecked) {
        		// We haven't used the full window once since we last checked.
        		_windowSize *= PACKET_DROP_DECREASE_MULTIPLE;
        		if(_windowSize < 1.0F) _windowSize = 1.0F;
            	_packetSeqWindowFullChecked += windowSize;
            	if(logMINOR) Logger.minor(this, "Window not used since we last checked: full="+_packetSeqWindowFull+" last checked="+_packetSeqWindowFullChecked+" window = "+_windowSize+" for "+this);
        		return;
        	}
        	_packetSeqWindowFullChecked += windowSize;
        }

    	if(slowStart) {
    		if(logMINOR) Logger.minor(this, "Still in slow start");
    		_windowSize += _windowSize / SLOW_START_DIVISOR;
    		// Avoid craziness if there is lag in detecting packet loss.
    		if(_windowSize > maxWindowSize) slowStart = false;
    		// Window size must not drop below 1.0. Partly this is because we need to be able to send one packet, so it is a logical lower bound.
    		// But mostly it is because of the non-slow-start division by _windowSize!
    		if(_windowSize < 1.0F) _windowSize = 1.0F;
    	} else {
    		_windowSize += (PACKET_TRANSMIT_INCREMENT / _windowSize);
    	}
    	// Ensure that we the window size does not grow dramatically larger than the largest window
    	// that has actually been in flight at one time. This works both on new and old packet format,
    	// although it is more relevant for new packet format because new packet format allows larger
    	// in flight windows in practice.
    	if(_windowSize > maxWindowSize)
    		_windowSize = (float) maxWindowSize;
    	if(_windowSize > (windowSize + 1))
    		notifyAll();
    	if(logMINOR)
    		Logger.minor(this, "notifyOfPacketAcked(): "+this);
    }
    
    /** Only used for diagnostics. We actually maintain a real window size. So we don't
     * need lots of sanity checking here. */
	public synchronized long getDelay() {
		// return (long) (_roundTripTime / _simulatedWindowSize);
		return Math.max(MIN_DELAY, (long) (_roundTripTime / _windowSize));
	}

	@Override
	public synchronized String toString() {
		return Double.toString(getBandwidth()) + " k/sec, (w: "
				+ _windowSize + ", r:" + _roundTripTime + ", d:"
				+ (((float) _droppedPackets / (float) _totalPackets)) + ") total="+_totalPackets+" : "+super.toString();
	}

	public synchronized long getRoundTripTime() {
		return _roundTripTime;
	}

	public synchronized double getWindowSize() {
		return Math.max(1.0, _windowSize);
	}

	/**
	 * returns the number of bytes-per-second in the transmition link (?).
	 * FIXME: Will not return more than 1M/s due to MIN_DELAY in getDelay().
	 */
	public synchronized double getBandwidth() {
		//PACKET_SIZE=1024 [bytes?]
		//1000 ms/sec
		return ((PACKET_SIZE * 1000.0 / getDelay()));
	}
	
	/** 
	 * Send a throttled message.
	 * @param cbForAsyncSend cbForAsyncSend Callback to call when we send the message, etc. We will try
	 * to call it even if we throw an exception etc. The caller may want to do this too,
	 * in which case the callback should ignore multiple calls, which is a good idea 
	 * anyway.
	 * 
	 * FIXME this would be significantly simpler, as well as faster, if it was asynchronous.
	 */
	public MessageItem sendThrottledMessage(Message msg, PeerContext peer, int packetSize, ByteCounter ctr, long deadline, boolean blockForSend, AsyncMessageCallback cbForAsyncSend, boolean isRealTime) throws NotConnectedException, WaitedTooLongException, SyncSendWaitedTooLongException, PeerRestartedException {
		long start = System.currentTimeMillis();
		long bootID = peer.getBootID();
		if(logMINOR) Logger.minor(this, "Sending throttled message "+msg+" to "+peer.shortToString()+" realtime="+isRealTime+" message.getPriority()="+msg.getPriority());
		try {
		synchronized(this) {
			final long thisTicket=_packetTicketGenerator++;
			// FIXME a list, or even a TreeMap by deadline, would use less CPU than waking up every waiter twice whenever a packet is acked.
			while(true) {
				int windowSize = (int) getWindowSize();
				// If we have different timeouts, and we have packets 1 and 2 timeout and 3 and 4 not timeout,
				// we could end up not sending 3 and 4 at all if we use == here.
				if(logMINOR) Logger.minor(this, "_packetSeq="+_packetSeq+" this ticket = "+thisTicket+" abandoned "+_abandonedTickets+" in flight "+_packetsInFlight+" window "+windowSize);
				boolean wereNext=(_packetSeq>=(thisTicket-_abandonedTickets));
				//If there is room for it in the window, break and send it immediately
				if(_packetsInFlight < windowSize && wereNext) {
					_packetsInFlight++;
					_packetSeq++;
					if(windowSize == _packetsInFlight) {
						_packetSeqWindowFull = _packetSeq;
						if(logMINOR) Logger.minor(this, "Window full at "+_packetSeq+" for "+this);
					}
					if(logMINOR) Logger.minor(this, "Sending, window size now "+windowSize+" packets in flight "+_packetsInFlight+" for "+this);
					break;
				}
				long waitingBehind=thisTicket-_abandonedTickets-_packetSeq;
				if(logMINOR) Logger.minor(this, "Window size: "+windowSize+" packets in flight "+_packetsInFlight+", "+waitingBehind+" in front of this thread for "+this);
				long now = System.currentTimeMillis();
				int waitFor = (int)Math.min(Integer.MAX_VALUE, deadline - now);
				if(waitFor <= 0) {
					// Double-check.
					if(!peer.isConnected()) {
						Logger.error(this, "Not notified of disconnection before timeout");
						_abandonedTickets++;
						throw new NotConnectedException();
					}
					if(bootID != peer.getBootID()) {
						Logger.error(this, "Not notified of reconnection before timeout");
						_abandonedTickets++;
						notifyAll();
						throw new NotConnectedException();
					}
					Logger.error(this, "Unable to send throttled message, waited "+(now-start)+"ms");
					_abandonedTickets++;
					notifyAll();
					throw new WaitedTooLongException();
				}
				try {
					wait(waitFor);
				} catch (InterruptedException e) {
					// Ignore
				}
				if(!peer.isConnected()) {
					_abandonedTickets++;
					throw new NotConnectedException();
				}
				long newBootID = peer.getBootID();
				if(bootID != newBootID) {
					_abandonedTickets++;
					notifyAll();
					Logger.normal(this, "Peer restarted: boot ID was "+bootID+" now "+newBootID);
					throw new PeerRestartedException();
				}
			}
			/** Because we send in order, we have to go around all the waiters again after sending.
			 * Otherwise, we will miss slots:
			 * Seq = 0
			 * A: Wait for seq = 1
			 * B: Wait for seq = 2
			 * Packet acked
			 * Packet acked
			 * B: I'm not next since seq = 0 and I'm waiting for 2. Do nothing.
			 * A: I'm next because seq = 0 and I'm waiting for 1. Send a packet.
			 * A sends, B doesn't, even though it ought to: its slot is lost, and this can cause big 
			 * problems if we are sending more than one packet at a time.
			 */
			notifyAll();
		}
		// Deal with this outside the lock, catch and re-throw.
		} catch (NotConnectedException e) {
			if (cbForAsyncSend != null)
				cbForAsyncSend.disconnected();
			throw e;
		} catch (PeerRestartedException e) {
			if (cbForAsyncSend != null)
				cbForAsyncSend.disconnected();
			throw e;
		} catch (WaitedTooLongException e) {
			if (cbForAsyncSend != null)
				cbForAsyncSend.fatalError();
			throw e;
		} catch (Error e) {
			if (cbForAsyncSend != null)
				cbForAsyncSend.fatalError();
			throw e;
		} catch (RuntimeException e) {
			if (cbForAsyncSend != null)
				cbForAsyncSend.fatalError();
			throw e;
		}
		long waitTime = System.currentTimeMillis() - start;
		if(waitTime > 60*1000)
			Logger.error(this, "Congestion control wait time: "+waitTime+" for "+this);
		else if(logMINOR)
			Logger.minor(this, "Congestion control wait time: "+waitTime+" for "+this);
		MyCallback callback = new MyCallback(cbForAsyncSend, packetSize, ctr, peer != null && peer instanceof PeerNode && ((PeerNode)peer).isOldFNP(), start, peer, isRealTime);
		MessageItem sent;
		try {
			sent = peer.sendAsync(msg, callback, ctr);
			if(logMINOR) Logger.minor(this, "Sending async for throttled message: "+msg);
			if(blockForSend) {
				synchronized(callback) {
					long timeout = System.currentTimeMillis() + 60*1000;
					long now;
					while((now = System.currentTimeMillis()) < timeout && !callback.finished) {
						try {
							callback.wait((int)(timeout - now));
						} catch (InterruptedException e) {
							// Ignore
						}
					}
					if(!callback.finished) {
						throw new SyncSendWaitedTooLongException();
					}
				}
			}
			return sent;
			
		} catch (RuntimeException e) {
			callback.fatalError();
			throw e;
		} catch (Error e) {
			callback.fatalError();
			throw e;
		} catch (NotConnectedException e) {
			synchronized(this) {
				callback.disconnected();
				notifyAll();
			}
			throw e;
		}
	}
	
	private class MyCallback implements AsyncMessageCallback {

		private boolean finished = false;
		private boolean sent = false;
		private final int packetSize;
		private final ByteCounter ctr;
		private final boolean isOldFNP;
		private final long startTime;
		private final PeerContext pn;
		private final boolean realTime;
		
		private AsyncMessageCallback chainCallback;
		
		public MyCallback(AsyncMessageCallback cbForAsyncSend, int packetSize, ByteCounter ctr, boolean isOldFNP, long startTime, PeerContext pn, boolean realTime) {
			this.chainCallback = cbForAsyncSend;
			this.packetSize = packetSize;
			this.ctr = ctr;
			this.isOldFNP = isOldFNP;
			this.startTime = startTime;
			this.pn = pn;
			this.realTime = realTime;
		}

		public void acknowledged() {
			sent(true); // Make sure it is called at least once.
			synchronized(PacketThrottle.this) {
				if(finished) {
					if(logMINOR) Logger.minor(this, "Already acked, ignoring callback: "+this);
					return;
				}
				finished = true;
				_packetsInFlight--;
				PacketThrottle.this.notifyAll();
			}
			if(logMINOR) Logger.minor(this, "Removed packet: acked for "+this);
			if(chainCallback != null) chainCallback.acknowledged();
		}

		public void disconnected() {
			synchronized(PacketThrottle.this) {
				if(finished) return;
				finished = true;
				_packetsInFlight--;
				PacketThrottle.this.notifyAll();
			}
			if(logMINOR) Logger.minor(this, "Removed packet: disconnected for "+this);
			if(chainCallback != null) chainCallback.disconnected();
		}

		public void fatalError() {
			synchronized(PacketThrottle.this) {
				if(finished) return;
				finished = true;
				_packetsInFlight--;
				PacketThrottle.this.notifyAll();
			}
			if(logMINOR) Logger.minor(this, "Removed packet: error for "+this);
			if(chainCallback != null) chainCallback.fatalError();
		}
		
		public void sent() {
			sent(false);
		}

		public void sent(boolean error) {
			synchronized(PacketThrottle.this) {
				if(sent) return;
				if(error) {
					if(!isOldFNP)
						Logger.error(this, "Acknowledged called but not sent, assuming it has been sent on "+this);
					else
						// This looks like an old-FNP bug. Log at a lower priority.
						Logger.normal(this, "Acknowledged called but not sent, assuming it has been sent on "+this);
				}
				sent = true;
			}
			ctr.sentPayload(packetSize);
			long now = System.currentTimeMillis();
			if(logMINOR) Logger.minor(this, "Total time taken for packet: "+(now - startTime)+" realtime="+realTime);
			pn.reportThrottledPacketSendTime(now - startTime, realTime);
			// Ignore
			if(chainCallback != null) chainCallback.sent();
		}
		
		@Override
		public String toString() {
			return super.toString()+":"+PacketThrottle.this.toString();
		}
		
	}

	public synchronized void maybeDisconnected() {
		notifyAll();
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.node.Node;
import freenet.node.PeerNode;
import freenet.support.Fields;
import freenet.support.SimpleFieldSet;

public class ListPeersMessage extends FCPMessage {

	final boolean withMetadata;
	final boolean withVolatile;
	final String identifier;
	static final String NAME = "ListPeers";
	
	public ListPeersMessage(SimpleFieldSet fs) {
		withMetadata = Fields.stringToBool(fs.get("WithMetadata"), false);
		withVolatile = Fields.stringToBool(fs.get("WithVolatile"), false);
		this.identifier = fs.get("Identifier");
		fs.removeValue("Identifier");
	}
	
	@Override
	public SimpleFieldSet getFieldSet() {
		return new SimpleFieldSet(true);
	}
	
	@Override
	public String getName() {
		return NAME;
	}
	
	@Override
	public void run(FCPConnectionHandler handler, Node node)
			throws MessageInvalidException {
		if(!handler.hasFullAccess()) {
			throw new MessageInvalidException(ProtocolErrorMessage.ACCESS_DENIED, "ListPeers requires full access", identifier, false);
		}
		PeerNode[] nodes = node.getPeerNodes();
		for(int i = 0; i < nodes.length; i++) {
			PeerNode pn = nodes[i];
			handler.outputHandler.queue(new PeerMessage(pn, withMetadata, withVolatile, identifier));
		}
		
		handler.outputHandler.queue(new EndListPeersMessage(identifier));
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		container.delete(this);
	}
	
}
package freenet.support;

import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;

import com.db4o.ObjectContainer;

import freenet.support.api.Bucket;

/**
 * Simple read-only array bucket. Just an adapter class to save some RAM.
 * Wraps a byte[], offset, length into a Bucket. Read-only. ArrayBucket on
 * the other hand is a chain of byte[]'s.
 */
public class SimpleReadOnlyArrayBucket implements Bucket {

	final byte[] buf;
	final int offset;
	final int length;
	
	public SimpleReadOnlyArrayBucket(byte[] buf, int offset, int length) {
		this.buf = buf;
		this.offset = offset;
		this.length = length;
	}
	
	public SimpleReadOnlyArrayBucket(byte[] buf) {
		this(buf, 0, buf.length);
	}
	
	public OutputStream getOutputStream() throws IOException {
		throw new IOException("Read only");
	}

	public InputStream getInputStream() throws IOException {
		return new ByteArrayInputStream(buf, offset, length);
	}

	public String getName() {
		return "SimpleReadOnlyArrayBucket: len="+length+ ' ' +super.toString();
	}

	public long size() {
		return length;
	}

	public boolean isReadOnly() {
		return true;
	}

	public void setReadOnly() {
		// Already read-only
	}

	public void free() {
		// Do nothing
	}

	public void storeTo(ObjectContainer container) {
		container.store(this);
	}

	public void removeFrom(ObjectContainer container) {
		container.delete(this);
	}

	public Bucket createShadow() {
		if(buf.length < 256*1024) {
			byte[] newBuf = new byte[length];
			System.arraycopy(buf, offset, newBuf, 0, length);
			return new SimpleReadOnlyArrayBucket(newBuf);
		}
		return null;
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.config;

import freenet.l10n.NodeL10n;
import freenet.support.Fields;
import freenet.support.api.IntCallback;

/** Integer config variable */
public class IntOption extends Option<Integer> {
	protected final boolean isSize;

	public IntOption(SubConfig conf, String optionName, String defaultValueString, int sortOrder, boolean expert,
	        boolean forceWrite, String shortDesc, String longDesc, IntCallback cb, boolean isSize) {
		this(conf, optionName, Fields.parseInt(defaultValueString), sortOrder, expert, forceWrite, shortDesc, longDesc,
		        cb, isSize);
	}
	
	public IntOption(SubConfig conf, String optionName, Integer defaultValue, int sortOrder, boolean expert,
	        boolean forceWrite, String shortDesc, String longDesc, IntCallback cb, boolean isSize) {
		super(conf, optionName, cb, sortOrder, expert, forceWrite, shortDesc, longDesc, Option.DataType.NUMBER);
		this.defaultValue = defaultValue;
		this.currentValue = defaultValue;
		this.isSize = isSize;
	}

	@Override
	protected Integer parseString(String val) throws InvalidConfigValueException {
		Integer x;
		try {
			x = Fields.parseInt(val);
		} catch (NumberFormatException e) {
			throw new InvalidConfigValueException(l10n("parseError", "val", val));
		}
		return x;
	}

	private String l10n(String key, String pattern, String value) {
		return NodeL10n.getBase().getString("IntOption." + key, pattern, value);
	}

	@Override
	protected String toString(Integer val) {
		return Fields.intToString(val, isSize);
	}
}
package freenet.client;

import java.io.IOException;

import com.db4o.ObjectContainer;
import com.db4o.ObjectSet;
import com.db4o.query.Predicate;

import freenet.client.ArchiveManager.ARCHIVE_TYPE;
import freenet.client.async.ClientContext;
import freenet.client.async.DBJob;
import freenet.client.async.DatabaseDisabledException;
import freenet.keys.FreenetURI;
import freenet.support.Logger;
import freenet.support.api.Bucket;
import freenet.support.api.BucketFactory;
import freenet.support.compress.Compressor.COMPRESSOR_TYPE;
import freenet.support.io.BucketTools;
import freenet.support.io.NativeThread;

// WARNING: THIS CLASS IS STORED IN DB4O -- THINK TWICE BEFORE ADD/REMOVE/RENAME FIELDS
class ArchiveHandlerImpl implements ArchiveHandler {

	private static volatile boolean logMINOR;

	static {
		Logger.registerClass(ArchiveHandlerImpl.class);
	}

	private final FreenetURI key;
	private boolean forceRefetchArchive;
	ARCHIVE_TYPE archiveType;
	COMPRESSOR_TYPE compressorType;

	ArchiveHandlerImpl(FreenetURI key, ARCHIVE_TYPE archiveType, COMPRESSOR_TYPE ctype, boolean forceRefetchArchive) {
		this.key = key;
		this.archiveType = archiveType;
		this.compressorType = ctype;
		this.forceRefetchArchive = forceRefetchArchive;
	}

	public Bucket get(String internalName, ArchiveContext archiveContext,
			ArchiveManager manager, ObjectContainer container)
			throws ArchiveFailureException, ArchiveRestartException,
			MetadataParseException, FetchException {

		// Do loop detection on the archive that we are about to fetch.
		archiveContext.doLoopDetection(key, container);

		if(forceRefetchArchive) return null;

		Bucket data;

		// Fetch from cache
		if(logMINOR)
			Logger.minor(this, "Checking cache: "+key+ ' ' +internalName);
		if((data = manager.getCached(key, internalName)) != null) {
			return data;
		}

		return null;
	}

	public Bucket getMetadata(ArchiveContext archiveContext,
			ArchiveManager manager, ObjectContainer container) throws ArchiveFailureException,
			ArchiveRestartException, MetadataParseException, FetchException {
		return get(".metadata", archiveContext, manager, container);
	}

	public void extractToCache(Bucket bucket, ArchiveContext actx,
			String element, ArchiveExtractCallback callback,
			ArchiveManager manager, ObjectContainer container, ClientContext context) throws ArchiveFailureException,
			ArchiveRestartException {
		forceRefetchArchive = false; // now we don't need to force refetch any more
		ArchiveStoreContext ctx = manager.makeContext(key, archiveType, compressorType, false);
		manager.extractToCache(key, archiveType, compressorType, bucket, actx, ctx, element, callback, container, context);
	}

	public ARCHIVE_TYPE getArchiveType() {
		return archiveType;
	}

	public COMPRESSOR_TYPE getCompressorType() {
		return compressorType;
	}

	public FreenetURI getKey() {
		return key;
	}

	/**
	 * Unpack a fetched archive on a separate thread for a persistent caller.
	 * This involves:
	 * - Add a tag to the database so that it will be restarted on a crash.
	 * - Run the actual unpack on a separate thread.
	 * - Copy the data to a persistent bucket.
	 * - Schedule a database job.
	 * - Call the callback.
	 * @param bucket
	 * @param actx
	 * @param element
	 * @param callback
	 * @param container
	 * @param context
	 */
	public void extractPersistentOffThread(Bucket bucket, boolean freeBucket, ArchiveContext actx, String element, ArchiveExtractCallback callback, ObjectContainer container, final ClientContext context) {
		assert(element != null); // no callback would be called...
		final ArchiveManager manager = context.archiveManager;
		final ArchiveExtractTag tag = new ArchiveExtractTag(this, bucket, freeBucket, actx, element, callback, context.nodeDBHandle);
		container.store(tag);
		runPersistentOffThread(tag, context, manager, context.persistentBucketFactory);
	}

	private static void runPersistentOffThread(final ArchiveExtractTag tag, final ClientContext context, final ArchiveManager manager, final BucketFactory bf) {
		final ProxyCallback proxyCallback = new ProxyCallback();

		if(logMINOR)
			Logger.minor(ArchiveHandlerImpl.class, "Scheduling off-thread extraction: "+tag.data+" for "+tag.handler.key+" element "+tag.element+" for "+tag.callback, new Exception("debug"));

		context.mainExecutor.execute(new Runnable() {

			public void run() {
				try {
					if(logMINOR)
						Logger.minor(this, "Extracting off-thread: "+tag.data+" for "+tag.handler.key+" element "+tag.element+" for "+tag.callback);
					tag.handler.extractToCache(tag.data, tag.actx, tag.element, proxyCallback, manager, null, context);
					if(logMINOR)
						Logger.minor(this, "Extracted");
					final Bucket data;
					if(proxyCallback.data == null)
						data = null;
					else {
						try {
							if(logMINOR)
								Logger.minor(this, "Copying data...");
							data = bf.makeBucket(proxyCallback.data.size());
							BucketTools.copy(proxyCallback.data, data);
							proxyCallback.data.free();
							if(logMINOR)
								Logger.minor(this, "Copied and freed original");
						} catch (IOException e) {
							throw new ArchiveFailureException("Failure copying data to persistent storage", e);
						}
					}
					context.jobRunner.queue(new DBJob() {

						public boolean run(ObjectContainer container, ClientContext context) {
							if(logMINOR)
								Logger.minor(this, "Calling callback for "+tag.data+" for "+tag.handler.key+" element "+tag.element+" for "+tag.callback);
							container.activate(tag.callback, 1);
							if(proxyCallback.data == null)
								tag.callback.notInArchive(container, context);
							else
								tag.callback.gotBucket(data, container, context);
							tag.callback.removeFrom(container);
							if(tag.freeBucket) {
								tag.data.free();
								tag.data.removeFrom(container);
							}
							container.deactivate(tag.callback, 1);
							container.delete(tag);
							return false;
						}

					}, NativeThread.NORM_PRIORITY, false);

				} catch (final ArchiveFailureException e) {

					try {
						context.jobRunner.queue(new DBJob() {

							public boolean run(ObjectContainer container, ClientContext context) {
								container.activate(tag.callback, 1);
								tag.callback.onFailed(e, container, context);
								tag.callback.removeFrom(container);
								if(tag.freeBucket) {
									tag.data.free();
									tag.data.removeFrom(container);
								}
								container.delete(tag);
								return false;
							}

						}, NativeThread.NORM_PRIORITY, false);
					} catch (DatabaseDisabledException e1) {
						Logger.error(this, "Extracting off thread but persistence is disabled");
					}

				} catch (final ArchiveRestartException e) {

					try {
						context.jobRunner.queue(new DBJob() {

							public boolean run(ObjectContainer container, ClientContext context) {
								container.activate(tag.callback, 1);
								tag.callback.onFailed(e, container, context);
								tag.callback.removeFrom(container);
								if(tag.freeBucket) {
									tag.data.free();
									tag.data.removeFrom(container);
								}
								container.delete(tag);
								return false;
							}

						}, NativeThread.NORM_PRIORITY, false);
					} catch (DatabaseDisabledException e1) {
						Logger.error(this, "Extracting off thread but persistence is disabled");
					}

				} catch (DatabaseDisabledException e) {
					Logger.error(this, "Extracting off thread but persistence is disabled");
				}
			}

		}, "Off-thread extract");
	}

	/** Called from ArchiveManager.init() */
	static void init(ObjectContainer container, ClientContext context, final long nodeDBHandle) {
		ObjectSet<ArchiveExtractTag> set = container.query(new Predicate<ArchiveExtractTag>() {
			final private static long serialVersionUID = 5769839072558476040L;
			@Override
			public boolean match(ArchiveExtractTag tag) {
				return tag.nodeDBHandle == nodeDBHandle;
			}
		});
		while(set.hasNext()) {
			ArchiveExtractTag tag = set.next();
			if(tag.checkBroken(container, context)) continue;
			tag.activateForExecution(container);
			runPersistentOffThread(tag, context, context.archiveManager, context.persistentBucketFactory);
		}
	}

	private static class ProxyCallback implements ArchiveExtractCallback {

		Bucket data;

		public void gotBucket(Bucket data, ObjectContainer container, ClientContext context) {
			this.data = data;
		}

		public void notInArchive(ObjectContainer container, ClientContext context) {
			this.data = null;
		}

		public void onFailed(ArchiveRestartException e, ObjectContainer container, ClientContext context) {
			// Must not be called.
			throw new UnsupportedOperationException();
		}

		public void onFailed(ArchiveFailureException e, ObjectContainer container, ClientContext context) {
			// Must not be called.
			throw new UnsupportedOperationException();
		}

		public void removeFrom(ObjectContainer container) {
			container.delete(this);
		}

	}

	public void activateForExecution(ObjectContainer container) {
		container.activate(this, 1);
		container.activate(key, 5);
	}

	public ArchiveHandler cloneHandler() {
		return new ArchiveHandlerImpl(key.clone(), archiveType, compressorType, forceRefetchArchive);
	}

	public void removeFrom(ObjectContainer container) {
		if(key == null) {
			Logger.error(this, "removeFrom() : key = null for "+this+" I exist = "+container.ext().isStored(this)+" I am active: "+container.ext().isActive(this), new Exception("error"));
		} else
			key.removeFrom(container);
		container.delete(this);
	}

}
package freenet.support;

import java.util.Enumeration;
import java.util.HashMap;
import java.util.Map;

import freenet.node.OpennetPeerNode;

/**
 * LRU Queue
 * 
 * push()'ing an existing object move it to tail, no duplicated object are ever added.
 */
public class LRUQueue<T> {

    /*
     * I've just converted this to using the DLList and Hashtable
     * this makes it Hashtable time instead of O(N) for push and
     * remove, and Hashtable time instead of O(1) for pop.  Since
     * push is by far the most done operation, this should be an
     * overall improvement.
     */
	private final DoublyLinkedListImpl<QItem<T>> list = new DoublyLinkedListImpl<QItem<T>>();
	private final Map<T, QItem<T>> hash = new HashMap<T, QItem<T>>();
    
    public LRUQueue() {
    }
    
    /**
     *       push()ing an object that is already in
     *       the queue moves that object to the most
     *       recently used position, but doesn't add
     *       a duplicate entry in the queue.
     */
	public final synchronized void push(T obj) {
		if (obj == null)
			throw new NullPointerException();

		QItem<T> insert = hash.get(obj);
        if (insert == null) {
			insert = new QItem<T>(obj);
            hash.put(obj,insert);
        } else {
            list.remove(insert);
        }

        list.unshift(insert);
    } 

    /**
     * push to bottom (least recently used position)
     */
	public synchronized void pushLeast(T obj) {
		if (obj == null)
			throw new NullPointerException();

		QItem<T> insert = hash.get(obj);
        if (insert == null) {
			insert = new QItem<T>(obj);
            hash.put(obj,insert);
        } else {
            list.remove(insert);
        }

        list.push(insert);
	}
	
    /**
     *  @return Least recently pushed Object.
     */
	public final synchronized T pop() {
        if ( list.size() > 0 ) {
			return hash.remove(list.pop().obj).obj;
        } else {
            return null;
        }
    }

    public final int size() {
        return list.size();
    }
    
    public final synchronized boolean remove(Object obj) {
		if (obj == null)
			throw new NullPointerException();

		QItem<T> i = hash.remove(obj);
	if(i != null) {
	    list.remove(i);
	    return true;
	} else {
	    return false;
	}
    }
    
    /**
     * Check if this queue contains obj
     * @param obj Object to match
     * @return true if this queue contains obj.
     */
    public final synchronized boolean contains(Object obj) {
        return hash.containsKey(obj);
    }
    
	public Enumeration<T> elements() {
        return new ItemEnumeration();
    }

	private class ItemEnumeration implements Enumeration<T> {

		private Enumeration<QItem<T>> source = list.reverseElements();
       
        public boolean hasMoreElements() {
            return source.hasMoreElements();
        }

		public T nextElement() {
			return source.nextElement().obj;
        }
    }

	private static class QItem<T> extends DoublyLinkedListImpl.Item<QItem<T>> {
		public T obj;

        public QItem(T obj) {
            this.obj = obj;
        }
    }

    /**
     * Return the objects in the queue as an array in an arbitrary and meaningless
     * order.
     */
	public synchronized Object[] toArray() {
		return hash.keySet().toArray();
	}

    /**
     * Return the objects in the queue as an array in an arbitrary and meaningless
     * order.
	 * @param array The array to fill in. If it is too small a new array of the same type will be allocated.
     */
	public synchronized <E> E[] toArray(E[] array) {
		return hash.keySet().toArray(array);
	}
	
	/**
	 * Return the objects in the queue as an array. The <strong>least</strong>
	 * recently used object is in <tt>[0]</tt>, the <strong>most</strong>
	 * recently used object is in <tt>[array.length-1]</tt>.
	 */

	public synchronized Object[] toArrayOrdered() {
		Object[] array = new Object[list.size()];
		int x = 0;
		for (Enumeration<QItem<T>> e = list.reverseElements(); e.hasMoreElements();) {
			array[x++] = e.nextElement().obj;
		}
		return array;
	}

	/**
	 * Return the objects in the queue as an array. The <strong>least</strong>
	 * recently used object is in <tt>[0]</tt>, the <strong>most</strong>
	 * recently used object is in <tt>[array.length-1]</tt>.
	 * 
	 * @param array
	 *            The array to fill in. If it is too small a new array of the
	 *            same type will be allocated.
	 */

	public synchronized <E> E[] toArrayOrdered(E[] array) {
		array = toArray(array);
		int listSize = list.size();
		if(array.length != listSize)
			throw new IllegalStateException("array.length="+array.length+" but list.size="+listSize);
		int x = 0;
		for (Enumeration<QItem<T>> e = list.reverseElements(); e.hasMoreElements();) {
			array[x++] = (E) e.nextElement().obj;
		}
		return array;
	}
	
	public synchronized boolean isEmpty() {
		return hash.isEmpty();
	}
	
	public synchronized void clear() {
		list.clear();
		hash.clear();
	}

	public synchronized T get(T obj) {
		QItem<T> val = hash.get(obj);
		if(val == null) return null;
		return val.obj;
	}
}

package freenet.node;

import freenet.support.Logger;
import freenet.support.TimeUtil;

/**
 * Tag tracking an offer reply.
 * @author Matthew Toseland <toad@amphibian.dyndns.org> (0xE43DA450)
 */
public class OfferReplyTag extends UIDTag {

	final boolean ssk;
	
	public OfferReplyTag(boolean isSSK, PeerNode source, boolean realTimeFlag, long uid, Node node) {
		super(source, realTimeFlag, uid, node);
		ssk = isSSK;
	}

	@Override
	public void logStillPresent(Long uid) {
		StringBuffer sb = new StringBuffer();
		sb.append("Still present after ").append(TimeUtil.formatTime(age()));
		sb.append(" : ssk=").append(ssk);
		Logger.error(this, sb.toString());
	}

	@Override
	public int expectedTransfersIn(boolean ignoreLocalVsRemote,
			int outwardTransfersPerInsert) {
		return 0;
	}

	@Override
	public int expectedTransfersOut(boolean ignoreLocalVsRemote,
			int outwardTransfersPerInsert) {
		return 1;
	}

	@Override
	public boolean isSSK() {
		return ssk;
	}

	@Override
	public boolean isInsert() {
		return false;
	}

	@Override
	public boolean isOfferReply() {
		return true;
	}

}
package freenet.node;

import java.io.File;
import java.io.IOException;
import java.net.URI;
import java.util.HashSet;

import org.tanukisoftware.wrapper.WrapperManager;

import com.db4o.ObjectContainer;
import com.db4o.ext.Db4oException;

import freenet.client.ArchiveManager;
import freenet.client.FECQueue;
import freenet.client.HighLevelSimpleClient;
import freenet.client.HighLevelSimpleClientImpl;
import freenet.client.InsertContext;
import freenet.client.async.BackgroundBlockEncoder;
import freenet.client.async.ClientContext;
import freenet.client.async.ClientRequestScheduler;
import freenet.client.async.ClientRequester;
import freenet.client.async.DBJob;
import freenet.client.async.DBJobRunner;
import freenet.client.async.DatabaseDisabledException;
import freenet.client.async.DatastoreChecker;
import freenet.client.async.HealingQueue;
import freenet.client.async.InsertCompressor;
import freenet.client.async.PersistentStatsPutter;
import freenet.client.async.SimpleHealingQueue;
import freenet.client.async.USKManager;
import freenet.client.events.SimpleEventProducer;
import freenet.client.filter.FilterCallback;
import freenet.client.filter.FoundURICallback;
import freenet.client.filter.GenericReadFilterCallback;
import freenet.clients.http.FProxyToadlet;
import freenet.clients.http.SimpleToadletServer;
import freenet.config.Config;
import freenet.config.InvalidConfigValueException;
import freenet.config.NodeNeedRestartException;
import freenet.config.SubConfig;
import freenet.config.WrapperConfig;
import freenet.crypt.RandomSource;
import freenet.io.xfer.AbortedException;
import freenet.io.xfer.PartiallyReceivedBlock;
import freenet.keys.CHKBlock;
import freenet.keys.CHKVerifyException;
import freenet.keys.ClientCHK;
import freenet.keys.ClientCHKBlock;
import freenet.keys.ClientKey;
import freenet.keys.ClientKeyBlock;
import freenet.keys.ClientSSK;
import freenet.keys.ClientSSKBlock;
import freenet.keys.FreenetURI;
import freenet.keys.Key;
import freenet.keys.KeyBlock;
import freenet.keys.NodeSSK;
import freenet.keys.SSKBlock;
import freenet.keys.SSKVerifyException;
import freenet.l10n.NodeL10n;
import freenet.node.NodeRestartJobsQueue.RestartDBJob;
import freenet.node.SecurityLevels.PHYSICAL_THREAT_LEVEL;
import freenet.node.fcp.FCPServer;
import freenet.node.useralerts.SimpleUserAlert;
import freenet.node.useralerts.UserAlert;
import freenet.node.useralerts.UserAlertManager;
import freenet.store.KeyCollisionException;
import freenet.support.Base64;
import freenet.support.Executor;
import freenet.support.ExecutorIdleCallback;
import freenet.support.Fields;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.MutableBoolean;
import freenet.support.OOMHandler;
import freenet.support.OOMHook;
import freenet.support.PrioritizedSerialExecutor;
import freenet.support.SimpleFieldSet;
import freenet.support.Ticker;
import freenet.support.Logger.LogLevel;
import freenet.support.SizeUtil;
import freenet.support.api.BooleanCallback;
import freenet.support.api.IntCallback;
import freenet.support.api.LongCallback;
import freenet.support.api.StringArrCallback;
import freenet.support.api.StringCallback;
import freenet.support.compress.Compressor;
import freenet.support.compress.RealCompressor;
import freenet.support.io.FileUtil;
import freenet.support.io.FilenameGenerator;
import freenet.support.io.NativeThread;
import freenet.support.io.PersistentTempBucketFactory;
import freenet.support.io.TempBucketFactory;

/**
 * The connection between the node and the client layer.
 */
public class NodeClientCore implements Persistable, DBJobRunner, OOMHook, ExecutorIdleCallback {
	private static volatile boolean logMINOR;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback() {
			@Override
			public void shouldUpdate() {
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	public final PersistentStatsPutter bandwidthStatsPutter;
	public final USKManager uskManager;
	public final ArchiveManager archiveManager;
	public final RequestStarterGroup requestStarters;
	private final HealingQueue healingQueue;
	public NodeRestartJobsQueue restartJobsQueue;
	/** Must be included as a hidden field in order for any dangerous HTTP operation to complete successfully. */
	public final String formPassword;
	final ProgramDirectory downloadsDir;
	private File[] downloadAllowedDirs;
	private boolean includeDownloadDir;
	private boolean downloadAllowedEverywhere;
	private File[] uploadAllowedDirs;
	private boolean uploadAllowedEverywhere;
	public final FilenameGenerator tempFilenameGenerator;
	public FilenameGenerator persistentFilenameGenerator;
	public final TempBucketFactory tempBucketFactory;
	public PersistentTempBucketFactory persistentTempBucketFactory;
	public final Node node;
	final NodeStats nodeStats;
	public final RandomSource random;
	final ProgramDirectory tempDir;	// Persistent temporary buckets
	final ProgramDirectory persistentTempDir;
	public FECQueue fecQueue;
	public final UserAlertManager alerts;
	final TextModeClientInterfaceServer tmci;
	TextModeClientInterface directTMCI;
	final FCPServer fcpServer;
	FProxyToadlet fproxyServlet;
	final SimpleToadletServer toadletContainer;
	public final BackgroundBlockEncoder backgroundBlockEncoder;
	public final RealCompressor compressor;
	/** If true, requests are resumed lazily i.e. startup does not block waiting for them. */
	protected final Persister persister;
	/** All client-layer database access occurs on a SerialExecutor, so that we don't need
	 * to have multiple parallel transactions. Advantages:
	 * - We never have two copies of the same object in RAM, and more broadly, we don't
	 *   need to worry about interactions between objects from different transactions.
	 * - Only one weak-reference cache for the database.
	 * - No need to refresh live objects.
	 * - Deactivation is simpler.
	 * Note that the priorities are thread priorities, not request priorities.
	 */
	public transient final PrioritizedSerialExecutor clientDatabaseExecutor;
	public final DatastoreChecker storeChecker;

	public transient final ClientContext clientContext;

	private static int maxBackgroundUSKFetchers;	// Client stuff that needs to be configged - FIXME
	static final int MAX_ARCHIVE_HANDLERS = 200; // don't take up much RAM... FIXME
	static final long MAX_CACHED_ARCHIVE_DATA = 32 * 1024 * 1024; // make a fixed fraction of the store by default? FIXME
	static final long MAX_ARCHIVED_FILE_SIZE = 1024 * 1024; // arbitrary... FIXME
	static final int MAX_CACHED_ELEMENTS = 256 * 1024; // equally arbitrary! FIXME hopefully we can cache many of these though
	/** Each FEC item can take a fair amount of RAM, since it's fully activated with all the buckets, potentially 256
	 * of them, so only cache a small number of them */
	private static final int FEC_QUEUE_CACHE_SIZE = 20;
	private UserAlert startingUpAlert;
	private RestartDBJob[] startupDatabaseJobs;
	private boolean alwaysCommit;

	NodeClientCore(Node node, Config config, SubConfig nodeConfig, SubConfig installConfig, int portNumber, int sortOrder, SimpleFieldSet oldConfig, SubConfig fproxyConfig, SimpleToadletServer toadlets, long nodeDBHandle, ObjectContainer container) throws NodeInitException {
		this.node = node;
		this.nodeStats = node.nodeStats;
		this.random = node.random;
		killedDatabase = container == null;
		if(killedDatabase)
			System.err.println("Database corrupted (before entering NodeClientCore)!");
		fecQueue = initFECQueue(node.nodeDBHandle, container, null);
		this.backgroundBlockEncoder = new BackgroundBlockEncoder();
		clientDatabaseExecutor = new PrioritizedSerialExecutor(NativeThread.NORM_PRIORITY, NativeThread.MAX_PRIORITY+1, NativeThread.NORM_PRIORITY, true, 30*1000, this, node.nodeStats);
		storeChecker = new DatastoreChecker(node);
		byte[] pwdBuf = new byte[16];
		random.nextBytes(pwdBuf);
		compressor = new RealCompressor(node.executor);
		this.formPassword = Base64.encode(pwdBuf);
		alerts = new UserAlertManager(this);
		if(container != null)
			initRestartJobs(nodeDBHandle, container);
		persister = new ConfigurablePersister(this, nodeConfig, "clientThrottleFile", "client-throttle.dat", sortOrder++, true, false,
			"NodeClientCore.fileForClientStats", "NodeClientCore.fileForClientStatsLong", node.ticker, node.getRunDir());

		SimpleFieldSet throttleFS = persister.read();
		if(logMINOR)
			Logger.minor(this, "Read throttleFS:\n" + throttleFS);

		if(logMINOR)
			Logger.minor(this, "Serializing RequestStarterGroup from:\n" + throttleFS);

		// Temp files

		this.tempDir = node.setupProgramDir(installConfig, "tempDir", node.runDir().file("temp-"+portNumber).toString(),
		  "NodeClientCore.tempDir", "NodeClientCore.tempDirLong", nodeConfig);
		FileUtil.setOwnerRWX(getTempDir());

		try {
			tempFilenameGenerator = new FilenameGenerator(random, true, getTempDir(), "temp-");
		} catch(IOException e) {
			String msg = "Could not find or create temporary directory (filename generator)";
			throw new NodeInitException(NodeInitException.EXIT_BAD_DIR, msg);
		}

		this.bandwidthStatsPutter = new PersistentStatsPutter(this.node);
		if (container != null) {
			bandwidthStatsPutter.restorePreviousData(container);
			this.getTicker().queueTimedJob(new Runnable() {
				public void run() {
					try {
						queue(bandwidthStatsPutter, NativeThread.LOW_PRIORITY, true);
						getTicker().queueTimedJob(this, "BandwidthStatsPutter", PersistentStatsPutter.OFFSET, false, true);
					} catch (DatabaseDisabledException e) {
						// Should be safe to ignore.
					}
				}
			}, PersistentStatsPutter.OFFSET);
		}

		uskManager = new USKManager(this);

		// Persistent temp files
		nodeConfig.register("encryptPersistentTempBuckets", true, sortOrder++, true, false, "NodeClientCore.encryptPersistentTempBuckets", "NodeClientCore.encryptPersistentTempBucketsLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				return (persistentTempBucketFactory == null ? true : persistentTempBucketFactory.isEncrypting());
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				if (get().equals(val) || (persistentTempBucketFactory == null))
					        return;
				persistentTempBucketFactory.setEncryption(val);
			}
		});

		this.persistentTempDir = node.setupProgramDir(installConfig, "persistentTempDir", node.userDir().file("persistent-temp-"+portNumber).toString(),
		  "NodeClientCore.persistentTempDir", "NodeClientCore.persistentTempDirLong", nodeConfig);
		initPTBF(container, nodeConfig);

		// Allocate 10% of the RAM to the RAMBucketPool by default
		int defaultRamBucketPoolSize;
		long maxMemory = Runtime.getRuntime().maxMemory();
		if(maxMemory == Long.MAX_VALUE || maxMemory <= 0)
			defaultRamBucketPoolSize = 10;
		else {
			maxMemory /= (1024 * 1024);
			if(maxMemory <= 0) // Still bogus
				defaultRamBucketPoolSize = 10;
			else {
				// 10% of memory above 64MB, with a minimum of 1MB.
				defaultRamBucketPoolSize = Math.min(Integer.MAX_VALUE, (int)((maxMemory - 64) / 10));
				if(defaultRamBucketPoolSize <= 0) defaultRamBucketPoolSize = 1;
			}
		}

		// Max bucket size 5% of the total, minimum 32KB (one block, vast majority of buckets)
		long maxBucketSize = Math.max(32768, (defaultRamBucketPoolSize * 1024 * 1024) / 20);

		nodeConfig.register("maxRAMBucketSize", SizeUtil.formatSizeWithoutSpace(maxBucketSize), sortOrder++, true, false, "NodeClientCore.maxRAMBucketSize", "NodeClientCore.maxRAMBucketSizeLong", new LongCallback() {

			@Override
			public Long get() {
				return (tempBucketFactory == null ? 0 : tempBucketFactory.getMaxRAMBucketSize());
			}

			@Override
			public void set(Long val) throws InvalidConfigValueException {
				if (get().equals(val) || (tempBucketFactory == null))
					        return;
				tempBucketFactory.setMaxRAMBucketSize(val);
			}
		}, true);

		nodeConfig.register("RAMBucketPoolSize", defaultRamBucketPoolSize+"MiB", sortOrder++, true, false, "NodeClientCore.ramBucketPoolSize", "NodeClientCore.ramBucketPoolSizeLong", new LongCallback() {

			@Override
			public Long get() {
				return (tempBucketFactory == null ? 0 : tempBucketFactory.getMaxRamUsed());
			}

			@Override
			public void set(Long val) throws InvalidConfigValueException {
				if (get().equals(val) || (tempBucketFactory == null))
					        return;
				tempBucketFactory.setMaxRamUsed(val);
			}
		}, true);

		nodeConfig.register("encryptTempBuckets", true, sortOrder++, true, false, "NodeClientCore.encryptTempBuckets", "NodeClientCore.encryptTempBucketsLong", new BooleanCallback() {

			@Override
			public Boolean get() {
				return (tempBucketFactory == null ? true : tempBucketFactory.isEncrypting());
			}

			@Override
			public void set(Boolean val) throws InvalidConfigValueException {
				if (get().equals(val) || (tempBucketFactory == null))
					        return;
				tempBucketFactory.setEncryption(val);
			}
		});
		tempBucketFactory = new TempBucketFactory(node.executor, tempFilenameGenerator, nodeConfig.getLong("maxRAMBucketSize"), nodeConfig.getLong("RAMBucketPoolSize"), random, node.fastWeakRandom, nodeConfig.getBoolean("encryptTempBuckets"));

		archiveManager = new ArchiveManager(MAX_ARCHIVE_HANDLERS, MAX_CACHED_ARCHIVE_DATA, MAX_ARCHIVED_FILE_SIZE, MAX_CACHED_ELEMENTS, tempBucketFactory);

		healingQueue = new SimpleHealingQueue(
				new InsertContext(
						0, 2, 0, 0, new SimpleEventProducer(),
						false, Node.FORK_ON_CACHEABLE_DEFAULT, false, Compressor.DEFAULT_COMPRESSORDESCRIPTOR, 0, 0, InsertContext.CompatibilityMode.COMPAT_CURRENT), RequestStarter.PREFETCH_PRIORITY_CLASS, 512 /* FIXME make configurable */);

		clientContext = new ClientContext(node.bootID, nodeDBHandle, this, fecQueue, node.executor, backgroundBlockEncoder, archiveManager, persistentTempBucketFactory, tempBucketFactory, persistentTempBucketFactory, healingQueue, uskManager, random, node.fastWeakRandom, node.getTicker(), tempFilenameGenerator, persistentFilenameGenerator, compressor, storeChecker);
		compressor.setClientContext(clientContext);
		storeChecker.setContext(clientContext);

		try {
			requestStarters = new RequestStarterGroup(node, this, portNumber, random, config, throttleFS, clientContext, nodeDBHandle, container);
		} catch (InvalidConfigValueException e1) {
			throw new NodeInitException(NodeInitException.EXIT_BAD_CONFIG, e1.toString());
		}
		clientContext.init(requestStarters, alerts);
		initKeys(container);

		node.securityLevels.addPhysicalThreatLevelListener(new SecurityLevelListener<PHYSICAL_THREAT_LEVEL>() {

			public void onChange(PHYSICAL_THREAT_LEVEL oldLevel, PHYSICAL_THREAT_LEVEL newLevel) {
				if(newLevel == PHYSICAL_THREAT_LEVEL.LOW) {
					if(tempBucketFactory.isEncrypting()) {
						tempBucketFactory.setEncryption(false);
					}
					if(persistentTempBucketFactory != null) {
					if(persistentTempBucketFactory.isEncrypting()) {
						persistentTempBucketFactory.setEncryption(false);
					}
					}
				} else { // newLevel >= PHYSICAL_THREAT_LEVEL.NORMAL
					if(!tempBucketFactory.isEncrypting()) {
						tempBucketFactory.setEncryption(true);
					}
					if(persistentTempBucketFactory != null) {
					if(!persistentTempBucketFactory.isEncrypting()) {
						persistentTempBucketFactory.setEncryption(true);
					}
					}
				}
			}

		});

		// Downloads directory

		this.downloadsDir = node.setupProgramDir(nodeConfig, "downloadsDir", node.userDir().file("downloads").getPath(),
		  "NodeClientCore.downloadsDir", "NodeClientCore.downloadsDirLong", l10n("couldNotFindOrCreateDir"), (SubConfig)null);

		// Downloads allowed, uploads allowed

		nodeConfig.register("downloadAllowedDirs", new String[]{"all"}, sortOrder++, true, true, "NodeClientCore.downloadAllowedDirs",
			"NodeClientCore.downloadAllowedDirsLong",
			new StringArrCallback() {

				@Override
				public String[] get() {
					synchronized(NodeClientCore.this) {
						if(downloadAllowedEverywhere)
							return new String[]{"all"};
						String[] dirs = new String[downloadAllowedDirs.length + (includeDownloadDir ? 1 : 0)];
						for(int i = 0; i < downloadAllowedDirs.length; i++)
							dirs[i] = downloadAllowedDirs[i].getPath();
						if(includeDownloadDir)
							dirs[downloadAllowedDirs.length] = "downloads";
						return dirs;
					}
				}

				@Override
				public void set(String[] val) throws InvalidConfigValueException {
					setDownloadAllowedDirs(val);
				}
			});
		setDownloadAllowedDirs(nodeConfig.getStringArr("downloadAllowedDirs"));

		nodeConfig.register("uploadAllowedDirs", new String[]{"all"}, sortOrder++, true, true, "NodeClientCore.uploadAllowedDirs",
			"NodeClientCore.uploadAllowedDirsLong",
			new StringArrCallback() {

				@Override
				public String[] get() {
					synchronized(NodeClientCore.this) {
						if(uploadAllowedEverywhere)
							return new String[]{"all"};
						String[] dirs = new String[uploadAllowedDirs.length];
						for(int i = 0; i < uploadAllowedDirs.length; i++)
							dirs[i] = uploadAllowedDirs[i].getPath();
						return dirs;
					}
				}

				@Override
				public void set(String[] val) throws InvalidConfigValueException {
					setUploadAllowedDirs(val);
				}
			});
		setUploadAllowedDirs(nodeConfig.getStringArr("uploadAllowedDirs"));

		Logger.normal(this, "Initializing USK Manager");
		System.out.println("Initializing USK Manager");
		uskManager.init(clientContext);
		initUSK(container);

		nodeConfig.register("maxBackgroundUSKFetchers", "64", sortOrder++, true, false, "NodeClientCore.maxUSKFetchers",
			"NodeClientCore.maxUSKFetchersLong", new IntCallback() {

			@Override
			public Integer get() {
				return maxBackgroundUSKFetchers;
			}

			@Override
			public void set(Integer uskFetch) throws InvalidConfigValueException {
				if(uskFetch <= 0)
					throw new InvalidConfigValueException(l10n("maxUSKFetchersMustBeGreaterThanZero"));
				maxBackgroundUSKFetchers = uskFetch;
			}
		}, false);

		maxBackgroundUSKFetchers = nodeConfig.getInt("maxBackgroundUSKFetchers");


		// This is all part of construction, not of start().
		// Some plugins depend on it, so it needs to be *created* before they are started.

		// TMCI
		try {
			tmci = TextModeClientInterfaceServer.maybeCreate(node, this, config);
		} catch(IOException e) {
			e.printStackTrace();
			throw new NodeInitException(NodeInitException.EXIT_COULD_NOT_START_TMCI, "Could not start TMCI: " + e);
		}

		// FCP (including persistent requests so needs to start before FProxy)
		try {
			fcpServer = FCPServer.maybeCreate(node, this, node.config, container);
			clientContext.setDownloadCache(fcpServer);
			if(!killedDatabase)
				fcpServer.load(container);
		} catch(IOException e) {
			throw new NodeInitException(NodeInitException.EXIT_COULD_NOT_START_FCP, "Could not start FCP: " + e);
		} catch(InvalidConfigValueException e) {
			throw new NodeInitException(NodeInitException.EXIT_COULD_NOT_START_FCP, "Could not start FCP: " + e);
		}

		// FProxy
		// FIXME this is a hack, the real way to do this is plugins
		this.alerts.register(startingUpAlert = new SimpleUserAlert(true, l10n("startingUpTitle"), l10n("startingUp"), l10n("startingUpShort"), UserAlert.ERROR));
		this.alerts.register(new SimpleUserAlert(true, NodeL10n.getBase().getString("QueueToadlet.persistenceBrokenTitle"),
				NodeL10n.getBase().getString("QueueToadlet.persistenceBroken",
						new String[]{ "TEMPDIR", "DBFILE" },
						new String[]{ new File(FileUtil.getCanonicalFile(getPersistentTempDir()), File.separator).toString(), new File(FileUtil.getCanonicalFile(node.getUserDir()), "node.db4o").toString() }
				), NodeL10n.getBase().getString("QueueToadlet.persistenceBrokenShortAlert"), UserAlert.CRITICAL_ERROR)
				{
			public boolean isValid() {
				synchronized(NodeClientCore.this) {
					if(!killedDatabase) return false;
				}
				if(NodeClientCore.this.node.awaitingPassword()) return false;
				if(NodeClientCore.this.node.isStopping()) return false;
				return true;
			}

			public boolean userCanDismiss() {
				return false;
			}

		});
		toadletContainer = toadlets;
		toadletContainer.setCore(this);
		toadletContainer.setBucketFactory(tempBucketFactory);
		if(fecQueue == null) throw new NullPointerException();
		fecQueue.init(RequestStarter.NUMBER_OF_PRIORITY_CLASSES, FEC_QUEUE_CACHE_SIZE, clientContext.jobRunner, node.executor, clientContext);
		OOMHandler.addOOMHook(this);
		if(killedDatabase)
			System.err.println("Database corrupted (leaving NodeClientCore)!");

		nodeConfig.register("alwaysCommit", false, sortOrder++, true, false, "NodeClientCore.alwaysCommit", "NodeClientCore.alwaysCommitLong",
				new BooleanCallback() {

					@Override
					public Boolean get() {
						return alwaysCommit;
					}

					@Override
					public void set(Boolean val) throws InvalidConfigValueException, NodeNeedRestartException {
						alwaysCommit = val;
					}

		});
		alwaysCommit = nodeConfig.getBoolean("alwaysCommit");
	}

	private void initUSK(ObjectContainer container) {
		if(!killedDatabase) {
			try {
				uskManager.init(container);
			} catch (Db4oException e) {
				killedDatabase = true;
			}
		}
	}

	private void initKeys(ObjectContainer container) {
		if(!killedDatabase) {
			try {
				ClientRequestScheduler.loadKeyListeners(container, clientContext);
			} catch (Db4oException e) {
				killedDatabase = true;
			}
		}
		if(!killedDatabase) {
			try {
				InsertCompressor.load(container, clientContext);
			} catch (Db4oException e) {
				killedDatabase = true;
			}
		}
		// FIXME get rid of this.
		if(container != null) {
			container.commit();
			ClientRequester.checkAll(container, clientContext);
		}
	}

	private void initPTBF(ObjectContainer container, SubConfig nodeConfig) throws NodeInitException {
		PersistentTempBucketFactory ptbf = null;
		FilenameGenerator pfg = null;
		try {
			String prefix = "freenet-temp-";
			if(!killedDatabase) {
				ptbf = PersistentTempBucketFactory.load(getPersistentTempDir(), prefix, random, node.fastWeakRandom, container, node.nodeDBHandle, nodeConfig.getBoolean("encryptPersistentTempBuckets"), this, node.getTicker());
				ptbf.init(getPersistentTempDir(), prefix, random, node.fastWeakRandom);
				pfg = ptbf.fg;
			}
		} catch(IOException e2) {
			String msg = "Could not find or create persistent temporary directory: "+e2;
			e2.printStackTrace();
			throw new NodeInitException(NodeInitException.EXIT_BAD_DIR, msg);
		} catch (Db4oException e) {
			killedDatabase = true;
		} catch (Throwable t) {
			// Let the rest of the node start up but kill the database
			System.err.println("Failed to load persistent temporary buckets factory: "+t);
			t.printStackTrace();
			killedDatabase = true;
		}
		if(killedDatabase) {
			persistentTempBucketFactory = null;
			persistentFilenameGenerator = null;
		} else {
			persistentTempBucketFactory = ptbf;
			persistentFilenameGenerator = pfg;
			if(clientContext != null) {
				clientContext.setPersistentBucketFactory(persistentTempBucketFactory, persistentFilenameGenerator);
			}
		}
	}

	private void initRestartJobs(long nodeDBHandle, ObjectContainer container) {
		// Restart jobs are handled directly by NodeClientCore, so no need to deal with ClientContext.
		NodeRestartJobsQueue rq = null;
		try {
			if(!killedDatabase) rq = container == null ? null : NodeRestartJobsQueue.init(node.nodeDBHandle, container);
		} catch (Db4oException e) {
			killedDatabase = true;
		}
		restartJobsQueue = rq;
		RestartDBJob[] startupJobs = null;
		try {
			if(!killedDatabase)
				startupJobs = restartJobsQueue.getEarlyRestartDatabaseJobs(container);
		} catch (Db4oException e) {
			killedDatabase = true;
		}
		startupDatabaseJobs = startupJobs;
		if(startupDatabaseJobs != null &&
				startupDatabaseJobs.length > 0) {
			try {
				queue(startupJobRunner, NativeThread.HIGH_PRIORITY, false);
			} catch (DatabaseDisabledException e1) {
				// Impossible
			}
		}
		if(!killedDatabase) {
			try {
				restartJobsQueue.addLateRestartDatabaseJobs(this, container);
			} catch (Db4oException e) {
				killedDatabase = true;
			} catch (DatabaseDisabledException e) {
				// addLateRestartDatabaseJobs only modifies the database in case of a job being deleted without being removed.
				// So it is safe to just ignore this.
			}
		}

	}

	private FECQueue initFECQueue(long nodeDBHandle, ObjectContainer container, FECQueue oldQueue) {
		FECQueue q;
		try {
			oldFECQueue = oldQueue;
			if(!killedDatabase) q = FECQueue.create(node.nodeDBHandle, container, oldQueue);
			else q = new FECQueue(node.nodeDBHandle);
		} catch (Db4oException e) {
			killedDatabase = true;
			q = new FECQueue(node.nodeDBHandle);
		}
		return q;
	}

	private FECQueue oldFECQueue;

	boolean lateInitDatabase(long nodeDBHandle, ObjectContainer container) throws NodeInitException {
		System.out.println("Late database initialisation: starting middle phase");
		synchronized(this) {
			killedDatabase = false;
		}
		// Don't actually start the database thread yet, messy concurrency issues.
		lateInitFECQueue(nodeDBHandle, container);
		initRestartJobs(nodeDBHandle, container);
		initPTBF(container, node.config.get("node"));
		requestStarters.lateStart(this, nodeDBHandle, container);
		// Must create the CRSCore's before telling them to load stuff.
		initKeys(container);
		if(!killedDatabase)
			fcpServer.load(container);
		synchronized(this) {
			if(killedDatabase) {
				startupDatabaseJobs = null;
				fecQueue = oldFECQueue;
				clientContext.setFECQueue(oldFECQueue);
				persistentTempBucketFactory = null;
				persistentFilenameGenerator = null;
				clientContext.setPersistentBucketFactory(null, null);
				return false;
			}
		}

		bandwidthStatsPutter.restorePreviousData(container);
		this.getTicker().queueTimedJob(new Runnable() {
			public void run() {
				try {
					queue(bandwidthStatsPutter, NativeThread.LOW_PRIORITY, false);
					getTicker().queueTimedJob(this, "BandwidthStatsPutter", PersistentStatsPutter.OFFSET, false, true);
				} catch (DatabaseDisabledException e) {
					// Should be safe to ignore.
				}
			}
		}, PersistentStatsPutter.OFFSET);

		// CONCURRENCY: We need everything to have hit its various memory locations.
		// How to ensure this?
		// FIXME This is a hack!!
		// I guess the standard solution would be to make ClientContext members volatile etc?
		// That sucks though ... they are only changed ONCE, and they are used constantly.
		// Also existing transient requests won't care about the changes; what we must guarantee
		// is that new persistent jobs will be accepted.
		node.getTicker().queueTimedJob(new Runnable() {

			public void run() {
				clientDatabaseExecutor.start(node.executor, "Client database access thread");
			}

		}, 1000);
		System.out.println("Late database initialisation completed.");
		return true;
	}

	private void lateInitFECQueue(long nodeDBHandle, ObjectContainer container) {
		fecQueue = initFECQueue(nodeDBHandle, container, fecQueue);
		clientContext.setFECQueue(fecQueue);
	}

	private static String l10n(String key) {
		return NodeL10n.getBase().getString("NodeClientCore." + key);
	}

	protected synchronized void setDownloadAllowedDirs(String[] val) {
		int x = 0;
		downloadAllowedEverywhere = false;
		includeDownloadDir = false;
		int i = 0;
		downloadAllowedDirs = new File[val.length];
		for(i = 0; i < downloadAllowedDirs.length; i++) {
			String s = val[i];
			if(s.equals("downloads"))
				includeDownloadDir = true;
			else if(s.equals("all"))
				downloadAllowedEverywhere = true;
			else
				downloadAllowedDirs[x++] = new File(val[i]);
		}
		if(x != i) {
			File[] newDirs = new File[x];
			System.arraycopy(downloadAllowedDirs, 0, newDirs, 0, x);
			downloadAllowedDirs = newDirs;
		}
	}

	protected synchronized void setUploadAllowedDirs(String[] val) {
		int x = 0;
		int i = 0;
		uploadAllowedEverywhere = false;
		uploadAllowedDirs = new File[val.length];
		for(i = 0; i < uploadAllowedDirs.length; i++) {
			String s = val[i];
			if(s.equals("all"))
				uploadAllowedEverywhere = true;
			else
				uploadAllowedDirs[x++] = new File(val[i]);
		}
		if(x != i) {
			File[] newDirs = new File[x];
			System.arraycopy(uploadAllowedDirs, 0, newDirs, 0, x);
			uploadAllowedDirs = newDirs;
		}
	}

	public void start(Config config) throws NodeInitException {
		backgroundBlockEncoder.setContext(clientContext);
		node.executor.execute(backgroundBlockEncoder, "Background block encoder");
		try {
			clientContext.jobRunner.queue(new DBJob() {

				public String toString() {
					return "Init ArchiveManager";
				}

				public boolean run(ObjectContainer container, ClientContext context) {
					ArchiveManager.init(container, context, context.nodeDBHandle);
					return false;
				}

			}, NativeThread.MAX_PRIORITY, false);
		} catch (DatabaseDisabledException e) {
			// Safe to ignore
		}

		persister.start();

		requestStarters.start();

		storeChecker.start(node.executor, "Datastore checker");
		if(fcpServer != null)
			fcpServer.maybeStart();
		if(tmci != null)
			tmci.start();
		backgroundBlockEncoder.runPersistentQueue(clientContext);
		node.executor.execute(compressor, "Compression scheduler");

		node.executor.execute(new PrioRunnable() {

			public void run() {
				Logger.normal(this, "Resuming persistent requests");
				if(persistentTempBucketFactory != null)
					persistentTempBucketFactory.completedInit();
				node.pluginManager.start(node.config);
				node.ipDetector.ipDetectorManager.start();
				// FIXME most of the work is done after this point on splitfile starter threads.
				// So do we want to make a fuss?
				// FIXME but a better solution is real request resuming.
				Logger.normal(this, "Completed startup: All persistent requests resumed or restarted");
				alerts.unregister(startingUpAlert);
			}

			public int getPriority() {
				return NativeThread.LOW_PRIORITY;
			}
		}, "Startup completion thread");

		if(!killedDatabase)
			clientDatabaseExecutor.start(node.executor, "Client database access thread");
	}

	private int startupDatabaseJobsDone = 0;

	private DBJob startupJobRunner = new DBJob() {

		public String toString() {
			return "Run startup jobs";
		}

		public boolean run(ObjectContainer container, ClientContext context) {
			RestartDBJob job = startupDatabaseJobs[startupDatabaseJobsDone];
			try {
				container.activate(job.job, 1);
				// Remove before execution, to allow it to re-add itself if it wants to
				System.err.println("Cleaning up after restart: "+job.job);
				restartJobsQueue.removeRestartJob(job.job, job.prio, container);
				job.job.run(container, context);
				container.commit();
			} catch (Throwable t) {
				Logger.error(this, "Caught "+t+" in startup job "+job, t);
				// Try again next time
				restartJobsQueue.queueRestartJob(job.job, job.prio, container, true);
			}
			startupDatabaseJobsDone++;
			if(startupDatabaseJobsDone == startupDatabaseJobs.length)
				startupDatabaseJobs = null;
			else
				try {
					context.jobRunner.queue(startupJobRunner, NativeThread.HIGH_PRIORITY, false);
				} catch (DatabaseDisabledException e) {
					// Do nothing
				}
			return true;
		}

	};

	public interface SimpleRequestSenderCompletionListener {

		public void completed(boolean success);
	}

	/** UID -1 is used internally, so never generate it.
	 * It is not however a problem if a node does use it; it will slow its messages down
	 * by them being round-robin'ed in PeerMessageQueue with messages with no UID, that's
	 * all. */
	long makeUID() {
		while(true) {
			long uid = random.nextLong();
			if(uid != -1) return uid;
		}
	}

	public void asyncGet(Key key, boolean offersOnly, final SimpleRequestSenderCompletionListener listener, boolean canReadClientCache, boolean canWriteClientCache, final boolean realTimeFlag) {
		final long uid = makeUID();
		final boolean isSSK = key instanceof NodeSSK;
		final RequestTag tag = new RequestTag(isSSK, RequestTag.START.ASYNC_GET, null, realTimeFlag, uid, node);
		if(!node.lockUID(uid, isSSK, false, false, true, realTimeFlag, tag)) {
			Logger.error(this, "Could not lock UID just randomly generated: " + uid + " - probably indicates broken PRNG");
			return;
		}
		short htl = node.maxHTL();
		// If another node requested it within the ULPR period at a lower HTL, that may allow
		// us to cache it in the datastore. Find the lowest HTL fetching the key in that period,
		// and use that for purposes of deciding whether to cache it in the store.
		if(offersOnly) {
			htl = node.failureTable.minOfferedHTL(key, htl);
			if(logMINOR) Logger.minor(this, "Using old HTL for GetOfferedKey: "+htl);
		}
		asyncGet(key, isSSK, offersOnly, uid, new RequestSender.Listener() {

			public void onCHKTransferBegins() {
				// Ignore
			}

			public void onReceivedRejectOverload() {
				// Ignore
			}

			/** The RequestSender finished.
			 * @param status The completion status.
			 * @param uidTransferred If this is set, the RequestSender has taken on
			 * responsibility for unlocking the UID specified. We should not unlock it.
			 */
			public void onRequestSenderFinished(int status, boolean fromOfferedKey) {
				tag.unlockHandler();
				if(listener != null)
					listener.completed(status == RequestSender.SUCCESS);
			}

			public void onAbortDownstreamTransfers(int reason, String desc) {
				// Ignore, onRequestSenderFinished will also be called.
			}
		}, tag, canReadClientCache, canWriteClientCache, htl, realTimeFlag);
	}

	/**
	 * Start an asynchronous fetch of the key in question, which will complete to the datastore.
	 * It will not decode the data because we don't provide a ClientKey. It will not return
	 * anything and will run asynchronously. Caller is responsible for unlocking the UID.
	 * @param key
	 */
	void asyncGet(Key key, boolean isSSK, boolean offersOnly, long uid, RequestSender.Listener listener, RequestTag tag, boolean canReadClientCache, boolean canWriteClientCache, short htl, boolean realTimeFlag) {
		try {
			Object o = node.makeRequestSender(key, htl, uid, tag, null, false, false, offersOnly, canReadClientCache, canWriteClientCache, realTimeFlag);
			if(o instanceof KeyBlock) {
				tag.servedFromDatastore = true;
				tag.unlockHandler();
				return; // Already have it.
			}
			RequestSender rs = (RequestSender) o;
			rs.addListener(listener);
			if(rs.uid != uid)
				tag.unlockHandler();
			// Else it has started a request.
			if(logMINOR)
				Logger.minor(this, "Started " + o + " for " + uid + " for " + key);
		} catch(RuntimeException e) {
			Logger.error(this, "Caught error trying to start request: " + e, e);
			tag.unlockHandler();
		} catch(Error e) {
			Logger.error(this, "Caught error trying to start request: " + e, e);
			tag.unlockHandler();
		}
	}

	public ClientKeyBlock realGetKey(ClientKey key, boolean localOnly, boolean ignoreStore, boolean canWriteClientCache, boolean realTimeFlag) throws LowLevelGetException {
		if(key instanceof ClientCHK)
			return realGetCHK((ClientCHK) key, localOnly, ignoreStore, canWriteClientCache, realTimeFlag);
		else if(key instanceof ClientSSK)
			return realGetSSK((ClientSSK) key, localOnly, ignoreStore, canWriteClientCache, realTimeFlag);
		else
			throw new IllegalArgumentException("Not a CHK or SSK: " + key);
	}

	/**
	 * Fetch a CHK.
	 * @param key
	 * @param localOnly
	 * @param cache
	 * @param ignoreStore
	 * @param canWriteClientCache Can we write to the client cache? This is a local request, so
	 * we can always read from it, but some clients will want to override to avoid polluting it.
	 * @return The fetched block.
	 * @throws LowLevelGetException
	 */
	ClientCHKBlock realGetCHK(ClientCHK key, boolean localOnly, boolean ignoreStore, boolean canWriteClientCache, boolean realTimeFlag) throws LowLevelGetException {
		long startTime = System.currentTimeMillis();
		long uid = makeUID();
		RequestTag tag = new RequestTag(false, RequestTag.START.LOCAL, null, realTimeFlag, uid, node);
		if(!node.lockUID(uid, false, false, false, true, realTimeFlag, tag)) {
			Logger.error(this, "Could not lock UID just randomly generated: " + uid + " - probably indicates broken PRNG");
			throw new LowLevelGetException(LowLevelGetException.INTERNAL_ERROR);
		}
		RequestSender rs = null;
		try {
			Object o = node.makeRequestSender(key.getNodeCHK(), node.maxHTL(), uid, tag, null, localOnly, ignoreStore, false, true, canWriteClientCache, realTimeFlag);
			if(o instanceof CHKBlock)
				try {
					tag.setServedFromDatastore();
					return new ClientCHKBlock((CHKBlock) o, key);
				} catch(CHKVerifyException e) {
					Logger.error(this, "Does not verify: " + e, e);
					throw new LowLevelGetException(LowLevelGetException.DECODE_FAILED);
				}
			if(o == null)
				throw new LowLevelGetException(LowLevelGetException.DATA_NOT_FOUND_IN_STORE);
			rs = (RequestSender) o;
			boolean rejectedOverload = false;
			short waitStatus = 0;
			while(true) {
				waitStatus = rs.waitUntilStatusChange(waitStatus);
				if((!rejectedOverload) && (waitStatus & RequestSender.WAIT_REJECTED_OVERLOAD) != 0) {
					// See below; inserts count both
					requestStarters.rejectedOverload(false, false, realTimeFlag);
					rejectedOverload = true;
				}

				int status = rs.getStatus();

				if(rs.abortedDownstreamTransfers())
					status = RequestSender.TRANSFER_FAILED;

				if(status == RequestSender.NOT_FINISHED)
					continue;

				if(status != RequestSender.TIMED_OUT && status != RequestSender.GENERATED_REJECTED_OVERLOAD && status != RequestSender.INTERNAL_ERROR) {
					if(logMINOR)
						Logger.minor(this, "CHK fetch cost " + rs.getTotalSentBytes() + '/' + rs.getTotalReceivedBytes() + " bytes (" + status + ')');
					nodeStats.localChkFetchBytesSentAverage.report(rs.getTotalSentBytes());
					nodeStats.localChkFetchBytesReceivedAverage.report(rs.getTotalReceivedBytes());
					if(status == RequestSender.SUCCESS)
						// See comments above declaration of successful* : We don't report sent bytes here.
						//nodeStats.successfulChkFetchBytesSentAverage.report(rs.getTotalSentBytes());
						nodeStats.successfulChkFetchBytesReceivedAverage.report(rs.getTotalReceivedBytes());
				}

				if((status == RequestSender.TIMED_OUT) ||
					(status == RequestSender.GENERATED_REJECTED_OVERLOAD)) {
					if(!rejectedOverload) {
						// See below
						requestStarters.rejectedOverload(false, false, realTimeFlag);
						rejectedOverload = true;
						long rtt = System.currentTimeMillis() - startTime;
						double targetLocation=key.getNodeCHK().toNormalizedDouble();
						node.nodeStats.reportCHKOutcome(rtt, false, targetLocation, realTimeFlag);
					}
				} else
					if(rs.hasForwarded() &&
						((status == RequestSender.DATA_NOT_FOUND) ||
						(status == RequestSender.RECENTLY_FAILED) ||
						(status == RequestSender.SUCCESS) ||
						(status == RequestSender.ROUTE_NOT_FOUND) ||
						(status == RequestSender.VERIFY_FAILURE) ||
						(status == RequestSender.GET_OFFER_VERIFY_FAILURE))) {
						long rtt = System.currentTimeMillis() - startTime;
						double targetLocation=key.getNodeCHK().toNormalizedDouble();
						if(!rejectedOverload)
							requestStarters.requestCompleted(false, false, key.getNodeKey(true), realTimeFlag);
						// Count towards RTT even if got a RejectedOverload - but not if timed out.
						requestStarters.getThrottle(false, false, realTimeFlag).successfulCompletion(rtt);
						node.nodeStats.reportCHKOutcome(rtt, status == RequestSender.SUCCESS, targetLocation, realTimeFlag);
						if(status == RequestSender.SUCCESS) {
							Logger.minor(this, "Successful CHK fetch took "+rtt);
						}
					}

				if(status == RequestSender.SUCCESS)
					try {
						return new ClientCHKBlock(rs.getPRB().getBlock(), rs.getHeaders(), key, true);
					} catch(CHKVerifyException e) {
						Logger.error(this, "Does not verify: " + e, e);
						throw new LowLevelGetException(LowLevelGetException.DECODE_FAILED);
					} catch(AbortedException e) {
						Logger.error(this, "Impossible: " + e, e);
						throw new LowLevelGetException(LowLevelGetException.INTERNAL_ERROR);
					}
				else {
					switch(status) {
						case RequestSender.NOT_FINISHED:
							Logger.error(this, "RS still running in getCHK!: " + rs);
							throw new LowLevelGetException(LowLevelGetException.INTERNAL_ERROR);
						case RequestSender.DATA_NOT_FOUND:
							throw new LowLevelGetException(LowLevelGetException.DATA_NOT_FOUND);
						case RequestSender.RECENTLY_FAILED:
							throw new LowLevelGetException(LowLevelGetException.RECENTLY_FAILED);
						case RequestSender.ROUTE_NOT_FOUND:
							throw new LowLevelGetException(LowLevelGetException.ROUTE_NOT_FOUND);
						case RequestSender.TRANSFER_FAILED:
						case RequestSender.GET_OFFER_TRANSFER_FAILED:
							throw new LowLevelGetException(LowLevelGetException.TRANSFER_FAILED);
						case RequestSender.VERIFY_FAILURE:
						case RequestSender.GET_OFFER_VERIFY_FAILURE:
							throw new LowLevelGetException(LowLevelGetException.VERIFY_FAILED);
						case RequestSender.GENERATED_REJECTED_OVERLOAD:
						case RequestSender.TIMED_OUT:
							throw new LowLevelGetException(LowLevelGetException.REJECTED_OVERLOAD);
						case RequestSender.INTERNAL_ERROR:
							throw new LowLevelGetException(LowLevelGetException.INTERNAL_ERROR);
						default:
							Logger.error(this, "Unknown RequestSender code in getCHK: " + status + " on " + rs);
							throw new LowLevelGetException(LowLevelGetException.INTERNAL_ERROR);
					}
				}
			}
		} finally {
			tag.unlockHandler();
		}
	}

	ClientSSKBlock realGetSSK(ClientSSK key, boolean localOnly, boolean ignoreStore, boolean canWriteClientCache, boolean realTimeFlag) throws LowLevelGetException {
		long startTime = System.currentTimeMillis();
		long uid = makeUID();
		RequestTag tag = new RequestTag(true, RequestTag.START.LOCAL, null, realTimeFlag, uid, node);
		if(!node.lockUID(uid, true, false, false, true, realTimeFlag, tag)) {
			Logger.error(this, "Could not lock UID just randomly generated: " + uid + " - probably indicates broken PRNG");
			throw new LowLevelGetException(LowLevelGetException.INTERNAL_ERROR);
		}
		RequestSender rs = null;
		try {
			Object o = node.makeRequestSender(key.getNodeKey(true), node.maxHTL(), uid, tag, null, localOnly, ignoreStore, false, true, canWriteClientCache, realTimeFlag);
			if(o instanceof SSKBlock)
				try {
					tag.setServedFromDatastore();
					SSKBlock block = (SSKBlock) o;
					key.setPublicKey(block.getPubKey());
					return ClientSSKBlock.construct(block, key);
				} catch(SSKVerifyException e) {
					Logger.error(this, "Does not verify: " + e, e);
					throw new LowLevelGetException(LowLevelGetException.DECODE_FAILED);
				}
			if(o == null)
				throw new LowLevelGetException(LowLevelGetException.DATA_NOT_FOUND_IN_STORE);
			rs = (RequestSender) o;
			boolean rejectedOverload = false;
			short waitStatus = 0;
			while(true) {
				waitStatus = rs.waitUntilStatusChange(waitStatus);
				if((!rejectedOverload) && (waitStatus & RequestSender.WAIT_REJECTED_OVERLOAD) != 0) {
					requestStarters.rejectedOverload(true, false, realTimeFlag);
					rejectedOverload = true;
				}

				int status = rs.getStatus();

				if(status == RequestSender.NOT_FINISHED)
					continue;

				if(status != RequestSender.TIMED_OUT && status != RequestSender.GENERATED_REJECTED_OVERLOAD && status != RequestSender.INTERNAL_ERROR) {
					if(logMINOR)
						Logger.minor(this, "SSK fetch cost " + rs.getTotalSentBytes() + '/' + rs.getTotalReceivedBytes() + " bytes (" + status + ')');
					nodeStats.localSskFetchBytesSentAverage.report(rs.getTotalSentBytes());
					nodeStats.localSskFetchBytesReceivedAverage.report(rs.getTotalReceivedBytes());
					if(status == RequestSender.SUCCESS)
						// See comments above successfulSskFetchBytesSentAverage : we don't relay the data, so
						// reporting the sent bytes would be inaccurate.
						//nodeStats.successfulSskFetchBytesSentAverage.report(rs.getTotalSentBytes());
						nodeStats.successfulSskFetchBytesReceivedAverage.report(rs.getTotalReceivedBytes());
				}

				long rtt = System.currentTimeMillis() - startTime;
				if((status == RequestSender.TIMED_OUT) ||
					(status == RequestSender.GENERATED_REJECTED_OVERLOAD)) {
					if(!rejectedOverload) {
						requestStarters.rejectedOverload(true, false, realTimeFlag);
						rejectedOverload = true;
					}
					node.nodeStats.reportSSKOutcome(rtt, false, realTimeFlag);
				} else
					if(rs.hasForwarded() &&
						((status == RequestSender.DATA_NOT_FOUND) ||
						(status == RequestSender.RECENTLY_FAILED) ||
						(status == RequestSender.SUCCESS) ||
						(status == RequestSender.ROUTE_NOT_FOUND) ||
						(status == RequestSender.VERIFY_FAILURE) ||
						(status == RequestSender.GET_OFFER_VERIFY_FAILURE))) {

						if(!rejectedOverload)
							requestStarters.requestCompleted(true, false, key.getNodeKey(true), realTimeFlag);
						// Count towards RTT even if got a RejectedOverload - but not if timed out.
						requestStarters.getThrottle(true, false, realTimeFlag).successfulCompletion(rtt);
						node.nodeStats.reportSSKOutcome(rtt, status == RequestSender.SUCCESS, realTimeFlag);
					}

				if(rs.getStatus() == RequestSender.SUCCESS)
					try {
						SSKBlock block = rs.getSSKBlock();
						key.setPublicKey(block.getPubKey());
						return ClientSSKBlock.construct(block, key);
					} catch(SSKVerifyException e) {
						Logger.error(this, "Does not verify: " + e, e);
						throw new LowLevelGetException(LowLevelGetException.DECODE_FAILED);
					}
				else
					switch(rs.getStatus()) {
						case RequestSender.NOT_FINISHED:
							Logger.error(this, "RS still running in getCHK!: " + rs);
							throw new LowLevelGetException(LowLevelGetException.INTERNAL_ERROR);
						case RequestSender.DATA_NOT_FOUND:
							throw new LowLevelGetException(LowLevelGetException.DATA_NOT_FOUND);
						case RequestSender.RECENTLY_FAILED:
							throw new LowLevelGetException(LowLevelGetException.RECENTLY_FAILED);
						case RequestSender.ROUTE_NOT_FOUND:
							throw new LowLevelGetException(LowLevelGetException.ROUTE_NOT_FOUND);
						case RequestSender.TRANSFER_FAILED:
						case RequestSender.GET_OFFER_TRANSFER_FAILED:
							Logger.error(this, "WTF? Transfer failed on an SSK? on " + uid);
							throw new LowLevelGetException(LowLevelGetException.TRANSFER_FAILED);
						case RequestSender.VERIFY_FAILURE:
						case RequestSender.GET_OFFER_VERIFY_FAILURE:
							throw new LowLevelGetException(LowLevelGetException.VERIFY_FAILED);
						case RequestSender.GENERATED_REJECTED_OVERLOAD:
						case RequestSender.TIMED_OUT:
							throw new LowLevelGetException(LowLevelGetException.REJECTED_OVERLOAD);
						case RequestSender.INTERNAL_ERROR:
						default:
							Logger.error(this, "Unknown RequestSender code in getCHK: " + rs.getStatus() + " on " + rs);
							throw new LowLevelGetException(LowLevelGetException.INTERNAL_ERROR);
					}
			}
		} finally {
			tag.unlockHandler();
		}
	}

	/**
	 * Start a local request to insert a block. Note that this is a KeyBlock not a ClientKeyBlock
	 * mainly because of random reinserts.
	 * @param block
	 * @param cache
	 * @param canWriteClientCache
	 * @throws LowLevelPutException
	 */
	public void realPut(KeyBlock block, boolean canWriteClientCache, boolean forkOnCacheable, boolean preferInsert, boolean ignoreLowBackoff, boolean realTimeFlag) throws LowLevelPutException {
		if(block instanceof CHKBlock)
			realPutCHK((CHKBlock) block, canWriteClientCache, forkOnCacheable, preferInsert, ignoreLowBackoff, realTimeFlag);
		else if(block instanceof SSKBlock)
			realPutSSK((SSKBlock) block, canWriteClientCache, forkOnCacheable, preferInsert, ignoreLowBackoff, realTimeFlag);
		else
			throw new IllegalArgumentException("Unknown put type " + block.getClass());
	}

	public void realPutCHK(CHKBlock block, boolean canWriteClientCache, boolean forkOnCacheable, boolean preferInsert, boolean ignoreLowBackoff, boolean realTimeFlag) throws LowLevelPutException {
		byte[] data = block.getData();
		byte[] headers = block.getHeaders();
		PartiallyReceivedBlock prb = new PartiallyReceivedBlock(Node.PACKETS_IN_BLOCK, Node.PACKET_SIZE, data);
		CHKInsertSender is;
		long uid = makeUID();
		InsertTag tag = new InsertTag(false, InsertTag.START.LOCAL, null, realTimeFlag, uid, node);
		if(!node.lockUID(uid, false, true, false, true, realTimeFlag, tag)) {
			Logger.error(this, "Could not lock UID just randomly generated: " + uid + " - probably indicates broken PRNG");
			throw new LowLevelPutException(LowLevelPutException.INTERNAL_ERROR);
		}
		try {
			long startTime = System.currentTimeMillis();
			is = node.makeInsertSender(block.getKey(),
				node.maxHTL(), uid, tag, null, headers, prb, false, canWriteClientCache, forkOnCacheable, preferInsert, ignoreLowBackoff, realTimeFlag);
			boolean hasReceivedRejectedOverload = false;
			// Wait for status
			while(true) {
				synchronized(is) {
					if(is.getStatus() == CHKInsertSender.NOT_FINISHED)
						try {
							is.wait(5 * 1000);
						} catch(InterruptedException e) {
							// Ignore
						}
					if(is.getStatus() != CHKInsertSender.NOT_FINISHED)
						break;
				}
				if((!hasReceivedRejectedOverload) && is.receivedRejectedOverload()) {
					hasReceivedRejectedOverload = true;
					requestStarters.rejectedOverload(false, true, realTimeFlag);
				}
			}

			// Wait for completion
			while(true) {
				synchronized(is) {
					if(is.completed())
						break;
					try {
						is.wait(10 * 1000);
					} catch(InterruptedException e) {
						// Go around again
					}
				}
				if(is.anyTransfersFailed() && (!hasReceivedRejectedOverload)) {
					hasReceivedRejectedOverload = true; // not strictly true but same effect
					requestStarters.rejectedOverload(false, true, realTimeFlag);
				}
			}

			if(logMINOR)
				Logger.minor(this, "Completed " + uid + " overload=" + hasReceivedRejectedOverload + ' ' + is.getStatusString());

			// Finished?
			if(!hasReceivedRejectedOverload)
				// Is it ours? Did we send a request?
				if(is.sentRequest() && (is.uid == uid) && ((is.getStatus() == CHKInsertSender.ROUTE_NOT_FOUND) || (is.getStatus() == CHKInsertSender.SUCCESS))) {
					// It worked!
					long endTime = System.currentTimeMillis();
					long len = endTime - startTime;

					// RejectedOverload requests count towards RTT (timed out ones don't).
					requestStarters.getThrottle(false, true, realTimeFlag).successfulCompletion(len);
					requestStarters.requestCompleted(false, true, block.getKey(), realTimeFlag);
				}

			// Get status explicitly, *after* completed(), so that it will be RECEIVE_FAILED if the receive failed.
			int status = is.getStatus();
			if(status != CHKInsertSender.TIMED_OUT && status != CHKInsertSender.GENERATED_REJECTED_OVERLOAD && status != CHKInsertSender.INTERNAL_ERROR && status != CHKInsertSender.ROUTE_REALLY_NOT_FOUND) {
				int sent = is.getTotalSentBytes();
				int received = is.getTotalReceivedBytes();
				if(logMINOR)
					Logger.minor(this, "Local CHK insert cost " + sent + '/' + received + " bytes (" + status + ')');
				nodeStats.localChkInsertBytesSentAverage.report(sent);
				nodeStats.localChkInsertBytesReceivedAverage.report(received);
				if(status == CHKInsertSender.SUCCESS)
					// Only report Sent bytes because we did not receive the data.
					nodeStats.successfulChkInsertBytesSentAverage.report(sent);
			}

			boolean deep = node.shouldStoreDeep(block.getKey(), null, is == null ? new PeerNode[0] : is.getRoutedTo());
			try {
				node.store(block, deep, canWriteClientCache, false, false);
			} catch (KeyCollisionException e) {
				// CHKs don't collide
			}

			if(status == CHKInsertSender.SUCCESS) {
				Logger.normal(this, "Succeeded inserting " + block);
				return;
			} else {
				String msg = "Failed inserting " + block + " : " + is.getStatusString();
				if(status == CHKInsertSender.ROUTE_NOT_FOUND)
					msg += " - this is normal on small networks; the data will still be propagated, but it can't find the 20+ nodes needed for full success";
				if(is.getStatus() != CHKInsertSender.ROUTE_NOT_FOUND)
					Logger.error(this, msg);
				else
					Logger.normal(this, msg);
				switch(is.getStatus()) {
					case CHKInsertSender.NOT_FINISHED:
						Logger.error(this, "IS still running in putCHK!: " + is);
						throw new LowLevelPutException(LowLevelPutException.INTERNAL_ERROR);
					case CHKInsertSender.GENERATED_REJECTED_OVERLOAD:
					case CHKInsertSender.TIMED_OUT:
						throw new LowLevelPutException(LowLevelPutException.REJECTED_OVERLOAD);
					case CHKInsertSender.ROUTE_NOT_FOUND:
						throw new LowLevelPutException(LowLevelPutException.ROUTE_NOT_FOUND);
					case CHKInsertSender.ROUTE_REALLY_NOT_FOUND:
						throw new LowLevelPutException(LowLevelPutException.ROUTE_REALLY_NOT_FOUND);
					case CHKInsertSender.INTERNAL_ERROR:
						throw new LowLevelPutException(LowLevelPutException.INTERNAL_ERROR);
					default:
						Logger.error(this, "Unknown CHKInsertSender code in putCHK: " + is.getStatus() + " on " + is);
						throw new LowLevelPutException(LowLevelPutException.INTERNAL_ERROR);
				}
			}
		} finally {
			tag.unlockHandler();
		}
	}

	public void realPutSSK(SSKBlock block, boolean canWriteClientCache, boolean forkOnCacheable, boolean preferInsert, boolean ignoreLowBackoff, boolean realTimeFlag) throws LowLevelPutException {
		SSKInsertSender is;
		long uid = makeUID();
		InsertTag tag = new InsertTag(true, InsertTag.START.LOCAL, null, realTimeFlag, uid, node);
		if(!node.lockUID(uid, true, true, false, true, realTimeFlag, tag)) {
			Logger.error(this, "Could not lock UID just randomly generated: " + uid + " - probably indicates broken PRNG");
			throw new LowLevelPutException(LowLevelPutException.INTERNAL_ERROR);
		}
		try {
			long startTime = System.currentTimeMillis();
			// Be consistent: use the client cache to check for collisions as this is a local insert.
			SSKBlock altBlock = node.fetch(block.getKey(), false, true, canWriteClientCache, false, false, null);
			if(altBlock != null && !altBlock.equals(block))
				throw new LowLevelPutException(altBlock);
			is = node.makeInsertSender(block,
				node.maxHTL(), uid, tag, null, false, canWriteClientCache, false, forkOnCacheable, preferInsert, ignoreLowBackoff, realTimeFlag);
			boolean hasReceivedRejectedOverload = false;
			// Wait for status
			while(true) {
				synchronized(is) {
					if(is.getStatus() == SSKInsertSender.NOT_FINISHED)
						try {
							is.wait(5 * 1000);
						} catch(InterruptedException e) {
							// Ignore
						}
					if(is.getStatus() != SSKInsertSender.NOT_FINISHED)
						break;
				}
				if((!hasReceivedRejectedOverload) && is.receivedRejectedOverload()) {
					hasReceivedRejectedOverload = true;
					requestStarters.rejectedOverload(true, true, realTimeFlag);
				}
			}

			// Wait for completion
			while(true) {
				synchronized(is) {
					if(is.getStatus() != SSKInsertSender.NOT_FINISHED)
						break;
					try {
						is.wait(10 * 1000);
					} catch(InterruptedException e) {
						// Go around again
					}
				}
			}

			if(logMINOR)
				Logger.minor(this, "Completed " + uid + " overload=" + hasReceivedRejectedOverload + ' ' + is.getStatusString());

			// Finished?
			if(!hasReceivedRejectedOverload)
				// Is it ours? Did we send a request?
				if(is.sentRequest() && (is.uid == uid) && ((is.getStatus() == SSKInsertSender.ROUTE_NOT_FOUND) || (is.getStatus() == SSKInsertSender.SUCCESS))) {
					// It worked!
					long endTime = System.currentTimeMillis();
					long rtt = endTime - startTime;
					requestStarters.requestCompleted(true, true, block.getKey(), realTimeFlag);
					requestStarters.getThrottle(true, true, realTimeFlag).successfulCompletion(rtt);
				}

			int status = is.getStatus();

			if(status != CHKInsertSender.TIMED_OUT && status != CHKInsertSender.GENERATED_REJECTED_OVERLOAD && status != CHKInsertSender.INTERNAL_ERROR && status != CHKInsertSender.ROUTE_REALLY_NOT_FOUND) {
				int sent = is.getTotalSentBytes();
				int received = is.getTotalReceivedBytes();
				if(logMINOR)
					Logger.minor(this, "Local SSK insert cost " + sent + '/' + received + " bytes (" + status + ')');
				nodeStats.localSskInsertBytesSentAverage.report(sent);
				nodeStats.localSskInsertBytesReceivedAverage.report(received);
				if(status == SSKInsertSender.SUCCESS)
					// Only report Sent bytes as we haven't received anything.
					nodeStats.successfulSskInsertBytesSentAverage.report(sent);
			}

			boolean deep = node.shouldStoreDeep(block.getKey(), null, is == null ? new PeerNode[0] : is.getRoutedTo());

			if(is.hasCollided()) {
				SSKBlock collided = is.getBlock();
				// Store it locally so it can be fetched immediately, and overwrites any locally inserted.
				try {
					// Has collided *on the network*, not locally.
					node.storeInsert(collided, deep, true, canWriteClientCache, false);
				} catch(KeyCollisionException e) {
					// collision race?
					// should be impossible.
					Logger.normal(this, "collision race? is="+is, e);
				}
				throw new LowLevelPutException(collided);
			} else
				try {
					node.storeInsert(block, deep, false, canWriteClientCache, false);
				} catch(KeyCollisionException e) {
					LowLevelPutException failed = new LowLevelPutException(LowLevelPutException.COLLISION);
					NodeSSK key = block.getKey();
					KeyBlock collided = node.fetch(key, true, canWriteClientCache, false, false, null);
					if(collided == null) {
						Logger.error(this, "Collided but no key?!");
						// Could be a race condition.
						try {
							node.store(block, false, canWriteClientCache, false, false);
						} catch (KeyCollisionException e2) {
							Logger.error(this, "Collided but no key and still collided!");
							throw new LowLevelPutException(LowLevelPutException.INTERNAL_ERROR, "Collided, can't find block, but still collides!", e);
						}
					}

					failed.setCollidedBlock(collided);
					throw failed;
				}


			if(status == SSKInsertSender.SUCCESS) {
				Logger.normal(this, "Succeeded inserting " + block);
				return;
			} else {
				String msg = "Failed inserting " + block + " : " + is.getStatusString();
				if(status == CHKInsertSender.ROUTE_NOT_FOUND)
					msg += " - this is normal on small networks; the data will still be propagated, but it can't find the 20+ nodes needed for full success";
				if(is.getStatus() != SSKInsertSender.ROUTE_NOT_FOUND)
					Logger.error(this, msg);
				else
					Logger.normal(this, msg);
				switch(is.getStatus()) {
					case SSKInsertSender.NOT_FINISHED:
						Logger.error(this, "IS still running in putCHK!: " + is);
						throw new LowLevelPutException(LowLevelPutException.INTERNAL_ERROR);
					case SSKInsertSender.GENERATED_REJECTED_OVERLOAD:
					case SSKInsertSender.TIMED_OUT:
						throw new LowLevelPutException(LowLevelPutException.REJECTED_OVERLOAD);
					case SSKInsertSender.ROUTE_NOT_FOUND:
						throw new LowLevelPutException(LowLevelPutException.ROUTE_NOT_FOUND);
					case SSKInsertSender.ROUTE_REALLY_NOT_FOUND:
						throw new LowLevelPutException(LowLevelPutException.ROUTE_REALLY_NOT_FOUND);
					case SSKInsertSender.INTERNAL_ERROR:
						throw new LowLevelPutException(LowLevelPutException.INTERNAL_ERROR);
					default:
						Logger.error(this, "Unknown CHKInsertSender code in putSSK: " + is.getStatus() + " on " + is);
						throw new LowLevelPutException(LowLevelPutException.INTERNAL_ERROR);
				}
			}
		} finally {
			tag.unlockHandler();
		}
	}

	/** @deprecated Only provided for compatibility with old plugins! Plugins must specify! */
	public HighLevelSimpleClient makeClient(short prioClass) {
		return makeClient(prioClass, false, false);
	}

	/** @deprecated Only provided for compatibility with old plugins! Plugins must specify! */
	public HighLevelSimpleClient makeClient(short prioClass, boolean forceDontIgnoreTooManyPathComponents) {
		return makeClient(prioClass, forceDontIgnoreTooManyPathComponents, false);
	}

	/**
	 * @param prioClass The priority to run requests at.
	 * @param realTimeFlag If true, requests are latency-optimised. If false, they are
	 * throughput-optimised. Fewer latency-optimised ("real time") requests are accepted
	 * but their transfers are faster. Latency-optimised requests are expected to be bursty,
	 * whereas throughput-optimised (bulk) requests can be constant.
	 */
	public HighLevelSimpleClient makeClient(short prioClass, boolean forceDontIgnoreTooManyPathComponents, boolean realTimeFlag) {
		return new HighLevelSimpleClientImpl(this, tempBucketFactory, random, prioClass, forceDontIgnoreTooManyPathComponents, realTimeFlag);
	}

	public FCPServer getFCPServer() {
		return fcpServer;
	}

	public FProxyToadlet getFProxy() {
		return fproxyServlet;
	}

	public SimpleToadletServer getToadletContainer() {
		return toadletContainer;
	}

	public TextModeClientInterfaceServer getTextModeClientInterface() {
		return tmci;
	}

	public void setFProxy(FProxyToadlet fproxy) {
		this.fproxyServlet = fproxy;
	}

	public TextModeClientInterface getDirectTMCI() {
		return directTMCI;
	}

	public void setDirectTMCI(TextModeClientInterface i) {
		this.directTMCI = i;
	}

	public File getDownloadsDir() {
		return downloadsDir.dir();
	}

	public ProgramDirectory downloadsDir() {
		return downloadsDir;
	}

	public HealingQueue getHealingQueue() {
		return healingQueue;
	}

	public void queueRandomReinsert(KeyBlock block) {
		SimpleSendableInsert ssi = new SimpleSendableInsert(this, block, RequestStarter.MAXIMUM_PRIORITY_CLASS);
		if(logMINOR)
			Logger.minor(this, "Queueing random reinsert for " + block + " : " + ssi);
		ssi.schedule();
	}

	public void storeConfig() {
		Logger.normal(this, "Trying to write config to disk", new Exception("debug"));
		node.config.store();
	}

	public boolean isTestnetEnabled() {
		return node.isTestnetEnabled();
	}

	public boolean isAdvancedModeEnabled() {
		return (getToadletContainer() != null) &&
			getToadletContainer().isAdvancedModeEnabled();
	}

	public boolean isFProxyJavascriptEnabled() {
		return (getToadletContainer() != null) &&
			getToadletContainer().isFProxyJavascriptEnabled();
	}

	public String getMyName() {
		return node.getMyName();
	}

	public FilterCallback createFilterCallback(URI uri, FoundURICallback cb) {
		if(logMINOR)
			Logger.minor(this, "Creating filter callback: " + uri + ", " + cb);
		return new GenericReadFilterCallback(uri, cb,null);
	}

	public int maxBackgroundUSKFetchers() {
		return maxBackgroundUSKFetchers;
	}

	public boolean allowDownloadTo(File filename) {
		PHYSICAL_THREAT_LEVEL physicalThreatLevel = node.securityLevels.getPhysicalThreatLevel();
		if(physicalThreatLevel == PHYSICAL_THREAT_LEVEL.MAXIMUM)
			return false;
		if(downloadAllowedEverywhere)
			return true;
		if(includeDownloadDir)
			if(FileUtil.isParent(getDownloadsDir(), filename))
				return true;
		for(int i = 0; i < downloadAllowedDirs.length; i++) {
			if(FileUtil.isParent(downloadAllowedDirs[i], filename))
				return true;
		}
		return false;
	}

	public boolean allowUploadFrom(File filename) {
		if(uploadAllowedEverywhere)
			return true;
		for(int i = 0; i < uploadAllowedDirs.length; i++) {
			if(FileUtil.isParent(uploadAllowedDirs[i], filename))
				return true;
		}
		return false;
	}

	public File[] getAllowedUploadDirs() {
		return uploadAllowedDirs;
	}

	public SimpleFieldSet persistThrottlesToFieldSet() {
		return requestStarters.persistToFieldSet();
	}

	public Ticker getTicker() {
		return node.getTicker();
	}

	public Executor getExecutor() {
		return node.executor;
	}

	public File getPersistentTempDir() {
		return persistentTempDir.dir();
	}

	public File getTempDir() {
		return tempDir.dir();
	}

	/** Queue the offered key. */
	public void queueOfferedKey(Key key, boolean realTime) {
		ClientRequestScheduler sched = requestStarters.getScheduler(key instanceof NodeSSK, false, realTime);
		sched.queueOfferedKey(key, realTime);
	}

	public void dequeueOfferedKey(Key key) {
		ClientRequestScheduler sched = requestStarters.getScheduler(key instanceof NodeSSK, false, false);
		sched.dequeueOfferedKey(key);
		sched = requestStarters.getScheduler(key instanceof NodeSSK, false, true);
		sched.dequeueOfferedKey(key);
	}

	public FreenetURI[] getBookmarkURIs() {
		return toadletContainer.getBookmarkURIs();
	}

	public long countTransientQueuedRequests() {
		return requestStarters.countTransientQueuedRequests();
	}

	public void queue(final DBJob job, int priority, boolean checkDupes) throws DatabaseDisabledException{
		synchronized(this) {
			if(killedDatabase) throw new DatabaseDisabledException();
		}
		if(checkDupes)
			this.clientDatabaseExecutor.executeNoDupes(new DBJobWrapper(job), priority, ""+job);
		else
			this.clientDatabaseExecutor.execute(new DBJobWrapper(job), priority, ""+job);
	}

	private boolean killedDatabase = false;

	private long lastCommitted = System.currentTimeMillis();

	static final int MAX_COMMIT_INTERVAL = 30*1000;

	static final int SOON_COMMIT_INTERVAL = 5*1000;

	class DBJobWrapper implements Runnable {

		DBJobWrapper(DBJob job) {
			this.job = job;
			if(job == null) throw new NullPointerException();
		}

		final DBJob job;

		public void run() {

			try {
				synchronized(NodeClientCore.this) {
					if(killedDatabase) {
						Logger.error(this, "Database killed already, not running job");
						return;
					}
				}
				if(job == null) throw new NullPointerException();
				if(node == null) throw new NullPointerException();
				boolean commit = job.run(node.db, clientContext);
				boolean killed;
				synchronized(NodeClientCore.this) {
					killed = killedDatabase;
					if(!killed) {
						long now = System.currentTimeMillis();
						if(now - lastCommitted > MAX_COMMIT_INTERVAL) {
							lastCommitted = now;
							commit = true;
						} else if(commitSoon && now - lastCommitted > SOON_COMMIT_INTERVAL) {
							commitSoon = false;
							lastCommitted = now;
							commit = true;
						} else if(commitSoon && !clientDatabaseExecutor.anyQueued()) {
							commitSoon = false;
							lastCommitted = now;
							commit = true;
						}
						if(alwaysCommit)
							commit = true;
						if(commitThisTransaction) {
							commit = true;
							commitThisTransaction = false;
						}
					}
				}
				if(killed) {
					node.db.rollback();
					return;
				} else if(commit) {
					persistentTempBucketFactory.preCommit(node.db);
					node.db.commit();
					synchronized(NodeClientCore.this) {
						lastCommitted = System.currentTimeMillis();
					}
					if(logMINOR) Logger.minor(this, "COMMITTED");
					persistentTempBucketFactory.postCommit(node.db);
				}
			} catch (Throwable t) {
				if(t instanceof OutOfMemoryError) {
					synchronized(NodeClientCore.this) {
						killedDatabase = true;
					}
					OOMHandler.handleOOM((OutOfMemoryError) t);
				} else {
					Logger.error(this, "Failed to run database job "+job+" : caught "+t, t);
				}
				boolean killed;
				synchronized(NodeClientCore.this) {
					killed = killedDatabase;
				}
				if(killed) {
					node.db.rollback();
				}
			}
		}

		@Override
		public int hashCode() {
			return job == null ? 0 : job.hashCode();
		}

		@Override
		public boolean equals(Object o) {
			if(!(o instanceof DBJobWrapper)) return false;
			DBJobWrapper cmp = (DBJobWrapper) o;
			return (cmp.job == job);
		}

		@Override
		public String toString() {
			return "DBJobWrapper:"+job;
		}

	}

	public boolean onDatabaseThread() {
		return clientDatabaseExecutor.onThread();
	}

	public int getQueueSize(int priority) {
		return clientDatabaseExecutor.getQueueSize(priority);
	}

	public void handleLowMemory() throws Exception {
		// Ignore
	}

	public void handleOutOfMemory() throws Exception {
		synchronized(this) {
			killedDatabase = true;
		}
		WrapperManager.requestThreadDump();
		System.err.println("Out of memory: Emergency shutdown to protect database integrity in progress...");
		WrapperManager.restart();
		System.exit(NodeInitException.EXIT_OUT_OF_MEMORY_PROTECTING_DATABASE);
	}

	/**
	 * Queue a job to be run soon after startup. The job must delete itself.
	 */
	public void queueRestartJob(DBJob job, int priority, ObjectContainer container, boolean early) throws DatabaseDisabledException {
		synchronized(this) {
			if(killedDatabase) throw new DatabaseDisabledException();
		}
		restartJobsQueue.queueRestartJob(job, priority, container, early);
	}

	public void removeRestartJob(DBJob job, int priority, ObjectContainer container) throws DatabaseDisabledException {
		synchronized(this) {
			if(killedDatabase) throw new DatabaseDisabledException();
		}
		restartJobsQueue.removeRestartJob(job, priority, container);
	}

	public void runBlocking(final DBJob job, int priority) throws DatabaseDisabledException {
		if(clientDatabaseExecutor.onThread()) {
			job.run(node.db, clientContext);
		} else {
			final MutableBoolean finished = new MutableBoolean();
			queue(new DBJob() {

				public boolean run(ObjectContainer container, ClientContext context) {
					try {
						return job.run(container, context);
					} finally {
						synchronized(finished) {
							finished.value = true;
							finished.notifyAll();
						}
					}
				}

				public String toString() {
					return job.toString();
				}

			}, priority, false);
			synchronized(finished) {
				while(!finished.value) {
					try {
						finished.wait();
					} catch (InterruptedException e) {
						// Ignore
					}
				}
			}
		}
	}

	public boolean objectCanNew(ObjectContainer container) {
		Logger.error(this, "Not storing NodeClientCore in database", new Exception("error"));
		return false;
	}

	public synchronized void killDatabase() {
		killedDatabase = true;
	}

	public synchronized boolean killedDatabase() {
		return killedDatabase;
	}

	public void onIdle() {
		synchronized(NodeClientCore.this) {
			if(killedDatabase) return;
		}
		persistentTempBucketFactory.preCommit(node.db);
		node.db.commit();
		synchronized(NodeClientCore.this) {
			lastCommitted = System.currentTimeMillis();
		}
		if(logMINOR) Logger.minor(this, "COMMITTED");
		persistentTempBucketFactory.postCommit(node.db);
	}

	private boolean commitThisTransaction;

	public synchronized void setCommitThisTransaction() {
		commitThisTransaction = true;
	}

	private boolean commitSoon;

	public synchronized void setCommitSoon() {
		commitSoon = true;
	}

	public static int getMaxBackgroundUSKFetchers() {
		return maxBackgroundUSKFetchers;
	}

	/* FIXME SECURITY When/if introduce tunneling or similar mechanism for starting requests
	 * at a distance this will need to be reconsidered. See the comments on the caller in
	 * RequestHandler (onAbort() handler). */
	public boolean wantKey(Key key) {
		boolean isSSK = key instanceof NodeSSK;
		if(this.clientContext.getFetchScheduler(isSSK, true).wantKey(key)) return true;
		if(this.clientContext.getFetchScheduler(isSSK, false).wantKey(key)) return true;
		return false;
	}

	public long checkRecentlyFailed(Key key, boolean realTime) {
		RecentlyFailedReturn r = new RecentlyFailedReturn();
		// We always decrement when we start a request. This feeds into the
		// routing decision. Depending on our decrementAtMax flag, it may or
		// may not actually go down one hop. But if we don't use it here then
		// this won't be comparable to the decisions taken by the RequestSender,
		// so we will keep on selecting and RF'ing locally, and wasting send
		// slots and CPU. FIXME SECURITY/NETWORK: Reconsider if we ever decide
		// not to decrement on the originator.
		short origHTL = node.decrementHTL(null, node.maxHTL());
		node.peers.closerPeer(null, new HashSet<PeerNode>(), key.toNormalizedDouble(), true, false, -1, null, 2.0, key, origHTL, 0, true, realTime, r, false, System.currentTimeMillis());
		return r.recentlyFailed();
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.api;

import java.io.IOException;


public interface BucketFactory {
	/**
	 * Create a bucket.
	 * @param size The maximum size of the data, or -1 if we don't know.
	 * Some buckets will throw IOException if you go over this length.
	 * @return
	 * @throws IOException
	 */
    public Bucket makeBucket(long size) throws IOException;
}

package freenet.node;

import java.net.InetAddress;
import java.net.UnknownHostException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

import freenet.config.InvalidConfigValueException;
import freenet.config.SubConfig;
import freenet.io.comm.FreenetInetAddress;
import freenet.io.comm.Peer;
import freenet.io.comm.UdpSocketHandler;
import freenet.l10n.NodeL10n;
import freenet.node.useralerts.IPUndetectedUserAlert;
import freenet.node.useralerts.InvalidAddressOverrideUserAlert;
import freenet.node.useralerts.SimpleUserAlert;
import freenet.node.useralerts.UserAlert;
import freenet.pluginmanager.DetectedIP;
import freenet.pluginmanager.FredPluginBandwidthIndicator;
import freenet.pluginmanager.FredPluginIPDetector;
import freenet.pluginmanager.FredPluginPortForward;
import freenet.support.HTMLNode;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.api.StringCallback;
import freenet.support.io.NativeThread;
import freenet.support.transport.ip.HostnameSyntaxException;
import freenet.support.transport.ip.IPAddressDetector;
import freenet.support.transport.ip.IPUtil;

/**
 * Detect the IP address of the node. Doesn't return port numbers, doesn't have access to per-port
 * information (NodeCrypto - UdpSocketHandler etc).
 */
public class NodeIPDetector {
	private static volatile boolean logMINOR;
	private static volatile boolean logDEBUG;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
				logDEBUG = Logger.shouldLog(LogLevel.DEBUG, this);
			}
		});
	}

	/** Parent node */
	final Node node;
	/** Ticker */
	/** Explicit forced IP address */
	FreenetInetAddress overrideIPAddress;
	/** Explicit forced IP address in string form because we want to keep it even if it's invalid and therefore unused */
	String overrideIPAddressString;
	/** IP address from last time */
	FreenetInetAddress oldIPAddress;
	/** Detected IP's and their NAT status from plugins */
	DetectedIP[] pluginDetectedIPs;
	/** Last detected IP address */
	FreenetInetAddress[] lastIPAddress;
	/** The minimum reported MTU on all detected interfaces */
	private int minimumMTU = Integer.MAX_VALUE;
	/** IP address detector */
	private final IPAddressDetector ipDetector;
	/** Plugin manager for plugin IP address detectors e.g. STUN */
	final IPDetectorPluginManager ipDetectorManager;
	/** UserAlert shown when ipAddressOverride has a hostname/IP address syntax error */
	private static InvalidAddressOverrideUserAlert invalidAddressOverrideAlert;
	private boolean hasValidAddressOverride;
	/** UserAlert shown when we can't detect an IP address */
	private static IPUndetectedUserAlert primaryIPUndetectedAlert;
	// FIXME redundant? see lastIPAddress
	FreenetInetAddress[] lastIP;
	/** Set when we have grounds to believe that we may be behind a symmetric NAT. */
	boolean maybeSymmetric;
	private boolean hasDetectedPM;
	private boolean hasDetectedIAD;
	/** Subsidiary detectors: NodeIPPortDetector's which rely on this object */
	private NodeIPPortDetector[] portDetectors;
	private boolean hasValidIP;
	private boolean firstDetection = true;
	
	SimpleUserAlert maybeSymmetricAlert;
	
	public NodeIPDetector(Node node) {
		this.node = node;
		ipDetectorManager = new IPDetectorPluginManager(node, this);
		ipDetector = new IPAddressDetector(10*1000, this);
		invalidAddressOverrideAlert = new InvalidAddressOverrideUserAlert(node);
		primaryIPUndetectedAlert = new IPUndetectedUserAlert(node);
		portDetectors = new NodeIPPortDetector[0];
	}

	public synchronized void addPortDetector(NodeIPPortDetector detector) {
		NodeIPPortDetector[] newDetectors = new NodeIPPortDetector[portDetectors.length+1];
		System.arraycopy(portDetectors, 0, newDetectors, 0, portDetectors.length);
		newDetectors[portDetectors.length] = detector;
		portDetectors = newDetectors;
	}
	
	/**
	 * What is my IP address? Use all globally available information (everything which isn't
	 * specific to a given port i.e. opennet or darknet) to determine our current IP addresses.
	 * Will include more than one IP in many cases when we are not strictly multi-homed. For 
	 * example, if we have a DNS name set, we will usually return an IP as well.
	 * 
	 * Will warn the user with a UserAlert if we don't have sufficient information.
	 */
	FreenetInetAddress[] detectPrimaryIPAddress(boolean dumpLocalAddresses) {
		boolean addedValidIP = false;
		Logger.minor(this, "Redetecting IPs...");
		ArrayList<FreenetInetAddress> addresses = new ArrayList<FreenetInetAddress>();
		if(overrideIPAddress != null) {
			// If the IP is overridden and the override is valid, the override has to be the first element.
			// overrideIPAddress will be null if the override is invalid
			addresses.add(overrideIPAddress);
			if(overrideIPAddress.isRealInternetAddress(false, true, false))
				addedValidIP = true;
		}
		
		if(!node.dontDetect()) {
			addedValidIP |= innerDetect(addresses);
		}
		
	   	if(node.clientCore != null) {
	   		boolean hadValidIP;
	   		synchronized(this) {
	   			hadValidIP = hasValidIP;
	   			hasValidIP = addedValidIP;
	   			if(firstDetection) {
	   				hadValidIP = !addedValidIP;
	   				firstDetection = false;
	   			}
	   		}
	   		if(hadValidIP != addedValidIP) {
	   			if (addedValidIP) {
	   				if(logMINOR) Logger.minor(this, "Got valid IP");
	   				onAddedValidIP();
	   			} else {
	   				if(logMINOR) Logger.minor(this, "No valid IP");
	   				onNotAddedValidIP();
	   			}
	   		}
	   	} else if(logMINOR)
	   		Logger.minor(this, "Client core not loaded");
	   	synchronized(this) {
	   		hasValidIP = addedValidIP;
	   	}
	   	lastIPAddress = addresses.toArray(new FreenetInetAddress[addresses.size()]);
	   	if(dumpLocalAddresses) {
	   		ArrayList<FreenetInetAddress> filtered = new ArrayList<FreenetInetAddress>(lastIPAddress.length);
	   		for(int i=0;i<lastIPAddress.length;i++) {
	   			if(lastIPAddress[i] == null) continue;
	   			if(lastIPAddress[i] == overrideIPAddress && lastIPAddress[i].hasHostnameNoIP())
	   				filtered.add(lastIPAddress[i]);
	   			else if(lastIPAddress[i].hasHostnameNoIP()) continue;
	   			else if(IPUtil.isValidAddress(lastIPAddress[i].getAddress(), false))
	   				filtered.add(lastIPAddress[i]);
	   		}
	   		return filtered.toArray(new FreenetInetAddress[filtered.size()]);
	   	}
	   	return lastIPAddress;
	}
	
	boolean hasValidIP() {
		synchronized(this) {
			return hasValidIP;
		}
	}
	
	private void onAddedValidIP() {
		node.clientCore.alerts.unregister(primaryIPUndetectedAlert);
		node.onAddedValidIP();
	}
	
	private void onNotAddedValidIP() {
		node.clientCore.alerts.register(primaryIPUndetectedAlert);
	}
	
	/**
	 * Core of the IP detection algorithm.
	 * @param addresses
	 * @param addedValidIP
	 * @return
	 */
	private boolean innerDetect(List<FreenetInetAddress> addresses) {
		boolean addedValidIP = false;
		InetAddress[] detectedAddrs = ipDetector.getAddress();
		assert(detectedAddrs != null);
		synchronized(this) {
			hasDetectedIAD = true;
		}
		for(int i=0;i<detectedAddrs.length;i++) {
			FreenetInetAddress addr = new FreenetInetAddress(detectedAddrs[i]);
			if(!addresses.contains(addr)) {
				Logger.normal(this, "Detected IP address: "+addr);
				addresses.add(addr);
				if(addr.isRealInternetAddress(false, false, false))
					addedValidIP = true;
			}
		}
		
		if((pluginDetectedIPs != null) && (pluginDetectedIPs.length > 0)) {
			for(int i=0;i<pluginDetectedIPs.length;i++) {
				InetAddress addr = pluginDetectedIPs[i].publicAddress;
				if(addr == null) continue;
				FreenetInetAddress a = new FreenetInetAddress(addr);
				if(!addresses.contains(a)) {
					Logger.normal(this, "Plugin detected IP address: "+a);
					addresses.add(a);
					if(a.isRealInternetAddress(false, false, false))
						addedValidIP = true;
				}
			}
		}
		
		boolean hadAddedValidIP = addedValidIP;
		
		int confidence = 0;
		
		// Try to pick it up from our connections
		if(node.peers != null) {
			PeerNode[] peerList = node.peers.myPeers;
			HashMap<FreenetInetAddress,Integer> countsByPeer = new HashMap<FreenetInetAddress,Integer>();
			// FIXME use a standard mutable int object, we have one somewhere
			for(int i=0;i<peerList.length;i++) {
				if(!peerList[i].isConnected()) {
					if(logDEBUG) Logger.minor(this, "Not connected");
					continue;
				}
				if(!peerList[i].isRealConnection()) {
					// Only let seed server connections through.
					// We have to trust them anyway.
					if(!(peerList[i] instanceof SeedServerPeerNode)) continue;
					if(logMINOR) Logger.minor(this, "Not a real connection and not a seed node: "+peerList[i]);
				}
				if(logMINOR) Logger.minor(this, "Maybe a usable connection for IP: "+peerList[i]);
				Peer p = peerList[i].getRemoteDetectedPeer();
				if(logMINOR) Logger.minor(this, "Remote detected peer: "+p);
				if(p == null || p.isNull()) continue;
				FreenetInetAddress addr = p.getFreenetAddress();
				if(logMINOR) Logger.minor(this, "Address: "+addr);
				if(addr == null) continue;
				if(!IPUtil.isValidAddress(addr.getAddress(false), false)) {
					if(logMINOR) Logger.minor(this, "Address not valid");
					continue;
				}
				if(logMINOR)
					Logger.minor(this, "Peer "+peerList[i].getPeer()+" thinks we are "+addr);
				if(countsByPeer.containsKey(addr)) {
					countsByPeer.put(addr, countsByPeer.get(addr) + 1);
				} else {
					countsByPeer.put(addr, 1);
				}
			}
			if(countsByPeer.size() == 1) {
				Iterator<FreenetInetAddress> it = countsByPeer.keySet().iterator();
				FreenetInetAddress addr = it.next();
				confidence = countsByPeer.get(addr);
				Logger.minor(this, "Everyone agrees we are "+addr);
				if(!addresses.contains(addr)) {
					if(addr.isRealInternetAddress(false, false, false))
						addedValidIP = true;
					addresses.add(addr);
				}
			} else if(countsByPeer.size() > 1) {
				// Take two most popular addresses.
				FreenetInetAddress best = null;
				FreenetInetAddress secondBest = null;
				int bestPopularity = 0;
				int secondBestPopularity = 0;
				for(Map.Entry<FreenetInetAddress,Integer> entry : countsByPeer.entrySet()) {
					FreenetInetAddress cur = entry.getKey();
					int curPop = entry.getValue();
					Logger.minor(this, "Detected peer: "+cur+" popularity "+curPop);
					if(curPop >= bestPopularity) {
						secondBestPopularity = bestPopularity;
						bestPopularity = curPop;
						secondBest = best;
						best = cur;
					}
				}
				if(best != null) {
					boolean hasRealDetectedAddress = false;
					for(int i=0;i<detectedAddrs.length;i++) {
						if(IPUtil.isValidAddress(detectedAddrs[i], false))
							hasRealDetectedAddress = true;
					}
					if((bestPopularity > 1) || !hasRealDetectedAddress) {
 						if(!addresses.contains(best)) {
							Logger.minor(this, "Adding best peer "+best+" ("+bestPopularity+ ')');
							addresses.add(best);
							if(best.isRealInternetAddress(false, false, false))
								addedValidIP = true;
						}
 						confidence = bestPopularity;
						if((secondBest != null) && (secondBestPopularity > 1)) {
							if(!addresses.contains(secondBest)) {
								Logger.minor(this, "Adding second best peer "+secondBest+" ("+secondBest+ ')');
								addresses.add(secondBest);
								if(secondBest.isRealInternetAddress(false, false, false))
									addedValidIP = true;
							}
						}
					}
				}
			}
		}
		
		// Add the old address only if we have no choice, or if we only have the word of two peers to go on.
		if((!(hadAddedValidIP || confidence > 2)) && (oldIPAddress != null) && !oldIPAddress.equals(overrideIPAddress)) {
			addresses.add(oldIPAddress);
			// Don't set addedValidIP.
			// There is an excellent chance that this is out of date.
			// So we still want to nag the user, until we have some confirmation.
		}
		
	   	return addedValidIP;
	}
	
	private String l10n(String key) {
		return NodeL10n.getBase().getString("NodeIPDetector."+key);
	}

	private String l10n(String key, String pattern, String value) {
		return NodeL10n.getBase().getString("NodeIPDetector."+key, pattern, value);
	}

	FreenetInetAddress[] getPrimaryIPAddress(boolean dumpLocal) {
		if(lastIPAddress == null) return detectPrimaryIPAddress(dumpLocal);
		return lastIPAddress;
	}
	
	public boolean hasDirectlyDetectedIP() {
		InetAddress[] addrs = ipDetector.getAddress();
		if(addrs == null || addrs.length == 0) return false;
		for(int i=0;i<addrs.length;i++) {
			if(IPUtil.isValidAddress(addrs[i], false)) {
				if(logMINOR)
					Logger.minor(this, "Has a directly detected IP: "+addrs[i]);
				return true;
			}
		}
		return false;
	}

	/**
	 * Process a list of DetectedIP's from the IP detector plugin manager.
	 * DetectedIP's can tell us what kind of NAT we are behind as well as our public
	 * IP address.
	 */
	public void processDetectedIPs(DetectedIP[] list) {
		pluginDetectedIPs = list;
		for(int i=0; i<pluginDetectedIPs.length; i++){
			int mtu = pluginDetectedIPs[i].mtu;
			if(minimumMTU > mtu && mtu > 0){
				minimumMTU = mtu;
				Logger.normal(this, "Reducing the MTU to "+minimumMTU);
				if(mtu < UdpSocketHandler.MIN_MTU)
					node.onTooLowMTU(minimumMTU, UdpSocketHandler.MIN_MTU);
			}
		}
		node.updateMTU();
		redetectAddress();
	}

	public void redetectAddress() {
		FreenetInetAddress[] newIP = detectPrimaryIPAddress(false);
		NodeIPPortDetector[] detectors;
		synchronized(this) {
			if(Arrays.equals(newIP, lastIP)) return;
			lastIP = newIP;
			detectors = portDetectors;
		}
		for(int i=0;i<detectors.length;i++)
			detectors[i].update();
		node.writeNodeFile();
	}

	public void setOldIPAddress(FreenetInetAddress freenetAddress) {
		this.oldIPAddress = freenetAddress;
	}

	public int registerConfigs(SubConfig nodeConfig, int sortOrder) {
		// IP address override
		nodeConfig.register("ipAddressOverride", "", sortOrder++, true, false, "NodeIPDectector.ipOverride", 
				"NodeIPDectector.ipOverrideLong", 
				new StringCallback() {

			@Override
			public String get() {
				if(overrideIPAddressString == null) return "";
				else return overrideIPAddressString;
			}
			
			@Override
			public void set(String val) throws InvalidConfigValueException {
				boolean hadValidAddressOverride = hasValidAddressOverride();
				// FIXME do we need to tell anyone?
				if(val.length() == 0) {
					// Set to null
					overrideIPAddressString = val;
					overrideIPAddress = null;
					lastIPAddress = null;
					redetectAddress();
					return;
				}
				FreenetInetAddress addr;
				try {
					addr = new FreenetInetAddress(val, false, true);
				} catch (HostnameSyntaxException e) {
					throw new InvalidConfigValueException(l10n("unknownHostErrorInIPOverride", "error", "hostname or IP address syntax error"));
				} catch (UnknownHostException e) {
					throw new InvalidConfigValueException(l10n("unknownHostErrorInIPOverride", "error", e.getMessage()));
				}
				// Compare as IPs.
				if(addr.equals(overrideIPAddress)) return;
				overrideIPAddressString = val;
				overrideIPAddress = addr;
				lastIPAddress = null;
				synchronized(this) {
					hasValidAddressOverride = true;
				}
				if(!hadValidAddressOverride) {
					onGetValidAddressOverride();
				}
				redetectAddress();
			}
		});
		
		hasValidAddressOverride = true;
		overrideIPAddressString = nodeConfig.getString("ipAddressOverride");
		if(overrideIPAddressString.length() == 0)
			overrideIPAddress = null;
		else {
			try {
				overrideIPAddress = new FreenetInetAddress(overrideIPAddressString, false, true);
			} catch (HostnameSyntaxException e) {
				synchronized(this) {
					hasValidAddressOverride = false;
				}
				String msg = "Invalid IP override syntax: "+overrideIPAddressString+" in config: "+e.getMessage();
				Logger.error(this, msg);
				System.err.println(msg+" but starting up anyway, ignoring the configured IP override");
				overrideIPAddress = null;
			} catch (UnknownHostException e) {
				// **FIXME** This never happens for this reason with current FreenetInetAddress(String, boolean, boolean) code; perhaps it needs review?
				String msg = "Unknown host: "+overrideIPAddressString+" in config: "+e.getMessage();
				Logger.error(this, msg);
				System.err.println(msg+" but starting up anyway with no IP override");
				overrideIPAddress = null;
			}
		}
		
		// Temporary IP address hint
		
		nodeConfig.register("tempIPAddressHint", "", sortOrder++, true, false, "NodeIPDectector.tempAddressHint", "NodeIPDectector.tempAddressHintLong", new StringCallback() {

			@Override
			public String get() {
				return "";
			}
			
			@Override
			public void set(String val) throws InvalidConfigValueException {
				if(val.length() == 0) {
					return;
				}
				if(overrideIPAddress != null) return;
				try {
					oldIPAddress = new FreenetInetAddress(val, false);
				} catch (UnknownHostException e) {
					throw new InvalidConfigValueException("Unknown host: "+e.getMessage());
				}
				redetectAddress();
			}
		});
		
		String ipHintString = nodeConfig.getString("tempIPAddressHint");
		if(ipHintString.length() > 0) {
			try {
				oldIPAddress = new FreenetInetAddress(ipHintString, false);
			} catch (UnknownHostException e) {
				String msg = "Unknown host: "+ipHintString+" in config: "+e.getMessage();
				Logger.error(this, msg);
				System.err.println(msg+"");
				oldIPAddress = null;
			}
		}
		
		return sortOrder;
	}

	/** Start all IP detection related processes */
	public void start() {
		boolean haveValidAddressOverride = hasValidAddressOverride();
		if(!haveValidAddressOverride) {
			onNotGetValidAddressOverride();
		}
		node.executor.execute(ipDetector, "IP address re-detector");
		redetectAddress();
		// 60 second delay for inserting ARK to avoid reinserting more than necessary if we don't detect IP on startup.
		// Not a FastRunnable as it can take a while to start the insert
		node.getTicker().queueTimedJob(new Runnable() {
			public void run() {
				NodeIPPortDetector[] detectors;
				synchronized(this) {
					detectors = portDetectors;
				}
				for(int i=0;i<detectors.length;i++)
					detectors[i].startARK();
			}
		}, 60*1000);
	}

	public void onConnectedPeer() {
		// Run off thread, but at high priority.
		// Initial messages don't need an up to date IP for the node itself, but
		// announcements do. However announcements are not sent instantly.
		node.executor.execute(new PrioRunnable() {

			public void run() {
				ipDetectorManager.maybeRun();
			}

			public int getPriority() {
				return NativeThread.HIGH_PRIORITY;
			}
			
		});
	}

	public void registerIPDetectorPlugin(FredPluginIPDetector detector) {
		ipDetectorManager.registerDetectorPlugin(detector);
	}

	public void unregisterIPDetectorPlugin(FredPluginIPDetector detector) {
		ipDetectorManager.unregisterDetectorPlugin(detector);
	}
	
	public synchronized boolean isDetecting() {
		return !(hasDetectedPM && hasDetectedIAD);
	}

	void hasDetectedPM() {
		if(logMINOR)
			Logger.minor(this, "hasDetectedPM() called", new Exception("debug"));
		synchronized(this) {
			hasDetectedPM = true;
		}
	}

	public int getMinimumDetectedMTU() {
		return minimumMTU > 0 ? minimumMTU : 1500;
	}

	public void setMaybeSymmetric() {
		if(ipDetectorManager != null && ipDetectorManager.isEmpty()) {
			if(maybeSymmetricAlert == null) {
				maybeSymmetricAlert = new SimpleUserAlert(true, l10n("maybeSymmetricTitle"), 
						l10n("maybeSymmetric"), l10n("maybeSymmetricShort"), UserAlert.ERROR);
			}
			if(node.clientCore != null && node.clientCore.alerts != null)
				node.clientCore.alerts.register(maybeSymmetricAlert);
		} else {
			if(maybeSymmetricAlert != null)
				node.clientCore.alerts.unregister(maybeSymmetricAlert);
		}
	}

	public void registerPortForwardPlugin(FredPluginPortForward forward) {
		ipDetectorManager.registerPortForwardPlugin(forward);
	}

	public void unregisterPortForwardPlugin(FredPluginPortForward forward) {
		ipDetectorManager.unregisterPortForwardPlugin(forward);
	}
	
	//TODO: ugly: deal with multiple instances properly
	public synchronized void registerBandwidthIndicatorPlugin(FredPluginBandwidthIndicator indicator) {
		bandwidthIndicator = indicator;
	}
	public synchronized void unregisterBandwidthIndicatorPlugin(FredPluginBandwidthIndicator indicator) {
		bandwidthIndicator = null;
	}
	public synchronized FredPluginBandwidthIndicator getBandwidthIndicator() {
		return bandwidthIndicator;
	}
	private FredPluginBandwidthIndicator bandwidthIndicator;
	
	boolean hasValidAddressOverride() {
		synchronized(this) {
			return hasValidAddressOverride;
		}
	}
	
	private void onGetValidAddressOverride() {
		node.clientCore.alerts.unregister(invalidAddressOverrideAlert);
	}
	
	private void onNotGetValidAddressOverride() {
		node.clientCore.alerts.register(invalidAddressOverrideAlert);
	}

	public void addConnectionTypeBox(HTMLNode contentNode) {
		ipDetectorManager.addConnectionTypeBox(contentNode);
	}

	public boolean noDetectPlugins() {
		return !ipDetectorManager.hasDetectors();
	}
}
package freenet.support.io;

import java.io.DataInputStream;
import java.io.EOFException;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.nio.ByteBuffer;
import java.nio.channels.Channels;
import java.nio.channels.ReadableByteChannel;
import java.nio.channels.WritableByteChannel;
import java.security.MessageDigest;
import java.util.ArrayList;
import java.util.List;

import freenet.support.math.MersenneTwister;

import com.db4o.ObjectContainer;

import freenet.crypt.SHA256;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;
import freenet.support.api.BucketFactory;

/**
 * Helper functions for working with Buckets.
 */
public class BucketTools {

	private static final int BUFFER_SIZE = 64 * 1024;
        
	private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}
	/**
	 * Copy from the input stream of <code>src</code> to the output stream of
	 * <code>dest</code>.
	 * 
	 * @param src
	 * @param dst
	 * @throws IOException
	 */
	public final static void copy(Bucket src, Bucket dst) throws IOException {
		OutputStream out = dst.getOutputStream();
		InputStream in = src.getInputStream();
		ReadableByteChannel readChannel = Channels.newChannel(in);
		WritableByteChannel writeChannel = Channels.newChannel(out);

		ByteBuffer buffer = ByteBuffer.allocateDirect(BUFFER_SIZE);
		while (readChannel.read(buffer) != -1) {
			buffer.flip();
			while(buffer.hasRemaining())
				writeChannel.write(buffer);
			buffer.clear();
		}

		writeChannel.close();
		readChannel.close();
	}

	public final static void zeroPad(Bucket b, long size) throws IOException {
		OutputStream out = b.getOutputStream();

		// Initialized to zero by default.
		byte[] buffer = new byte[16384];

		long count = 0;
		while (count < size) {
			long nRequired = buffer.length;
			if (nRequired > size - count) {
				nRequired = size - count;
			}
			out.write(buffer, 0, (int) nRequired);
			count += nRequired;
		}

		out.close();
	}

	public final static void paddedCopy(Bucket from, Bucket to, long nBytes,
			int blockSize) throws IOException {

		if (nBytes > blockSize) {
			throw new IllegalArgumentException("nBytes > blockSize");
		}

		OutputStream out = null;
		InputStream in = null;

		try {

			out = to.getOutputStream();
			byte[] buffer = new byte[16384];
			in = from.getInputStream();

			long count = 0;
			while (count != nBytes) {
				long nRequired = nBytes - count;
				if (nRequired > buffer.length) {
					nRequired = buffer.length;
				}
				long nRead = in.read(buffer, 0, (int) nRequired);
				if (nRead == -1) {
					throw new IOException("Not enough data in source bucket.");
				}
				out.write(buffer, 0, (int) nRead);
				count += nRead;
			}

			if (count < blockSize) {
				// hmmm... better to just allocate a new buffer
				// instead of explicitly zeroing the old one?
				// Zero pad to blockSize
				long padLength = buffer.length;
				if (padLength > blockSize - nBytes) {
					padLength = blockSize - nBytes;
				}
				for (int i = 0; i < padLength; i++) {
					buffer[i] = 0;
				}

				while (count != blockSize) {
					long nRequired = blockSize - count;
					if (blockSize - count > buffer.length) {
						nRequired = buffer.length;
					}
					out.write(buffer, 0, (int) nRequired);
					count += nRequired;
				}
			}
		} finally {
			if (in != null)
				in.close();
			if (out != null)
				out.close();
		}
	}

	public static Bucket[] makeBuckets(BucketFactory bf, int count, int size)
		throws IOException {
		Bucket[] ret = new Bucket[count];
		for (int i = 0; i < count; i++) {
			ret[i] = bf.makeBucket(size);
		}
		return ret;
	}

	public final static int[] nullIndices(Bucket[] array) {
		List<Integer> list = new ArrayList<Integer>();
		for (int i = 0; i < array.length; i++) {
			if (array[i] == null) {
				list.add(i);
			}
		}

		int[] ret = new int[list.size()];
		for (int i = 0; i < list.size(); i++) {
			ret[i] = list.get(i);
		}
		return ret;
	}

	public final static int[] nonNullIndices(Bucket[] array) {
		List<Integer> list = new ArrayList<Integer>();
		for (int i = 0; i < array.length; i++) {
			if (array[i] != null) {
				list.add(i);
			}
		}

		int[] ret = new int[list.size()];
		for (int i = 0; i < list.size(); i++) {
			ret[i] = list.get(i);
		}
		return ret;
	}

	public final static Bucket[] nonNullBuckets(Bucket[] array) {
		List<Bucket> list = new ArrayList<Bucket>(array.length);
		for (int i = 0; i < array.length; i++) {
			if (array[i] != null) {
				list.add(array[i]);
			}
		}

		Bucket[] ret = new Bucket[list.size()];
		return list.toArray(ret);
	}

	/**
	 * Read the entire bucket in as a byte array.
	 * Not a good idea unless it is very small!
	 * Don't call if concurrent writes may be happening.
	 * @throws IOException If there was an error reading from the bucket.
	 * @throws OutOfMemoryError If it was not possible to allocate enough 
	 * memory to contain the entire bucket.
	 */
	public final static byte[] toByteArray(Bucket bucket) throws IOException {
		long size = bucket.size();
		if(size > Integer.MAX_VALUE) throw new OutOfMemoryError();
		byte[] data = new byte[(int)size];
		InputStream is = bucket.getInputStream();
		DataInputStream dis = null;
		try {
			dis = new DataInputStream(is);
			dis.readFully(data);
		} finally {
			Closer.close(dis);
			Closer.close(is);
		}
		return data;
	}

	public static int toByteArray(Bucket bucket, byte[] output) throws IOException {
		long size = bucket.size();
		if(size > output.length)
			throw new IllegalArgumentException("Data does not fit in provided buffer");
		InputStream is = null;
		try {
			is = bucket.getInputStream();
			int moved = 0;
			while(true) {
				if(moved == size) return moved;
				int x = is.read(output, moved, (int)(size - moved));
				if(x == -1) return moved;
				moved += x;
			}
		} finally {
			if(is != null) is.close();
		}
	}
	
	public static Bucket makeImmutableBucket(BucketFactory bucketFactory, byte[] data) throws IOException {
		return makeImmutableBucket(bucketFactory, data, data.length);
	}
	
	public static Bucket makeImmutableBucket(BucketFactory bucketFactory, byte[] data, int length) throws IOException {
		return makeImmutableBucket(bucketFactory, data, 0, length);
	}
	
	public static Bucket makeImmutableBucket(BucketFactory bucketFactory, byte[] data, int offset, int length) throws IOException {
		Bucket bucket = bucketFactory.makeBucket(length);
		OutputStream os = bucket.getOutputStream();
		os.write(data, offset, length);
		os.close();
		bucket.setReadOnly();
		return bucket;
	}

	public static byte[] hash(Bucket data) throws IOException {
		InputStream is = data.getInputStream();
		try {
			MessageDigest md = SHA256.getMessageDigest();
			try { 
				long bucketLength = data.size();
				long bytesRead = 0;
				byte[] buf = new byte[BUFFER_SIZE];
				while ((bytesRead < bucketLength) || (bucketLength == -1)) {
					int readBytes = is.read(buf);
					if (readBytes < 0)
						break;
					bytesRead += readBytes;
					if (readBytes > 0)
						md.update(buf, 0, readBytes);
				}
				if ((bytesRead < bucketLength) && (bucketLength > 0))
					throw new EOFException();
				if ((bytesRead != bucketLength) && (bucketLength > 0))
					throw new IOException("Read " + bytesRead + " but bucket length " + bucketLength + " on " + data + '!');
				byte[] retval = md.digest();
				return retval;
			} finally {
				SHA256.returnMessageDigest(md);
			}
		} finally {
			if(is != null) is.close();
		}
	}

	/** Copy the given quantity of data from the given bucket to the given OutputStream. 
	 * @throws IOException If there was an error reading from the bucket or writing to the stream. */
	public static long copyTo(Bucket decodedData, OutputStream os, long truncateLength) throws IOException {
		if(truncateLength == 0) return 0;
		if(truncateLength < 0) truncateLength = Long.MAX_VALUE;
		InputStream is = decodedData.getInputStream();
		try {
			int bufferSize = BUFFER_SIZE;
			if(truncateLength > 0 && truncateLength < (long)bufferSize) bufferSize = (int) truncateLength;
			byte[] buf = new byte[bufferSize];
			long moved = 0;
			while(moved < truncateLength) {
				// DO NOT move the (int) inside the Math.min()! big numbers truncate to negative numbers.
				int bytes = (int) Math.min(buf.length, truncateLength - moved);
				if(bytes <= 0)
					throw new IllegalStateException("bytes="+bytes+", truncateLength="+truncateLength+", moved="+moved);
				bytes = is.read(buf, 0, bytes);
				if(bytes <= 0) {
					if(truncateLength == Long.MAX_VALUE)
						break;
					IOException ioException = new IOException("Could not move required quantity of data in copyTo: "+bytes+" (moved "+moved+" of "+truncateLength+"): unable to read from "+is);
					ioException.printStackTrace();
					throw ioException; 
				}
				os.write(buf, 0, bytes);
				moved += bytes;
			}
			return moved;
		} finally {
			is.close();
			os.flush();
		}
	}

	/** Copy data from an InputStream into a Bucket. */
	public static void copyFrom(Bucket bucket, InputStream is, long truncateLength) throws IOException {
		OutputStream os = bucket.getOutputStream();
		byte[] buf = new byte[BUFFER_SIZE];
		if(truncateLength < 0) truncateLength = Long.MAX_VALUE;
		try {
			long moved = 0;
			while(moved < truncateLength) {
				// DO NOT move the (int) inside the Math.min()! big numbers truncate to negative numbers.
				int bytes = (int) Math.min(buf.length, truncateLength - moved);
				if(bytes <= 0)
					throw new IllegalStateException("bytes="+bytes+", truncateLength="+truncateLength+", moved="+moved);
				bytes = is.read(buf, 0, bytes);
				if(bytes <= 0) {
					if(truncateLength == Long.MAX_VALUE)
						break;
					IOException ioException = new IOException("Could not move required quantity of data in copyFrom: "
					        + bytes + " (moved " + moved + " of " + truncateLength + "): unable to read from " + is);
					ioException.printStackTrace();
					throw ioException;
				}
				os.write(buf, 0, bytes);
				moved += bytes;
			}
		} finally {
			os.close();
		}
	}

	/**
	 * Split the data into a series of read-only Bucket's.
	 * @param origData The original data Bucket.
	 * @param splitSize The number of bytes to put into each bucket.
	 *
	 * If the passed-in Bucket is a FileBucket, will be efficiently
	 * split into ReadOnlyFileSliceBuckets, otherwise new buckets are created
	 * and the data written to them.
	 * 
	 * Note that this method will allocate a buffer of size splitSize.
	 * @param freeData 
	 * @param persistent If true, the data is persistent. This method is responsible for ensuring that the returned
	 * buckets HAVE ALREADY BEEN STORED TO THE DATABASE, using the provided handle. The point? SegmentedBCB's buckets
	 * have already been stored!!
	 * @param container Database handle, only needed if persistent = true. 
	 * @throws IOException If there is an error creating buckets, reading from
	 * the provided bucket, or writing to created buckets.
	 */
	public static Bucket[] split(Bucket origData, int splitSize, BucketFactory bf, boolean freeData, boolean persistent, ObjectContainer container) throws IOException {
		if(origData instanceof FileBucket) {
			if(freeData) {
				Logger.error(BucketTools.class, "Asked to free data when splitting a FileBucket ?!?!? Not freeing as this would clobber the split result...");
			}
			Bucket[] buckets = ((FileBucket)origData).split(splitSize);
			if(persistent)
			for(Bucket bucket : buckets)
				bucket.storeTo(container);
			return buckets;
		}
		if(origData instanceof BucketChainBucket) {
			if(persistent) throw new IllegalArgumentException("Splitting a BucketChainBucket but persistent = true!");
			BucketChainBucket data = (BucketChainBucket)origData;
			if(data.bucketSize == splitSize) {
				Bucket[] buckets = data.getBuckets();
				if(freeData)
					data.clear();
				return buckets;
			} else {
				Logger.error(BucketTools.class, "Incompatible split size splitting a BucketChainBucket: his split size is "+data.bucketSize+" but mine is "+splitSize+" - we will copy the data, but this suggests a bug", new Exception("debug"));
			}
		}
		if(origData instanceof SegmentedBucketChainBucket) {
			SegmentedBucketChainBucket data = (SegmentedBucketChainBucket)origData;
			if(data.bucketSize == splitSize) {
				Bucket[] buckets = data.getBuckets();
				if(freeData)
					data.clear();
				if(persistent && freeData)
					data.removeFrom(container);
				// Buckets have already been stored, no need to storeTo().
				return buckets;
			} else {
				Logger.error(BucketTools.class, "Incompatible split size splitting a BucketChainBucket: his split size is "+data.bucketSize+" but mine is "+splitSize+" - we will copy the data, but this suggests a bug", new Exception("debug"));
			}
		}
		long length = origData.size();
		if(length > ((long)Integer.MAX_VALUE) * splitSize)
			throw new IllegalArgumentException("Way too big!: "+length+" for "+splitSize);
		int bucketCount = (int) (length / splitSize);
		if(length % splitSize > 0) bucketCount++;
		if(logMINOR)
			Logger.minor(BucketTools.class, "Splitting bucket "+origData+" of size "+length+" into "+bucketCount+" buckets");
		Bucket[] buckets = new Bucket[bucketCount];
		InputStream is = origData.getInputStream();
		DataInputStream dis = null;
		try {
			dis = new DataInputStream(is);
			long remainingLength = length;
			byte[] buf = new byte[splitSize];
			for(int i=0;i<bucketCount;i++) {
				int len = (int) Math.min(splitSize, remainingLength);
				Bucket bucket = bf.makeBucket(len);
				buckets[i] = bucket;
				dis.readFully(buf, 0, len);
				remainingLength -= len;
				OutputStream os = bucket.getOutputStream();
				try {
					os.write(buf, 0, len);
				} finally {
					os.close();
				}
			}
		} finally {
			if(dis != null)
				dis.close();
			else
				is.close();
		}
		if(freeData)
			origData.free();
		if(persistent && freeData)
			origData.removeFrom(container);
		if(persistent) {
			for(Bucket bucket : buckets)
				bucket.storeTo(container);
		}
		return buckets;
	}
	
	/**
	 * Pad a bucket with random data
	 * 
	 * @param oldBucket
	 * @param blockLength
	 * @param BucketFactory
	 * @param length
	 * 
	 * @return the paded bucket
	 */
	public static Bucket pad(Bucket oldBucket, int blockLength, BucketFactory bf, int length) throws IOException {
		byte[] hash = BucketTools.hash(oldBucket);
		Bucket b = bf.makeBucket(blockLength);
		MersenneTwister mt = new MersenneTwister(hash);
		OutputStream os = b.getOutputStream();
		try {
			BucketTools.copyTo(oldBucket, os, length);
			byte[] buf = new byte[BUFFER_SIZE];
			for(int x=length;x<blockLength;) {
				int remaining = blockLength - x;
				int thisCycle = Math.min(remaining, buf.length);
				mt.nextBytes(buf); // FIXME??
				os.write(buf, 0, thisCycle);
				x += thisCycle;
			}
			os.close();
			os = null;
			if(b.size() != blockLength)
				throw new IllegalStateException("The bucket's size is "+b.size()+" whereas it should be "+blockLength+'!');
			return b;
		} finally { Closer.close(os); }
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.client.filter;

import java.io.ByteArrayOutputStream;
import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.io.UnsupportedEncodingException;
import java.util.Arrays;
import java.util.HashMap;

import freenet.l10n.NodeL10n;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.io.CountedInputStream;

/**
 * Content filter for JPEG's.
 * Just check the header.
 * 
 * http://www.obrador.com/essentialjpeg/headerinfo.htm
 * Also the JFIF spec.
 * Also http://cs.haifa.ac.il/~nimrod/Compression/JPEG/J6sntx2005.pdf
 * http://svn.xiph.org/experimental/giles/jpegdump.c
 * http://it.jeita.or.jp/document/publica/standard/exif/english/jeida49e.htm
 *
 * L10n: Only the overall explanation message and the "too short" messages are localised.
 * It's probably not worth doing the others, they're way too detailed.
 */
public class JPEGFilter implements ContentDataFilter {

	private final boolean deleteComments;
	private final boolean deleteExif;

	private static final int MARKER_EOI = 0xD9; // End of image
	//private static final int MARKER_SOI = 0xD8; // Start of image
	private static final int MARKER_RST0 = 0xD0; // First reset marker
	private static final int MARKER_RST7 = 0xD7; // Last reset marker

        private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	JPEGFilter(boolean deleteComments, boolean deleteExif) {
		this.deleteComments = deleteComments;
		this.deleteExif = deleteExif;
	}

	static final byte[] soi = new byte[] {
		(byte)0xFF, (byte)0xD8 // Start of Image
	};
	static final byte[] identifier = new byte[] {
		(byte)'J', (byte)'F', (byte)'I', (byte)'F', 0
	};
	static final byte[] extensionIdentifier = new byte[] {
		(byte)'J', (byte)'F', (byte)'X', (byte)'X', 0
	};

	public void readFilter(InputStream input, OutputStream output, String charset, HashMap<String, String> otherParams,
			FilterCallback cb) throws DataFilterException, IOException {
		readFilter(input, output, charset, otherParams, cb, deleteComments, deleteExif);
		output.flush();
	}

	public void readFilter(InputStream input, OutputStream output, String charset, HashMap<String, String> otherParams,
			FilterCallback cb, boolean deleteComments, boolean deleteExif)
	throws DataFilterException, IOException {
		CountedInputStream cis = new CountedInputStream(input);
		DataInputStream dis = new DataInputStream(cis);
		assertHeader(dis, soi);
		output.write(soi);

		ByteArrayOutputStream baos = new ByteArrayOutputStream();
		DataOutputStream dos = new DataOutputStream(baos);

		// Check the chunks.

		boolean finished = false;
		int forceMarkerType = -1;
		while(!finished) {
			baos.reset();
			int markerType;
			if(forceMarkerType != -1) {
				markerType = forceMarkerType;
				forceMarkerType = -1;
			} else {
				int markerStart = dis.read();
				if(markerStart == -1) {
					// No more chunks to scan.
					break;
				} else if(finished) {
					if(logMINOR)
						Logger.minor(this, "More data after EOI, copying to truncate");
					return;
				}
				if(markerStart != 0xFF) {
					throwError("Invalid marker", "The file includes an invalid marker start "+Integer.toHexString(markerStart)+" and cannot be parsed further.");
				}
				if(baos != null) baos.write(0xFF);
				markerType = dis.readUnsignedByte();
				if(baos != null) baos.write(markerType);
			}
			if(logMINOR)
				Logger.minor(this, "Marker type: "+Integer.toHexString(markerType));
			long countAtStart = cis.count(); // After marker but before type
			int blockLength;
			if(markerType == MARKER_EOI || markerType >= MARKER_RST0 && markerType <= MARKER_RST7)
				blockLength = 0;
			else {
				blockLength = dis.readUnsignedShort();
				dos.writeShort(blockLength);
			}
			if(markerType == 0xDA) {
				// Start of scan marker

				// Copy marker
				if(blockLength < 2)
					throwError("Invalid frame length", "The file includes an invalid frame (length "+blockLength+").");
				byte[] buf = new byte[blockLength - 2];
				dis.readFully(buf);
				dos.write(buf);
				Logger.minor(this, "Copied start-of-frame marker length "+(blockLength-2));

				if(baos != null)
					baos.writeTo(output); // will continue; at end

				// Now copy the scan itself

				int prevChar = -1;
				while(true) {
					int x = dis.read();
					if(prevChar != -1 && output != null) {
						output.write(prevChar);
					}
					if(x == -1) {
						// Termination inside a scan; valid I suppose
						break;
					}
					if(prevChar == 0xFF && x != 0 &&
							!(x >= MARKER_RST0 && x <= MARKER_RST7)) { // reset markers can occur in the scan

						forceMarkerType = x;
						if(logMINOR)
							Logger.minor(this, "Moved scan at "+cis.count()+", found a marker type "+Integer.toHexString(x));
						if(output != null) output.write(x);
						break; // End of scan, new marker
					}
					prevChar = x;
				}

				continue; // Avoid writing the header twice

			} else if(markerType == 0xE0) { // APP0
				if(logMINOR) Logger.minor(this, "APP0");
				String type = readNullTerminatedAsciiString(dis);
				if(baos != null) writeNullTerminatedString(baos, type);
				if(logMINOR) Logger.minor(this, "Type: "+type+" length "+type.length());
				if(type.equals("JFIF")) {
					Logger.minor(this, "JFIF Header");
					// File header
					int majorVersion = dis.readUnsignedByte();
					if(majorVersion != 1)
						throwError("Invalid header", "Unrecognized major version "+majorVersion+".");
					dos.write(majorVersion);
					int minorVersion = dis.readUnsignedByte();
					if(minorVersion > 2)
						throwError("Invalid header", "Unrecognized version 1."+minorVersion+".");
					dos.write(minorVersion);
					int units = dis.readUnsignedByte();
					if(units > 2)
						throwError("Invalid header", "Unrecognized units type "+units+".");
					dos.write(units);
					dos.writeShort(dis.readShort()); // Copy Xdensity
					dos.writeShort(dis.readShort()); // Copy Ydensity
					int thumbX = dis.readUnsignedByte();
					dos.writeByte(thumbX);
					int thumbY = dis.readUnsignedByte();
					dos.writeByte(thumbY);
					int thumbLen = thumbX * thumbY * 3;
					byte[] buf = new byte[thumbLen];
					dis.readFully(buf);
					dos.write(buf);
				} else if(type.equals("JFXX")) {
					// JFIF extension marker
					int extensionCode = dis.readUnsignedByte();
					if(extensionCode == 0x10 || extensionCode == 0x11 || extensionCode == 0x13) {
						// Alternate thumbnail, perfectly valid
						skipRest(blockLength, countAtStart, cis, dis, dos, "thumbnail frame");
						Logger.minor(this, "Thumbnail frame");
					} else
						throwError("Unknown JFXX extension "+extensionCode, "The file contains an unknown JFXX extension.");
				} else {
					if(logMINOR)
						Logger.minor(this, "Dropping application-specific APP0 chunk named "+type);
					// Application-specific extension
					skipRest(blockLength, countAtStart, cis, dis, dos, "application-specific frame");
					continue; // Don't write the frame.
				}
			} else if(markerType == 0xE1) { // EXIF
				if(deleteExif) {
					if(logMINOR)
						Logger.minor(this, "Dropping EXIF data");
					skipBytes(dis, blockLength - 2);
					continue; // Don't write the frame
				}
				skipRest(blockLength, countAtStart, cis, dis, dos, "EXIF frame");
			} else if(markerType == 0xFE) {
				// Comment
				if(deleteComments) {
					skipBytes(dis, blockLength - 2);
					if(logMINOR)
						Logger.minor(this, "Dropping comment length "+(blockLength - 2)+'.');
					continue; // Don't write the frame
				}
				skipRest(blockLength, countAtStart, cis, dis, dos, "comment");
			} else if(markerType == 0xD9) {
				// End of image
				finished = true;
				if(logMINOR)
					Logger.minor(this, "End of image");
			} else {
				boolean valid = false;
				// We used to support only DB C4 C0, because some website said they were
				// sufficient for decoding a JPEG. Unfortunately they are not, JPEG is a 
				// very complex standard and the full spec is only available for a fee.
				// FIXME somebody who has access to the spec should have a look at this,
				// and ideally write some chunk sanitizers.
				switch(markerType) {
				// descriptions from http://svn.xiph.org/experimental/giles/jpegdump.c (GPL)
				case 0xc0: // start of frame
				case 0xc1: // extended sequential, huffman
				case 0xc2: // progressive, huffman
				case 0xc3: // lossless, huffman
				case 0xc5: // differential sequential, huffman
				case 0xc6: // differential progressive, huffman
				case 0xc7: // differential lossless, huffman
					// DELETE 0xc8 - "reserved for JPEG extension" - likely to be used for Bad Things
				case 0xc9: // extended sequential, arithmetic
				case 0xca: // progressive, arithmetic
				case 0xcb: // lossless, arithmetic
				case 0xcd: // differential sequential, arithmetic
				case 0xcf: // differential lossless, arithmetic
				case 0xc4: // define huffman tables
				case 0xcc: // define arithmetic-coding conditioning
					// Restart markers
				case 0xd0:
				case 0xd1:
				case 0xd2:
				case 0xd3:
				case 0xd4:
				case 0xd5:
				case 0xd6:
				case 0xd7:
					// Delimiters:
				case 0xd8: // start of image
				case 0xd9: // end of image
				case 0xda: // start of scan
				case 0xdb: // define quantization tables
				case 0xdc: // define number of lines
				case 0xdd: // define restart interval
				case 0xde: // define hierarchical progression
				case 0xdf: // expand reference components
					// DELETE APP0 - APP15 - application data sections, likely to be troublesome.
					// DELETE extension data sections JPG0-6,SOF48,LSE,JPG9-JPG13, JCOM (comment!!), TEM ("temporary private use for arithmetic coding")
					// DELETE 0x02 - 0xbf reserved sections.
					// Do not support JPEG2000 at the moment. Probably has different headers. FIXME.
					valid = true;
				}
				if(valid) {
					// Essential, non-terminal, but unparsed frames.
					if(blockLength < 2)
						throwError("Invalid frame length", "The file includes an invalid frame (length "+blockLength+").");
					byte[] buf = new byte[blockLength - 2];
					dis.readFully(buf);
					dos.write(buf);
					Logger.minor(this, "Essential frame type "+Integer.toHexString(markerType)+" length "+(blockLength-2)+" offset at end "+cis.count());
				} else {
					if(markerType >= 0xE0 && markerType <= 0xEF) {
						// APP marker. Can be safely deleted.
						if(logMINOR)
							Logger.minor(this, "Dropping application marker type "+Integer.toHexString(markerType)+" length "+blockLength);
					} else {
						if(logMINOR)
							Logger.minor(this, "Dropping unknown frame type "+Integer.toHexString(markerType)+" blockLength");
					}
					// Delete frame
					skipBytes(dis, blockLength - 2);
					continue;
				}
			}

			if(cis.count() != countAtStart + blockLength)
				throwError("Invalid frame", "The length of the frame is incorrect (read "+
						(cis.count()-countAtStart)+" bytes, frame length "+blockLength+" for type "+Integer.toHexString(markerType)+").");
			// Write frame
			baos.writeTo(output);
		}

		// In future, maybe we will check the other chunks too.
		// In particular, we may want to delete, or filter, the comment blocks.
		// FIXME
	}

	private static String l10n(String key) {
		return NodeL10n.getBase().getString("JPEGFilter."+key);
	}

	private void writeNullTerminatedString(ByteArrayOutputStream baos, String type) throws IOException {
		try {
			byte[] data = type.getBytes("ISO-8859-1"); // ascii, near enough
			baos.write(data);
			baos.write(0);
		} catch (UnsupportedEncodingException e) {
			throw new Error("Impossible: JVM doesn't support ISO-8859-1: " + e, e);
		}
	}

	private String readNullTerminatedAsciiString(DataInputStream dis) throws IOException {
		StringBuilder sb = new StringBuilder();
		while(true) {
			int x = dis.read();
			if(x == -1)
				throwError("Invalid extension frame", "Could not read an extension frame name.");
			if(x == 0) break;
			char c = (char) x; // ASCII
			if(x > 128 || (c < 32 && c != 10 && c != 13))
				throwError("Invalid extension frame name", "Non-ASCII character in extension frame name");
			sb.append(c);
		}
		return sb.toString();
	}

	private void skipRest(int blockLength, long countAtStart, CountedInputStream cis, DataInputStream dis, DataOutputStream dos, String thing) throws IOException {
		// Skip the rest of the data
		int skip = (int) (blockLength - (cis.count() - countAtStart));
		if(skip < 0)
			throwError("Invalid "+thing, "The file includes an invalid "+thing+'.');
		if(skip == 0) return;
		byte[] buf = new byte[skip];
		dis.readFully(buf);
		dos.write(buf);
	}

	// FIXME factor this out somewhere ... an IOUtil class maybe
	private void skipBytes(DataInputStream dis, int skip) throws IOException {
		int skipped = 0;
		while(skipped < skip) {
			long x = dis.skip(skip - skipped);
			if(x <= 0) {
				byte[] buf = new byte[Math.min(4096, skip - skipped)];
				dis.readFully(buf);
				skipped += buf.length;
			} else
				skipped += x;
		}
	}

	private void assertHeader(DataInputStream dis, byte[] expected) throws IOException {
		byte[] read = new byte[expected.length];
		dis.readFully(read);
		if(!Arrays.equals(read, expected))
			throwError("Invalid header", "The file does not start with a valid JPEG (JFIF) header.");
	}

	private void throwError(String shortReason, String reason) throws DataFilterException {
		// Throw an exception
		String message = l10n("notJpeg");
		if(reason != null) 
			message += ' ' + reason;
		if(shortReason != null)
			message += " - " + shortReason;
		DataFilterException e = new DataFilterException(shortReason, shortReason, message);
		if(logMINOR)
			Logger.normal(this, "Throwing "+e.getMessage(), e);
		throw e;
	}

	public void writeFilter(InputStream input, OutputStream output, String charset, HashMap<String, String> otherParams,
			FilterCallback cb) throws DataFilterException, IOException {
		return;
	}

}
package freenet.client.events;

public class SendingToNetworkEvent implements ClientEvent {
	
	final static int CODE = 0x0A;

	public int getCode() {
		return CODE;
	}

	public String getDescription() {
		return "Sending to network";
	}

}
package freenet.support.io;

import freenet.support.api.Bucket;

// A Bucket which does not support being stored to the database. E.g. SegmentedBCB.
public interface NotPersistentBucket extends Bucket {

	// No methods
	
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.pluginmanager;

import freenet.node.Node;
import freenet.support.Logger;
import freenet.support.SimpleFieldSet;
import freenet.support.api.Bucket;

/**
 * @author saces
 *
 */
public class PluginReplySenderDirect extends PluginReplySender {
	
	private final Node node;
	private final FredPluginTalker target;

	public PluginReplySenderDirect(Node node2, FredPluginTalker target2, String pluginname2, String identifier2) {
		super(pluginname2, identifier2);
		node = node2;
		target = target2;
	}

	@Override
	public void send(final SimpleFieldSet params, final Bucket bucket) {
		
		node.executor.execute(new Runnable() {

			public void run() {

				try {
					target.onReply(pluginname, identifier, params, bucket);
				} catch (Throwable t) {
					Logger.error(this, "Cought error while handling plugin reply: " + t.getMessage(), t);
				}

			}
		}, "FCPPlugin reply runner for " + pluginname);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

import java.util.HashSet;
import java.util.Vector;

import freenet.io.comm.AsyncMessageFilterCallback;
import freenet.io.comm.ByteCounter;
import freenet.io.comm.DMT;
import freenet.io.comm.DisconnectedException;
import freenet.io.comm.Message;
import freenet.io.comm.MessageFilter;
import freenet.io.comm.NotConnectedException;
import freenet.io.comm.PeerContext;
import freenet.io.comm.SlowAsyncMessageFilterCallback;
import freenet.io.xfer.AbortedException;
import freenet.io.xfer.BlockTransmitter;
import freenet.io.xfer.BlockTransmitter.BlockTransmitterCompletion;
import freenet.io.xfer.PartiallyReceivedBlock;
import freenet.keys.CHKBlock;
import freenet.keys.CHKVerifyException;
import freenet.keys.NodeCHK;
import freenet.support.Logger;
import freenet.support.OOMHandler;
import freenet.support.Logger.LogLevel;
import freenet.support.io.NativeThread;

public final class CHKInsertSender implements PrioRunnable, AnyInsertSender, ByteCounter {
	
	private class BackgroundTransfer implements PrioRunnable, AsyncMessageFilterCallback {
		private final long uid;
		/** Node we are waiting for response from */
		final PeerNode pn;
		/** We may be sending data to that node */
		BlockTransmitter bt;
		/** Have we received notice of the downstream success
		 * or failure of dependant transfers from that node?
		 * Includes timing out. */
		boolean receivedCompletionNotice;
		/** Set when we fatally timeout, or when we get a completion other than a timeout. */
		boolean finishedWaiting;

		/** Was the notification of successful transfer? */
		boolean completionSucceeded;
		
		/** Have we completed the immediate transfer? */
		boolean completedTransfer;
		/** Did it succeed? */
		boolean transferSucceeded;
		
		private final InsertTag thisTag;
		
		BackgroundTransfer(final PeerNode pn, PartiallyReceivedBlock prb, InsertTag thisTag) {
			this.pn = pn;
			this.uid = CHKInsertSender.this.uid;
			this.thisTag = thisTag;
			bt = new BlockTransmitter(node.usm, node.getTicker(), pn, uid, prb, CHKInsertSender.this, BlockTransmitter.NEVER_CASCADE, 
					new BlockTransmitterCompletion() {

				public void blockTransferFinished(boolean success) {
					BackgroundTransfer.this.completedTransfer(success);
					// Double-check that the node is still connected. Pointless to wait otherwise.
					if (pn.isConnected() && transferSucceeded) {
						//synch-version: this.receivedNotice(waitForReceivedNotification(this));
						//Add ourselves as a listener for the longterm completion message of this transfer, then gracefully exit.
						try {
							node.usm.addAsyncFilter(getNotificationMessageFilter(), BackgroundTransfer.this, null);
						} catch (DisconnectedException e) {
							// Normal
							if(logMINOR)
								Logger.minor(this, "Disconnected while adding filter");
							BackgroundTransfer.this.completedTransfer(false);
							BackgroundTransfer.this.receivedNotice(false, false);
						}
					} else {
						BackgroundTransfer.this.receivedNotice(false, false);
						pn.localRejectedOverload("TransferFailedInsert", realTimeFlag);
					}
				}
				
			}, realTimeFlag, node.nodeStats);
		}
		
		void start() {
			node.executor.execute(this, "CHKInsert-BackgroundTransfer for "+uid+" to "+pn.getPeer());
		}
		
		public void run() {
			freenet.support.Logger.OSThread.logPID(this);
			try {
				this.realRun();
			} catch (Throwable t) {
				this.completedTransfer(false);
				this.receivedNotice(false, false);
				Logger.error(this, "Caught "+t, t);
			}
		}
		
		private void realRun() {
			bt.sendAsync();
			// REDFLAG: Load limiting:
			// No confirmation that it has finished, and it won't finish immediately on the transfer finishing.
			// So don't try to thisTag.removeRoutingTo(next), just assume it keeps running until the whole insert finishes.
		}
		
		private void completedTransfer(boolean success) {
			synchronized(backgroundTransfers) {
				transferSucceeded = success;
				completedTransfer = true;
				backgroundTransfers.notifyAll();
			}
			if(!success) {
				setTransferTimedOut();
			}
		}
		
		/** @param timeout Whether this completion is the result of a timeout.
		 * @return True if we should wait again, false if we have already received a notice or timed out. */
		private boolean receivedNotice(boolean success, boolean timeout) {
			if(logMINOR) Logger.minor(this, "Received notice: "+success+(timeout ? " (timeout)" : "")+" on "+this);
			boolean noUnlockPeer = false;
			boolean noNotifyOriginator = false;
			synchronized(backgroundTransfers) {
				if(finishedWaiting) {
					Logger.error(this, "Finished waiting already yet receivedNotice("+success+","+timeout+")", new Exception("error"));
					return false;
				}
				if (receivedCompletionNotice) {
					if(logMINOR) Logger.minor(this, "receivedNotice("+success+"), already had receivedNotice("+completionSucceeded+")");
					if(timeout) {
						// Fatal timeout.
						finishedWaiting = true;
						noNotifyOriginator = true;
					}
				} else {
					completionSucceeded = success;
					receivedCompletionNotice = true;
					if(!timeout) // Any completion mode other than a timeout immediately sets finishedWaiting, because we won't wait any longer.
						finishedWaiting = true;
					else {
						// First timeout but not had second timeout yet.
						// Unlock downstream (below), but will wait here for the peer to fatally timeout.
						// UIDTag will automatically reassign to self when the time comes if we call handlingTimeout() here, and will avoid unnecessarily logging errors.
						// LOCKING: Note that it is safe to call the tag within the lock since we always take the UIDTag lock last.
						thisTag.handlingTimeout(pn);
						noUnlockPeer = true;
					}
				}
				if(!noNotifyOriginator) {
					backgroundTransfers.notifyAll();
				}
			}
			if((!noNotifyOriginator) && (!success)) {
				setTransferTimedOut();
			}
			if(!noUnlockPeer)
				// Downstream (away from originator), we need to stay locked on the peer until the fatal timeout / the delayed notice.
				// Upstream (towards originator), of course, we can unlockHandler() as soon as all the transfers are finished.
				// LOCKING: Do this outside the lock as pn can do heavy stuff in response (new load management).
				pn.noLongerRoutingTo(thisTag, false);
			if(noNotifyOriginator) return false;
			return true;
		}
		
		public void onMatched(Message m) {
			pn.successNotOverload(realTimeFlag);
			PeerNode pn = (PeerNode) m.getSource();
			// pn cannot be null, because the filters will prevent garbage collection of the nodes
			
			if(this.pn.equals(pn)) {
				boolean anyTimedOut = m.getBoolean(DMT.ANY_TIMED_OUT);
				if(anyTimedOut) {
					CHKInsertSender.this.setTransferTimedOut();
				}
				receivedNotice(!anyTimedOut, false);
			} else {
				Logger.error(this, "received completion notice for wrong node: "+pn+" != "+this.pn);
			}			
		}
		
		public boolean shouldTimeout() {
			//AFIACS, this will still let the filter timeout, but not call onMatched() twice.
			return finishedWaiting;
		}
		
		private MessageFilter getNotificationMessageFilter() {
			return MessageFilter.create().setField(DMT.UID, uid).setType(DMT.FNPInsertTransfersCompleted).setSource(pn).setTimeout(transferCompletionTimeout);
		}

		public void onTimeout() {
			/* FIXME: Cascading timeout...
			   if this times out, we don't have any time to report to the node of origin the timeout notification (anyTimedOut?).
			 */
			// NORMAL priority because it is normally caused by a transfer taking too long downstream, and that doesn't usually indicate a bug.
			Logger.normal(this, "Timed out waiting for a final ack from: "+pn+" on "+this, new Exception("debug"));
			if(receivedNotice(false, true)) {
				pn.localRejectedOverload("InsertTimeoutNoFinalAck", realTimeFlag);
				// First timeout. Wait for second timeout.
				try {
					node.usm.addAsyncFilter(getNotificationMessageFilter(), this, CHKInsertSender.this);
				} catch (DisconnectedException e) {
					// Normal
					if(logMINOR)
						Logger.minor(this, "Disconnected while adding filter after first timeout");
				}
			} else {
				Logger.error(this, "Second timeout waiting for final ack from "+pn+" on "+this);
				pn.fatalTimeout(thisTag, false);
			}
		}

		public void onDisconnect(PeerContext ctx) {
			Logger.normal(this, "Disconnected "+ctx+" for "+this);
			receivedNotice(true, false); // as far as we know
			thisTag.removeRoutingTo(pn);
		}

		public void onRestarted(PeerContext ctx) {
			Logger.normal(this, "Restarted "+ctx+" for "+this);
			receivedNotice(true, false);
			thisTag.removeRoutingTo(pn);
		}

		public int getPriority() {
			return NativeThread.HIGH_PRIORITY;
		}
		
		public String toString() {
			return super.toString()+":"+uid+":"+pn;
		}
	}
	
	CHKInsertSender(NodeCHK myKey, long uid, InsertTag tag, byte[] headers, short htl, 
            PeerNode source, Node node, PartiallyReceivedBlock prb, boolean fromStore,
            boolean canWriteClientCache, boolean forkOnCacheable, boolean preferInsert, boolean ignoreLowBackoff, boolean realTimeFlag) {
        this.myKey = myKey;
        this.target = myKey.toNormalizedDouble();
        this.origUID = uid;
        this.uid = uid;
        this.origTag = tag;
        this.headers = headers;
        this.htl = htl;
        this.source = source;
        this.node = node;
        this.prb = prb;
        this.fromStore = fromStore;
        this.startTime = System.currentTimeMillis();
        this.backgroundTransfers = new Vector<BackgroundTransfer>();
        this.forkOnCacheable = forkOnCacheable;
        this.preferInsert = preferInsert;
        this.ignoreLowBackoff = ignoreLowBackoff;
        this.realTimeFlag = realTimeFlag;
        if(realTimeFlag) {
        	searchTimeout = SEARCH_TIMEOUT_REALTIME;
        	transferCompletionTimeout = TRANSFER_COMPLETION_ACK_TIMEOUT_REALTIME;
        } else {
        	searchTimeout = SEARCH_TIMEOUT_BULK;
        	transferCompletionTimeout = TRANSFER_COMPLETION_ACK_TIMEOUT_BULK;
        }
        logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
    }

	void start() {
		node.executor.execute(this, "CHKInsertSender for UID "+uid+" on "+node.getDarknetPortNumber()+" at "+System.currentTimeMillis());
	}

	static boolean logMINOR;
	
    // Constants
    static final int ACCEPTED_TIMEOUT = 10000;
    static final int SEARCH_TIMEOUT_REALTIME = 60*1000;
    static final int SEARCH_TIMEOUT_BULK = 300*1000;
    static final int TRANSFER_COMPLETION_ACK_TIMEOUT_REALTIME = 60*1000;
    static final int TRANSFER_COMPLETION_ACK_TIMEOUT_BULK = 300*1000;

    final int searchTimeout;
    final int transferCompletionTimeout;
    
    // Basics
    final NodeCHK myKey;
    final double target;
    final long origUID;
    final InsertTag origTag;
    long uid;
    private InsertTag forkedRequestTag;
    short htl;
    final PeerNode source;
    final Node node;
    final byte[] headers; // received BEFORE creation => we handle Accepted elsewhere
    final PartiallyReceivedBlock prb;
    final boolean fromStore;
    private boolean receiveFailed;
    final long startTime;
    private boolean sentRequest;
    private final boolean forkOnCacheable;
    private final boolean preferInsert;
    private final boolean ignoreLowBackoff;
    private final boolean realTimeFlag;
    private HashSet<PeerNode> nodesRoutedTo = new HashSet<PeerNode>();

    
    /** List of nodes we are waiting for either a transfer completion
     * notice or a transfer completion from. Also used as a sync object for waiting for transfer completion. */
    private Vector<BackgroundTransfer> backgroundTransfers;
    
    /** Have all transfers completed and all nodes reported completion status? */
    private boolean allTransfersCompleted;
    
    /** Has a transfer timed out, either directly or downstream? */
    private volatile boolean transferTimedOut;
    
    private int status = -1;
    /** Still running */
    static final int NOT_FINISHED = -1;
    /** Successful insert */
    static final int SUCCESS = 0;
    /** Route not found */
    static final int ROUTE_NOT_FOUND = 1;
    /** Internal error */
    static final int INTERNAL_ERROR = 3;
    /** Timed out waiting for response */
    static final int TIMED_OUT = 4;
    /** Locally Generated a RejectedOverload */
    static final int GENERATED_REJECTED_OVERLOAD = 5;
    /** Could not get off the node at all! */
    static final int ROUTE_REALLY_NOT_FOUND = 6;
    /** Receive failed. Not used internally; only used by CHKInsertHandler. */
    static final int RECEIVE_FAILED = 7;
    
    @Override
	public String toString() {
        return super.toString()+" for "+uid;
    }
    
    public void run() {
	    freenet.support.Logger.OSThread.logPID(this);
        short origHTL;
    	synchronized (this) {
            origHTL = htl;
		}
    	origTag.startedSender();
        try {
        	realRun();
		} catch (OutOfMemoryError e) {
			OOMHandler.handleOOM(e);
        } catch (Throwable t) {
            Logger.error(this, "Caught "+t, t);
        } finally {
        	// Always check: we ALWAYS set status, even if receiveFailed.
            int myStatus;
            synchronized (this) {
				myStatus = status;
			}
            if(myStatus == NOT_FINISHED)
            	finish(INTERNAL_ERROR, null);
            origTag.finishedSender();
        	if(forkedRequestTag != null)
        		forkedRequestTag.finishedSender();
        }
    }
    
	static final int MAX_HIGH_HTL_FAILURES = 5;
	
    private void realRun() {
        
        PeerNode next = null;
        // While in no-cache mode, we don't decrement HTL on a RejectedLoop or similar, but we only allow a limited number of such failures before RNFing.
        int highHTLFailureCount = 0;
        boolean starting = true;
        while(true) {
        	if(failIfReceiveFailed(null, null)) return; // don't need to set status as killed by CHKInsertHandler
            
            /*
             * If we haven't routed to any node yet, decrement according to the source.
             * If we have, decrement according to the node which just failed.
             * Because:
             * 1) If we always decrement according to source then we can be at max or min HTL
             * for a long time while we visit *every* peer node. This is BAD!
             * 2) The node which just failed can be seen as the requestor for our purposes.
             */
            // Decrement at this point so we can DNF immediately on reaching HTL 0.
            boolean canWriteStorePrev = node.canWriteDatastoreInsert(htl);
            if((!starting) && (!canWriteStorePrev)) {
            	// We always decrement on starting a sender.
            	// However, after that, if our HTL is above the no-cache threshold,
            	// we do not want to decrement the HTL for trivial rejections (e.g. RejectedLoop),
            	// because we would end up caching data too close to the originator.
            	// So allow 5 failures and then RNF.
            	if(highHTLFailureCount++ >= MAX_HIGH_HTL_FAILURES) {
            		if(logMINOR) Logger.minor(this, "Too many failures at non-cacheable HTL");
                    finish(ROUTE_NOT_FOUND, null);
                    return;
            	}
            	if(logMINOR) Logger.minor(this, "Allowing failure "+highHTLFailureCount+" htl is still "+htl);
            } else {
                htl = node.decrementHTL(sentRequest ? next : source, htl);
                if(logMINOR) Logger.minor(this, "Decremented HTL to "+htl);
            }
            starting = false;
            boolean successNow = false;
            boolean noRequest = false;
            synchronized (this) {
            	if(htl == 0) {
            		successNow = true;
            		// Send an InsertReply back
            		noRequest = !sentRequest;
            	}
            }
            if(successNow) {
        		if(noRequest)
        			origTag.setNotRoutedOnwards();
        		finish(SUCCESS, null);
        		return;
            }
            
            if( node.canWriteDatastoreInsert(htl) && (!canWriteStorePrev) && forkOnCacheable && forkedRequestTag == null) {
            	// FORK! We are now cacheable, and it is quite possible that we have already gone over the ideal sink nodes,
            	// in which case if we don't fork we will miss them, and greatly reduce the insert's reachability.
            	// So we fork: Create a new UID so we can go over the previous hops again if they happen to be good places to store the data.
            	
            	// Existing transfers will keep their existing UIDs, since they copied the UID in the constructor.
            	
            	uid = node.clientCore.makeUID();
				forkedRequestTag = new InsertTag(false, InsertTag.START.REMOTE, source, realTimeFlag, uid, node);
				forkedRequestTag.reassignToSelf();
				forkedRequestTag.startedSender();
				forkedRequestTag.unlockHandler();
            	Logger.normal(this, "FORKING CHK INSERT "+origUID+" to "+uid);
            	nodesRoutedTo.clear();
            	node.lockUID(uid, false, true, false, false, realTimeFlag, forkedRequestTag);
            }
            
            // Route it
            // Can backtrack, so only route to nodes closer than we are to target.
            next = node.peers.closerPeer(forkedRequestTag == null ? source : null, nodesRoutedTo, target, true, node.isAdvancedModeEnabled(), -1, null,
			        null, htl, ignoreLowBackoff ? Node.LOW_BACKOFF : 0, source == null, realTimeFlag);
			
            if(next == null) {
                // Backtrack
        		if(!sentRequest)
        			origTag.setNotRoutedOnwards();
                finish(ROUTE_NOT_FOUND, null);
                return;
            }
			
            if(logMINOR) Logger.minor(this, "Routing insert to "+next);
            nodesRoutedTo.add(next);
            
            Message req;
            
            req = DMT.createFNPInsertRequest(uid, htl, myKey);
            if(forkOnCacheable != Node.FORK_ON_CACHEABLE_DEFAULT) {
            	req.addSubMessage(DMT.createFNPSubInsertForkControl(forkOnCacheable));
            }
            if(ignoreLowBackoff != Node.IGNORE_LOW_BACKOFF_DEFAULT) {
            	req.addSubMessage(DMT.createFNPSubInsertIgnoreLowBackoff(ignoreLowBackoff));
            }
            if(preferInsert != Node.PREFER_INSERT_DEFAULT) {
            	req.addSubMessage(DMT.createFNPSubInsertPreferInsert(preferInsert));
            }
        	req.addSubMessage(DMT.createFNPRealTimeFlag(realTimeFlag));
            
            InsertTag thisTag = forkedRequestTag;
            if(forkedRequestTag == null) thisTag = origTag;
            
            thisTag.addRoutedTo(next, false);
            
            // Send to next node
            
            try {
				/*
				 Note also that we won't fork here, unlike in RequestSender, because the data won't be sent after a timeout, and the
				 insert will not be routed any further without the DataInsert.
				 */
				next.sendSync(req, this, realTimeFlag);
			} catch (NotConnectedException e1) {
				if(logMINOR) Logger.minor(this, "Not connected to "+next);
				thisTag.removeRoutingTo(next);
				continue;
			} catch (SyncSendWaitedTooLongException e) {
				Logger.warning(this, "Failed to send request to "+next);
				thisTag.removeRoutingTo(next);
				continue;
			}
			synchronized (this) {
				sentRequest = true;				
			}

			if(failIfReceiveFailed(thisTag, next)) {
				// Need to tell the peer that the DataInsert is not forthcoming.
				// DataInsertRejected is overridden to work both ways.
				try {
					next.sendAsync(DMT.createFNPDataInsertRejected(uid, DMT.DATA_INSERT_REJECTED_RECEIVE_FAILED), null, this);
				} catch (NotConnectedException e) {
					// Ignore
				}
				return;
			}
			
            Message msg = null;
            
            if(!waitAccepted(next, thisTag)) {
				thisTag.removeRoutingTo(next);
				if(failIfReceiveFailed(thisTag, next)) {
					// Need to tell the peer that the DataInsert is not forthcoming.
					// DataInsertRejected is overridden to work both ways.
					try {
						next.sendAsync(DMT.createFNPDataInsertRejected(uid, DMT.DATA_INSERT_REJECTED_RECEIVE_FAILED), null, this);
					} catch (NotConnectedException e) {
						// Ignore
					}
					return;
				}
				continue; // Try another node
            }
            	
            if(logMINOR) Logger.minor(this, "Got Accepted on "+this);
            
            // Send them the data.
            // Which might be the new data resulting from a collision...

            Message dataInsert;
            dataInsert = DMT.createFNPDataInsert(uid, headers);
            /** What are we waiting for now??:
             * - FNPRouteNotFound - couldn't exhaust HTL, but send us the 
             *   data anyway please
             * - FNPInsertReply - used up all HTL, yay
             * - FNPRejectOverload - propagating an overload error :(
             * - FNPRejectTimeout - we took too long to send the DataInsert
             * - FNPDataInsertRejected - the insert was invalid
             */
            
            MessageFilter mfInsertReply = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPInsertReply);
            MessageFilter mfRejectedOverload = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPRejectedOverload);
            MessageFilter mfRouteNotFound = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPRouteNotFound);
            MessageFilter mfDataInsertRejected = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPDataInsertRejected);
            MessageFilter mfTimeout = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPRejectedTimeout);
            
            MessageFilter mf = mfInsertReply.or(mfRouteNotFound.or(mfDataInsertRejected.or(mfTimeout.or(mfRejectedOverload))));

            if(logMINOR) Logger.minor(this, "Sending DataInsert");
            try {
				next.sendSync(dataInsert, this, realTimeFlag);
			} catch (NotConnectedException e1) {
				if(logMINOR) Logger.minor(this, "Not connected sending DataInsert: "+next+" for "+uid);
				thisTag.removeRoutingTo(next);
				continue;
			} catch (SyncSendWaitedTooLongException e) {
				Logger.error(this, "Unable to send "+dataInsert+" to "+next+" in a reasonable time");
				// Other side will fail. No need to do anything.
				thisTag.removeRoutingTo(next);
				continue;
			}

			if(logMINOR) Logger.minor(this, "Sending data");
			startBackgroundTransfer(next, prb, thisTag);
			
			// Once the transfer has started, we only unlock the tag after the transfer completes (successfully or not).
			
            while (true) {

    			if(failIfReceiveFailed(thisTag, next)) {
    				// The transfer has started, it will be cancelled.
    				return;
    			}
				
				try {
					msg = node.usm.waitFor(mf, this);
				} catch (DisconnectedException e) {
					Logger.normal(this, "Disconnected from " + next
							+ " while waiting for InsertReply on " + this);
					break;
				}
    			if(failIfReceiveFailed(thisTag, next)) {
    				// The transfer has started, it will be cancelled.
    				return;
    			}
				
				if (msg == null) {
					
					Logger.warning(this, "Timeout on insert "+this+" to "+next);
					
					// First timeout.
					// Could be caused by the next node, or could be caused downstream.
					next.localRejectedOverload("AfterInsertAcceptedTimeout2", realTimeFlag);
					forwardRejectedOverload();

					synchronized(this) {
						status = TIMED_OUT;
						notifyAll();
					}
					
					// Wait for the second timeout off-thread.
					// FIXME wait asynchronously.
					
					final InsertTag tag = thisTag;
					final PeerNode waitingFor = next;
					
					Runnable r = new Runnable() {

						public void run() {
							// We do not need to unlock the tag here.
							// That will happen in the BackgroundTransfer, which has already started.
							
							// FIXME factor out
			                MessageFilter mfInsertReply = MessageFilter.create().setSource(waitingFor).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPInsertReply);
			                MessageFilter mfRejectedOverload = MessageFilter.create().setSource(waitingFor).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPRejectedOverload);
			                MessageFilter mfRouteNotFound = MessageFilter.create().setSource(waitingFor).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPRouteNotFound);
			                MessageFilter mfDataInsertRejected = MessageFilter.create().setSource(waitingFor).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPDataInsertRejected);
			                MessageFilter mfTimeout = MessageFilter.create().setSource(waitingFor).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPRejectedTimeout);
			                
			                MessageFilter mf = mfInsertReply.or(mfRouteNotFound.or(mfDataInsertRejected.or(mfTimeout.or(mfRejectedOverload))));

				            while (true) {
				            	
				            	Message msg;

				    			if(failIfReceiveFailed(tag, waitingFor)) return;
								
								try {
									msg = node.usm.waitFor(mf, CHKInsertSender.this);
								} catch (DisconnectedException e) {
									Logger.normal(this, "Disconnected from " + waitingFor
											+ " while waiting for InsertReply on " + CHKInsertSender.this);
									return;
								}
								
				    			if(failIfReceiveFailed(tag, waitingFor)) return;
								
								if(msg == null) {
									// Second timeout.
									// Definitely caused by the next node, fatal.
									Logger.error(this, "Got second (local) timeout on "+CHKInsertSender.this+" from "+waitingFor);
									waitingFor.fatalTimeout();
									return;
								}
								
								if (msg.getSpec() == DMT.FNPRejectedTimeout) {
									// Next node timed out awaiting our DataInsert.
									// But we already sent it, so something is wrong. :(
									handleRejectedTimeout(msg, waitingFor);
									return;
								}

								if (msg.getSpec() == DMT.FNPRejectedOverload) {
									if(handleRejectedOverload(msg, waitingFor, tag)) {
										// Already set the status, and handle... will have unlocked the next node, so no need to call finished().
										return; // Don't try another node.
									}
									else continue;
								}

								if (msg.getSpec() == DMT.FNPRouteNotFound) {
									return; // Don't try another node.
								}
								
								if (msg.getSpec() == DMT.FNPDataInsertRejected) {
									handleDataInsertRejected(msg, waitingFor, tag);
									return; // Don't try another node.
								}
								
								if (msg.getSpec() != DMT.FNPInsertReply) {
									Logger.error(this, "Unknown reply: " + msg);
									return;
								} else {
									// Our task is complete, one node (quite deep), has accepted the insert.
									// The request will not be routed to any other nodes, this is where the data *should* be.
									// We will removeRoutingTo() after the node has sent the transfer completion notice, which never happens before the InsertReply.
									return;
								}
				            }
						}
						
					};
					
					// Wait for the timeout off-thread.
					node.executor.execute(r);
					// Meanwhile, finish() to update allTransfersCompleted and hence allow the CHKInsertHandler to send the message downstream.
					// We have already set the status code, this is necessary in order to avoid race conditions.
					// However since it is set to TIMED_OUT, we are allowed to set it again.
					finish(TIMED_OUT, next);
					return;
				}

				if (msg.getSpec() == DMT.FNPRejectedTimeout) {
					// Next node timed out awaiting our DataInsert.
					// But we already sent it, so something is wrong. :(
					handleRejectedTimeout(msg, next);
					return;
				}

				if (msg.getSpec() == DMT.FNPRejectedOverload) {
					if(handleRejectedOverload(msg, next, thisTag)) break;
					else continue;
				}

				if (msg.getSpec() == DMT.FNPRouteNotFound) {
					//RNF means that the HTL was not exhausted, but that the data will still be stored.
					handleRNF(msg, next, thisTag);
					break;
				}

				//Can occur after reception of the entire chk block
				if (msg.getSpec() == DMT.FNPDataInsertRejected) {
					handleDataInsertRejected(msg, next, thisTag);
					break;
				}
				
				if (msg.getSpec() != DMT.FNPInsertReply) {
					Logger.error(this, "Unknown reply: " + msg);
					finish(INTERNAL_ERROR, next);
					return;
				} else {
					// Our task is complete, one node (quite deep), has accepted the insert.
					// The request will not be routed to any other nodes, this is where the data *should* be.
					// We will removeRoutingTo() after the node has sent the transfer completion notice, which never happens before the InsertReply.
					finish(SUCCESS, next);
					return;
				}
			}
			if (logMINOR) Logger.debug(this, "Trying alternate node for insert");
		}
	}

    private void handleRejectedTimeout(Message msg, PeerNode next) {
    	// Some severe lag problem.
    	// However it is not fatal because we can be confident now that even if the DataInsert
    	// is delivered late, it will not be acted on. I.e. we are certain how many requests
    	// are running, which is what fatal timeouts are designed to deal with.
		Logger.warning(this, "Node timed out waiting for our DataInsert (" + msg + " from " + next
				+ ") after Accepted in insert - treating as fatal timeout");
		// Terminal overload
		// Try to propagate back to source
		next.localRejectedOverload("AfterInsertAcceptedRejectedTimeout", realTimeFlag);
		
		// We have always started the transfer by the time this is called, so we do NOT need to removeRoutingTo().
		finish(TIMED_OUT, next);
	}

	/** @return True if fatal i.e. we should try another node. */
	private boolean handleRejectedOverload(Message msg, PeerNode next, InsertTag thisTag) {
		// Probably non-fatal, if so, we have time left, can try next one
		if (msg.getBoolean(DMT.IS_LOCAL)) {
			next.localRejectedOverload("ForwardRejectedOverload6", realTimeFlag);
			if(logMINOR) Logger.minor(this,
					"Local RejectedOverload, moving on to next peer");
			// Give up on this one, try another
			return true;
		} else {
			forwardRejectedOverload();
		}
		return false; // Wait for any further response
	}

	private void handleRNF(Message msg, PeerNode next, InsertTag thisTag) {
		if(logMINOR) Logger.minor(this, "Rejected: RNF");
		short newHtl = msg.getShort(DMT.HTL);
		synchronized (this) {
			if (htl > newHtl)
				htl = newHtl;						
		}
		// Finished as far as this node is concerned - except for the data transfer, which will continue until it finishes.
		next.successNotOverload(realTimeFlag);
	}

	private void handleDataInsertRejected(Message msg, PeerNode next, InsertTag thisTag) {
		next.successNotOverload(realTimeFlag);
		short reason = msg
				.getShort(DMT.DATA_INSERT_REJECTED_REASON);
		if(logMINOR) Logger.minor(this, "DataInsertRejected: " + reason);
		if (reason == DMT.DATA_INSERT_REJECTED_VERIFY_FAILED) {
			if (fromStore) {
				// That's odd...
				Logger.error(this,"Verify failed on next node "
						+ next + " for DataInsert but we were sending from the store!");
			} else {
				try {
					if (!prb.allReceived())
						Logger.error(this,
								"Did not receive all packets but next node says invalid anyway!");
					else {
						// Check the data
						new CHKBlock(prb.getBlock(), headers,
								myKey);
						Logger.error(this,
								"Verify failed on " + next
								+ " but data was valid!");
					}
				} catch (CHKVerifyException e) {
					Logger.normal(this,
									"Verify failed because data was invalid");
				} catch (AbortedException e) {
					onReceiveFailed();
				}
			}
		} else if (reason == DMT.DATA_INSERT_REJECTED_RECEIVE_FAILED) {
			boolean recvFailed;
			synchronized(backgroundTransfers) {
				recvFailed = receiveFailed;
			}
			if (recvFailed) {
				if(logMINOR) Logger.minor(this, "Failed to receive data, so failed to send data");
			} else {
				try {
					if (prb.allReceived()) {
						// Probably caused by transient connectivity problems.
						// Only fatal timeouts warrant ERROR's because they indicate something seriously wrong that didn't result in a disconnection, and because they cause disconnections.
						Logger.warning(this, "Received all data but send failed to " + next);
					} else {
						if (prb.isAborted()) {
							Logger.normal(this, "Send failed: aborted: " + prb.getAbortReason() + ": " + prb.getAbortDescription());
						} else
							Logger.normal(this, "Send failed; have not yet received all data but not aborted: " + next);
					}
				} catch (AbortedException e) {
					onReceiveFailed();
				}
			}
		}
		Logger.error(this, "DataInsert rejected! Reason="
				+ DMT.getDataInsertRejectedReason(reason));
	}

	/** @return True if accepted, false if we should try another node. */
	private boolean waitAccepted(PeerNode next, InsertTag thisTag) {
		
		// We have not yet started the data transfer, so if we fail here we need to tell the tag we are not routing to next any more.
		
		Message msg = null;
		
        // Wait for ack or reject... will come before even a locally generated DataReply
    	MessageFilter mf = makeAcceptedRejectedFilter(next, ACCEPTED_TIMEOUT);
        
        /*
         * Because messages may be re-ordered, it is
         * entirely possible that we get a non-local RejectedOverload,
         * followed by an Accepted. So we must loop here.
         */
        
        while ((msg==null) || (msg.getSpec() != DMT.FNPAccepted)) {
        	
			try {
				msg = node.usm.waitFor(mf, this);
			} catch (DisconnectedException e) {
				Logger.normal(this, "Disconnected from " + next
						+ " while waiting for Accepted");
				next.noLongerRoutingTo(thisTag, false);
				break;
			}

			if(failIfReceiveFailed(thisTag, next)) return false;
			
			if (msg == null) {
				// Terminal overload
				// Try to propagate back to source
				if(logMINOR) Logger.minor(this, "Timeout");
				next.localRejectedOverload("Timeout3", realTimeFlag);
				// Try another node.
				forwardRejectedOverload();
    			handleAcceptedRejectedTimeout(next, thisTag);
				break;
			}
			
			if (msg.getSpec() == DMT.FNPRejectedOverload) {
				// Non-fatal - probably still have time left
				if (msg.getBoolean(DMT.IS_LOCAL)) {
					next.localRejectedOverload("ForwardRejectedOverload5", realTimeFlag);
					if(logMINOR) Logger.minor(this,
									"Local RejectedOverload, moving on to next peer");
					// Give up on this one, try another
					next.noLongerRoutingTo(thisTag, false);
					break;
				} else {
					forwardRejectedOverload();
				}
				continue;
			}
			
			if (msg.getSpec() == DMT.FNPRejectedLoop) {
				if(logMINOR) Logger.minor(this, "Rejected (loop) on "+this);
				next.successNotOverload(realTimeFlag);
				// Loop - we don't want to send the data to this one
				next.noLongerRoutingTo(thisTag, false);
				break;
			}
			
			if (msg.getSpec() != DMT.FNPAccepted) {
				Logger.error(this,
						"Unexpected message waiting for Accepted: "
								+ msg);
				next.noLongerRoutingTo(thisTag, false);
				break;
			}
			// Otherwise is an FNPAccepted
		}
        
        if((msg == null) || (msg.getSpec() != DMT.FNPAccepted)) return false;
        return true;
        
	}
	
	private MessageFilter makeAcceptedRejectedFilter(PeerNode next, int acceptedTimeout) {
        MessageFilter mfAccepted = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(acceptedTimeout).setType(DMT.FNPAccepted);
        MessageFilter mfRejectedLoop = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(acceptedTimeout).setType(DMT.FNPRejectedLoop);
        MessageFilter mfRejectedOverload = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(acceptedTimeout).setType(DMT.FNPRejectedOverload);
        
        // mfRejectedOverload must be the last thing in the or
        // So its or pointer remains null
        // Otherwise we need to recreate it below
        mfRejectedOverload.clearOr();
        return mfAccepted.or(mfRejectedLoop.or(mfRejectedOverload));
	}
	
	private final int TIMEOUT_AFTER_ACCEPTEDREJECTED_TIMEOUT = 60*1000;

	private void handleAcceptedRejectedTimeout(final PeerNode next, final InsertTag tag) {
		// It could still be running. So the timeout is fatal to the node.
		// This is a WARNING not an ERROR because it's possible that the problem is we simply haven't been able to send the message yet, because we don't use sendSync().
		// FIXME use a callback to rule this out and log an ERROR.
		Logger.warning(this, "Timeout awaiting Accepted/Rejected "+this+" to "+next);
		tag.handlingTimeout(next);
		// The node didn't accept the request. So we don't need to send them the data.
		// However, we do need to wait a bit longer to try to postpone the fatalTimeout().
		// Somewhat intricate logic to try to avoid fatalTimeout() if at all possible.
		MessageFilter mf = makeAcceptedRejectedFilter(next, TIMEOUT_AFTER_ACCEPTEDREJECTED_TIMEOUT);
		try {
			node.usm.addAsyncFilter(mf, new SlowAsyncMessageFilterCallback() {

				public void onMatched(Message m) {
					if(m.getSpec() == DMT.FNPRejectedLoop ||
							m.getSpec() == DMT.FNPRejectedOverload) {
						// Ok.
						tag.removeRoutingTo(next);
					} else {
						assert(m.getSpec() == DMT.FNPAccepted);
						// We are not going to send the DataInsert.
						// We have moved on, and we don't want inserts to fork unnecessarily.
			            MessageFilter mfTimeout = MessageFilter.create().setSource(next).setField(DMT.UID, uid).setTimeout(searchTimeout).setType(DMT.FNPRejectedTimeout);
			            try {
							node.usm.addAsyncFilter(mfTimeout, new AsyncMessageFilterCallback() {

								public void onMatched(Message m) {
									// Cool.
								}

								public boolean shouldTimeout() {
									return false;
								}

								public void onTimeout() {
									// Grrr!
									Logger.error(this, "Timed out awaiting FNPRejectedTimeout on insert to "+next);
									next.fatalTimeout(tag, false);
								}

								public void onDisconnect(PeerContext ctx) {
									tag.removeRoutingTo(next);
								}

								public void onRestarted(PeerContext ctx) {
									tag.removeRoutingTo(next);
								}
								
							}, CHKInsertSender.this);
						} catch (DisconnectedException e) {
							tag.removeRoutingTo(next);
						}
					}
				}

				public boolean shouldTimeout() {
					return false;
				}

				public void onTimeout() {
					Logger.error(this, "Fatal: No Accepted/Rejected for "+CHKInsertSender.this);
					next.fatalTimeout(tag, false);
				}

				public void onDisconnect(PeerContext ctx) {
					tag.removeRoutingTo(next);
				}

				public void onRestarted(PeerContext ctx) {
					tag.removeRoutingTo(next);
				}

				public int getPriority() {
					return NativeThread.NORM_PRIORITY;
				}
				
			}, this);
		} catch (DisconnectedException e) {
			tag.removeRoutingTo(next);
		}
	}

	private void startBackgroundTransfer(PeerNode node, PartiallyReceivedBlock prb, InsertTag tag) {
		BackgroundTransfer ac = new BackgroundTransfer(node, prb, tag);
		synchronized(backgroundTransfers) {
			backgroundTransfers.add(ac);
			backgroundTransfers.notifyAll();
		}
		ac.start();
	}
	
	private boolean hasForwardedRejectedOverload;
    
    synchronized boolean receivedRejectedOverload() {
    	return hasForwardedRejectedOverload;
    }
    
    /** Forward RejectedOverload to the request originator.
     * DO NOT CALL if have a *local* RejectedOverload.
     */
    private synchronized void forwardRejectedOverload() {
    	if(hasForwardedRejectedOverload) return;
    	hasForwardedRejectedOverload = true;
   		notifyAll();
	}
	
	private void setTransferTimedOut() {
		synchronized(this) {
			if(!transferTimedOut) {
				transferTimedOut = true;
				notifyAll();
			}
		}
	}
    
    /**
     * Finish the insert process. Will set status, wait for underlings to complete, and report success
     * if appropriate.
     * @param code The status code to set. 
     * @param next The node we successfully inserted to.
     */
    private void finish(int code, PeerNode next) {
    	if(logMINOR) Logger.minor(this, "Finished: "+code+" on "+this, new Exception("debug"));
     
    	// If there is an InsertReply, it always happens before the transfer completion notice.
    	// So we do NOT need to removeRoutingTo().
    	
        synchronized(this) {
        	if(allTransfersCompleted) return; // Already called. Doesn't prevent race condition resulting in the next bit running but that's not really a problem.
        	if((code == ROUTE_NOT_FOUND) && !sentRequest)
        		code = ROUTE_REALLY_NOT_FOUND;

        	if(status != NOT_FINISHED) {
        		if(status == RECEIVE_FAILED) {
        			if(code == SUCCESS)
        				Logger.error(this, "Request succeeded despite receive failed?! on "+this);
        		} else if(status != TIMED_OUT)
        			throw new IllegalStateException("finish() called with "+code+" when was already "+status);
        	} else {
                status = code;
        	}
        	
        	notifyAll();
        	if(logMINOR) Logger.minor(this, "Set status code: "+getStatusString()+" on "+uid);
        }
		
        boolean failedRecv = false; // receiveFailed is protected by backgroundTransfers but status by this
        // Now wait for transfers, or for downstream transfer notifications.
        // Note that even the data receive may not have completed by this point.
        boolean mustWait = false;
		synchronized(backgroundTransfers) {
			if (backgroundTransfers.isEmpty()) {
				if(logMINOR) Logger.minor(this, "No background transfers");
				failedRecv = receiveFailed;
			} else {
				mustWait = true;
			}
		}
		if(mustWait) { 
			waitForBackgroundTransferCompletions();
			synchronized(backgroundTransfers) {
				failedRecv = receiveFailed;
			}
		}
        
		synchronized(this) {
			// waitForBackgroundTransferCompletions() may have already set it.
			if(!allTransfersCompleted) {
				if(failedRecv)
					status = RECEIVE_FAILED;
				allTransfersCompleted = true;
				notifyAll();
			}
		}
        	
        if(status == SUCCESS && next != null)
        	next.onSuccess(true, false);
        
        if(logMINOR) Logger.minor(this, "Returning from finish()");
    }

    public synchronized int getStatus() {
        return status;
    }
    
    public synchronized short getHTL() {
        return htl;
    }
    
    public boolean failIfReceiveFailed(InsertTag tag, PeerNode next) {
    	synchronized(backgroundTransfers) {
    		if(!receiveFailed) return false;
    	}
    	if(logMINOR) Logger.minor(this, "Failing because receive failed on "+this);
    	if(tag != null && next != null) {
   			next.noLongerRoutingTo(tag, false);
    	}
    	return true;
    }

    /**
     * Called by CHKInsertHandler to notify that the receive has
     * failed.
     */
    public void onReceiveFailed() {
    	if(logMINOR) Logger.minor(this, "Receive failed on "+this);
    	synchronized(backgroundTransfers) {
    		receiveFailed = true;
    		backgroundTransfers.notifyAll();
    		// Locking is safe as UIDTag always taken last.
    		for(BackgroundTransfer t : backgroundTransfers)
    			t.thisTag.handlingTimeout(t.pn);
    	}
    	// Set status immediately.
    	// The code (e.g. waitForStatus()) relies on a status eventually being set,
    	// so we may as well set it here. The alternative is to set it in realRun()
    	// when we notice that receiveFailed = true.
    	synchronized(this) {
    		status = RECEIVE_FAILED;
    		allTransfersCompleted = true;
    		notifyAll();
    	}
    	// Do not call finish(), that can only be called on the main thread and it will block.
    }

    /**
     * @return The current status as a string
     */
    public synchronized String getStatusString() {
        if(status == SUCCESS)
            return "SUCCESS";
        if(status == ROUTE_NOT_FOUND)
            return "ROUTE NOT FOUND";
        if(status == NOT_FINISHED)
            return "NOT FINISHED";
        if(status == INTERNAL_ERROR)
        	return "INTERNAL ERROR";
        if(status == TIMED_OUT)
        	return "TIMED OUT";
        if(status == GENERATED_REJECTED_OVERLOAD)
        	return "GENERATED REJECTED OVERLOAD";
        if(status == ROUTE_REALLY_NOT_FOUND)
        	return "ROUTE REALLY NOT FOUND";
        return "UNKNOWN STATUS CODE: "+status;
    }

	public synchronized boolean sentRequest() {
		return sentRequest;
	}
		
		private void waitForBackgroundTransferCompletions() {
			try {
				freenet.support.Logger.OSThread.logPID(this);
				if(logMINOR) Logger.minor(this, "Waiting for background transfer completions: "+this);
				
				// We must presently be at such a stage that no more background transfers will be added.
				
				BackgroundTransfer[] transfers;
				synchronized(backgroundTransfers) {
					transfers = new BackgroundTransfer[backgroundTransfers.size()];
					transfers = backgroundTransfers.toArray(transfers);
				}
				
				// Wait for the outgoing transfers to complete.
				if(!waitForBackgroundTransfers(transfers)) {
					setTransferTimedOut();
					return;
				}
			} finally {
				synchronized(CHKInsertSender.this) {
					allTransfersCompleted = true;
					CHKInsertSender.this.notifyAll();
				}
			}
		}

		/**
		 * Block until all transfers have reached a final-terminal state (success/failure). On success this means that a
		 * successful 'received-notification' has been received.
		 * @return True if all background transfers were successful.
		 */
		private boolean waitForBackgroundTransfers(BackgroundTransfer[] transfers) {
			long start = System.currentTimeMillis();
			// Generous deadline so we catch bugs more obviously
			long deadline = start + transferCompletionTimeout * 3;
			// MAYBE all done
			while(true) {
				if(System.currentTimeMillis() > deadline) {
					// NORMAL priority because it is normally caused by a transfer taking too long downstream, and that doesn't usually indicate a bug.
					Logger.normal(this, "Timed out waiting for background transfers! Probably caused by async filter not getting a timeout notification! DEBUG ME!");
					return false;
				}
				//If we want to be sure to exit as-soon-as the transfers are done, then we must hold the lock while we check.
				synchronized(backgroundTransfers) {
					if(receiveFailed) return false;
					
					boolean noneRouteable = true;
					boolean completedTransfers = true;
					boolean completedNotifications = true;
					boolean someFailed = false;
					for(int i=0;i<transfers.length;i++) {
						if(!transfers[i].pn.isRoutable()) {
							if(logMINOR)
								Logger.minor(this, "Ignoring transfer to "+transfers[i].pn+" for "+this+" as not routable");
							continue;
						}
						noneRouteable = false;
						if(!transfers[i].completedTransfer) {
							if(logMINOR)
								Logger.minor(this, "Waiting for transfer completion to "+transfers[i].pn+" : "+transfers[i]);
							//must wait
							completedTransfers = false;
							break;
						}
						if (!transfers[i].receivedCompletionNotice) {
							if(logMINOR)
								Logger.minor(this, "Waiting for completion notice from "+transfers[i].pn+" : "+transfers[i]);
							//must wait
							completedNotifications = false;
							break;
						}
						if (!transfers[i].completionSucceeded)
							someFailed = true;
					}
					if(noneRouteable) return false;
					if(completedTransfers && completedNotifications) return !someFailed;
					
					if(logMINOR) Logger.minor(this, "Waiting: transfer completion=" + completedTransfers + " notification="+completedNotifications); 
					try {
						backgroundTransfers.wait(100*1000);
					} catch (InterruptedException e) {
						// Ignore
					}
				}				
			}
		}


	public synchronized boolean completed() {
		return allTransfersCompleted;
	}

	/** Block until status has been set to something other than NOT_FINISHED */
	public synchronized void waitForStatus() {
		while(status == NOT_FINISHED) {
			try {
				CHKInsertSender.this.wait(100*1000);
			} catch (InterruptedException e) {
				// Ignore
			}
		}
	}

	public boolean anyTransfersFailed() {
		return transferTimedOut;
	}

	public byte[] getPubkeyHash() {
		return headers;
	}

	public byte[] getHeaders() {
		return headers;
	}

	public long getUID() {
		return uid;
	}

	private final Object totalBytesSync = new Object();
	private int totalBytesSent;
	
	public void sentBytes(int x) {
		synchronized(totalBytesSync) {
			totalBytesSent += x;
		}
		node.nodeStats.insertSentBytes(false, x);
	}
	
	public int getTotalSentBytes() {
		synchronized(totalBytesSync) {
			return totalBytesSent;
		}
	}
	
	private int totalBytesReceived;
	
	public void receivedBytes(int x) {
		synchronized(totalBytesSync) {
			totalBytesReceived += x;
		}
		node.nodeStats.insertReceivedBytes(false, x);
	}
	
	public int getTotalReceivedBytes() {
		synchronized(totalBytesSync) {
			return totalBytesReceived;
		}
	}

	public void sentPayload(int x) {
		node.sentPayload(x);
		node.nodeStats.insertSentBytes(false, -x);
	}

	public boolean failedReceive() {
		return receiveFailed;
	}

	public synchronized boolean startedSendingData() {
		return !backgroundTransfers.isEmpty();
	}

	public int getPriority() {
		return NativeThread.HIGH_PRIORITY;
	}

	public PeerNode[] getRoutedTo() {
		return this.nodesRoutedTo.toArray(new PeerNode[nodesRoutedTo.size()]);
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.simulator;

import java.io.File;

import freenet.crypt.DummyRandomSource;
import freenet.io.comm.DMT;
import freenet.io.comm.DisconnectedException;
import freenet.io.comm.Message;
import freenet.io.comm.MessageFilter;
import freenet.io.comm.NotConnectedException;
import freenet.io.comm.PeerParseException;
import freenet.io.comm.ReferenceSignatureVerificationException;
import freenet.node.FSParseException;
import freenet.node.Location;
import freenet.node.Node;
import freenet.node.NodeInitException;
import freenet.node.NodeStarter;
import freenet.node.PeerNode;
import freenet.node.DarknetPeerNode.FRIEND_TRUST;
import freenet.node.SyncSendWaitedTooLongException;
import freenet.support.Executor;
import freenet.support.Logger;
import freenet.support.PooledExecutor;
import freenet.support.Logger.LogLevel;
import freenet.support.LoggerHook.InvalidThresholdException;
import freenet.support.math.BootstrappingDecayingRunningAverage;
import freenet.support.math.RunningAverage;

/**
 * @author amphibian
 * 
 * Create a mesh of nodes
 * Connect them in a s.w. network (rather than just letting them sort out their locations)
 * Then run some cross-peer challenge/response pings.
 */
public class RealNodeSecretPingTest {

    //static final int NUMBER_OF_NODES = 150;
	static final int NUMBER_OF_NODES = 15;
    static final short MAX_HTL = (short)6;
	static final int DEGREE = 5;
	
	static final short PING_HTL = 6;
	static final short DAWN_HTL = 4;
	static final int SECRETPONG_TIMEOUT=5000;
	static final long storeSize = 1024*1024;
	
	static final FRIEND_TRUST trust = FRIEND_TRUST.LOW;

	public static int DARKNET_PORT_BASE = RealNodeRoutingTest.DARKNET_PORT_END;
	public static final int DARKNET_PORT_END = DARKNET_PORT_BASE + NUMBER_OF_NODES;
	
    public static void main(String[] args) throws FSParseException, PeerParseException, InvalidThresholdException, NodeInitException, ReferenceSignatureVerificationException {
        //Logger.setupStdoutLogging(LogLevel.NORMAL, "freenet.node.CPUAdjustingSwapRequestInterval:minor" /*"freenet.node.LocationManager:debug,freenet.node.FNPPacketManager:normal,freenet.io.comm.MessageCore:debug"*/);
        System.out.println("SecretPing (CRAM) test using real nodes:");
        System.out.println();
        String wd = "realNodeSecretPingTest";
        new File(wd).mkdir();
        //NOTE: globalTestInit returns in ignored random source
        NodeStarter.globalTestInit(wd, false, LogLevel.ERROR, "freenet.node.Location:normal,freenet.node.simulator.RealNodeSecretPingTest:normal,freenet.node.NetworkIDManager:normal", true);

        DummyRandomSource random = new DummyRandomSource();
        //DiffieHellman.init(random);
        Node[] nodes = new Node[NUMBER_OF_NODES];
        Logger.normal(RealNodeRoutingTest.class, "Creating nodes...");
        Executor executor = new PooledExecutor();
		
		//Allow secret pings, but don't automatically send them (this is the test for them!)
		freenet.node.NetworkIDManager.disableSecretPings=false;
		freenet.node.NetworkIDManager.disableSecretPinger=true;
		
        for(int i=0;i<NUMBER_OF_NODES;i++) {
            nodes[i] = 
            	NodeStarter.createTestNode(DARKNET_PORT_BASE+i, 0, wd, true, MAX_HTL, 0 /* no dropped packets */, random, executor, 500*NUMBER_OF_NODES, storeSize, true, true, false, false, false, true, true, 0, true, false, true, false, null);
            Logger.normal(RealNodeRoutingTest.class, "Created node "+i);
        }
        Logger.normal(RealNodeRoutingTest.class, "Created "+NUMBER_OF_NODES+" nodes");
        // Now link them up
        makeKleinbergNetwork(nodes);
        Logger.normal(RealNodeRoutingTest.class, "Added small-world links");
        
        for(int i=0;i<NUMBER_OF_NODES;i++)
            nodes[i].start(false);
		
        // Now sit back and watch the fireworks!
        int cycleNumber = 0;
        RunningAverage avg2 = new BootstrappingDecayingRunningAverage(0.0, 0.0, 1.0, 100, null);
        while(true) {
            cycleNumber++;
			
			try {
                Thread.sleep(2000);
            } catch (InterruptedException e) {
                // Ignore
            }
			
			Node source = nodes[random.nextInt(NUMBER_OF_NODES)];
			PeerNode verify = source.peers.getRandomPeer();
			PeerNode pathway = source.peers.getRandomPeer(verify);
			
			Logger.error(source, "verify ("+getPortNumber(verify)+") through: "+getPortNumber(pathway)+"; so far "+avg2.currentValue());
			
			long uid=random.nextLong();
			long secret=random.nextLong();
			
			if (verify==null) {
				Logger.error(source, "verify peernode is null");
				continue;
			}
			
			if (pathway==null) {
				Logger.error(source, "pathway peernode is null");
				continue;
			}
			
			try {
				//Send the FNPStoreSecret message to the 'verify' node
				verify.sendSync(DMT.createFNPStoreSecret(uid, secret), null, false);
				
				if (!getAck(source, verify, uid)) {
					Logger.error(source, "did not get storesecret ack for "+uid);
					avg2.report(0.0);
					continue;
				}
				
				//Send the request for the secret through the 'pathway' node.
				pathway.sendSync(DMT.createFNPSecretPing(uid, verify.getLocation(), PING_HTL, DAWN_HTL, 0, verify.getIdentity()), null, false);
				
				long result=getSecretPingResponse(source, pathway, uid);
				if (result!=secret) {
					Logger.error(source, "not matched: "+secret+" != "+result);
					avg2.report(0.0);
				} else {
					Logger.error(source, "match: "+secret);
					avg2.report(1.0);
				}
			} catch (NotConnectedException e) {
				Logger.error(source, "what?",e);
				avg2.report(0.0);
			} catch (DisconnectedException e) {
				Logger.error(source, "huh?",e);
				avg2.report(0.0);
			} catch (SyncSendWaitedTooLongException e) {
				Logger.error(source, "eh?", e);
				avg2.report(0.0);
			}
        }
    }
	
	private static boolean getAck(Node source, PeerNode pathway, long uid) throws DisconnectedException {
		//wait for an accepted
		MessageFilter mfAccepted = MessageFilter.create().setSource(pathway).setField(DMT.UID, uid).setTimeout(SECRETPONG_TIMEOUT).setType(DMT.FNPAccepted);
		Message msg = source.getUSM().waitFor(mfAccepted, null);
		
		if (msg==null) {
			return false;
		}
		
		if (msg.getSpec() == DMT.FNPAccepted) {
			return true;
		}
		
		Logger.error(source, "got "+msg);
		return false;
	}
	
	private static long getSecretPingResponse(Node source, PeerNode pathway, long uid) throws DisconnectedException {
		//wait for a reject or pong
		MessageFilter mfPong = MessageFilter.create().setSource(pathway).setField(DMT.UID, uid).setTimeout(SECRETPONG_TIMEOUT).setType(DMT.FNPSecretPong);
		MessageFilter mfRejectLoop = MessageFilter.create().setSource(pathway).setField(DMT.UID, uid).setTimeout(SECRETPONG_TIMEOUT).setType(DMT.FNPRejectedLoop);
		Message msg = source.getUSM().waitFor(mfPong.or(mfRejectLoop), null);
		
		if (msg==null) {
			Logger.error(source, "fatal timeout in waiting for secretpong from "+getPortNumber(pathway));
			return -2;
		}
		
		if (msg.getSpec() == DMT.FNPSecretPong) {
			int suppliedCounter=msg.getInt(DMT.COUNTER);
			long secret=msg.getLong(DMT.SECRET);
			Logger.normal(source, "got secret, counter="+suppliedCounter);
			return secret;
		}
		
		if (msg.getSpec() == DMT.FNPRejectedLoop) {
			Logger.error(source, "top level secret ping should not reject!: "+getPortNumber(source)+" -> "+getPortNumber(pathway));
			return -1;
		}
		
		return -3;
	}
	
	/*
	 Borrowed from mrogers simulation code (February 6, 2008)
	 */
	static void makeKleinbergNetwork (Node[] nodes)
	{
		for (int i=0; i<nodes.length; i++) {
			Node a = nodes[i];
			// Normalise the probabilities
			double norm = 0.0;
			for (int j=0; j<nodes.length; j++) {
				Node b = nodes[j];
				if (a.getLocation() == b.getLocation()) continue;
				norm += 1.0 / distance (a, b);
			}
			// Create DEGREE/2 outgoing connections
			for (int k=0; k<nodes.length; k++) {
				Node b = nodes[k];
				if (a.getLocation() == b.getLocation()) continue;
				double p = 1.0 / distance (a, b) / norm;
				for (int n = 0; n < DEGREE / 2; n++) {
					if (Math.random() < p) {
						try {
							a.connect (b, trust);
							b.connect (a, trust);
						} catch (FSParseException e) {
							Logger.error(RealNodeSecretPingTest.class, "cannot connect!!!!", e);
						} catch (PeerParseException e) {
							Logger.error(RealNodeSecretPingTest.class, "cannot connect #2!!!!", e);
						} catch (freenet.io.comm.ReferenceSignatureVerificationException e) {
							Logger.error(RealNodeSecretPingTest.class, "cannot connect #3!!!!", e);
						}
						break;
					}
				}
			}
		}
	}
	
	static double distance(Node a, Node b) {
		double aL=a.getLocation();
		double bL=b.getLocation();
		return Location.distance(aL, bL);
	}
	
	static String getPortNumber(PeerNode p) {
		if (p == null || p.getPeer() == null)
			return "null";
		return Integer.toString(p.getPeer().getPort());
	}
	
	static String getPortNumber(Node n) {
		if (n == null)
			return "null";
		return Integer.toString(n.getDarknetPortNumber());
	}
	
}
package freenet.support;

import java.io.BufferedOutputStream;
import java.io.Closeable;
import java.io.DataInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.PrintStream;
import java.net.InetAddress;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.Date;
import java.util.GregorianCalendar;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.Locale;
import java.util.StringTokenizer;
import java.util.TimeZone;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.zip.GZIPOutputStream;

import freenet.node.SemiOrderedShutdownHook;
import freenet.node.Version;
import freenet.support.io.FileUtil;

/**
 * Converted the old StandardLogger to Ian's loggerhook interface.
 * 
 * @author oskar
 */
public class FileLoggerHook extends LoggerHook implements Closeable {

	/** Verbosity types */
	public static final int DATE = 1,
		CLASS = 2,
		HASHCODE = 3,
		THREAD = 4,
		PRIORITY = 5,
		MESSAGE = 6,
		UNAME = 7;

	private volatile boolean closed = false;
	private boolean closedFinished = false;

	protected int INTERVAL = Calendar.MINUTE;
	protected int INTERVAL_MULTIPLIER = 5;

        private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	/** Name of the local host (called uname in Unix-like operating systems). */
	private static String uname;
	static {
		uname = "unknown";
	}

	static synchronized final void getUName() {
		if(!uname.equals("unknown")) return;
		System.out.println("Getting uname for logging");
		try {
			InetAddress addr = InetAddress.getLocalHost();
			if (addr != null) {
				uname =
					new StringTokenizer(addr.getHostName(), ".").nextToken();
			}
		} catch (Exception e) {
			// Ignored.
		}
	}
	
	private DateFormat df;
	private int[] fmt;
	private String[] str;

	/** Stream to write data to (compressed if rotate is on) */
	protected OutputStream logStream;
	/** Other stream to write data to (may be null) */
	protected OutputStream altLogStream;

	protected final boolean logOverwrite;

	/* Base filename for rotating logs */
	protected String baseFilename = null;
	
	protected File latestFile;
	protected File previousFile;

	/* Whether to redirect stdout */
	protected boolean redirectStdOut = false;
	/* Whether to redirect stderr */
	protected boolean redirectStdErr = false;

	protected final int MAX_LIST_SIZE;
	protected long MAX_LIST_BYTES = 10 * (1 << 20);
	protected long LIST_WRITE_THRESHOLD;

	/**
	 * Something weird happens when the disk gets full, also we don't want to
	 * block So run the actual write on another thread
	 * 
	 * Unfortunately, we can't use ConcurrentBlockingQueue because we need to dump stuff when the queue gets
	 * too big.
	 * 
	 * FIXME PERFORMANCE: Using an ArrayBlockingQueue avoids some unnecessary memory allocations, but it 
	 * means we have to take two locks. 
	 * Seriously consider reverting 88268b99856919df0d42c2787d9ea3674a9f6f0d..e359b4005ef728a159fdee988c483de8ce8f3f6b
	 * to go back to one lock and a LinkedList.
	 */
	protected final ArrayBlockingQueue<byte[]> list;
	protected long listBytes = 0;

	long maxOldLogfilesDiskUsage;
	protected final LinkedList<OldLogFile> logFiles = new LinkedList<OldLogFile>();
	private long oldLogFilesDiskSpaceUsage = 0;

	private static class OldLogFile {
		public OldLogFile(File currentFilename, long startTime, long endTime, long length) {
			this.filename = currentFilename;
			this.start = startTime;
			this.end = endTime;
			this.size = length;
		}
		final File filename;
		final long start; // inclusive
		final long end; // exclusive
		final long size;
	}
	
	public void setMaxListBytes(long len) {
		synchronized(list) {
			MAX_LIST_BYTES = len;
			LIST_WRITE_THRESHOLD = MAX_LIST_BYTES / 4;
		}
	}

	public void setInterval(String intervalName) throws IntervalParseException {
		StringBuilder sb = new StringBuilder(intervalName.length());
		for(int i=0;i<intervalName.length();i++) {
			char c = intervalName.charAt(i);
			if(!Character.isDigit(c)) break;
			sb.append(c);
		}
		if(sb.length() > 0) {
			String prefix = sb.toString();
			intervalName = intervalName.substring(prefix.length());
			INTERVAL_MULTIPLIER = Integer.parseInt(prefix);
		} else {
			INTERVAL_MULTIPLIER = 1;
		}
		if (intervalName.endsWith("S")) {
			intervalName = intervalName.substring(0, intervalName.length()-1);
		}
		if (intervalName.equalsIgnoreCase("MINUTE"))
			INTERVAL = Calendar.MINUTE;
		else if (intervalName.equalsIgnoreCase("HOUR"))
			INTERVAL = Calendar.HOUR;
		else if (intervalName.equalsIgnoreCase("DAY"))
			INTERVAL = Calendar.DAY_OF_MONTH;
		else if (intervalName.equalsIgnoreCase("WEEK"))
			INTERVAL = Calendar.WEEK_OF_YEAR;
		else if (intervalName.equalsIgnoreCase("MONTH"))
			INTERVAL = Calendar.MONTH;
		else if (intervalName.equalsIgnoreCase("YEAR"))
			INTERVAL = Calendar.YEAR;
		else
			throw new IntervalParseException("invalid interval " + intervalName);
		System.out.println("Set interval to "+INTERVAL+" and multiplier to "+INTERVAL_MULTIPLIER);
	}

	public static class IntervalParseException extends Exception {

		private static final long serialVersionUID = 69847854744673572L;

		public IntervalParseException(String string) {
			super(string);
		}

	}
	
	/**
	 * The extra parameter int digit is to be used for creating a logfile name
	 * when a log exists already with the same date.
	 * @param c
	 * @param digit
	 *			log file name suffix. ignored if this is {@code < 0}
	 * @param compressed
	 * @return
	 */
	protected String getHourLogName(Calendar c, int digit, boolean compressed){
		StringBuilder buf = new StringBuilder(50);
		buf.append(baseFilename).append('-');
		buf.append(Version.buildNumber());
		buf.append('-');
		buf.append(c.get(Calendar.YEAR)).append('-');
		pad2digits(buf, c.get(Calendar.MONTH) + 1);
		buf.append('-');
		pad2digits(buf, c.get(Calendar.DAY_OF_MONTH));
		buf.append('-');
		pad2digits(buf, c.get(Calendar.HOUR_OF_DAY));
		if (INTERVAL == Calendar.MINUTE) {
			buf.append('-');
			pad2digits(buf, c.get(Calendar.MINUTE));
		}
		if (digit > 0) {
			buf.append("-");
			buf.append(digit);
		}
		buf.append(".log");
		if(compressed) buf.append(".gz");
		return buf.toString();
	}

	private StringBuilder pad2digits(StringBuilder buf, int x) {
		String s = Integer.toString(x);
		if (s.length() == 1) {
			buf.append('0');
		}
		buf.append(s);
		return buf;
	}
	
	// Unless we are writing flat out, everything will hit disk within this period.
	private long flushTime = 1000; // Default is 1 second. Will be set by setMaxBacklogNotBusy().

	class WriterThread extends Thread {
		WriterThread() {
			super("Log File Writer Thread");
		}

		@Override
		@SuppressWarnings("fallthrough")
		public void run() {
			File currentFilename = null;
			byte[] o = null;
			long thisTime;
			long lastTime = -1;
			long startTime;
			long nextHour = -1;
			GregorianCalendar gc = null;
			if (baseFilename != null) {
				latestFile = new File(baseFilename+"-latest.log");
				previousFile = new File(baseFilename+"-previous.log");
				gc = new GregorianCalendar();
				switch (INTERVAL) {
					case Calendar.YEAR :
						gc.set(Calendar.MONTH, 0);
					case Calendar.MONTH :
						gc.set(Calendar.DAY_OF_MONTH, 0);
					case Calendar.WEEK_OF_YEAR :
						if (INTERVAL == Calendar.WEEK_OF_YEAR)
							gc.set(Calendar.DAY_OF_WEEK, 0);
					case Calendar.DAY_OF_MONTH :
						gc.set(Calendar.HOUR, 0);
					case Calendar.HOUR :
						gc.set(Calendar.MINUTE, 0);
					case Calendar.MINUTE :
						gc.set(Calendar.SECOND, 0);
						gc.set(Calendar.MILLISECOND, 0);
				}
				if(INTERVAL_MULTIPLIER > 1) {
					int x = gc.get(INTERVAL);
					gc.set(INTERVAL, (x / INTERVAL_MULTIPLIER) * INTERVAL_MULTIPLIER);
				}
				findOldLogFiles(gc);
				currentFilename = new File(getHourLogName(gc, -1, true));
				synchronized(logFiles) {
					if((!logFiles.isEmpty()) && logFiles.getLast().filename.equals(currentFilename)) {
						logFiles.removeLast();
					}
				}
				logStream = openNewLogFile(currentFilename, true);
				if(latestFile != null) {
					altLogStream = openNewLogFile(latestFile, false);
				}
				System.err.println("Created log files");
				startTime = gc.getTimeInMillis();
		    	if(logMINOR)
		    		Logger.minor(this, "Start time: "+gc+" -> "+startTime);
				lastTime = startTime;
				gc.add(INTERVAL, INTERVAL_MULTIPLIER);
				nextHour = gc.getTimeInMillis();
			}
			long timeWaitingForSync = -1;
			long flush;
			synchronized(this) {
				flush = flushTime;
			}
			while (true) {
				try {
					thisTime = System.currentTimeMillis();
					if (baseFilename != null) {
						if ((thisTime > nextHour) || switchedBaseFilename) {
							currentFilename = rotateLog(currentFilename, lastTime, nextHour, gc);
							
							gc.add(INTERVAL, INTERVAL_MULTIPLIER);
							lastTime = nextHour;
							nextHour = gc.getTimeInMillis();

							if(switchedBaseFilename) {
								synchronized(FileLoggerHook.class) {
									switchedBaseFilename = false;
								}
							}
						}
					}
					boolean died = false;
					boolean timeoutFlush = false;
					synchronized (list) {
						flush = flushTime;
						long maxWait;
						if(timeWaitingForSync == -1)
							maxWait = Long.MAX_VALUE;
						else
							maxWait = timeWaitingForSync + flush;
						o = list.poll();
						while(o == null) {
							if (closed) {
								died = true;
								break;
							}
							try {
								if(thisTime < maxWait) {
									// Wait no more than 500ms since the CloserThread might be waiting for closedFinished.
									list.wait(Math.min(500, (int)(Math.min(maxWait-thisTime, Integer.MAX_VALUE))));
									thisTime = System.currentTimeMillis();
									if(listBytes < LIST_WRITE_THRESHOLD) {
										// Don't write at all until the lower bytes threshold is exceeded, or the time threshold is.
										assert((listBytes == 0) == (list.peek() == null));
										if(listBytes != 0 && maxWait == Long.MAX_VALUE)
											maxWait = thisTime + flush;
										if(closed) // If closing, write stuff ASAP.
											o = list.poll();
										else if(maxWait != Long.MAX_VALUE) {
											continue;
										}
									} else {
										// Do NOT use list.poll(timeout) because it uses a separate lock.
										o = list.poll();
									}
								}
							} catch (InterruptedException e) {
								// Ignored.
							}
							if(o == null) {
								if(timeWaitingForSync == -1) {
									timeWaitingForSync = thisTime;
									maxWait = thisTime + flush;
								}
								if(thisTime >= maxWait) {
									timeoutFlush = true;
									timeWaitingForSync = -1; // We have stuff to write, we are no longer waiting.
									break;
								}
							} else break;
						}
						if(o != null) {
							listBytes -= o.length + LINE_OVERHEAD;
						}
					}
					if(timeoutFlush || died) {
						// Flush to disk 
						myWrite(logStream, null);
				        if(altLogStream != null)
				        	myWrite(altLogStream, null);
					}
					if(died) {
						try {
							logStream.close();
						} catch (IOException e) {
							System.err.println("Failed to close log stream: "+e);
						}
						if(altLogStream != null) {
							try {
								altLogStream.close();
							} catch (IOException e) {
								System.err.println("Failed to close compressed log stream: "+e);
							}
						}
						synchronized(list) {
							closedFinished = true;
							list.notifyAll();
						}
						return;
					}
					if(o == null) continue;
					myWrite(logStream,  o);
			        if(altLogStream != null)
			        	myWrite(altLogStream, o);
				} catch (OutOfMemoryError e) {
					System.err.println(e.getClass());
					System.err.println(e.getMessage());
					e.printStackTrace();
				    // FIXME
					//freenet.node.Main.dumpInterestingObjects();
				} catch (Throwable t) {
					System.err.println("FileLoggerHook log writer caught " + t);
					t.printStackTrace(System.err);
				}
			}
		}

		private File rotateLog(File currentFilename, long lastTime, long nextHour, GregorianCalendar gc) {
	        // Switch logs
	        try {
	        	logStream.flush();
	        	if(altLogStream != null) altLogStream.flush();
	        } catch (IOException e) {
	        	System.err.println(
	        		"Flushing on change caught " + e);
	        }
	        try {
	        	logStream.close();
	        } catch (IOException e) {
	        	System.err.println(
	        			"Closing on change caught " + e);
	        }
	        long length = currentFilename.length();
	        OldLogFile olf = new OldLogFile(currentFilename, lastTime, nextHour, length);
	        synchronized(logFiles) {
	        	logFiles.addLast(olf);
	        }
	        oldLogFilesDiskSpaceUsage += length;
	        trimOldLogFiles();
	        // Rotate primary log stream
	        currentFilename = new File(getHourLogName(gc, -1, true));
	        logStream = openNewLogFile(currentFilename, true);
	        if(latestFile != null) {
	        	try {
	        		altLogStream.close();
	        	} catch (IOException e) {
	        		System.err.println(
	        				"Closing alt on change caught " + e);
	        	}
	        	if(previousFile != null && previousFile.exists())
	        		FileUtil.renameTo(latestFile, previousFile);
	        	latestFile.delete();
	        	altLogStream = openNewLogFile(latestFile, false);
	        }
	        return currentFilename;
        }

		// Check every minute
		static final int maxSleepTime = 60 * 1000;
		/**
		 * @param b
		 *            the bytes to write, null to flush
		 */
		protected void myWrite(OutputStream os, byte[] b) {
			long sleepTime = 1000;
			while (true) {
				boolean thrown = false;
				try {
					if (b != null)
						os.write(b);
					else
						os.flush();
				} catch (IOException e) {
					System.err.println(
						"Exception writing to log: "
							+ e
							+ ", sleeping "
							+ sleepTime);
					thrown = true;
				}
				if (thrown) {
					try {
						Thread.sleep(sleepTime);
					} catch (InterruptedException e) {
					}
					sleepTime += sleepTime;
					if (sleepTime > maxSleepTime)
						sleepTime = maxSleepTime;
				} else
					return;
			}
		}

		protected OutputStream openNewLogFile(File filename, boolean compress) {
			while (true) {
				long sleepTime = 1000;
				try {
					OutputStream o = new FileOutputStream(filename, !logOverwrite);
					if(compress) {
						// buffer -> gzip -> buffer -> file
						o = new BufferedOutputStream(o, 512*1024); // to file
						o = new GZIPOutputStream(o);
						// gzip block size is 32kB
						o = new BufferedOutputStream(o, 65536); // to gzipper
					} else {
						// buffer -> file
						o = new BufferedOutputStream(o, 512*1024);
					}
					return o;
				} catch (IOException e) {
					System.err.println(
						"Could not create FOS " + filename + ": " + e);
					System.err.println(
						"Sleeping " + sleepTime / 1000 + " seconds");
					try {
						Thread.sleep(sleepTime);
					} catch (InterruptedException ex) {
					}
					sleepTime += sleepTime;
				}
			}
		}
	}

	protected int runningCompressors = 0;
	protected Object runningCompressorsSync = new Object();

	private Date myDate = new Date();

	/**
	 * Create a Logger to append to the given file. If the file does not exist
	 * it will be created.
	 * 
	 * @param filename
	 *            the name of the file to log to.
	 * @param fmt
	 *            log message format string
	 * @param dfmt
	 *            date format string
	 * @param threshold
	 *            Lowest logged priority
	 * @param assumeWorking
	 *            If false, check whether stderr and stdout are writable and if
	 *            not, redirect them to the log file
	 * @exception IOException
	 *                if the file couldn't be opened for append.
	 * @throws IntervalParseException 
	 */
	public FileLoggerHook(
		String filename,
		String fmt,
		String dfmt,
		String logRotateInterval,
		LogLevel threshold,
		boolean assumeWorking,
		boolean logOverwrite,
		long maxOldLogfilesDiskUsage, int maxListSize)
		throws IOException, IntervalParseException {
		this(
			false,
			filename,
			fmt,
			dfmt,
			logRotateInterval,
			threshold,
			assumeWorking,
			logOverwrite,
			maxOldLogfilesDiskUsage,
			maxListSize);
	}
	
	private final Object trimOldLogFilesLock = new Object();
	
	public void trimOldLogFiles() {
		synchronized(trimOldLogFilesLock) {
			while(oldLogFilesDiskSpaceUsage > maxOldLogfilesDiskUsage) {
				OldLogFile olf;
				// TODO: creates a double lock situation, but only here. I think this is okay because the inner lock is only used for trivial things.
				synchronized(logFiles) {
					if(logFiles.isEmpty()) {
						System.err.println("ERROR: INCONSISTENT LOGGER TOTALS: Log file list is empty but still used "+oldLogFilesDiskSpaceUsage+" bytes!");
					}
					olf = logFiles.removeFirst();
				}
				olf.filename.delete();
				oldLogFilesDiskSpaceUsage -= olf.size;
		    	if(logMINOR)
		    		Logger.minor(this, "Deleting "+olf.filename+" - saving "+olf.size+
						" bytes, disk usage now: "+oldLogFilesDiskSpaceUsage+" of "+maxOldLogfilesDiskUsage);
			}
		}
	}

	/** Initialize oldLogFiles */
	public void findOldLogFiles(GregorianCalendar gc) {
		File currentFilename = new File(getHourLogName(gc, -1, true));
		System.out.println("Finding old log files. New log file is "+currentFilename);
		File numericSameDateFilename;
		int slashIndex = baseFilename.lastIndexOf(File.separatorChar);
		File dir;
		String prefix;
		if(slashIndex == -1) {
			dir = new File(System.getProperty("user.dir"));
			prefix = baseFilename.toLowerCase();
		} else {
			dir = new File(baseFilename.substring(0, slashIndex));
			prefix = baseFilename.substring(slashIndex+1).toLowerCase();
		}
		File[] files = dir.listFiles();
		if(files == null) return;
		java.util.Arrays.sort(files);
		long lastStartTime = -1;
		File oldFile = null;
        if(latestFile.exists())
        	FileUtil.renameTo(latestFile, previousFile);

		for(int i=0;i<files.length;i++) {
			File f = files[i];
			String name = f.getName();
			if(name.toLowerCase().startsWith(prefix)) {
				if(name.equals(previousFile.getName()) || name.equals(latestFile.getName())) {
					continue;
				}
				if(!name.endsWith(".log.gz")) {
					if(logMINOR) Logger.minor(this, "Does not end in .log.gz: "+name);
					f.delete();
					continue;
				} else {
					name = name.substring(0, name.length()-".log.gz".length());
				}
				name = name.substring(prefix.length());
				if((name.length() == 0) || (name.charAt(0) != '-')) {
					if(logMINOR) Logger.minor(this, "Deleting unrecognized: "+name+" ("+f.getPath()+ ')');
					f.delete();
					continue;
				} else
					name = name.substring(1);
				String[] tokens = name.split("-");
				int[] nums = new int[tokens.length];
				for(int j=0;j<tokens.length;j++) {
					try {
						nums[j] = Integer.parseInt(tokens[j]);
					} catch (NumberFormatException e) {
						Logger.normal(this, "Could not parse: "+tokens[j]+" into number from "+name);
						// Broken
						f.delete();
						continue;
					}
				}
				// First field: version
				if(nums[0] != Version.buildNumber()) {
					if(logMINOR) Logger.minor(this, "Deleting old log from build "+nums[0]+", current="+Version.buildNumber());
					// Logs that old are useless
					f.delete();
					continue;
				}
				if(nums.length > 1)
					gc.set(Calendar.YEAR, nums[1]);
				if(nums.length > 2)
					gc.set(Calendar.MONTH, nums[2]-1);
				if(nums.length > 3)
					gc.set(Calendar.DAY_OF_MONTH, nums[3]);
				if(nums.length > 4)
					gc.set(Calendar.HOUR_OF_DAY, nums[4]);
				if(nums.length > 5)
					gc.set(Calendar.MINUTE, nums[5]);
				gc.set(Calendar.SECOND, 0);
				gc.set(Calendar.MILLISECOND, 0);
				long startTime = gc.getTimeInMillis();
				if(oldFile != null) {
					long l = oldFile.length();
					OldLogFile olf = new OldLogFile(oldFile, lastStartTime, startTime, l);
					synchronized(logFiles) {
						logFiles.addLast(olf);
					}
					synchronized(trimOldLogFilesLock) {
						oldLogFilesDiskSpaceUsage += l;
					}
				}
				lastStartTime = startTime;
				oldFile = f;
			} else {
				// Nothing to do with us
				Logger.normal(this, "Unknown file: "+name+" in the log directory");
			}
		}
		//If a compressed log file already exists for a given date,
		//add a number to the end of the file that already exists
		if(currentFilename != null && currentFilename.exists()) {
			System.out.println("Old log file exists for this time period: "+currentFilename);
			for(int a = 1;; a++){
				numericSameDateFilename = new File(getHourLogName(gc, a, true));
				if(numericSameDateFilename == null || !numericSameDateFilename.exists()) {
					if(numericSameDateFilename != null) {
						System.out.println("Renaming to: "+numericSameDateFilename);
						FileUtil.renameTo(currentFilename, numericSameDateFilename);
					}
					break;
				}
			}
		}
		if(oldFile != null) {
			long l = oldFile.length();
			OldLogFile olf = new OldLogFile(oldFile, lastStartTime, System.currentTimeMillis(), l);
			synchronized(logFiles) {
				logFiles.addLast(olf);
			}
			synchronized(trimOldLogFilesLock) {
				oldLogFilesDiskSpaceUsage += l;
			}
		}
		trimOldLogFiles();
	}

	public FileLoggerHook(
			String filename,
			String fmt,
			String dfmt,
			String threshold,
			String logRotateInterval,
			boolean assumeWorking,
			boolean logOverwrite,
			long maxOldLogFilesDiskUsage,
			int maxListSize)
			throws IOException, InvalidThresholdException, IntervalParseException {
			this(filename,
				fmt,
				dfmt,
				logRotateInterval,
				LogLevel.valueOf(threshold.toUpperCase()),
				assumeWorking,
				logOverwrite,
				maxOldLogFilesDiskUsage,
				maxListSize);
		}

	private void checkStdStreams() {
		// Redirect System.err and System.out to the Logger Printstream
		// if they don't exist (like when running under javaw)
		System.out.print(" \b");
		if (System.out.checkError()) {
			redirectStdOut = true;
		}
		System.err.print(" \b");
		if (System.err.checkError()) {
			redirectStdErr = true;
		}
	}

	public FileLoggerHook(
		OutputStream os,
		String fmt,
		String dfmt,
		LogLevel threshold) throws IntervalParseException {
		this(new PrintStream(os), fmt, dfmt, threshold, true);
		logStream = os;
	}
	
	public FileLoggerHook(
			OutputStream os,
			String fmt,
			String dfmt,
			String threshold) throws InvalidThresholdException, IntervalParseException {
			this(new PrintStream(os), fmt, dfmt, LogLevel.valueOf(threshold.toUpperCase()), true);
			logStream = os;
		}

	/**
	 * Create a Logger to send log output to the given PrintStream.
	 * 
	 * @param stream
	 *            the PrintStream to send log output to.
	 * @param fmt
	 *            log message format string
	 * @param dfmt
	 *            date format string
	 * @param threshold
	 *            Lowest logged priority
	 * @throws IntervalParseException 
	 */
	public FileLoggerHook(
		PrintStream stream,
		String fmt,
		String dfmt,
		LogLevel threshold,
		boolean overwrite) throws IntervalParseException {
		this(fmt, dfmt, threshold, "HOUR", overwrite, -1, 10000);
		logStream = stream;
	}

	public void start() {
		if(redirectStdOut)
			System.setOut(new PrintStream(new OutputStreamLogger(LogLevel.NORMAL, "Stdout: ")));
		if(redirectStdErr)
			System.setErr(new PrintStream(new OutputStreamLogger(LogLevel.ERROR, "Stderr: ")));
		WriterThread wt = new WriterThread();
		wt.setDaemon(true);
		CloserThread ct = new CloserThread();
		SemiOrderedShutdownHook.get().addLateJob(ct);
		wt.start();
	}
	
	public FileLoggerHook(
		boolean rotate,
		String baseFilename,
		String fmt,
		String dfmt,
		String logRotateInterval,
		LogLevel threshold,
		boolean assumeWorking,
		boolean logOverwrite,
		long maxOldLogfilesDiskUsage, int maxListSize)
		throws IOException, IntervalParseException {
		this(fmt, dfmt, threshold, logRotateInterval, logOverwrite, maxOldLogfilesDiskUsage, maxListSize);
		//System.err.println("Creating FileLoggerHook with threshold
		// "+threshold);
		if (!assumeWorking)
			checkStdStreams();
		if (rotate) {
			this.baseFilename = baseFilename;
		} else {
			logStream = new BufferedOutputStream(new FileOutputStream(baseFilename, !logOverwrite), 65536);
		}
	}
	
	public FileLoggerHook(
			boolean rotate,
			String baseFilename,
			String fmt,
			String dfmt,
			String threshold,
			String logRotateInterval,
			boolean assumeWorking,
			boolean logOverwrite,
			long maxOldLogFilesDiskUsage, int maxListSize) throws IOException, InvalidThresholdException, IntervalParseException{
		this(rotate,baseFilename,fmt,dfmt,logRotateInterval,LogLevel.valueOf(threshold.toUpperCase()),assumeWorking,logOverwrite,maxOldLogFilesDiskUsage,maxListSize);
	}

	private FileLoggerHook(String fmt, String dfmt, LogLevel threshold, String logRotateInterval, boolean overwrite, long maxOldLogfilesDiskUsage, int maxListSize) throws IntervalParseException {
		super(threshold);
		this.maxOldLogfilesDiskUsage = maxOldLogfilesDiskUsage;
		this.logOverwrite = overwrite;
		setInterval(logRotateInterval);
		
		MAX_LIST_SIZE = maxListSize;
		list = new ArrayBlockingQueue<byte[]>(MAX_LIST_SIZE);
		
		setDateFormat(dfmt);
		setLogFormat(fmt);
	}

	private void setLogFormat(String fmt) {
		if ((fmt == null) || (fmt.length() == 0))
			fmt = "d:c:h:t:p:m";
		char[] f = fmt.toCharArray();

		ArrayList<Integer> fmtVec = new ArrayList<Integer>();
		ArrayList<String> strVec = new ArrayList<String>();

		StringBuilder sb = new StringBuilder();

		boolean comment = false;
		for (int i = 0; i < f.length; ++i) {
			int type = numberOf(f[i]);
			if(type == UNAME)
				getUName();
			if (!comment && (type != 0)) {
				if (sb.length() > 0) {
					strVec.add(sb.toString());
					fmtVec.add(0);
					sb = new StringBuilder();
				}
				fmtVec.add(type);
			} else if (f[i] == '\\') {
				comment = true;
			} else {
				comment = false;
				sb.append(f[i]);
			}
		}
		if (sb.length() > 0) {
			strVec.add(sb.toString());
			fmtVec.add(0);
		}

		this.fmt = new int[fmtVec.size()];
		int size = fmtVec.size();
		for (int i = 0; i < size; ++i)
			this.fmt[i] = fmtVec.get(i);

		this.str = new String[strVec.size()];
		str = strVec.toArray(str);
	}

	private void setDateFormat(String dfmt) {
		if ((dfmt != null) && (dfmt.length() != 0)) {
			try {
				df = new SimpleDateFormat(dfmt);
			} catch (RuntimeException e) {
				df = DateFormat.getDateTimeInstance();
			}
		} else
			df = DateFormat.getDateTimeInstance();

		df.setTimeZone(TimeZone.getTimeZone("UTC"));
	}

	@Override
	public void log(Object o, Class<?> c, String msg, Throwable e, LogLevel priority) {
		if (!instanceShouldLog(priority, c))
			return;

		if (closed)
			return;
		
		StringBuilder sb = new StringBuilder( e == null ? 512 : 1024 );
		int sctr = 0;

		for (int i = 0; i < fmt.length; ++i) {
			switch (fmt[i]) {
				case 0 :
					sb.append(str[sctr++]);
					break;
				case DATE :
					long now = System.currentTimeMillis();
					synchronized (this) {
						myDate.setTime(now);
						sb.append(df.format(myDate));
					}
					break;
				case CLASS :
					sb.append(c == null ? "<none>" : c.getName());
					break;
				case HASHCODE :
					sb.append(
						o == null
							? "<none>"
							: Integer.toHexString(o.hashCode()));
					break;
				case THREAD :
					sb.append(Thread.currentThread().getName());
					break;
				case PRIORITY :
					sb.append(priority.name());
					break;
				case MESSAGE :
					sb.append(msg);
					break;
				case UNAME :
					sb.append(uname);
					break;
			}
		}
		sb.append('\n');

		// Write stacktrace if available
		for(int j=0;j<20 && e != null;j++) {
			sb.append(e.toString());
			
			StackTraceElement[] trace = e.getStackTrace();
			
			if(trace == null)
				sb.append("(null)\n");
			else if(trace.length == 0)
				sb.append("(no stack trace)\n");
			else {
				sb.append('\n');
				for(int i=0;i<trace.length;i++) {
					sb.append("\tat ");
					sb.append(trace[i].toString());
					sb.append('\n');
				}
			}
			
			Throwable cause = e.getCause();
			if(cause != e) e = cause;
			else break;
		}

		logString(sb.toString().getBytes());
	}

	/** Memory allocation overhead (estimated through experimentation with bsh) */
	private static final int LINE_OVERHEAD = 60;
	
	public void logString(byte[] b) {
		synchronized (list) {
			int sz = list.size();
			if(!list.offer(b)) {
				byte[] ss = list.poll();
				if(ss != null) listBytes -= ss.length + LINE_OVERHEAD;
				if(list.offer(b))
					listBytes += (b.length + LINE_OVERHEAD);
			} else
				listBytes += (b.length + LINE_OVERHEAD);
			int x = 0;
			if (listBytes > MAX_LIST_BYTES) {
				while ((list.size() > (MAX_LIST_SIZE * 0.9F))
					|| (listBytes > (MAX_LIST_BYTES * 0.9F))) {
					byte[] ss;
					ss = list.poll();
					listBytes -= (ss.length + LINE_OVERHEAD);
					x++;
				}
				String err =
					"GRRR: ERROR: Logging too fast, chopped "
						+ x
						+ " entries, "
						+ listBytes
						+ " bytes in memory\n";
				byte[] buf = err.getBytes();
				if(!list.offer(buf)) {
					byte[] ss = list.poll();
					if(ss != null) listBytes -= ss.length + LINE_OVERHEAD;
					if(list.offer(buf))
						listBytes += (buf.length + LINE_OVERHEAD);
				} else
					listBytes += (buf.length + LINE_OVERHEAD);
			}
			if (sz == 0)
				list.notifyAll();
		}
	}

	public long listBytes() {
		synchronized (list) {
			return listBytes;
		}
	}

	public static int numberOf(char c) {
		switch (c) {
			case 'd' :
				return DATE;
			case 'c' :
				return CLASS;
			case 'h' :
				return HASHCODE;
			case 't' :
				return THREAD;
			case 'p' :
				return PRIORITY;
			case 'm' :
				return MESSAGE;
			case 'u' :
				return UNAME;
			default :
				return 0;
		}
	}

	public void close() {
		closed = true;
	}

	class CloserThread extends Thread {
		@Override
		public void run() {
			synchronized(list) {
				closed = true;
				long deadline = System.currentTimeMillis() + 10*1000;
				while(!closedFinished) {
					int wait = (int) (deadline - System.currentTimeMillis());
					if(wait <= 0) return;
					try {
						list.wait(wait);
					} catch (InterruptedException e) {
						// Ok.
					}
				}
				System.out.println("Completed writing logs to disk.");
			}
		}
	}

	/**
	 * Print a human- and script- readable list of available log files.
	 * @throws IOException 
	 */
	public void listAvailableLogs(OutputStreamWriter writer) throws IOException {
		OldLogFile[] oldLogFiles;
		synchronized(logFiles) {
			oldLogFiles = logFiles.toArray(new OldLogFile[logFiles.size()]);
		}
		DateFormat tempDF = DateFormat.getDateTimeInstance(DateFormat.SHORT, DateFormat.SHORT, Locale.ENGLISH);
		tempDF.setTimeZone(TimeZone.getTimeZone("GMT"));
		for(int i=0;i<oldLogFiles.length;i++) {
			OldLogFile olf = oldLogFiles[i];
			writer.write(olf.filename.getName()+" : "+tempDF.format(new Date(olf.start))+" to "+tempDF.format(new Date(olf.end))+ " - "+olf.size+" bytes\n");
		}
	}

	public void sendLogByContainedDate(long time, OutputStream os) throws IOException {
		OldLogFile toReturn = null;
		synchronized(logFiles) {
			Iterator<OldLogFile> i = logFiles.iterator();
			while(i.hasNext()) {
				OldLogFile olf = i.next();
		    	if(logMINOR)
		    		Logger.minor(this, "Checking "+time+" against "+olf.filename+" : start="+olf.start+", end="+olf.end);
				if((time >= olf.start) && (time < olf.end)) {
					toReturn = olf;
					if(logMINOR) Logger.minor(this, "Found "+olf);
					break;
				}
			}
			if(toReturn == null)
				return; // couldn't find it
		}
		FileInputStream fis = new FileInputStream(toReturn.filename);
		DataInputStream dis = new DataInputStream(fis);
		long written = 0;
		long size = toReturn.size;
		byte[] buf = new byte[4096];
		while(written < size) {
			int toRead = (int) Math.min(buf.length, (size - written));
			try {
				dis.readFully(buf, 0, toRead);
			} catch (IOException e) {
				Logger.error(this, "Could not read bytes "+written+" to "+(written + toRead)+" from file "+toReturn.filename+" which is supposed to be "+size+" bytes ("+toReturn.filename.length()+ ')');
				return;
			}
			os.write(buf, 0, toRead);
			written += toRead;
		}
		dis.close();
		fis.close();
	}

	/** Set the maximum size of old (gzipped) log files to keep.
	 * Will start to prune old files immediately, but this will likely not be completed
	 * by the time the function returns as it is run off-thread.
	 */
	public void setMaxOldLogsSize(long val) {
		synchronized(trimOldLogFilesLock) {
			maxOldLogfilesDiskUsage = val;
		}
		Runnable r = new Runnable() {
			public void run() {
				trimOldLogFiles();
			}
		};
		Thread t = new Thread(r, "Shrink logs");
		t.setDaemon(true);
		t.start();
	}

	private boolean switchedBaseFilename;
	
	public void switchBaseFilename(String filename) {
		synchronized(this) {
			this.baseFilename = filename;
			switchedBaseFilename = true;
		}
	}

	public void waitForSwitch() {
		long now = System.currentTimeMillis();
		synchronized(this) {
			if(!switchedBaseFilename) return;
			long startTime = now;
			long endTime = startTime + 10000;
			while(((now = System.currentTimeMillis()) < endTime) && !switchedBaseFilename) {
				try {
					wait(Math.max(1, endTime-now));
				} catch (InterruptedException e) {
					// Ignore
				}
			}
		}
	}

	public void deleteAllOldLogFiles() {
		synchronized(trimOldLogFilesLock) {
			while(true) {
				OldLogFile olf;
				synchronized(logFiles) {
					if(logFiles.isEmpty()) return;
					olf = logFiles.removeFirst();
				}
				olf.filename.delete();
				oldLogFilesDiskSpaceUsage -= olf.size;
				if(logMINOR)
					Logger.minor(this, "Deleting "+olf.filename+" - saving "+olf.size+
							" bytes, disk usage now: "+oldLogFilesDiskSpaceUsage+" of "+maxOldLogfilesDiskUsage);
			}
		}
	}

	/**
	 * This is used by the lost-lock deadlock detector so MUST NOT TAKE A LOCK ever!
	 */
	public boolean hasRedirectedStdOutErrNoLock() {
		return redirectStdOut || redirectStdErr;
	}

	public synchronized void setMaxBacklogNotBusy(long val) {
		flushTime = val;
	}
}
/*
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 */

package freenet.io;

import java.net.Inet6Address;
import java.net.InetAddress;
import java.util.Arrays;
import java.util.StringTokenizer;

import freenet.io.AddressIdentifier.AddressType;

/**
 * @author David Roden &lt;droden@gmail.com&gt;
 * @version $Id$
 */
public class Inet6AddressMatcher implements AddressMatcher {
	public AddressType getAddressType() {
		return AddressType.IPv6;
	}

	private static final byte[] FULL_MASK = new byte[16];
	static {
		Arrays.fill(FULL_MASK, (byte) 0xff);
	}
	
	private byte[] address;
	private byte[] netmask;

	public Inet6AddressMatcher(String pattern) {
		if (pattern.indexOf('/') != -1) {
			address = convertToBytes(pattern.substring(0, pattern.indexOf('/')));
			String netmaskString = pattern.substring(pattern.indexOf('/') + 1).trim();
			if (netmaskString.indexOf(':') != -1) {
				netmask = convertToBytes(netmaskString);
			} else {
				netmask = new byte[16];
				int bits = Integer.parseInt(netmaskString);
				for (int index = 0; index < 16; index++) {
					netmask[index] = (byte) (255 << (8 - Math.min(bits, 8)));
					bits = Math.max(bits - 8, 0);
				}
			}
			if(Arrays.equals(netmask, FULL_MASK)) netmask = FULL_MASK;
		} else {
			address = convertToBytes(pattern);
			netmask = FULL_MASK;
		}
		if (address.length != 16) {
			throw new IllegalArgumentException("address is not IPv6");
		}
	}

	private byte[] convertToBytes(String address) {
		StringTokenizer addressTokens = new StringTokenizer(address, ":");
		if (addressTokens.countTokens() != 8) {
			throw new IllegalArgumentException(address + " is not an IPv6 address.");
		}
		byte[] addressBytes = new byte[16];
		int count = 0;
		while (addressTokens.hasMoreTokens()) {
			int addressWord = Integer.parseInt(addressTokens.nextToken(), 16);
			addressBytes[count * 2] = (byte) ((addressWord >> 8) & 0xff);
			addressBytes[count * 2 + 1] = (byte) (addressWord & 0xff);
			count++;
		}
		return addressBytes;
	}

	public boolean matches(InetAddress address) {
		if (!(address instanceof Inet6Address)) return false;
		byte[] addressBytes = address.getAddress();
		for (int index = 0; index < 16; index++) {
			if ((addressBytes[index] & netmask[index]) != (this.address[index] & netmask[index])) {
				return false;
			}
		}
		return true;
	}

	public static boolean matches(String pattern, InetAddress address) {
		return new Inet6AddressMatcher(pattern).matches(address);
	}

	public String getHumanRepresentation() {
		if(netmask == FULL_MASK)
			return convertToString(address);
		else
			return convertToString(address)+'/'+convertToString(netmask);
	}

	private String convertToString(byte[] addr) {
		StringBuilder sb = new StringBuilder(4*8+7);
		for(int i=0;i<8;i++) {
			if(i != 0) sb.append(':');
			int token = ((addr[i*2] & 0xff) << 8) + (addr[i*2+1] & 0xff);
			sb.append(Integer.toHexString(token));
		}
		return sb.toString();
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.config.Config;
import freenet.config.Option;
import freenet.config.SubConfig;
import freenet.node.Node;
import freenet.support.Logger;
import freenet.support.SimpleFieldSet;
import freenet.support.Logger.LogLevel;

public class ModifyConfig extends FCPMessage {

	static final String NAME = "ModifyConfig";
	
	final SimpleFieldSet fs;
	final String identifier;
	
	public ModifyConfig(SimpleFieldSet fs) {
		this.fs = fs;
		this.identifier = fs.get("Identifier");
		fs.removeValue("Identifier");
	}

	@Override
	public SimpleFieldSet getFieldSet() {
		return new SimpleFieldSet(true);
	}

	@Override
	public String getName() {
		return NAME;
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node) throws MessageInvalidException {
		if(!handler.hasFullAccess()) {
			throw new MessageInvalidException(ProtocolErrorMessage.ACCESS_DENIED, "ModifyConfig requires full access", identifier, false);
		}
		Config config = node.config;
		SubConfig[] sc = config.getConfigs();
		
		boolean logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
		
		for(int i=0; i<sc.length ; i++){
			Option<?>[] o = sc[i].getOptions();
			String prefix = sc[i].getPrefix();
			String configName;
			
			for(int j=0; j<o.length; j++){
				configName=o[j].getName();
				if(logMINOR) Logger.minor(this, "Setting "+prefix+ '.' +configName);
				
				// we ignore unreconized parameters 
				if(fs.get(prefix+ '.' +configName) != null) {
					if(!(o[j].getValueString().equals(fs.get(prefix+ '.' +configName)))){
						if(logMINOR) Logger.minor(this, "Setting "+prefix+ '.' +configName+" to "+fs.get(prefix+ '.' +configName));
						try{
							o[j].setValue(fs.get(prefix+ '.' +configName));
						}catch(Exception e){
							// Bad values silently fail from an FCP perspective, but the FCP client can tell if a change took by comparing ConfigData messages before and after
							Logger.error(this, "Caught "+e, e);
						}
					}
				}
			}
		}
		node.clientCore.storeConfig();
		handler.outputHandler.queue(new ConfigData(node, true, false, false, false, false, false, false, false, identifier));
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		throw new UnsupportedOperationException();
	}
}
package freenet.node;

public class MasterKeysWrongPasswordException extends Exception {

	final private static long serialVersionUID = 5075431515279831718L;

}
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.node.RequestClient;
import freenet.support.Logger;

public class FCPClientRequestClient implements RequestClient {
	
	// WARNING: THIS CLASS IS STORED IN DB4O -- THINK TWICE BEFORE ADD/REMOVE/RENAME FIELDS
	
	public final FCPClient client;
	public final boolean forever;
	public final boolean realTimeFlag;
	
	public FCPClientRequestClient(FCPClient fcpClient, boolean forever2, boolean realTime) {
		this.client = fcpClient;
		this.forever = forever2;
		this.realTimeFlag = realTime;
	}
	
	public boolean persistent() {
		return forever;
	}
	
	public void removeFrom(ObjectContainer container) {
		if(forever)
			container.delete(this);
		else
			throw new UnsupportedOperationException();
	}
	
	public boolean objectCanDelete(ObjectContainer container) {
		container.activate(client, 1);
		if(client.isGlobalQueue) {
			Logger.error(this, "Trying to remove the RequestClient for the global queue!!!", new Exception("error"));
			return false;
		}
		return true;
	}

	public boolean realTimeFlag() {
		return realTimeFlag;
	}


}
/**
 * This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL.
 */
package freenet.support.io;

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;

import com.db4o.ObjectContainer;

import freenet.crypt.RandomSource;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.SimpleFieldSet;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;

public class DelayedFreeBucket implements Bucket {

	private final PersistentFileTracker factory;
	Bucket bucket;
	boolean freed;
	boolean removed;
	boolean reallyRemoved;

        private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}
	
	public boolean toFree() {
		return freed;
	}
	
	public boolean toRemove() {
		return removed;
	}
	
	public DelayedFreeBucket(PersistentTempBucketFactory factory, Bucket bucket) {
		this.factory = factory;
		this.bucket = bucket;
		if(bucket == null) throw new NullPointerException();
	}

	public OutputStream getOutputStream() throws IOException {
		if(freed) throw new IOException("Already freed");
		return bucket.getOutputStream();
	}

	public InputStream getInputStream() throws IOException {
		if(freed) throw new IOException("Already freed");
		return bucket.getInputStream();
	}

	public String getName() {
		return bucket.getName();
	}

	public long size() {
		return bucket.size();
	}

	public boolean isReadOnly() {
		return bucket.isReadOnly();
	}

	public void setReadOnly() {
		bucket.setReadOnly();
	}

	public Bucket getUnderlying() {
		if(freed) return null;
		return bucket;
	}
	
	public void free() {
		synchronized(this) { // mutex on just this method; make a separate lock if necessary to lock the above
			if(freed) return;
			if(logMINOR)
				Logger.minor(this, "Freeing "+this+" underlying="+bucket, new Exception("debug"));
			this.factory.delayedFreeBucket(this);
			freed = true;
		}
	}

	public void storeTo(ObjectContainer container) {
		bucket.storeTo(container);
		container.store(this);
	}

	public void removeFrom(ObjectContainer container) {
		if(logMINOR)
			Logger.minor(this, "Removing from database: "+this);
		synchronized(this) {
			boolean wasQueued = freed || removed;
			if(!freed)
				Logger.error(this, "Asking to remove from database but not freed: "+this, new Exception("error"));
			removed = true;
			if(!wasQueued)
				this.factory.delayedFreeBucket(this);
		}
	}

	@Override
	public String toString() {
		return super.toString()+":"+bucket;
	}
	
	private transient int _activationCount = 0;
	
	public void objectOnActivate(ObjectContainer container) {
//		StackTraceElement[] elements = Thread.currentThread().getStackTrace();
//		if(elements != null && elements.length > 100) {
//			System.err.println("Infinite recursion in progress...");
//		}
		if(logMINOR)
			Logger.minor(this, "Activating "+super.toString()+" : "+bucket.getClass());
		if(bucket == this) {
			Logger.error(this, "objectOnActivate on DelayedFreeBucket: wrapping self!!!");
			return;
		}
		// Cascading activation of dependancies
		container.activate(bucket, 1);
	}

	public Bucket createShadow() {
		return bucket.createShadow();
	}

	public void realFree() {
		bucket.free();
	}

	public void realRemoveFrom(ObjectContainer container) {
		synchronized(this) {
			if(reallyRemoved)
				Logger.error(this, "Calling realRemoveFrom() twice on "+this);
			reallyRemoved = true;
		}
		bucket.removeFrom(container);
		container.delete(this);
	}
	
	public boolean objectCanNew(ObjectContainer container) {
		if(reallyRemoved) {
			Logger.error(this, "objectCanNew() on "+this+" but really removed = "+reallyRemoved+" already freed="+freed+" removed="+removed, new Exception("debug"));
			return false;
		}
		assert(bucket != null);
		return true;
	}
	
	public boolean objectCanUpdate(ObjectContainer container) {
		if(reallyRemoved) {
			Logger.error(this, "objectCanUpdate() on "+this+" but really removed = "+reallyRemoved+" already freed="+freed+" removed="+removed, new Exception("debug"));
			return false;
		}
		assert(bucket != null);
		return true;
	}

}/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.client;

import com.db4o.ObjectContainer;

import freenet.keys.FreenetURI;
import freenet.l10n.NodeL10n;
import freenet.support.Logger;

/**
 * Thrown when a high-level request (fetch) fails. Indicates why, whether it is worth retrying, and may give a 
 * new URI to try, the expected size of the file, its expected MIME type, and whether these are reliable.
 * For most failure modes, except INTERNAL_ERROR there will be no stack trace, or it will be unhelpful or 
 * inaccurate. 
 */
public class FetchException extends Exception {
	private static volatile boolean logMINOR;
	
	static {
		Logger.registerClass(FetchException.class);
	}

	private static final long serialVersionUID = -1106716067841151962L;
	
	/** Failure mode */
	public final int mode;
	
	/** Try this URI instead. If we fetch a USK and there is a more recent version, for example, we will get
	 * a FetchException, but it will give a new URI to try so we can update our links, bookmarks, or convert
	 * it to an HTTP Permanent Redirect. */
	public final FreenetURI newURI;
	
	/** The expected size of the data had the fetch succeeded, or -1. May not be accurate. If retrying 
	 * after TOO_BIG, you need to set the temporary and final data limits to at least this big! */
	public long expectedSize;
	
	/** The expected final MIME type, or null. */
	String expectedMimeType;
	
	/** If true, the expected MIME type and size are probably accurate. */
	boolean finalizedSizeAndMimeType;
	
	/** Do we know the expected MIME type of the data? */
	public String getExpectedMimeType() {
		return expectedMimeType;
	}

	/** Do we have any idea of the final size of the data? */
	public boolean finalizedSize() {
		return finalizedSizeAndMimeType;
	}
	
	/** If there are many failures, usually in a splitfile fetch, tracks the number of failures of each 
	 * type. */
	public final FailureCodeTracker errorCodes;
	
	/** Extra information about the failure. */
	public final String extraMessage;
	
	/** Get the failure mode. */
	public int getMode() {
		return mode;
	}
	
	public FetchException(int m) {
		super(getMessage(m));
		if(m == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		extraMessage = null;
		mode = m;
		errorCodes = null;
		newURI = null;
		expectedSize = -1;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	public FetchException(int m, long expectedSize, boolean finalizedSize, String expectedMimeType) {
		super(getMessage(m));
		if(m == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		extraMessage = null;
		this.finalizedSizeAndMimeType = finalizedSize;
		mode = m;
		errorCodes = null;
		newURI = null;
		this.expectedSize = expectedSize;
		this.expectedMimeType = expectedMimeType;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}
	
	public FetchException(int m, long expectedSize, boolean finalizedSize, String expectedMimeType, FreenetURI uri) {
		super(getMessage(m));
		if(m == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		extraMessage = null;
		this.finalizedSizeAndMimeType = finalizedSize;
		mode = m;
		errorCodes = null;
		newURI = uri;
		this.expectedSize = expectedSize;
		this.expectedMimeType = expectedMimeType;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}
	
	public FetchException(MetadataParseException e) {
		super(getMessage(INVALID_METADATA)+": "+e.getMessage());
		extraMessage = e.getMessage();
		mode = INVALID_METADATA;
		errorCodes = null;
		initCause(e);
		newURI = null;
		expectedSize = -1;
		if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	public FetchException(ArchiveFailureException e) {
		super(getMessage(ARCHIVE_FAILURE)+": "+e.getMessage());
		extraMessage = e.getMessage();
		mode = ARCHIVE_FAILURE;
		errorCodes = null;
		newURI = null;
		initCause(e);
		expectedSize = -1;
		if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	public FetchException(ArchiveRestartException e) {
		super(getMessage(ARCHIVE_RESTART)+": "+e.getMessage());
		extraMessage = e.getMessage();
		mode = ARCHIVE_FAILURE;
		errorCodes = null;
		initCause(e);
		newURI = null;
		expectedSize = -1;
		if(logMINOR)
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	public FetchException(int mode, Throwable t) {
		super(getMessage(mode)+": "+t.getMessage());
		if(mode == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		extraMessage = t.getMessage();
		this.mode = mode;
		errorCodes = null;
		initCause(t);
		newURI = null;
		expectedSize = -1;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	public FetchException(int mode, String reason, Throwable t) {
		super(reason+" : "+getMessage(mode)+": "+t.getMessage());
		if(mode == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		extraMessage = t.getMessage();
		this.mode = mode;
		errorCodes = null;
		initCause(t);
		newURI = null;
		expectedSize = -1;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	public FetchException(int mode, long expectedSize, String reason, Throwable t, String expectedMimeType) {
		super(reason+" : "+getMessage(mode)+": "+t.getMessage());
		if(mode == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		extraMessage = t.getMessage();
		this.mode = mode;
		this.expectedSize = expectedSize;
		this.expectedMimeType = expectedMimeType;
		errorCodes = null;
		initCause(t);
		newURI = null;
		expectedSize = -1;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	public FetchException(int mode, long expectedSize, Throwable t, String expectedMimeType) {
		super(getMessage(mode)+": "+t.getMessage());
		if(mode == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		extraMessage = t.getMessage();
		this.mode = mode;
		this.expectedSize = expectedSize;
		this.expectedMimeType = expectedMimeType;
		errorCodes = null;
		initCause(t);
		newURI = null;
		expectedSize = -1;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	public FetchException(int mode, FailureCodeTracker errorCodes) {
		super(getMessage(mode));
		if(mode == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		extraMessage = null;
		this.mode = mode;
		this.errorCodes = errorCodes;
		newURI = null;
		expectedSize = -1;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}
	
	public FetchException(int mode, String msg) {
		super(getMessage(mode)+": "+msg);
		if(mode == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		extraMessage = msg;
		errorCodes = null;
		this.mode = mode;
		newURI = null;
		expectedSize = -1;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	public FetchException(int mode, FreenetURI newURI) {
		super(getMessage(mode));
		if(mode == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		extraMessage = null;
		this.mode = mode;
		errorCodes = null;
		this.newURI = newURI;
		expectedSize = -1;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}
	
	public FetchException(int mode, String msg, FreenetURI uri) {
		super(getMessage(mode)+": "+msg);
		if(mode == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		extraMessage = msg;
		errorCodes = null;
		this.mode = mode;
		newURI = uri;
		expectedSize = -1;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	public FetchException(FetchException e, int newMode) {
		super(getMessage(newMode)+(e.extraMessage != null ? ": "+e.extraMessage : ""));
		if(newMode == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		this.mode = newMode;
		this.newURI = e.newURI;
		this.errorCodes = e.errorCodes;
		this.expectedMimeType = e.expectedMimeType;
		this.expectedSize = e.expectedSize;
		this.extraMessage = e.extraMessage;
		this.finalizedSizeAndMimeType = e.finalizedSizeAndMimeType;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	public FetchException(FetchException e, FreenetURI uri) {
		super(e.getMessage());
		if(e.getCause() != null)
			initCause(e.getCause());
		if(e.mode == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		this.mode = e.mode;
		this.newURI = uri;
		this.errorCodes = e.errorCodes;
		this.expectedMimeType = e.expectedMimeType;
		this.expectedSize = e.expectedSize;
		this.extraMessage = e.extraMessage;
		this.finalizedSizeAndMimeType = e.finalizedSizeAndMimeType;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	public FetchException(FetchException e) {
		super(e.getMessage());
		initCause(e);
		if(e.mode == 0)
			Logger.error(this, "Can't increment failure mode 0, not a valid mode", new Exception("error"));
		this.mode = e.mode;
		this.newURI = e.newURI == null ? null : e.newURI.clone();
		this.errorCodes = e.errorCodes == null ? null : e.errorCodes.clone();
		this.expectedMimeType = e.expectedMimeType;
		this.expectedSize = e.expectedSize;
		this.extraMessage = e.extraMessage;
		this.finalizedSizeAndMimeType = e.finalizedSizeAndMimeType;
		if(mode == INTERNAL_ERROR)
			Logger.error(this, "Internal error: "+this);
		else if(logMINOR) 
			Logger.minor(this, "FetchException("+getMessage(mode)+ ')', this);
	}

	/** Get the short name of this exception's failure. */
	public String getShortMessage() {
		if (getCause() == null) return getShortMessage(mode);
		else return getCause().toString();
	}

	/** Get the (localised) short name of this failure mode. */
	public static String getShortMessage(int mode) {
		String ret = NodeL10n.getBase().getString("FetchException.shortError."+mode);
		if(ret == null || ret.equals(""))
			return "Unknown code "+mode;
		else return ret;
	}
	
	@Override
	public String toString() {
		StringBuilder sb = new StringBuilder(200);
		sb.append("FetchException:");
		sb.append(getShortMessage(mode));
		sb.append(':');
		sb.append(newURI);
		sb.append(':');
		sb.append(expectedSize);
		sb.append(':');
		sb.append(expectedMimeType);
		sb.append(':');
		sb.append(finalizedSizeAndMimeType);
		sb.append(':');
		sb.append(errorCodes);
		sb.append(':');
		sb.append(extraMessage);
		return sb.toString();
	}
	
	/** Get the (localised) long explanation for this failure mode. */
	public static String getMessage(int mode) {
		String ret = NodeL10n.getBase().getString("FetchException.longError."+mode);
		if(ret == null)
			return "Unknown fetch error code: "+mode;
		else return ret;
	}
	
	// FIXME many of these are not used any more
	
	/** Too many levels of recursion into archives */
	@Deprecated // not used
	public static final int TOO_DEEP_ARCHIVE_RECURSION = 1;
	/** Don't know what to do with splitfile */
	@Deprecated // not used
	public static final int UNKNOWN_SPLITFILE_METADATA = 2;
	/** Too many redirects */
	@Deprecated // not used
	public static final int TOO_MANY_REDIRECTS = 16;
	/** Don't know what to do with metadata */
	public static final int UNKNOWN_METADATA = 3;
	/** Got a MetadataParseException */
	public static final int INVALID_METADATA = 4;
	/** Got an ArchiveFailureException */
	public static final int ARCHIVE_FAILURE = 5;
	/** Failed to decode a block. But we found it i.e. it is valid on the network level. */
	public static final int BLOCK_DECODE_ERROR = 6;
	/** Too many split metadata levels */
	@Deprecated // not used
	public static final int TOO_MANY_METADATA_LEVELS = 7;
	/** Too many archive restarts */
	public static final int TOO_MANY_ARCHIVE_RESTARTS = 8;
	/** Too deep recursion */
	// FIXME some TOO_MUCH_RECURSION may be TOO_DEEP_ARCHIVE_RECURSION 
	public static final int TOO_MUCH_RECURSION = 9;
	/** Tried to access an archive file but not in an archive */
	public static final int NOT_IN_ARCHIVE = 10;
	/** Too many meta strings. E.g. requesting CHK@blah,blah,blah as CHK@blah,blah,blah/filename.ext */
	public static final int TOO_MANY_PATH_COMPONENTS = 11;
	/** Failed to read from or write to a bucket; a kind of internal error */
	public static final int BUCKET_ERROR = 12;
	/** Data not found */
	public static final int DATA_NOT_FOUND = 13;
	/** Not all data was found; some DNFs but some successes */
	public static final int ALL_DATA_NOT_FOUND = 28;
	/** Route not found */
	public static final int ROUTE_NOT_FOUND = 14;
	/** Downstream overload */
	public static final int REJECTED_OVERLOAD = 15;
	/** An internal error occurred */
	public static final int INTERNAL_ERROR = 17;
	/** The node found the data but the transfer failed */
	public static final int TRANSFER_FAILED = 18;
	/** Splitfile error. This should be a SplitFetchException. */
	public static final int SPLITFILE_ERROR = 19;
	/** Invalid URI. */
	public static final int INVALID_URI = 20;
	/** Too big */
	public static final int TOO_BIG = 21;
	/** Metadata too big */
	public static final int TOO_BIG_METADATA = 22;
	/** Splitfile has too big segments */
	public static final int TOO_MANY_BLOCKS_PER_SEGMENT = 23;
	/** Not enough meta strings in URI given and no default document */
	public static final int NOT_ENOUGH_PATH_COMPONENTS = 24;
	/** Explicitly cancelled */
	public static final int CANCELLED = 25;
	/** Archive restart */
	public static final int ARCHIVE_RESTART = 26;
	/** There is a more recent version of the USK, ~= HTTP 301; FProxy will turn this into a 301 */
	public static final int PERMANENT_REDIRECT = 27;
	/** Requestor specified a list of allowed MIME types, and the key's type wasn't in the list */
	public static final int WRONG_MIME_TYPE = 29;
	/** A node killed the request because it had recently been tried and had DNFed */
	public static final int RECENTLY_FAILED = 30;
	/** Content filtration has generally failed to produce clean data */
	public static final int CONTENT_VALIDATION_FAILED = 31;
	/** The content filter does not recognize this data type */
	public static final int CONTENT_VALIDATION_UNKNOWN_MIME = 32;
	/** The content filter knows this data type is dangerous */
	public static final int CONTENT_VALIDATION_BAD_MIME = 33;
	/** The metadata specified a hash but the data didn't match it. */
	public static final int CONTENT_HASH_FAILED = 34;
	/** FEC decode produced a block that doesn't match the data in the original splitfile. */
	public static final int SPLITFILE_DECODE_ERROR = 35;

	/** Is an error fatal i.e. is there no point retrying? */
	public boolean isFatal() {
		return isFatal(mode);
	}

	/** Is an error mode fatal i.e. is there no point retrying? */
	@SuppressWarnings("deprecation")
	public static boolean isFatal(int mode) {
		switch(mode) {
		// Problems with the data as inserted, or the URI given. No point retrying.
		case ARCHIVE_FAILURE:
		case BLOCK_DECODE_ERROR:
		case TOO_MANY_PATH_COMPONENTS:
		case NOT_ENOUGH_PATH_COMPONENTS:
		case INVALID_METADATA:
		case NOT_IN_ARCHIVE:
		case TOO_DEEP_ARCHIVE_RECURSION:
		case TOO_MANY_ARCHIVE_RESTARTS:
		case TOO_MANY_METADATA_LEVELS:
		case TOO_MANY_REDIRECTS:
		case TOO_MUCH_RECURSION:
		case UNKNOWN_METADATA:
		case UNKNOWN_SPLITFILE_METADATA:
		case INVALID_URI:
		case TOO_BIG:
		case TOO_BIG_METADATA:
		case TOO_MANY_BLOCKS_PER_SEGMENT:
		case CONTENT_HASH_FAILED:
		case SPLITFILE_DECODE_ERROR:
			return true;

		// Low level errors, can be retried
		case DATA_NOT_FOUND:
		case ROUTE_NOT_FOUND:
		case REJECTED_OVERLOAD:
		case TRANSFER_FAILED:
		case ALL_DATA_NOT_FOUND:
		case RECENTLY_FAILED: // wait a bit, but fine
		// Not usually fatal
		case SPLITFILE_ERROR:
			return false;
			
		case BUCKET_ERROR:
		case INTERNAL_ERROR:
			// No point retrying.
			return true;
		
		//The ContentFilter failed to validate the data. Retrying won't fix this.
			case CONTENT_VALIDATION_FAILED:
			case CONTENT_VALIDATION_UNKNOWN_MIME:
			case CONTENT_VALIDATION_BAD_MIME:
				return true;

		// Wierd ones
		case CANCELLED:
		case ARCHIVE_RESTART:
		case PERMANENT_REDIRECT:
		case WRONG_MIME_TYPE:
			// Fatal
			return true;
			
		default:
			Logger.error(FetchException.class, "Do not know if error code is fatal: "+getMessage(mode));
			return false; // assume it isn't
		}
	}

	/** Call to indicate the expected size and MIME type are unreliable. */
	public void setNotFinalizedSize() {
		this.finalizedSizeAndMimeType = false;
	}

	/** Remove from the database. */
	public void removeFrom(ObjectContainer container) {
		if(errorCodes != null)
			errorCodes.removeFrom(container);
		if(newURI != null)
			newURI.removeFrom(container);
		StackTraceElement[] elements = getStackTrace();
		if(elements != null)
			for(StackTraceElement element : elements)
				container.delete(element);
		container.delete(this);
	}
	
	@Override
	public FetchException clone() {
		return new FetchException(this);
	}

	public boolean isDataFound() {
		return isDataFound(mode, errorCodes);
	}
	
	public static boolean isDataFound(int mode, FailureCodeTracker errorCodes) {
		switch(mode) {
		case TOO_DEEP_ARCHIVE_RECURSION:
		case UNKNOWN_SPLITFILE_METADATA:
		case TOO_MANY_REDIRECTS:
		case UNKNOWN_METADATA:
		case INVALID_METADATA:
		case ARCHIVE_FAILURE:
		case BLOCK_DECODE_ERROR:
		case TOO_MANY_METADATA_LEVELS:
		case TOO_MANY_ARCHIVE_RESTARTS:
		case TOO_MUCH_RECURSION:
		case NOT_IN_ARCHIVE:
		case TOO_MANY_PATH_COMPONENTS:
		case TOO_BIG:
		case TOO_BIG_METADATA:
		case TOO_MANY_BLOCKS_PER_SEGMENT:
		case NOT_ENOUGH_PATH_COMPONENTS:
		case ARCHIVE_RESTART:
		case CONTENT_VALIDATION_FAILED:
		case CONTENT_VALIDATION_UNKNOWN_MIME:
		case CONTENT_VALIDATION_BAD_MIME:
		case CONTENT_HASH_FAILED:
		case SPLITFILE_DECODE_ERROR:
			return true;
		case SPLITFILE_ERROR:
			return errorCodes.isDataFound();
		default:
			return false;
		}
	}
	
	public boolean isDNF() {
		switch(mode) {
		case DATA_NOT_FOUND:
		case ALL_DATA_NOT_FOUND:
		case RECENTLY_FAILED:
			return true;
		default:
			return false;
		}
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.io.xfer;

import java.io.IOException;

import freenet.io.comm.MessageCore;
import freenet.io.comm.RetrievalException;
import freenet.support.BitArray;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.io.RandomAccessThing;

/**
 * Equivalent of PartiallyReceivedBlock, for large(ish) file transfers.
 * As presently implemented, we keep a bitmap in RAM of blocks received, so it should be adequate
 * for fairly large files (128kB for a 1GB file e.g.). We can compress this structure later on if
 * need be.
 * @author toad
 */
public class PartiallyReceivedBulk {
	
	/** The size of the data being received. Does *not* have to be a multiple of blockSize. */
	final long size;
	/** The size of the blocks sent as packets. */
	final int blockSize;
	private final RandomAccessThing raf;
	/** Which blocks have been received and written? */
	private final BitArray blocksReceived;
	final int blocks;
	private BulkTransmitter[] transmitters;
	final MessageCore usm;
	/** The one and only BulkReceiver */
	BulkReceiver recv;
	private int blocksReceivedCount;
	// Abort status
	boolean _aborted;
	int _abortReason;
	String _abortDescription;

        private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}
	
	/**
	 * Construct a PartiallyReceivedBulk.
	 * @param size Size of the file, does not have to be a multiple of blockSize.
	 * @param blockSize Block size.
	 * @param raf Where to store the data.
	 * @param initialState If true, assume all blocks have been received. If false, assume no blocks have
	 * been received.
	 */
	public PartiallyReceivedBulk(MessageCore usm, long size, int blockSize, RandomAccessThing raf, boolean initialState) {
		this.size = size;
		this.blockSize = blockSize;
		this.raf = raf;
		this.usm = usm;
		long blocks = size / blockSize + (size % blockSize > 0 ? 1 : 0);
		if(blocks > Integer.MAX_VALUE)
			throw new IllegalArgumentException("Too big");
		this.blocks = (int)blocks;
		blocksReceived = new BitArray(this.blocks);
		if(initialState) {
			blocksReceived.setAllOnes();
			blocksReceivedCount = this.blocks;
		}
	}

	/**
	 * Clone the blocksReceived BitArray. Used by BulkTransmitter to find what blocks are available on 
	 * creation. BulkTransmitter will have already taken the lock and will keep it over the add() also.
	 * @return A copy of blocksReceived.
	 */
	synchronized BitArray cloneBlocksReceived() {
		return new BitArray(blocksReceived);
	}
	
	/**
	 * Add a BulkTransmitter to the list of BulkTransmitters. When a block comes in, we will tell each
	 * BulkTransmitter about it.
	 * @param bt The BulkTransmitter to register.
	 */
	synchronized void add(BulkTransmitter bt) {
		if(transmitters == null)
			transmitters = new BulkTransmitter[] { bt };
		else {
			BulkTransmitter[] t = new BulkTransmitter[transmitters.length+1];
			System.arraycopy(transmitters, 0, t, 0, transmitters.length);
			t[transmitters.length] = bt;
			transmitters = t;
		}
	}
	
	/**
	 * Called when a block has been received. Will copy the data from the provided buffer and store it.
	 * @param blockNum The block number.
	 * @param data The byte array from which to read the data.
	 * @param offset The start of the data in the buffer.
	 */
	void received(int blockNum, byte[] data, int offset, int length) {
		if(blockNum > blocks) {
			Logger.error(this, "Received block "+blockNum+" of "+blocks+" !");
			return;
		}
		if(logMINOR)
			Logger.minor(this, "Received block "+blockNum);
		BulkTransmitter[] notifyBTs;
		long fileOffset = (long)blockNum * (long)blockSize;
		int bs = (int) Math.min(blockSize, size - fileOffset);
		if(length < bs) {
			String err = "Data too short! Should be "+bs+" actually "+length;
			Logger.error(this, err+" for "+this);
			abort(RetrievalException.PREMATURE_EOF, err);
			return;
		}
		synchronized(this) {
			if(blocksReceived.bitAt(blockNum)) return; // ignore
			blocksReceived.setBit(blockNum, true); // assume the rest of the function succeeds
			blocksReceivedCount++;
			notifyBTs = transmitters;
		}
		try {
			raf.pwrite(fileOffset, data, offset, bs);
		} catch (Throwable t) {
			Logger.error(this, "Failed to store received block "+blockNum+" on "+this+" : "+t, t);
			abort(RetrievalException.IO_ERROR, t.toString());
		}
		if(notifyBTs == null) return;
		for(int i=0;i<notifyBTs.length;i++) {
			// Not a generic callback, so no catch{} guard
			notifyBTs[i].blockReceived(blockNum);
		}
	}

	public void abort(int errCode, String why) {
		if(logMINOR)
			Logger.normal(this, "Aborting "+this+": "+errCode+" : "+why+" first missing is "+blocksReceived.firstZero(0), new Exception("debug"));
		BulkTransmitter[] notifyBTs;
		BulkReceiver notifyBR;
		synchronized(this) {
			_aborted = true;
			_abortReason = errCode;
			_abortDescription = why;
			notifyBTs = transmitters;
			notifyBR = recv;
		}
		if(notifyBTs != null) {
			for(int i=0;i<notifyBTs.length;i++) {
				notifyBTs[i].onAborted();
			}
		}
		if(notifyBR != null)
			notifyBR.onAborted();
		raf.close();
	}

	public synchronized boolean isAborted() {
		return _aborted;
	}

	public boolean hasWholeFile() {
		return blocksReceivedCount >= blocks;
	}

	public byte[] getBlockData(int blockNum) {
		long fileOffset = (long)blockNum * (long)blockSize;
		int bs = (int) Math.min(blockSize, size - fileOffset);
		byte[] data = new byte[bs];
		try {
			raf.pread(fileOffset, data, 0, bs);
		} catch (IOException e) {
			Logger.error(this, "Failed to read stored block "+blockNum+" on "+this+" : "+e, e);
			abort(RetrievalException.IO_ERROR, e.toString());
			return null;
		}
		return data;
	}

	public synchronized void remove(BulkTransmitter remove) {
		boolean found = false;
		for(int i=0;i<transmitters.length;i++) {
			if(transmitters[i] == remove) found = true;
		}
		if(!found) return;
		BulkTransmitter[] newTrans = new BulkTransmitter[transmitters.length-1];
		int j = 0;
		for(int i=0;i<transmitters.length;i++) {
			BulkTransmitter t = transmitters[i];
			if(t == remove) continue;
			newTrans[j++] = t;
		}
		transmitters = newTrans;
	}
	
	public int getAbortReason() {
		return _abortReason;
	}
	
	public String getAbortDescription() {
		return _abortDescription;
	}
}
/*
 * freenet - FredPluginMultiple.java Copyright © 2007 David Roden
 * 
 * This program is free software; you can redistribute it and/or modify it under
 * the terms of the GNU General Public License as published by the Free Software
 * Foundation; either version 2 of the License, or (at your option) any later
 * version.
 * 
 * This program is distributed in the hope that it will be useful, but WITHOUT
 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
 * FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
 * details.
 * 
 * You should have received a copy of the GNU General Public License along with
 * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
 * Place - Suite 330, Boston, MA 02111-1307, USA.
 */

package freenet.pluginmanager;

/**
 * Interface that has to be implemented for plugins that can be loaded more than
 * once.
 * 
 * @author David &lsquo;Bombe&rsquo; Roden &lt;bombe@freenetproject.org&gt;
 * @version $Id$
 */
public interface FredPluginMultiple {

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.client.async;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Random;

import com.db4o.ObjectContainer;

import freenet.client.ArchiveManager.ARCHIVE_TYPE;
import freenet.client.ClientMetadata;
import freenet.client.FECCodec;
import freenet.client.FailureCodeTracker;
import freenet.client.InsertContext;
import freenet.client.InsertContext.CompatibilityMode;
import freenet.client.InsertException;
import freenet.client.Metadata;
import freenet.crypt.HashResult;
import freenet.keys.CHKBlock;
import freenet.keys.ClientCHK;
import freenet.support.Executor;
import freenet.support.HexUtil;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;
import freenet.support.compress.Compressor.COMPRESSOR_TYPE;
import freenet.support.io.BucketTools;
import freenet.support.math.MersenneTwister;

public class SplitFileInserter implements ClientPutState {

	private static volatile boolean logMINOR;
	private static volatile boolean logDEBUG;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback() {

			@Override
			public void shouldUpdate() {
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
				logDEBUG = Logger.shouldLog(LogLevel.DEBUG, this);
			}
		});
	}

	final BaseClientPutter parent;
	final InsertContext ctx;
	final PutCompletionCallback cb;
	final long dataLength;
	final COMPRESSOR_TYPE compressionCodec;
	final short splitfileAlgorithm;
	/** The number of data blocks in a typical segment. Does not include cross-check blocks. */
	final int segmentSize;
	/** The number of check blocks in a typical segment. Does not include cross-check blocks. */
	final int deductBlocksFromSegments;
	/** The number of cross-check blocks in any segment, or in any cross-segment. */
	final int checkSegmentSize;
	final SplitFileInserterSegment[] segments;
	final boolean getCHKOnly;
	final int countCheckBlocks;
	final int countDataBlocks;
	private boolean haveSentMetadata;
	final ClientMetadata cm;
	final boolean isMetadata;
	private volatile boolean finished;
	private boolean fetchable;
	public final Object token;
	final ARCHIVE_TYPE archiveType;
	private boolean forceEncode;
	private final long decompressedLength;
	final boolean persistent;
	final HashResult[] hashes;
	final byte[] hashThisLayerOnly;
	private byte splitfileCryptoAlgorithm;
	private byte[] splitfileCryptoKey;
	private final boolean specifySplitfileKeyInMetadata;
	private final boolean realTimeFlag;
	
	public final long topSize;
	public final long topCompressedSize;
	
	// Cross-segment splitfile redundancy
	private final int crossCheckBlocks;
	private final SplitFileInserterCrossSegment[] crossSegments;

	// A persistent hashCode is helpful in debugging, and also means we can put
	// these objects into sets etc when we need to.

	private final int hashCode;
	
	@Override
	public int hashCode() {
		return hashCode;
	}

	/**
	 * zero arg c'tor for db4o on jamvm
	 */
	@SuppressWarnings("unused")
	private SplitFileInserter() {
		topSize = 0;
		topCompressedSize = 0;
		token = null;
		splitfileAlgorithm = 0;
		specifySplitfileKeyInMetadata = false;
		segments = null;
		segmentSize = 0;
		persistent = false;
		parent = null;
		isMetadata = false;
		hashes = null;
		hashThisLayerOnly = null;
		hashCode = 0;
		getCHKOnly = false;
		deductBlocksFromSegments = 0;
		decompressedLength = 0;
		dataLength = 0;
		ctx = null;
		crossSegments = null;
		crossCheckBlocks = 0;
		countDataBlocks = 0;
		countCheckBlocks = 0;
		compressionCodec = null;
		cm = null;
		checkSegmentSize = 0;
		cb = null;
		archiveType = null;
		realTimeFlag = false;
	}

	public SplitFileInserter(BaseClientPutter put, PutCompletionCallback cb, Bucket data, COMPRESSOR_TYPE bestCodec, long decompressedLength, ClientMetadata clientMetadata, InsertContext ctx, boolean getCHKOnly, boolean isMetadata, Object token, ARCHIVE_TYPE archiveType, boolean freeData, boolean persistent, boolean realTimeFlag, ObjectContainer container, ClientContext context, HashResult[] hashes, byte[] hashThisLayerOnly, long origTopSize, long origTopCompressedSize, byte cryptoAlgorithm, byte[] splitfileKey) throws InsertException {
		hashCode = super.hashCode();
		if(put == null) throw new NullPointerException();
		this.parent = put;
		this.realTimeFlag = realTimeFlag;
		this.archiveType = archiveType;
		this.compressionCodec = bestCodec;
		this.token = token;
		this.finished = false;
		this.isMetadata = isMetadata;
		this.cm = clientMetadata;
		this.getCHKOnly = getCHKOnly;
		this.cb = cb;
		this.ctx = ctx;
		this.decompressedLength = decompressedLength;
		this.dataLength = data.size();
		this.hashes = hashes;
		this.topSize = origTopSize;
		this.topCompressedSize = origTopCompressedSize;
		this.hashThisLayerOnly = hashThisLayerOnly;
		Bucket[] dataBuckets;
		context.jobRunner.setCommitThisTransaction();
		try {
			dataBuckets = BucketTools.split(data, CHKBlock.DATA_LENGTH, persistent ? context.persistentBucketFactory : context.tempBucketFactory, freeData, persistent, container);
				if(dataBuckets[dataBuckets.length-1].size() < CHKBlock.DATA_LENGTH) {
					Bucket oldData = dataBuckets[dataBuckets.length-1];
					dataBuckets[dataBuckets.length-1] = BucketTools.pad(oldData, CHKBlock.DATA_LENGTH, context.getBucketFactory(persistent), (int) oldData.size());
					if(persistent) dataBuckets[dataBuckets.length-1].storeTo(container);
					oldData.free();
					if(persistent) oldData.removeFrom(container);
				}
			if(logMINOR)
				Logger.minor(this, "Data size "+data.size()+" buckets "+dataBuckets.length);
		} catch (IOException e) {
			throw new InsertException(InsertException.BUCKET_ERROR, e, null);
		}
		countDataBlocks = dataBuckets.length;
		// Encoding is done by segments
		this.splitfileAlgorithm = ctx.splitfileAlgorithm;
		
		// Segment size cannot be greater than ctx.splitfileSegmentDataBlocks.
		// But IT CAN BE SMALLER!
		int segs;
		CompatibilityMode cmode = ctx.getCompatibilityMode();
		if(cmode == CompatibilityMode.COMPAT_1250_EXACT) {
			segs = countDataBlocks / 128 + (countDataBlocks % 128 == 0 ? 0 : 1);
			segmentSize = 128;
			deductBlocksFromSegments = 0;
		} else {
			if(cmode == CompatibilityMode.COMPAT_1251) {
				// Max 131 blocks per segment.
				segs = (int)Math.ceil(((double)countDataBlocks) / 131);
			} else {
				// Algorithm from evanbd, see bug #2931.
				if(countDataBlocks > 520) {
					segs = (int)Math.ceil(((double)countDataBlocks) / 128);
				} else if(countDataBlocks > 393) {
					//maxSegSize = 130;
					segs = 4;
				} else if(countDataBlocks > 266) {
					//maxSegSize = 131;
					segs = 3;
				} else if(countDataBlocks > 136) {
					//maxSegSize = 133;
					segs = 2;
				} else {
					//maxSegSize = 136;
					segs = 1;
				}
			}
			int segSize = (int)Math.ceil(((double)countDataBlocks) / ((double)segs));
			if(ctx.splitfileSegmentDataBlocks < segSize) {
				segs = (int)Math.ceil(((double)countDataBlocks) / ((double)ctx.splitfileSegmentDataBlocks));
				segSize = (int)Math.ceil(((double)countDataBlocks) / ((double)segs));
			}
			segmentSize = segSize;
			if(cmode == CompatibilityMode.COMPAT_CURRENT || cmode.ordinal() >= CompatibilityMode.COMPAT_1255.ordinal()) {
				// Even with basic even segment splitting, it is possible for the last segment to be a lot smaller than the rest.
				// So drop a single data block from each of the last [segmentSize-lastSegmentSize] segments instead.
				// Hence all the segments are within 1 block of segmentSize.
				int lastSegmentSize = countDataBlocks - (segmentSize * (segs - 1));
				deductBlocksFromSegments = segmentSize - lastSegmentSize;
			} else {
				deductBlocksFromSegments = 0;
			}
		}
		
		int crossCheckBlocks = 0;
		
		// Cross-segment splitfile redundancy becomes useful at 20 segments.
		if(segs >= 20 && (cmode == CompatibilityMode.COMPAT_CURRENT || cmode.ordinal() >= CompatibilityMode.COMPAT_1255.ordinal())) {
			// The optimal number of cross-check blocks per segment (and per cross-segment since there are the same number of cross-segments as segments) is 3.
			crossCheckBlocks = 3;
		}
		
		this.crossCheckBlocks = crossCheckBlocks;
		
		if(splitfileAlgorithm == Metadata.SPLITFILE_NONREDUNDANT)
			checkSegmentSize = 0;
		else
			checkSegmentSize = FECCodec.getCheckBlocks(splitfileAlgorithm, segmentSize + crossCheckBlocks, cmode);
		
		this.persistent = persistent;
		if(persistent) {
			container.activate(parent, 1);
		}

		// Create segments
		this.splitfileCryptoAlgorithm = cryptoAlgorithm;
		if(splitfileKey != null) {
			this.splitfileCryptoKey = splitfileKey;
			specifySplitfileKeyInMetadata = true;
		} else if(cmode == CompatibilityMode.COMPAT_CURRENT || cmode.ordinal() >= CompatibilityMode.COMPAT_1255.ordinal()) {
			if(hashThisLayerOnly != null) {
				this.splitfileCryptoKey = Metadata.getCryptoKey(hashThisLayerOnly);
			} else {
				if(persistent) {
					// array elements are treated as part of the parent object, but the hashes themselves may not be activated?
					for(HashResult res : hashes) {
						if(res == null) throw new NullPointerException();
						container.activate(res, Integer.MAX_VALUE);
					}
				}
				this.splitfileCryptoKey = Metadata.getCryptoKey(hashes);
			}
			specifySplitfileKeyInMetadata = false;
		} else
			specifySplitfileKeyInMetadata = false;
		segments = splitIntoSegments(segmentSize, crossCheckBlocks, segs, deductBlocksFromSegments, dataBuckets, context.mainExecutor, container, context, persistent, put, cryptoAlgorithm, splitfileCryptoKey);
		if(persistent) {
			// Deactivate all buckets, and let dataBuckets be GC'ed
			for(int i=0;i<dataBuckets.length;i++) {
				// If we don't set them now, they will be set when the segment is set, which means they will be set deactivated, and cause NPEs.
				dataBuckets[i].storeTo(container);
				container.deactivate(dataBuckets[i], 1);
				if(dataBuckets.length > segmentSize) // Otherwise we are nulling out within the segment
					dataBuckets[i] = null;
			}
		}
		dataBuckets = null;
		
		if(crossCheckBlocks != 0) {
			byte[] seed = Metadata.getCrossSegmentSeed(hashes, hashThisLayerOnly);
			if(logMINOR) Logger.minor(this, "Cross-segment seed: "+HexUtil.bytesToHex(seed));
			Random random = new MersenneTwister(seed);
			// Cross segment redundancy: Allocate the blocks.
			crossSegments = new SplitFileInserterCrossSegment[segs];
			int segLen = segmentSize;
			for(int i=0;i<crossSegments.length;i++) {
				if(logMINOR) Logger.minor(this, "Allocating blocks for cross segment "+i);
				if(segments.length - i == deductBlocksFromSegments) {
					segLen--;
				}

				SplitFileInserterCrossSegment seg = new SplitFileInserterCrossSegment(persistent, segLen, crossCheckBlocks, put, splitfileAlgorithm, this, i);
				crossSegments[i] = seg;
				for(int j=0;j<segLen;j++) {
					// Allocate random data blocks
					allocateCrossDataBlock(seg, random);
				}
				for(int j=0;j<crossCheckBlocks;j++) {
					// Allocate check blocks
					allocateCrossCheckBlock(seg, random);
				}
				if(persistent) seg.storeTo(container);
			}
		} else {
			crossSegments = null;
		}
		
		
		int count = 0;
		for(int i=0;i<segments.length;i++)
			count += segments[i].countCheckBlocks();
		countCheckBlocks = count;
		// Save progress to disk, don't want to do all that again (probably includes compression in caller)
		parent.onMajorProgress(container);
		if(persistent) {
			for(int i=0;i<segments.length;i++) {
				container.store(segments[i]);
				container.deactivate(segments[i], 1);
			}
		}
	}

	private void allocateCrossDataBlock(SplitFileInserterCrossSegment segment, Random random) {
		int x = 0;
		for(int i=0;i<10;i++) {
			x = random.nextInt(segments.length);
			SplitFileInserterSegment seg = segments[x];
			int blockNum = seg.allocateCrossDataBlock(segment, random);
			if(blockNum >= 0) {
				segment.addDataBlock(seg, blockNum);
				return;
			}
		}
		for(int i=0;i<segments.length;i++) {
			x++;
			if(x == segments.length) x = 0;
			SplitFileInserterSegment seg = segments[x];
			int blockNum = seg.allocateCrossDataBlock(segment, random);
			if(blockNum >= 0) {
				segment.addDataBlock(seg, blockNum);
				return;
			}
		}
		throw new IllegalStateException("Unable to allocate cross data block!");
	}

	private void allocateCrossCheckBlock(SplitFileInserterCrossSegment segment, Random random) {
		int x = 0;
		for(int i=0;i<10;i++) {
			x = random.nextInt(segments.length);
			SplitFileInserterSegment seg = segments[x];
			int blockNum = seg.allocateCrossCheckBlock(segment, random);
			if(blockNum >= 0) {
				segment.addDataBlock(seg, blockNum);
				return;
			}
		}
		for(int i=0;i<segments.length;i++) {
			x++;
			if(x == segments.length) x = 0;
			SplitFileInserterSegment seg = segments[x];
			int blockNum = seg.allocateCrossCheckBlock(segment, random);
			if(blockNum >= 0) {
				segment.addDataBlock(seg, blockNum);
				return;
			}
		}
		throw new IllegalStateException("Unable to allocate cross data block!");
	}

	/**
	 * Group the blocks into segments.
	 * @param deductBlocksFromSegments 
	 */
	private SplitFileInserterSegment[] splitIntoSegments(int segmentSize, int crossCheckBlocks, int segCount, int deductBlocksFromSegments, Bucket[] origDataBlocks, Executor executor, ObjectContainer container, ClientContext context, boolean persistent, BaseClientPutter putter, byte cryptoAlgorithm, byte[] splitfileCryptoKey) {
		int dataBlocks = origDataBlocks.length;

		ArrayList<SplitFileInserterSegment> segs = new ArrayList<SplitFileInserterSegment>();

		CompatibilityMode cmode = ctx.getCompatibilityMode();
		// First split the data up
		if(segCount == 1) {
			// Single segment
			SplitFileInserterSegment onlySeg = new SplitFileInserterSegment(this, persistent, realTimeFlag, putter, splitfileAlgorithm, crossCheckBlocks, FECCodec.getCheckBlocks(splitfileAlgorithm, origDataBlocks.length + crossCheckBlocks, cmode), origDataBlocks, ctx, getCHKOnly, 0, cryptoAlgorithm, splitfileCryptoKey, container);
			segs.add(onlySeg);
		} else {
			int j = 0;
			int segNo = 0;
			int data = segmentSize;
			int check = FECCodec.getCheckBlocks(splitfileAlgorithm, data + crossCheckBlocks, cmode);
			for(int i=segmentSize;;) {
				if(i > dataBlocks) i = dataBlocks;
				if(data > (i-j)) {
					// Last segment.
					assert(segNo == segCount-1);
					data = i-j;
					check = FECCodec.getCheckBlocks(splitfileAlgorithm, data + crossCheckBlocks, cmode);
				}
				Bucket[] seg = new Bucket[i-j];
				System.arraycopy(origDataBlocks, j, seg, 0, data);
				j = i;
				for(int x=0;x<seg.length;x++)
					if(seg[x] == null) throw new NullPointerException("In splitIntoSegs: "+x+" is null of "+seg.length+" of "+segNo);
				SplitFileInserterSegment s = new SplitFileInserterSegment(this, persistent, realTimeFlag, putter, splitfileAlgorithm, crossCheckBlocks, check, seg, ctx, getCHKOnly, segNo, cryptoAlgorithm, splitfileCryptoKey, container);
				segs.add(s);
				
				if(deductBlocksFromSegments != 0)
					if(logMINOR) Logger.minor(this, "INSERTING: Segment "+segNo+" of "+segCount+" : "+data+" data blocks "+check+" check blocks");

				segNo++;
				if(i == dataBlocks) break;
				// Deduct one block from each later segment, rather than having a really short last segment.
				if(segCount - segNo == deductBlocksFromSegments) {
					data--;
					// Don't change check.
				}
				i += data;
			}
			assert(segNo == segCount);
		}
		if(persistent)
			container.activate(parent, 1);
		parent.notifyClients(container, context);
		return segs.toArray(new SplitFileInserterSegment[segs.size()]);
	}

	public void start(ObjectContainer container, final ClientContext context) throws InsertException {
		if(crossCheckBlocks != 0) {
			for(SplitFileInserterCrossSegment seg : crossSegments) {
				if(persistent)
					container.activate(seg, 1);
				seg.start(container, context);
				if(persistent)
					container.deactivate(seg, 1);
			}
		}
		for(int i=0;i<segments.length;i++) {
			if(persistent)
				container.activate(segments[i], 1);
			segments[i].start(container, context);
			if(persistent)
				container.deactivate(segments[i], 1);
		}
		if(persistent)
			container.activate(parent, 1);

		if(countDataBlocks > 32)
			parent.onMajorProgress(container);
		parent.notifyClients(container, context);

	}

	public void encodedSegment(SplitFileInserterSegment segment, ObjectContainer container, ClientContext context) {
		if(logMINOR) Logger.minor(this, "Encoded segment "+segment.segNo+" of "+this);
		boolean ret = false;
		boolean encode;
		synchronized(this) {
			encode = forceEncode;
			for(int i=0;i<segments.length;i++) {
				if(segments[i] != segment) {
					if(persistent)
						container.activate(segments[i], 1);
				}
				if((segments[i] == null) || !segments[i].isEncoded()) {
					ret = true;
					if(segments[i] != segment && persistent)
						container.deactivate(segments[i], 1);
					break;
				}
				if(segments[i] != segment && persistent)
					container.deactivate(segments[i], 1);
			}
		}
		if(encode) segment.forceEncode(container, context);
		if(ret) return;
		if(persistent)
			container.activate(cb, 1);
		cb.onBlockSetFinished(this, container, context);
		if(persistent)
			container.deactivate(cb, 1);
		if(countDataBlocks > 32) {
			if(persistent)
				container.activate(parent, 1);
			parent.onMajorProgress(container);
		}
	}

	public boolean segmentHasURIs(SplitFileInserterSegment segment, ObjectContainer container, ClientContext context) {
		if(logMINOR) Logger.minor(this, "Segment has URIs: "+segment);
		synchronized(this) {
			if(haveSentMetadata) {
				return false;
			}

			for(int i=0;i<segments.length;i++) {
				if(persistent)
					container.activate(segments[i], 1);
				boolean hasURIs = segments[i].hasURIs();
				if(persistent && segments[i] != segment)
					container.deactivate(segments[i], 1);
				if(!hasURIs) {
					if(logMINOR) Logger.minor(this, "Segment does not have URIs: "+segments[i]);
					return false;
				}
			}
		}

		if(logMINOR) Logger.minor(this, "Have URIs from all segments");
		encodeMetadata(container, context, segment);
		return true;
	}

	private void encodeMetadata(ObjectContainer container, ClientContext context, SplitFileInserterSegment dontDeactivateSegment) {
		context.jobRunner.setCommitThisTransaction();
		boolean missingURIs;
		Metadata m = null;
		ClientCHK[] dataURIs = new ClientCHK[countDataBlocks + crossCheckBlocks * segments.length];
		ClientCHK[] checkURIs = new ClientCHK[countCheckBlocks];
		synchronized(this) {
			int dpos = 0;
			int cpos = 0;
			for(int i=0;i<segments.length;i++) {
				if(persistent)
					container.activate(segments[i], 1);
				ClientCHK[] data = segments[i].getDataCHKs();
				System.arraycopy(data, 0, dataURIs, dpos, data.length);
				dpos += data.length;
				ClientCHK[] check = segments[i].getCheckCHKs();
				System.arraycopy(check, 0, checkURIs, cpos, check.length);
				cpos += check.length;
				if(persistent && segments[i] != dontDeactivateSegment)
					container.deactivate(segments[i], 1);
			}
			// Create metadata

			if(logMINOR) Logger.minor(this, "Data URIs: "+dataURIs.length+", check URIs: "+checkURIs.length);

			missingURIs = anyNulls(dataURIs) || anyNulls(checkURIs);

			if(persistent) {
				// Copy the URIs. We don't know what the callee wants the metadata for:
				// he might well ignore it, as in SimpleManifestPutter.onMetadata().
				// This way he doesn't need to worry about removing them.
				for(int i=0;i<dataURIs.length;i++) {
					container.activate(dataURIs[i], 5);
					dataURIs[i] = dataURIs[i].cloneKey();
				}
				for(int i=0;i<checkURIs.length;i++) {
					container.activate(checkURIs[i], 5);
					checkURIs[i] = checkURIs[i].cloneKey();
				}
			}

			if(!missingURIs) {
				// Create Metadata
				if(persistent) container.activate(cm, 5);
				ClientMetadata meta = cm;
				if(persistent) meta = meta == null ? null : meta.clone();
				boolean allowTopBlocks = topSize != 0;
				int req = 0;
				int total = 0;
				long data = 0;
				long compressed = 0;
				boolean topDontCompress = false;
				short topCompatibilityMode = 0;
				if(allowTopBlocks) {
					boolean wasActive = true;
					boolean ctxWasActive = true;
					if(persistent) {
						wasActive = container.ext().isActive(parent);
						if(!wasActive)
							container.activate(parent, 1);
						ctxWasActive = container.ext().isActive(ctx);
						if(!ctxWasActive)
							container.activate(ctx, 1);
					}
					req = parent.getMinSuccessFetchBlocks();
					total = parent.totalBlocks;
					if(!wasActive) container.deactivate(parent, 1);
					data = topSize;
					compressed = topCompressedSize;
				}
				if(persistent) container.activate(hashes, Integer.MAX_VALUE);
				HashResult[] h;
				if(persistent) h = HashResult.copy(hashes);
				else h = hashes;
				if(persistent) container.activate(compressionCodec, Integer.MAX_VALUE);
				if(persistent) container.activate(archiveType, Integer.MAX_VALUE);
				m = new Metadata(splitfileAlgorithm, dataURIs, checkURIs, segmentSize, checkSegmentSize, deductBlocksFromSegments, meta, dataLength, archiveType, compressionCodec, decompressedLength, isMetadata, h, hashThisLayerOnly, data, compressed, req, total, topDontCompress, topCompatibilityMode, splitfileCryptoAlgorithm, splitfileCryptoKey, specifySplitfileKeyInMetadata, crossCheckBlocks);
			}
			haveSentMetadata = true;
		}
		if(missingURIs) {
			if(logMINOR) Logger.minor(this, "Missing URIs");
			// Error
			fail(new InsertException(InsertException.INTERNAL_ERROR, "Missing URIs after encoding", null), container, context);
			return;
		} else {
			if(persistent)
				container.activate(cb, 1);
			cb.onMetadata(m, this, container, context);
			if(persistent)
				container.deactivate(cb, 1);
		}
	}

	private void fail(InsertException e, ObjectContainer container, ClientContext context) {
		synchronized(this) {
			if(finished) return;
			finished = true;
		}
		if(persistent) {
			container.store(this);
			container.activate(cb, 1);
		}
		cb.onFailure(e, this, container, context);
		if(persistent) {
			container.deactivate(cb, 1);
		}
	}

	// FIXME move this to somewhere
	private static boolean anyNulls(Object[] array) {
		for(int i=0;i<array.length;i++)
			if(array[i] == null) return true;
		return false;
	}

	public BaseClientPutter getParent() {
		return parent;
	}

	public void segmentFinished(SplitFileInserterSegment segment, ObjectContainer container, ClientContext context) {
		if(logMINOR) Logger.minor(this, "Segment finished: "+segment, new Exception("debug"));
		boolean allGone = true;
		if(countDataBlocks > 32) {
			if(persistent)
				container.activate(parent, 1);
			parent.onMajorProgress(container);
		}
		synchronized(this) {
			if(finished) {
				if(logMINOR) Logger.minor(this, "Finished already");
				return;
			}
			for(int i=0;i<segments.length;i++) {
				if(persistent && segments[i] != segment)
					container.activate(segments[i], 1);
				if(!segments[i].isFinished()) {
					if(logMINOR) Logger.minor(this, "Segment not finished: "+i+": "+segments[i]+" for "+this);
					allGone = false;
					if(persistent && segments[i] != segment)
						container.deactivate(segments[i], 1);
					break;
				}
				if(!segments[i].hasURIs()) {
					if(segments[i].getException(container) == null)
						Logger.error(this, "Segment finished but hasURIs() is false: "+segments[i]+" for "+this);
				}
				if(persistent && segments[i] != segment)
					container.deactivate(segments[i], 1);
			}

			InsertException e = segment.getException(container);
			if((e != null) && e.isFatal()) {
				cancel(container, context);
			} else {
				if(!allGone) return;
			}
			finished = true;
		}
		if(persistent)
			container.store(this);
		onAllFinished(container, context);
	}

	public void segmentFetchable(SplitFileInserterSegment segment, ObjectContainer container) {
		if(logMINOR) Logger.minor(this, "Segment fetchable: "+segment);
		synchronized(this) {
			if(finished) return;
			if(fetchable) return;
			for(int i=0;i<segments.length;i++) {
				if(persistent && segments[i] != segment)
					container.activate(segments[i], 1);
				if(!segments[i].isFetchable()) {
					if(logMINOR) Logger.minor(this, "Segment not fetchable: "+i+": "+segments[i]);
					if(persistent && segments[i] != segment)
						container.deactivate(segments[i], 1);
					return;
				}
				if(persistent && segments[i] != segment)
					container.deactivate(segments[i], 1);
			}
			fetchable = true;
		}
		if(persistent) {
			container.activate(cb, 1);
			container.store(this);
		}
		cb.onFetchable(this, container);
	}

	private void onAllFinished(ObjectContainer container, ClientContext context) {
		if(logMINOR) Logger.minor(this, "All finished");
		try {
			// Finished !!
			FailureCodeTracker tracker = new FailureCodeTracker(true);
			boolean allSucceeded = true;
			for(int i=0;i<segments.length;i++) {
				if(persistent)
					container.activate(segments[i], 1);
				InsertException e = segments[i].getException(container);
				if(e == null) continue;
				if(logMINOR) Logger.minor(this, "Failure on segment "+i+" : "+segments[i]+" : "+e, e);
				allSucceeded = false;
				if(e.errorCodes != null)
					tracker.merge(e.errorCodes);
				tracker.inc(e.getMode());
			}
			if(persistent)
				container.activate(cb, 1);
			if(allSucceeded)
				cb.onSuccess(this, container, context);
			else {
				cb.onFailure(InsertException.construct(tracker), this, container, context);
			}
		} catch (Throwable t) {
			// We MUST tell the parent *something*!
			Logger.error(this, "Caught "+t, t);
			cb.onFailure(new InsertException(InsertException.INTERNAL_ERROR), this, container, context);
		}
	}

	public void cancel(ObjectContainer container, ClientContext context) {
		if(logMINOR)
			Logger.minor(this, "Cancelling "+this);
		synchronized(this) {
			if(finished) return;
			finished = true;
		}
		if(persistent)
			container.store(this);
		for(int i=0;i<segments.length;i++) {
			if(persistent)
				container.activate(segments[i], 1);
			segments[i].cancel(container, context);
		}
		// The segments will call segmentFinished, but it will ignore them because finished=true.
		// Hence we need to call the callback here, since the caller expects us to.
		if(persistent)
			container.activate(cb, 1);
		cb.onFailure(new InsertException(InsertException.CANCELLED), this, container, context);
	}

	public void schedule(ObjectContainer container, ClientContext context) throws InsertException {
		start(container, context);
	}

	public Object getToken() {
		return token;
	}

	public long getLength() {
		return dataLength;
	}

	/** Force the remaining blocks which haven't been encoded so far to be encoded ASAP. */
	public void forceEncode(ObjectContainer container, ClientContext context) {
		if(persistent)
			container.activate(this, 1);
		Logger.minor(this, "Forcing encode on "+this);
		synchronized(this) {
			forceEncode = true;
		}
		for(int i=0;i<segments.length;i++) {
			if(persistent)
				container.activate(segments[i], 1);
			segments[i].forceEncode(container, context);
			if(persistent)
				container.deactivate(segments[i], 1);
		}
	}

	public void removeFrom(ObjectContainer container, ClientContext context) {
		// parent can remove itself
		// ctx will be removed by parent
		// cb will remove itself
		// cm will be removed by parent
		// token setter can remove token
		for(SplitFileInserterSegment segment : segments) {
			container.activate(segment, 1);
			segment.removeFrom(container, context);
		}
		if(hashes != null) {
			for(HashResult res : hashes) {
				container.activate(res, Integer.MAX_VALUE);
				res.removeFrom(container);
			}
		}
		container.delete(this);
	}

	public boolean objectCanUpdate(ObjectContainer container) {
		if(logDEBUG)
			Logger.debug(this, "objectCanUpdate() on "+this, new Exception("debug"));
		return true;
	}

	public boolean objectCanNew(ObjectContainer container) {
		if(finished)
			Logger.error(this, "objectCanNew but finished on "+this, new Exception("error"));
		else if(logDEBUG)
			Logger.debug(this, "objectCanNew() on "+this, new Exception("debug"));
		return true;
	}

	public void dump(ObjectContainer container) {
		System.out.println("This: "+this);
		System.out.println("Persistent: "+persistent);
		System.out.println("Finished: "+finished);
		System.out.println("Data length: "+dataLength);
		System.out.println("Segment count: "+segments.length);
		System.out.println("Fetchable: "+fetchable);
		container.activate(parent,1);
		System.out.println("Parent: "+parent);
		parent.dump(container);
		container.deactivate(parent, 1);
	}

	public void clearCrossSegment(int segNum, SplitFileInserterCrossSegment segment, ObjectContainer container, ClientContext context) {
		boolean clearedAll = true;
		synchronized(this) {
			assert(crossSegments[segNum] == segment);
			crossSegments[segNum] = null;
			for(SplitFileInserterCrossSegment seg : crossSegments) {
				if(seg != null) clearedAll = false;
			}
		}
		if(persistent) container.store(this);
		if(clearedAll) {
			for(int i=0;i<segments.length;i++) {
				if(persistent)
					container.activate(segments[i], 1);
				try {
					segments[i].start(container, context);
				} catch (InsertException e) {
					fail(e, container, context);
				}
				if(persistent)
					container.deactivate(segments[i], 1);
			}
			if(persistent)
				container.activate(parent, 1);

			if(countDataBlocks > 32)
				parent.onMajorProgress(container);
			parent.notifyClients(container, context);
		}
	}

}
package freenet.client.async;

import com.db4o.ObjectContainer;

import freenet.client.InsertException;
import freenet.client.Metadata;
import freenet.keys.BaseClientKey;

/**
 * Callback called when part of a put request completes.
 */
public interface PutCompletionCallback {

	public void onSuccess(ClientPutState state, ObjectContainer container, ClientContext context);
	
	public void onFailure(InsertException e, ClientPutState state, ObjectContainer container, ClientContext context);

	/** Called when we know the final URI of the state in question. The currentState eventually calls this
	 * on the ClientPutter, which relays to the fcp layer, which sends a URIGenerated message. */
	public void onEncode(BaseClientKey usk, ClientPutState state, ObjectContainer container, ClientContext context);
	
	public void onTransition(ClientPutState oldState, ClientPutState newState, ObjectContainer container);
	
	/** Only called if explicitly asked for, in which case, generally
	 * the metadata won't be inserted. Won't be called if there isn't
	 * any!
	 */
	public void onMetadata(Metadata m, ClientPutState state, ObjectContainer container, ClientContext context);
	
	/** Called when enough data has been inserted that the file can be
	 * retrieved, even if not all data has been inserted yet. Note that this
	 * is only supported for splitfiles; if you get onSuccess() first, assume
	 * that onFetchable() isn't coming. */
	public void onFetchable(ClientPutState state, ObjectContainer container);
	
	/** Called when the ClientPutState knows that it knows about
	 * all the blocks it will need to put.
	 */
	public void onBlockSetFinished(ClientPutState state, ObjectContainer container, ClientContext context);

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.useralerts;

import freenet.config.Option;
import freenet.config.SubConfig;
import freenet.l10n.NodeL10n;
import freenet.node.Node;
import freenet.support.HTMLNode;

public class InvalidAddressOverrideUserAlert extends AbstractUserAlert {
	
	public InvalidAddressOverrideUserAlert(Node n) {
		super(false, null, null, null, null, (short) 0, true, null, false, null);
		this.node = n;
	}
	
	final Node node;
	
	@Override
	public String getTitle() {
		return l10n("unknownAddressTitle");
	}

	@Override
	public String getText() {
		return l10n("unknownAddress");
	}

	private String l10n(String key) {
		return NodeL10n.getBase().getString("InvalidAddressOverrideUserAlert."+key);
	}

	@Override
	public HTMLNode getHTMLText() {
		SubConfig sc = node.config.get("node");
		Option<?> o = sc.getOption("ipAddressOverride");
		
		HTMLNode textNode = new HTMLNode("div");
		NodeL10n.getBase().addL10nSubstitution(textNode, "InvalidAddressOverrideUserAlert.unknownAddressWithConfigLink", 
				new String[] { "link" }, 
				new HTMLNode[] { HTMLNode.link("/config/node")});
		HTMLNode formNode = textNode.addChild("form", new String[] { "action", "method" }, new String[] { "/config/node", "post" });
		formNode.addChild("input", new String[] { "type", "name", "value" }, new String[] { "hidden", "formPassword", node.clientCore.formPassword });
		HTMLNode listNode = formNode.addChild("ul", "class", "config");
		HTMLNode itemNode = listNode.addChild("li");
		itemNode.addChild("span", "class", "configshortdesc", NodeL10n.getBase().getString(o.getShortDesc())).addChild("input", new String[] { "type", "name", "value" }, new String[] { "text", sc.getPrefix() + ".ipAddressOverride", o.getValueString() });
		itemNode.addChild("span", "class", "configlongdesc", NodeL10n.getBase().getString(o.getLongDesc()));
		formNode.addChild("input", new String[] { "type", "value" }, new String[] { "submit", NodeL10n.getBase().getString("UserAlert.apply") });
		formNode.addChild("input", new String[] { "type", "value" }, new String[] { "reset", NodeL10n.getBase().getString("UserAlert.reset") });
		return textNode;
	}

	@Override
	public short getPriorityClass() {
		return UserAlert.ERROR;
	}

	@Override
	public String getShortText() {
		return l10n("unknownAddressShort");
	}

}
/* Copyright 2007 Freenet Project Inc.
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 */
package freenet.clients.http;

import java.io.IOException;
import java.net.URI;

import freenet.client.HighLevelSimpleClient;
import freenet.io.AddressTracker;
import freenet.io.AddressTrackerItem;
import freenet.io.InetAddressAddressTrackerItem;
import freenet.io.PeerAddressTrackerItem;
import freenet.io.AddressTrackerItem.Gap;
import freenet.io.comm.UdpSocketHandler;
import freenet.l10n.NodeL10n;
import freenet.node.FSParseException;
import freenet.node.Node;
import freenet.node.NodeClientCore;
import freenet.support.HTMLNode;
import freenet.support.SimpleFieldSet;
import freenet.support.TimeUtil;
import freenet.support.api.HTTPRequest;

/**
 * Toadlet displaying information on the node's connectivity status.
 * Eventually this will include all information gathered by the node on its
 * connectivity from plugins, local IP detection, packet monitoring etc.
 * For the moment it's just a dump of the AddressTracker.
 * @author toad
 */
public class ConnectivityToadlet extends Toadlet {
	
	private final Node node;
	private final NodeClientCore core;

	protected ConnectivityToadlet(HighLevelSimpleClient client, Node node, NodeClientCore core) {
		super(client);
		this.node = node;
		this.core = core;
	}

	public void handleMethodGET(URI uri, final HTTPRequest request, ToadletContext ctx) throws ToadletContextClosedException, IOException {
		PageMaker pageMaker = ctx.getPageMaker();
		
		final int mode = ctx.getPageMaker().parseMode(request, container);
		PageNode page = pageMaker.getPageNode(NodeL10n.getBase().getString("ConnectivityToadlet.title", new String[]{ "nodeName" }, new String[]{ core.getMyName() }), ctx);
		HTMLNode pageNode = page.outer;
		HTMLNode contentNode = page.content;

		/* add alert summary box */
		if(ctx.isAllowedFullAccess())
			contentNode.addChild(core.alerts.createSummary());

		// our ports
		HTMLNode portInfobox = contentNode.addChild("div", "class", "infobox infobox-normal");
		portInfobox.addChild("div", "class", "infobox-header", l10nConn("nodePortsTitle"));
		HTMLNode portInfoboxContent = portInfobox.addChild("div", "class", "infobox-content");
		HTMLNode portInfoList = portInfoboxContent.addChild("ul");
		SimpleFieldSet fproxyConfig = node.config.get("fproxy").exportFieldSet(true);
		SimpleFieldSet fcpConfig = node.config.get("fcp").exportFieldSet(true);
		SimpleFieldSet tmciConfig = node.config.get("console").exportFieldSet(true);
		portInfoList.addChild("li", NodeL10n.getBase().getString("DarknetConnectionsToadlet.darknetFnpPort", new String[] { "port" }, new String[] { Integer.toString(node.getFNPPort()) }));
		int opennetPort = node.getOpennetFNPPort();
		if(opennetPort > 0)
			portInfoList.addChild("li", NodeL10n.getBase().getString("DarknetConnectionsToadlet.opennetFnpPort", new String[] { "port" }, new String[] { Integer.toString(opennetPort) }));
		try {
			if(fproxyConfig.getBoolean("enabled", false)) {
				portInfoList.addChild("li", NodeL10n.getBase().getString("DarknetConnectionsToadlet.fproxyPort", new String[] { "port" }, new String[] { Integer.toString(fproxyConfig.getInt("port")) }));
			} else {
				portInfoList.addChild("li", l10nConn("fproxyDisabled"));
			}
			if(fcpConfig.getBoolean("enabled", false)) {
				portInfoList.addChild("li", NodeL10n.getBase().getString("DarknetConnectionsToadlet.fcpPort", new String[] { "port" }, new String[] { Integer.toString(fcpConfig.getInt("port")) }));
			} else {
				portInfoList.addChild("li", l10nConn("fcpDisabled"));
			}
			if(tmciConfig.getBoolean("enabled", false)) {
				portInfoList.addChild("li", NodeL10n.getBase().getString("DarknetConnectionsToadlet.tmciPort", new String[] { "port" }, new String[] { Integer.toString(tmciConfig.getInt("port")) }));
			} else {
				portInfoList.addChild("li", l10nConn("tmciDisabled"));
			}
		} catch (FSParseException e) {
			// ignore
		}
		
		// Add connection type box.
		
		node.ipDetector.addConnectionTypeBox(contentNode);
		
		UdpSocketHandler[] handlers = node.getPacketSocketHandlers();
		
		HTMLNode summaryContent = pageMaker.getInfobox("#", NodeL10n.getBase().getString("ConnectivityToadlet.summaryTitle"), contentNode, "connectivity-summary", true);
		
		HTMLNode table = summaryContent.addChild("table", "border", "0");
		
		for(int i=0;i<handlers.length;i++) {
			UdpSocketHandler handler = handlers[i];
			AddressTracker tracker = handlers[i].getAddressTracker();
			HTMLNode row = table.addChild("tr");
			row.addChild("td", handler.getTitle());
			row.addChild("td", AddressTracker.statusString(tracker.getPortForwardStatus()));
		}
		
		if(mode >= PageMaker.MODE_ADVANCED) {
		
		// One box per port
		
		String noreply = l10n("noreply");
		String local = l10n("local");
		String remote = l10n("remote");
		long now = System.currentTimeMillis();
		
		for(int i=0;i<handlers.length;i++) {
			// Peers
			AddressTracker tracker = handlers[i].getAddressTracker();
			HTMLNode portsContent = pageMaker.getInfobox("#", NodeL10n.getBase().getString("ConnectivityToadlet.byPortTitle", new String[] { "port", "status", "tunnelLength" }, new String[] { handlers[i].getTitle(), AddressTracker.statusString(tracker.getPortForwardStatus()), TimeUtil.formatTime(tracker.getLongestSendReceiveGap()) }), contentNode, "connectivity-port", false);
			PeerAddressTrackerItem[] items = tracker.getPeerAddressTrackerItems();
			table = portsContent.addChild("table");
			HTMLNode row = table.addChild("tr");
			row.addChild("th", l10n("addressTitle"));
			row.addChild("th", l10n("sentReceivedTitle"));
			row.addChild("th", l10n("localRemoteTitle"));
			row.addChild("th", l10n("firstSendLeadTime"));
			row.addChild("th", l10n("firstReceiveLeadTime"));
			for(int j=0;j<AddressTrackerItem.TRACK_GAPS;j++) {
				row.addChild("th", " "); // FIXME is <th/> valid??
			}
			for(int j=0;j<items.length;j++) {
				row = table.addChild("tr");
				PeerAddressTrackerItem item = items[j];
				// Address
				row.addChild("td", item.peer.toString());
				// Sent/received packets
				row.addChild("td", item.packetsSent() + "/ " + item.packetsReceived());
				// Initiator: local/remote FIXME something more graphical e.g. colored cells
				row.addChild("td", item.packetsReceived() == 0 ? noreply :
						(item.weSentFirst() ? local : remote));
				// Lead in time to first packet sent
				row.addChild("td", TimeUtil.formatTime(item.timeFromStartupToFirstSentPacket()));
				// Lead in time to first packet received
				row.addChild("td", TimeUtil.formatTime(item.timeFromStartupToFirstReceivedPacket()));
				Gap[] gaps = item.getGaps();
				for(int k=0;k<AddressTrackerItem.TRACK_GAPS;k++) {
					row.addChild("td", gaps[k].receivedPacketAt == 0 ? "" : 
						(TimeUtil.formatTime(gaps[k].gapLength)+" @ "+TimeUtil.formatTime(now - gaps[k].receivedPacketAt)+" ago" /* fixme l10n */));
				}
			}

			// IPs
			portsContent = pageMaker.getInfobox("#", NodeL10n.getBase().getString("ConnectivityToadlet.byIPTitle", new String[] { "ip", "status", "tunnelLength" }, new String[] { handlers[i].getTitle(), AddressTracker.statusString(tracker.getPortForwardStatus()), TimeUtil.formatTime(tracker.getLongestSendReceiveGap()) }), contentNode, "connectivity-ip", false);
			InetAddressAddressTrackerItem[] ipItems = tracker.getInetAddressTrackerItems();
			table = portsContent.addChild("table");
			row = table.addChild("tr");
			row.addChild("th", l10n("addressTitle"));
			row.addChild("th", l10n("sentReceivedTitle"));
			row.addChild("th", l10n("localRemoteTitle"));
			row.addChild("th", l10n("firstSendLeadTime"));
			row.addChild("th", l10n("firstReceiveLeadTime"));
			for(int j=0;j<AddressTrackerItem.TRACK_GAPS;j++) {
				row.addChild("th", " "); // FIXME is <th/> valid??
			}
			for(int j=0;j<ipItems.length;j++) {
				row = table.addChild("tr");
				InetAddressAddressTrackerItem item = ipItems[j];
				// Address
				row.addChild("td", item.addr.toString());
				// Sent/received packets
				row.addChild("td", item.packetsSent() + "/ " + item.packetsReceived());
				// Initiator: local/remote FIXME something more graphical e.g. colored cells
				row.addChild("td", item.packetsReceived() == 0 ? noreply :
						(item.weSentFirst() ? local : remote));
				// Lead in time to first packet sent
				row.addChild("td", TimeUtil.formatTime(item.timeFromStartupToFirstSentPacket()));
				// Lead in time to first packet received
				row.addChild("td", TimeUtil.formatTime(item.timeFromStartupToFirstReceivedPacket()));
				Gap[] gaps = item.getGaps();
				for(int k=0;k<AddressTrackerItem.TRACK_GAPS;k++) {
					row.addChild("td", gaps[k].receivedPacketAt == 0 ? "" : 
						(TimeUtil.formatTime(gaps[k].gapLength)+" @ "+TimeUtil.formatTime(now - gaps[k].receivedPacketAt)+" ago" /* fixme l10n */));
				}
			}

		}
		
		}
		
		writeHTMLReply(ctx, 200, "OK", pageNode.generate());
	}
	
	private String l10nConn(String string) {
		return NodeL10n.getBase().getString("DarknetConnectionsToadlet."+string);
	}

	private String l10n(String key) {
		return NodeL10n.getBase().getString("ConnectivityToadlet."+key);
	}

	public static final String PATH = "/connectivity/";
	
	@Override
	public String path() {
		return PATH;
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.pluginmanager;

/**
 * Force the download of something to disk
 * 
 * @author Florent Daigni&egrave;re &lt;nextgens@freenetproject.org&gt;
 */
public class DownloadPluginHTTPException extends PluginHTTPException {
	private static final long serialVersionUID = -1;
	
	public static final short CODE = 200; // Found
	public final String filename;
	public final String mimeType;
	public final byte[] data;

	public DownloadPluginHTTPException(byte[] data, String filename, String mimeType) {
		super("Ok", "none");
		this.data = data;
		this.filename = filename;
		this.mimeType = mimeType;
	}
}
package freenet.node;

/** Everything that a PacketFormat needs which is associated with a single SessionKey. */
public interface PacketFormatKeyContext {

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

import com.db4o.ObjectContainer;

import freenet.client.async.ChosenBlock;
import freenet.client.async.ClientContext;
import freenet.client.async.HasCooldownCacheItem;
import freenet.client.async.TransientChosenBlock;
import freenet.keys.Key;
import freenet.node.NodeStats.RejectReason;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.OOMHandler;
import freenet.support.RandomGrabArrayItem;
import freenet.support.RandomGrabArrayItemExclusionList;
import freenet.support.TokenBucket;
import freenet.support.Logger.LogLevel;
import freenet.support.math.RunningAverage;

/**
 * Starts requests.
 * Nobody starts a request directly, you have to go through RequestStarter.
 * And you have to provide a RequestStarterClient. We do round robin between 
 * clients on the same priority level.
 */
public class RequestStarter implements Runnable, RandomGrabArrayItemExclusionList {
	private static volatile boolean logMINOR;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	/*
	 * Priority classes
	 */
	/** Anything more important than FProxy */
	public static final short MAXIMUM_PRIORITY_CLASS = 0;
	/** FProxy etc */
	public static final short INTERACTIVE_PRIORITY_CLASS = 1;
	/** FProxy splitfile fetches */
	public static final short IMMEDIATE_SPLITFILE_PRIORITY_CLASS = 2;
	/** USK updates etc */
	public static final short UPDATE_PRIORITY_CLASS = 3;
	/** Bulk splitfile fetches */
	public static final short BULK_SPLITFILE_PRIORITY_CLASS = 4;
	/** Prefetch */
	public static final short PREFETCH_PRIORITY_CLASS = 5;
	/** Anything less important than prefetch (redundant??) */
	public static final short MINIMUM_PRIORITY_CLASS = 6;
	
	public static final short NUMBER_OF_PRIORITY_CLASSES = MINIMUM_PRIORITY_CLASS - MAXIMUM_PRIORITY_CLASS + 1; // include 0 and max !!
	
	/** If true, local requests are subject to shouldRejectRequest(). If false, they are only subject to the token
	 * buckets and the thread limit. FIXME make configurable. */
	static final boolean LOCAL_REQUESTS_COMPETE_FAIRLY = true;
	
	public static boolean isValidPriorityClass(int prio) {
		return !((prio < MAXIMUM_PRIORITY_CLASS) || (prio > MINIMUM_PRIORITY_CLASS));
	}
	
	final BaseRequestThrottle throttle;
	final TokenBucket inputBucket;
	final TokenBucket outputBucket;
	final RunningAverage averageInputBytesPerRequest;
	final RunningAverage averageOutputBytesPerRequest;
	RequestScheduler sched;
	final NodeClientCore core;
	final NodeStats stats;
	private long sentRequestTime;
	private final boolean isInsert;
	private final boolean isSSK;
	final boolean realTime;
	
	public RequestStarter(NodeClientCore node, BaseRequestThrottle throttle, String name, TokenBucket outputBucket, TokenBucket inputBucket,
			RunningAverage averageOutputBytesPerRequest, RunningAverage averageInputBytesPerRequest, boolean isInsert, boolean isSSK, boolean realTime) {
		this.core = node;
		this.stats = core.nodeStats;
		this.throttle = throttle;
		this.name = name + (realTime ? " (realtime)" : " (bulk)");
		this.outputBucket = outputBucket;
		this.inputBucket = inputBucket;
		this.averageOutputBytesPerRequest = averageOutputBytesPerRequest;
		this.averageInputBytesPerRequest = averageInputBytesPerRequest;
		this.isInsert = isInsert;
		this.isSSK = isSSK;
		this.realTime = realTime;
	}

	void setScheduler(RequestScheduler sched) {
		this.sched = sched;
	}
	
	void start() {
		sched.start(core);
		core.getExecutor().execute(this, name);
		sched.queueFillRequestStarterQueue();
	}
	
	final String name;
	
	@Override
	public String toString() {
		return name;
	}
	
	void realRun() {
		ChosenBlock req = null;
		sentRequestTime = System.currentTimeMillis();
		// The last time at which we sent a request or decided not to
		long cycleTime = sentRequestTime;
		while(true) {
			// Allow 5 minutes before we start killing requests due to not connecting.
			OpennetManager om;
			if(core.node.peers.countConnectedPeers() < 3 && (om = core.node.getOpennet()) != null &&
					System.currentTimeMillis() - om.getCreationTime() < 5*60*1000) {
				try {
					synchronized(this) {
						wait(1000);
					}
				} catch (InterruptedException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				}
				continue;
			}
			if(req == null) {
				req = sched.grabRequest();
			}
			if(req != null) {
				if(logMINOR) Logger.minor(this, "Running "+req+" priority "+req.getPriority());
				if(!req.localRequestOnly) {
					// Wait
					long delay = throttle.getDelay();
					if(logMINOR) Logger.minor(this, "Delay="+delay+" from "+throttle);
					long sleepUntil = cycleTime + delay;
					if(!LOCAL_REQUESTS_COMPETE_FAIRLY) {
						inputBucket.blockingGrab((int)(Math.max(0, averageInputBytesPerRequest.currentValue())));
						outputBucket.blockingGrab((int)(Math.max(0, averageOutputBytesPerRequest.currentValue())));
					}
					long now;
					do {
						now = System.currentTimeMillis();
						if(now < sleepUntil)
							try {
								Thread.sleep(sleepUntil - now);
								if(logMINOR) Logger.minor(this, "Slept: "+(sleepUntil-now)+"ms");
							} catch (InterruptedException e) {
								// Ignore
							}
					} while(now < sleepUntil);
				}
				RejectReason reason;
				assert(req.realTimeFlag == realTime);
				if(LOCAL_REQUESTS_COMPETE_FAIRLY && !req.localRequestOnly) {
					if((reason = stats.shouldRejectRequest(true, isInsert, isSSK, true, false, null, false, isInsert && Node.PREFER_INSERT_DEFAULT, req.realTimeFlag)) != null) {
						if(logMINOR)
							Logger.minor(this, "Not sending local request: "+reason);
						// Wait one throttle-delay before trying again
						cycleTime = System.currentTimeMillis();
						continue; // Let local requests compete with all the others
					}
				} else {
					stats.waitUntilNotOverloaded(isInsert);
				}
			} else {
				if(logMINOR) Logger.minor(this, "Waiting...");				
				// Always take the lock on RequestStarter first. AFAICS we don't synchronize on RequestStarter anywhere else.
				// Nested locks here prevent extra latency when there is a race, and therefore allow us to sleep indefinitely
				synchronized(this) {
					req = sched.grabRequest();
					if(req == null) {
						try {
							wait(1*1000); // this can happen when most but not all stuff is already running but there is still stuff to fetch, so don't wait *too* long.
							// FIXME increase when we can be *sure* there is nothing left in the queue (especially for transient requests).
						} catch (InterruptedException e) {
							// Ignore
						}
					}
				}
			}
			if(req == null) continue;
			if(!startRequest(req, logMINOR)) {
				// Don't log if it's a cancelled transient request.
				if(!((!req.isPersistent()) && req.isCancelled()))
					Logger.normal(this, "No requests to start on "+req);
			}
			if(!req.localRequestOnly)
				cycleTime = sentRequestTime = System.currentTimeMillis();
			req = null;
		}
	}

	private boolean startRequest(ChosenBlock req, boolean logMINOR) {
		if((!req.isPersistent()) && req.isCancelled()) {
			req.onDumped();
			return false;
		}
		if(req.key != null) {
			if(!sched.addToFetching(req.key)) {
				req.onDumped();
				return false;
			}
		} else if((!req.isPersistent()) && ((TransientChosenBlock)req).request instanceof SendableInsert) {
			if(!sched.addTransientInsertFetching((SendableInsert)(((TransientChosenBlock)req).request), req.token)) {
				req.onDumped();
				return false;
			}
		}
		if(logMINOR) Logger.minor(this, "Running request "+req+" priority "+req.getPriority());
		core.getExecutor().execute(new SenderThread(req, req.key), "RequestStarter$SenderThread for "+req);
		return true;
	}

	public void run() {
	    freenet.support.Logger.OSThread.logPID(this);
		while(true) {
			try {
				realRun();
            } catch (OutOfMemoryError e) {
				OOMHandler.handleOOM(e);
			} catch (Throwable t) {
				Logger.error(this, "Caught "+t, t);
			}
		}
	}
	
	private class SenderThread implements Runnable {

		private final ChosenBlock req;
		private final Key key;
		
		public SenderThread(ChosenBlock req, Key key) {
			this.req = req;
			this.key = key;
		}

		public void run() {
			try {
		    freenet.support.Logger.OSThread.logPID(this);
		    // FIXME ? key is not known for inserts here
		    if (key != null)
		    	stats.reportOutgoingLocalRequestLocation(key.toNormalizedDouble());
		    if(!req.send(core, sched)) {
				if(!((!req.isPersistent()) && req.isCancelled()))
					Logger.error(this, "run() not able to send a request on "+req);
				else
					Logger.normal(this, "run() not able to send a request on "+req+" - request was cancelled");
			}
			if(logMINOR) 
				Logger.minor(this, "Finished "+req);
			} finally {
				if(key != null) sched.removeFetchingKey(key);
				else if((!req.isPersistent()) && ((TransientChosenBlock)req).request instanceof SendableInsert)
					sched.removeTransientInsertFetching((SendableInsert)(((TransientChosenBlock)req).request), req.token);
				// Something might be waiting for a request to complete (e.g. if we have two requests for the same key), 
				// so wake the starter thread.
				wakeUp();
			}
		}
		
	}

	/** LOCKING: Caller must avoid locking while calling this function. In particular,
	 * if the RequestStarter lock is held we will get a deadlock. */
	public void wakeUp() {
		synchronized(this) {
			notifyAll();
		}
	}

	/** Can this item be excluded, based on e.g. already running requests? It must have 
	 * been activated already if it is persistent.
	 */
	public long exclude(RandomGrabArrayItem item, ObjectContainer container, ClientContext context, long now) {
		if(sched.isRunningOrQueuedPersistentRequest((SendableRequest)item)) {
			Logger.normal(this, "Excluding already-running request: "+item, new Exception("debug"));
			return Long.MAX_VALUE;
		}
		if(isInsert) return -1;
		if(!(item instanceof BaseSendableGet)) {
			Logger.error(this, "On a request scheduler, exclude() called with "+item, new Exception("error"));
			return -1;
		}
		BaseSendableGet get = (BaseSendableGet) item;
		return get.getCooldownTime(container, context, now);
	}

	/** Can this item be excluded based solely on the cooldown queue?
	 * @return -1 if the item can be run now, or the time at which it is on the cooldown queue until.
	 */
	public long excludeSummarily(HasCooldownCacheItem item,
			HasCooldownCacheItem parent, ObjectContainer container, boolean persistent, long now) {
		return core.clientContext.cooldownTracker.getCachedWakeup(item, persistent, container, now);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.io;

import java.io.File;

public interface PersistentFileTracker {

	public void register(File file);

	/** Notify that we have finished with a bucket and it should be freed after the
	 * next serialization to disk.
	 * @param bucket The bucket to free. Should be a DelayedFreeBucket.
	 */
	public void delayedFreeBucket(DelayedFreeBucket bucket);

	/**
	 * Get the persistent temp files directory.
	 */
	public File getDir();

	/**
	 * Is the file in question one of our persistent temp files?
	 */
	public boolean matches(File file);

	public FilenameGenerator getGenerator();

	public long getID(File file);
	
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

import java.io.BufferedReader;
import java.io.EOFException;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.UnsupportedEncodingException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.Vector;
import java.util.concurrent.CopyOnWriteArrayList;

import freenet.io.comm.AsyncMessageCallback;
import freenet.io.comm.ByteCounter;
import freenet.io.comm.DMT;
import freenet.io.comm.FreenetInetAddress;
import freenet.io.comm.Message;
import freenet.io.comm.NotConnectedException;
import freenet.io.comm.Peer;
import freenet.io.comm.PeerParseException;
import freenet.io.comm.ReferenceSignatureVerificationException;
import freenet.keys.Key;
import freenet.node.DarknetPeerNode.FRIEND_TRUST;
import freenet.node.useralerts.PeerManagerUserAlert;
import freenet.support.ByteArrayWrapper;
import freenet.support.Logger;
import freenet.support.ShortBuffer;
import freenet.support.SimpleFieldSet;
import freenet.support.io.Closer;
import freenet.support.io.FileUtil;

/**
 * @author amphibian
 * 
 * Maintains:
 * - A list of peers we want to connect to.
 * - A list of peers we are actually connected to.
 * - Each peer's Location.
 */
public class PeerManager {

        private static volatile boolean logMINOR;
        static {
            Logger.registerClass(PeerManager.class);
        }
	/** Our Node */
	final Node node;
	/** All the peers we want to connect to */
	PeerNode[] myPeers;
	/** All the peers we are actually connected to */
	PeerNode[] connectedPeers;
	private String darkFilename;
        private String openFilename;
        private String oldOpennetPeersFilename;
        // FIXME either track dirty status separately for each of the three files,
        // or implement a cheaper cache e.g. a secure hash, maybe with file size as a 
        // short-cut. String.hashCode() unfortunately will give way too many false 
        // positives and thus result in not writing important changes.
        private String darknetPeersStringCache = null;
        private String opennetPeersStringCache = null;
        private String oldOpennetPeersStringCache = null;
        private PeerManagerUserAlert ua;	// Peers stuff
	/** age of oldest never connected peer (milliseconds) */
	private long oldestNeverConnectedDarknetPeerAge;
	/** Next time to update oldestNeverConnectedPeerAge */
	private long nextOldestNeverConnectedDarknetPeerAgeUpdateTime = -1;
	/** oldestNeverConnectedPeerAge update interval (milliseconds) */
	private static final long oldestNeverConnectedPeerAgeUpdateInterval = 5000;
	/** Next time to log the PeerNode status summary */
	private long nextPeerNodeStatusLogTime = -1;
	/** PeerNode status summary log interval (milliseconds) */
	private static final long peerNodeStatusLogInterval = 5000;
	/** PeerNode statuses, by status */
	private final HashMap<Integer, HashSet<PeerNode>> peerNodeStatuses;
	/** DarknetPeerNode statuses, by status */
	private final HashMap<Integer, HashSet<PeerNode>> peerNodeStatusesDarknet;
	/** PeerNode routing backoff reasons, by reason (realtime) */
	private final HashMap<String, HashSet<PeerNode>> peerNodeRoutingBackoffReasonsRT;
	/** PeerNode routing backoff reasons, by reason (bulk) */
	private final HashMap<String, HashSet<PeerNode>> peerNodeRoutingBackoffReasonsBulk;
	/** Next time to update routableConnectionStats */
	private long nextRoutableConnectionStatsUpdateTime = -1;
	/** routableConnectionStats update interval (milliseconds) */
	private static final long routableConnectionStatsUpdateInterval = 7 * 1000;  // 7 seconds
	
	/** Should update the peer-file ? */
	private volatile boolean shouldWritePeers = false;
	private static final int MIN_WRITEPEERS_DELAY = 5*1000; // 5sec
	private final Runnable writePeersRunnable = new Runnable() {

		public void run() {
			try {
				if(shouldWritePeers) {
					shouldWritePeers = false;
					writePeersInner();
				}
			} finally {
				node.getTicker().queueTimedJob(writePeersRunnable, MIN_WRITEPEERS_DELAY);
			}
		}
	};
	
	public static final int PEER_NODE_STATUS_CONNECTED = 1;
	public static final int PEER_NODE_STATUS_ROUTING_BACKED_OFF = 2;
	public static final int PEER_NODE_STATUS_TOO_NEW = 3;
	public static final int PEER_NODE_STATUS_TOO_OLD = 4;
	public static final int PEER_NODE_STATUS_DISCONNECTED = 5;
	public static final int PEER_NODE_STATUS_NEVER_CONNECTED = 6;
	public static final int PEER_NODE_STATUS_DISABLED = 7;
	public static final int PEER_NODE_STATUS_BURSTING = 8;
	public static final int PEER_NODE_STATUS_LISTENING = 9;
	public static final int PEER_NODE_STATUS_LISTEN_ONLY = 10;
	public static final int PEER_NODE_STATUS_CLOCK_PROBLEM = 11;
	public static final int PEER_NODE_STATUS_CONN_ERROR = 12;
	public static final int PEER_NODE_STATUS_DISCONNECTING = 13;
	public static final int PEER_NODE_STATUS_ROUTING_DISABLED = 14;
	
	/** The list of listeners that needs to be notified when peers' statuses changed*/
	private List<PeerStatusChangeListener> listeners=new CopyOnWriteArrayList<PeerStatusChangeListener>();

	/**
	 * Create a PeerManager by reading a list of peers from
	 * a file.
	 * @param node
	 * @param filename
	 */
	public PeerManager(Node node) {
		Logger.normal(this, "Creating PeerManager");
		peerNodeStatuses = new HashMap<Integer, HashSet<PeerNode>>();
		peerNodeStatusesDarknet = new HashMap<Integer, HashSet<PeerNode>>();
		peerNodeRoutingBackoffReasonsRT = new HashMap<String, HashSet<PeerNode>>();
		peerNodeRoutingBackoffReasonsBulk = new HashMap<String, HashSet<PeerNode>>();
		System.out.println("Creating PeerManager");
		myPeers = new PeerNode[0];
		connectedPeers = new PeerNode[0];
		this.node = node;
	}

	/**
	 * Attempt to read a file full of noderefs. Try the file as named first, then the .bak if it is empty or
	 * otherwise doesn't work.
	 * @param filename The filename to read from. If this doesn't work, we try the .bak file.
	 * @param crypto The cryptographic identity which these nodes are connected to.
	 * @param opennet The opennet manager for the nodes. Only needed (for constructing the nodes) if isOpennet.
	 * @param isOpennet Whether the file contains opennet peers.
	 * @param oldOpennetPeers If true, don't add the nodes to the routing table, pass them to the opennet
	 * manager as "old peers" i.e. inactive nodes which may try to reconnect.
	 */
	void tryReadPeers(String filename, NodeCrypto crypto, OpennetManager opennet, boolean isOpennet, boolean oldOpennetPeers) {
		synchronized(writePeersSync) {
			if(!oldOpennetPeers)
				if(isOpennet)
					openFilename = filename;
				else
					darkFilename = filename;
		}
		OutgoingPacketMangler mangler = crypto.packetMangler;
		File peersFile = new File(filename);
		File backupFile = new File(filename + ".bak");
		// Try to read the node list from disk
		if(peersFile.exists())
			if(readPeers(peersFile, mangler, crypto, opennet, oldOpennetPeers)) {
				String msg;
				if(oldOpennetPeers)
					msg = "Read " + opennet.countOldOpennetPeers() + " old-opennet-peers from " + peersFile;
				else if(isOpennet)
					msg = "Read " + getOpennetPeers().length + " opennet peers from " + peersFile;
				else
					msg = "Read " + getDarknetPeers().length + " darknet peers from " + peersFile;
				Logger.normal(this, msg);
				System.out.println(msg);
				return;
			}
		// Try the backup
		if(backupFile.exists())
			if(readPeers(backupFile, mangler, crypto, opennet, oldOpennetPeers)) {
				String msg;
				if(oldOpennetPeers)
					msg = "Read " + opennet.countOldOpennetPeers() + " old-opennet-peers from " + peersFile;
				else if(isOpennet)
					msg = "Read " + getOpennetPeers().length + " opennet peers from " + peersFile;
				else
					msg = "Read " + getDarknetPeers().length + " darknet peers from " + peersFile;
				Logger.normal(this, msg);
				System.out.println(msg);
			} else {
				Logger.error(this, "No (readable) peers file with peers in it found");
				System.err.println("No (readable) peers file with peers in it found");
			}
	}

	private boolean readPeers(File peersFile, OutgoingPacketMangler mangler, NodeCrypto crypto, OpennetManager opennet, boolean oldOpennetPeers) {
		boolean gotSome = false;
		FileInputStream fis;
		try {
			fis = new FileInputStream(peersFile);
		} catch(FileNotFoundException e4) {
			Logger.normal(this, "Peers file not found: " + peersFile);
			return false;
		}
		InputStreamReader ris;
		try {
			ris = new InputStreamReader(fis, "UTF-8");
		} catch(UnsupportedEncodingException e4) {
			throw new Error("Impossible: JVM doesn't support UTF-8: " + e4, e4);
		}
		BufferedReader br = new BufferedReader(ris);
		try { // FIXME: no better way?
			while(true) {
				// Read a single NodePeer
				SimpleFieldSet fs;
				fs = new SimpleFieldSet(br, false, true);
				PeerNode pn;
				try {
					pn = PeerNode.create(fs, node, crypto, opennet, this, mangler);
				} catch(FSParseException e2) {
					Logger.error(this, "Could not parse peer: " + e2 + '\n' + fs.toString(), e2);
					continue;
				} catch(PeerParseException e2) {
					Logger.error(this, "Could not parse peer: " + e2 + '\n' + fs.toString(), e2);
					continue;
				} catch(ReferenceSignatureVerificationException e2) {
					Logger.error(this, "Could not parse peer: " + e2 + '\n' + fs.toString(), e2);
					continue;
				}
				if(oldOpennetPeers)
					opennet.addOldOpennetNode(pn);
				else
					addPeer(pn, true, false);
				gotSome = true;
			}
		} catch(EOFException e) {
			// End of file, fine
		} catch(IOException e1) {
			Logger.error(this, "Could not read peers file: " + e1, e1);
		}
		try {
			br.close();
		} catch(IOException e3) {
			Logger.error(this, "Ignoring " + e3 + " caught reading " + peersFile, e3);
		}
		return gotSome;
	}

	public boolean addPeer(PeerNode pn) {
		return addPeer(pn, false, false);
	}

	/**
	 * Add a peer.
	 * @param pn The node to add to the routing table.
	 * @param ignoreOpennet If true, don't check for opennet peers. If false, check for opennet peers and if so,
	 * if opennet is enabled auto-add them to the opennet LRU, otherwise fail.
	 * @param reactivate If true, re-enable the peer if it is in state DISCONNECTING before re-adding it.
	 * @return True if the node was successfully added. False if it was already present, or if we tried to add
	 * an opennet peer when opennet was disabled.
	 */
	boolean addPeer(PeerNode pn, boolean ignoreOpennet, boolean reactivate) {
		assert (pn != null);
		if(reactivate)
			pn.forceCancelDisconnecting();
		synchronized(this) {
			for(int i = 0; i < myPeers.length; i++) {
				if(myPeers[i].equals(pn)) {
					if(logMINOR)
						Logger.minor(this, "Can't add peer " + pn + " because already have " + myPeers[i], new Exception("debug"));
					return false;
				}
			}
			PeerNode[] newMyPeers = new PeerNode[myPeers.length + 1];
			System.arraycopy(myPeers, 0, newMyPeers, 0, myPeers.length);
			newMyPeers[myPeers.length] = pn;
			myPeers = newMyPeers;
			Logger.normal(this, "Added " + pn);
		}
		if(pn.recordStatus())
			addPeerNodeStatus(pn.getPeerNodeStatus(), pn, false);
		pn.setPeerNodeStatus(System.currentTimeMillis());
		if(!pn.isSeed())
                    updatePMUserAlert();
		if((!ignoreOpennet) && pn instanceof OpennetPeerNode) {
			OpennetManager opennet = node.getOpennet();
			if(opennet != null)
				opennet.forceAddPeer(pn, true);
			else {
				Logger.error(this, "Adding opennet peer when no opennet enabled!!!: " + pn + " - removing...");
				removePeer(pn);
				return false;
			}
		}
		notifyPeerStatusChangeListeners();
		return true;
	}

	synchronized boolean havePeer(PeerNode pn) {
		for(int i = 0; i < myPeers.length; i++) {
			if(myPeers[i] == pn)
				return true;
		}
		return false;
	}

	private boolean removePeer(PeerNode pn) {
		if(logMINOR)
			Logger.minor(this, "Removing " + pn);
		boolean isInPeers = false;
		synchronized(this) {
			for(int i = 0; i < myPeers.length; i++) {
				if(myPeers[i] == pn)
					isInPeers = true;
			}
			if(pn instanceof DarknetPeerNode)
				((DarknetPeerNode) pn).removeExtraPeerDataDir();
			if(isInPeers) {
				int peerNodeStatus = pn.getPeerNodeStatus();
				if(pn.recordStatus())
					removePeerNodeStatus(peerNodeStatus, pn, !isInPeers);
				String peerNodePreviousRoutingBackoffReason = pn.getPreviousBackoffReason(true);
				if(peerNodePreviousRoutingBackoffReason != null)
					removePeerNodeRoutingBackoffReason(peerNodePreviousRoutingBackoffReason, pn, true);
				peerNodePreviousRoutingBackoffReason = pn.getPreviousBackoffReason(false);
				if(peerNodePreviousRoutingBackoffReason != null)
					removePeerNodeRoutingBackoffReason(peerNodePreviousRoutingBackoffReason, pn, false);

				// removing from connectedPeers
				ArrayList<PeerNode> a = new ArrayList<PeerNode>();
				for(PeerNode mp : myPeers) {
					if((mp != pn) && mp.isConnected() && mp.isRealConnection())
						a.add(mp);
				}

				PeerNode[] newConnectedPeers = new PeerNode[a.size()];
				newConnectedPeers = a.toArray(newConnectedPeers);
				connectedPeers = newConnectedPeers;

				// removing from myPeers
				PeerNode[] newMyPeers = new PeerNode[myPeers.length - 1];
				int positionInNewArray = 0;
				for(PeerNode mp : myPeers) {
					if(mp != pn) {
						newMyPeers[positionInNewArray] = mp;
						positionInNewArray++;
					}
				}
				myPeers = newMyPeers;

				Logger.normal(this, "Removed " + pn);
			}
		}
		pn.onRemove();
		if(isInPeers && !pn.isSeed())
			updatePMUserAlert();
		notifyPeerStatusChangeListeners();
		return true;
	}

	public boolean removeAllPeers() {
		Logger.normal(this, "removeAllPeers!");
		PeerNode[] oldPeers;
		synchronized(this) {
			oldPeers = myPeers;
			myPeers = new PeerNode[0];
			connectedPeers = new PeerNode[0];
		}
		for(int i = 0; i < oldPeers.length; i++)
			oldPeers[i].onRemove();
		notifyPeerStatusChangeListeners();
		return true;
	}

	public boolean disconnected(PeerNode pn) {
		synchronized(this) {
			boolean isInPeers = false;
			for(int i = 0; i < connectedPeers.length; i++) {
				if(connectedPeers[i] == pn)
					isInPeers = true;
			}
			if(!isInPeers)
				return false;
			// removing from connectedPeers
			ArrayList<PeerNode> a = new ArrayList<PeerNode>();
			for(PeerNode mp : myPeers) {
				if((mp != pn) && mp.isRoutable())
					a.add(mp);
			}
			PeerNode[] newConnectedPeers = new PeerNode[a.size()];
			newConnectedPeers = a.toArray(newConnectedPeers);
			connectedPeers = newConnectedPeers;
		}
                if(!pn.isSeed())
                    updatePMUserAlert();
		node.lm.announceLocChange();
		return true;
	}
	long timeFirstAnyConnections = 0;

	public long getTimeFirstAnyConnections() {
		return timeFirstAnyConnections;
	}

	public void addConnectedPeer(PeerNode pn) {
		if(!pn.isRealConnection()) {
			if(logMINOR)
				Logger.minor(this, "Not a real connection: " + pn);
			return;
		}
		if(!pn.isConnected()) {
			if(logMINOR)
				Logger.minor(this, "Not connected: " + pn);
			return;
		}
		long now = System.currentTimeMillis();
		synchronized(this) {
			if(timeFirstAnyConnections == 0)
				timeFirstAnyConnections = now;
			for(int i = 0; i < connectedPeers.length; i++) {
				if(connectedPeers[i] == pn) {
					if(logMINOR)
						Logger.minor(this, "Already connected: " + pn);
					return;
				}
			}
			boolean inMyPeers = false;
			for(int i = 0; i < myPeers.length; i++) {
				if(myPeers[i] == pn) {
					inMyPeers = true;
					break;
				}
			}
			if(!inMyPeers) {
				Logger.error(this, "Connecting to " + pn + " but not in peers!");
				addPeer(pn);
			}
			if(logMINOR)
				Logger.minor(this, "Connecting: " + pn);
			PeerNode[] newConnectedPeers = new PeerNode[connectedPeers.length + 1];
			System.arraycopy(connectedPeers, 0, newConnectedPeers, 0, connectedPeers.length);
			newConnectedPeers[connectedPeers.length] = pn;
			connectedPeers = newConnectedPeers;
			if(logMINOR)
				Logger.minor(this, "Connected peers: " + connectedPeers.length);
		}
		if(!pn.isSeed())
                    updatePMUserAlert();
		node.lm.announceLocChange();
	}
//    NodePeer route(double targetLocation, RoutingContext ctx) {
//        double minDist = 1.1;
//        NodePeer best = null;
//        for(int i=0;i<connectedPeers.length;i++) {
//            NodePeer p = connectedPeers[i];
//            if(ctx.alreadyRoutedTo(p)) continue;
//            double loc = p.getLocation().getValue();
//            double dist = Math.abs(loc - targetLocation);
//            if(dist < minDist) {
//                minDist = dist;
//                best = p;
//            }
//        }
//        return best;
//    }
//    
//    NodePeer route(Location target, RoutingContext ctx) {
//        return route(target.getValue(), ctx);
//    }
//
	/**
	 * Find the node with the given Peer address. Used by FNPPacketMangler to try to 
	 * quickly identify a peer by the address of the packet. Includes 
	 * non-isRealConnection()'s since they can also be connected.
	 */
	public PeerNode getByPeer(Peer peer) {
		PeerNode[] peerList = myPeers;
		for(int i = 0; i < peerList.length; i++) {
			if(peerList[i].matchesPeerAndPort(peer))
				return peerList[i];
		}
		// Try a match by IP address if we can't match exactly by IP:port.
		FreenetInetAddress addr = peer.getFreenetAddress();
		for(int i = 0; i < peerList.length; i++) {
			if(peerList[i].matchesIP(addr))
				return peerList[i];
		}
		return null;
	}
	
	/**
	 * Find the node with the given Peer address, or IP address. Checks the outgoing
	 * packet mangler as well.
	 * @param peer
	 * @param mangler
	 * @return
	 */
	public PeerNode getByPeer(Peer peer, FNPPacketMangler mangler) {
		PeerNode[] peerList = myPeers;
		for(int i = 0; i < peerList.length; i++) {
			if(peerList[i].matchesPeerAndPort(peer) && peerList[i].getOutgoingMangler() == mangler)
				return peerList[i];
		}
		// Try a match by IP address if we can't match exactly by IP:port.
		FreenetInetAddress addr = peer.getFreenetAddress();
		for(int i = 0; i < peerList.length; i++) {
			if(peerList[i].matchesIP(addr) && peerList[i].getOutgoingMangler() == mangler)
				return peerList[i];
		}
		return null;
	}


	/**
	 * Connect to a node provided the fieldset representing it.
	 */
	public void connect(SimpleFieldSet noderef, OutgoingPacketMangler mangler, FRIEND_TRUST trust) throws FSParseException, PeerParseException, ReferenceSignatureVerificationException {
		PeerNode pn = node.createNewDarknetNode(noderef, trust);
		PeerNode[] peerList = myPeers;
		for(int i = 0; i < peerList.length; i++) {
			if(Arrays.equals(peerList[i].identity, pn.identity))
				return;
		}
		addPeer(pn);
	}
	
	public void disconnect(final PeerNode pn, boolean sendDisconnectMessage, final boolean waitForAck, boolean purge) {
		disconnect(pn, sendDisconnectMessage, waitForAck, purge, false, true, Node.MAX_PEER_INACTIVITY);
	}

	/**
	 * Disconnect from a specified node
	 * @param sendDisconnectMessage If false, don't send the FNPDisconnected message.
	 * @param waitForAck If false, don't wait for the ack for the FNPDisconnected message.
	 * @param purge If true, set the purge flag on the disconnect, causing the other peer
	 * to purge this node from e.g. its old opennet peers list.
	 * @param dumpMessagesNow If true, dump queued messages immediately, before the 
	 * disconnect completes.
	 * @param remove If true, remove the node from the routing table and tell the peer to do so.
	 */
	public void disconnect(final PeerNode pn, boolean sendDisconnectMessage, final boolean waitForAck, boolean purge, boolean dumpMessagesNow, final boolean remove, int timeout) {
		if(logMINOR)
			Logger.minor(this, "Disconnecting " + pn.shortToString(), new Exception("debug"));
		synchronized(this) {
			if(!havePeer(pn))
				return;
		}
		if(pn.notifyDisconnecting(dumpMessagesNow)) {
			if(logMINOR)
				Logger.minor(this, "Already disconnecting "+pn.shortToString());
			return;
		}
		if(sendDisconnectMessage) {
			Message msg = DMT.createFNPDisconnect(remove, purge, -1, new ShortBuffer(new byte[0]));
			try {
				pn.sendAsync(msg, new AsyncMessageCallback() {

					boolean done = false;

					public void acknowledged() {
						done();
					}

					public void disconnected() {
						done();
					}

					public void fatalError() {
						done();
					}

					public void sent() {
						if(!waitForAck)
							done();
					}

					void done() {
						synchronized(this) {
							if(done)
								return;
							done = true;
						}
						if(remove) {
							if(removePeer(pn) && !pn.isSeed())
								writePeers();
						}
					}
				}, ctrDisconn);
			} catch(NotConnectedException e) {
				if(remove) {
					if(pn.isDisconnecting() && removePeer(pn) && !pn.isSeed())
						writePeers();
				}
				return;
			}
                        if(!pn.isSeed()) {
                            node.getTicker().queueTimedJob(new Runnable() {

                                public void run() {
                                    if(pn.isDisconnecting()) {
                                    	if(remove) {
                                    		if(removePeer(pn)) {
                                    			if(!pn.isSeed()) {
                                    				writePeers();
                                    			}
                                    		}
                                    	}
                                    	pn.disconnected(true, true);
                                    }
                                }
                            }, timeout);
                        }
		} else {
			if(remove) {
				if(removePeer(pn) && !pn.isSeed())
					writePeers();
			}
		}
	}
	final ByteCounter ctrDisconn = new ByteCounter() {

		public void receivedBytes(int x) {
			node.nodeStats.disconnBytesReceived(x);
		}

		public void sentBytes(int x) {
			node.nodeStats.disconnBytesSent(x);
		}

		public void sentPayload(int x) {
			// Ignore
		}
	};

	protected static class LocationUIDPair implements Comparable<LocationUIDPair> {
		double location;
		long uid;

		LocationUIDPair(PeerNode pn) {
			location = pn.getLocation();
			uid = pn.swapIdentifier;
		}

		public int compareTo(LocationUIDPair p) {
			// Compare purely on location, so result is the same as getPeerLocationDoubles()
			if(p.location > location)
				return 1;
			if(p.location < location)
				return -1;
			return 0;
		}
	}

	/**
	 * @return An array of the current locations (as doubles) of all
	 * our connected peers or double[0] if Node.shallWePublishOurPeersLocation() is false
	 */
	public double[] getPeerLocationDoubles(boolean pruneBackedOffPeers) {
		double[] locs;
		if(!node.shallWePublishOurPeersLocation())
			return new double[0];
		PeerNode[] conns;
		synchronized(this) {
			conns = connectedPeers;
		}
		locs = new double[conns.length];
		int x = 0;
		for(int i = 0; i < conns.length; i++) {
			if(conns[i].isRoutable()) {
				if(!pruneBackedOffPeers || !conns[i].shouldBeExcludedFromPeerList()) {
					locs[x++] = conns[i].getLocation();
				}
			}
		}
		// Wipe out any information contained in the order
		java.util.Arrays.sort(locs, 0, x);
		if(x != locs.length) {
			double[] newLocs = new double[x];
			System.arraycopy(locs, 0, newLocs, 0, x);
			return newLocs;
		} else
			return locs;
	}

	/** Just like getPeerLocationDoubles, except it also
	 * returns the UID for each node. */
	public LocationUIDPair[] getPeerLocationsAndUIDs() {
		PeerNode[] conns;
		LocationUIDPair[] locPairs;
		synchronized(this) {
			conns = myPeers;
		}
		locPairs = new LocationUIDPair[conns.length];
		int x = 0;
		for(int i = 0; i < conns.length; i++) {
			if(conns[i].isRoutable())
				locPairs[x++] = new LocationUIDPair(conns[i]);
		}
		// Sort it
		Arrays.sort(locPairs, 0, x);
		if(x != locPairs.length) {
			LocationUIDPair[] newLocs = new LocationUIDPair[x];
			System.arraycopy(locPairs, 0, newLocs, 0, x);
			return newLocs;
		} else
			return locPairs;
	}

	/**
	 * @return A random routable connected peer.
	 * FIXME: should this take performance into account?
	 * DO NOT remove the "synchronized". See below for why.
	 */
	public synchronized PeerNode getRandomPeer(PeerNode exclude) {
		if(connectedPeers.length == 0)
			return null;
		for(int i = 0; i < 5; i++) {
			PeerNode pn = connectedPeers[node.random.nextInt(connectedPeers.length)];
			if(pn == exclude)
				continue;
			if(pn.isRoutable())
				return pn;
		}
		// None of them worked
		// Move the un-connected ones out
		// This is safe as they will add themselves when they
		// reconnect, and they can't do it yet as we are synchronized.
		ArrayList<PeerNode> v = new ArrayList<PeerNode>(connectedPeers.length);
		for(PeerNode pn : myPeers) {
			if(pn == exclude)
				continue;
			if(pn.isRoutable())
				v.add(pn);
			else
				if(logMINOR)
					Logger.minor(this, "Excluding " + pn + " because is disconnected");
		}
		int lengthWithoutExcluded = v.size();
		if((exclude != null) && exclude.isRoutable())
			v.add(exclude);
		PeerNode[] newConnectedPeers = new PeerNode[v.size()];
		newConnectedPeers = v.toArray(newConnectedPeers);
		if(logMINOR)
			Logger.minor(this, "Connected peers (in getRandomPeer): " + newConnectedPeers.length + " was " + connectedPeers.length);
		connectedPeers = newConnectedPeers;
		if(lengthWithoutExcluded == 0)
			return null;
		return connectedPeers[node.random.nextInt(lengthWithoutExcluded)];
	}

	/**
	 * Asynchronously send this message to every connected peer.
	 */
	public void localBroadcast(Message msg, boolean ignoreRoutability, boolean onlyRealConnections, ByteCounter ctr) {
		PeerNode[] peers;
		synchronized(this) {
			// myPeers not connectedPeers as connectedPeers only contains
			// ROUTABLE peers, and we may want to send to non-routable peers
			peers = myPeers;
		}
		for(int i = 0; i < peers.length; i++) {
			if(ignoreRoutability) {
				if(!peers[i].isConnected())
					continue;
			} else
				if(!peers[i].isRoutable())
					continue;
			if(onlyRealConnections && !peers[i].isRealConnection())
				continue;
			try {
				peers[i].sendAsync(msg, null, ctr);
			} catch(NotConnectedException e) {
				// Ignore
			}
		}
	}

	/**
	 * Asynchronously send a differential node reference to every isConnected() peer.
	 */
	public void locallyBroadcastDiffNodeRef(SimpleFieldSet fs, boolean toDarknetOnly, boolean toOpennetOnly) {
		PeerNode[] peers;
		synchronized(this) {
			// myPeers not connectedPeers as connectedPeers only contains
			// ROUTABLE peers and we want to also send to non-routable peers
			peers = myPeers;
		}
		for(int i = 0; i < peers.length; i++) {
			if(!peers[i].isConnected())
				continue;
			if(toDarknetOnly && !peers[i].isDarknet())
				continue;
			if(toOpennetOnly && !peers[i].isOpennet())
				continue;
			peers[i].sendNodeToNodeMessage(fs, Node.N2N_MESSAGE_TYPE_DIFFNODEREF, false, 0, false);
		}
	}

	public PeerNode getRandomPeer() {
		return getRandomPeer(null);
	}

	public double closestPeerLocation(double loc, double ignoreLoc, int minUptimePercent) {
		PeerNode[] peers;
		synchronized(this) {
			peers = connectedPeers;
		}
		double bestDiff = 1.0;
		double bestLoc = Double.MAX_VALUE;
		boolean foundOne = false;
		for(int i = 0; i < peers.length; i++) {
			PeerNode p = peers[i];
			if(!p.isRoutable())
				continue;
			if(p.isRoutingBackedOffEither()) {
				if(logMINOR) Logger.minor(this, "Skipping (backoff): "+p+" loc "+p.getLocation());
				continue;
			}
			if(p.getUptime() < minUptimePercent)
				continue;
			double peerloc = p.getLocation();
			if(Math.abs(peerloc - ignoreLoc) < Double.MIN_VALUE * 2)
				continue;
			double diff = Location.distance(peerloc, loc);
			if(diff < bestDiff) {
				foundOne = true;
				bestDiff = diff;
				if(logMINOR) Logger.minor(this, "Found best loc "+peerloc+" from "+p+" diff = "+diff);
				bestLoc = peerloc;
			}
		}
		if(!foundOne)
			if(logMINOR)
				Logger.minor(this, "closerPeerLocation() not found, trying backed off nodes...");
			for(int i = 0; i < peers.length; i++) {
				PeerNode p = peers[i];
				if(!p.isRoutable())
					continue;
				if(p.getUptime() < minUptimePercent)
					continue;
				// Ignore backoff state
				double peerloc = p.getLocation();
				if(Math.abs(peerloc - ignoreLoc) < Double.MIN_VALUE * 2)
					continue;
				double diff = Location.distance(peerloc, loc);
				if(diff < bestDiff) {
					foundOne = true;
					bestDiff = diff;
					if(logMINOR) Logger.minor(this, "Found best loc "+peerloc+" from "+p+" (second round) diff="+diff);
					bestLoc = peerloc;
				}
			}
		return bestLoc;
	}

	public boolean isCloserLocation(double loc, int minUptimePercent) {
		double nodeLoc = node.lm.getLocation();
		double nodeDist = Location.distance(nodeLoc, loc);
		if(logMINOR) Logger.minor(this, "My loc is "+nodeLoc+" my dist is "+nodeDist+" target is "+loc);
		double closest = closestPeerLocation(loc, nodeLoc, minUptimePercent);
		if(closest > 1.0)
			// No peers found
			return false;
		double closestDist = Location.distance(closest, loc);
		return closestDist < nodeDist;
	}

	public PeerNode closerPeer(PeerNode pn, Set<PeerNode> routedTo, double loc, boolean ignoreSelf, boolean calculateMisrouting,
	        int minVersion, List<Double> addUnpickedLocsTo, Key key, short outgoingHTL, int ignoreBackoffUnder, boolean isLocal, boolean realTime) {
		return closerPeer(pn, routedTo, loc, ignoreSelf, calculateMisrouting, minVersion, addUnpickedLocsTo, 2.0, key, outgoingHTL, ignoreBackoffUnder, isLocal, realTime, null, false, System.currentTimeMillis());
	}

	/**
	 * Find the peer, if any, which is closer to the target location than we are, and is not included in the provided set.
	 * If ignoreSelf==false, and we are closer to the target than any peers, this function returns null.
	 * This function returns two values, the closest such peer which is backed off, and the same which is not backed off.
	 * It is possible for either to be null independent of the other, 'closest' is the closer of the two in either case, and
	 * will not be null if any of the other two return values is not null.
	 * @param addUnpickedLocsTo Add all locations we didn't choose which we could have routed to to 
	 * this array. Remove the location of the peer we pick from it.
	 * @param maxDistance If a node is further away from the target than this distance, ignore it.
	 * @param key The original key, if we have it, and if we want to consult with the FailureTable
	 * to avoid routing to nodes which have recently failed for the same key.
	 * @param isLocal We don't just check pn == null because in some cases pn can be null here: If an insert is forked, for
	 * a remote requests, we can route back to the originator, so we set pn to null. Whereas for stats we want to know 
	 * accurately whether this was originated remotely.
	 * @param recentlyFailed If non-null, we should check for recently failed: If we have routed to, and got
	 * a failed response from, and are still connected to and within the timeout for, our top two routing choices,
	 * *and* the same is true of at least 3 nodes, we fill in this object and return null. This will cause a
	 * RecentlyFailed message to be returned to the originator, allowing them to retry in a little while. Note that the
	 * scheduler is not clever enough to retry immediately when that timeout elapses, and even if it was, it probably
	 * wouldn't be a good idea due to introducing a round-trip-to-request-originator; FIXME consider this.
	 */
	public PeerNode closerPeer(PeerNode pn, Set<PeerNode> routedTo, double target, boolean ignoreSelf,
	        boolean calculateMisrouting, int minVersion, List<Double> addUnpickedLocsTo, double maxDistance, Key key, short outgoingHTL, int ignoreBackoffUnder, boolean isLocal, boolean realTime,
	        RecentlyFailedReturn recentlyFailed, boolean ignoreTimeout, long now) {
		
		int countWaiting = 0;
		long soonestTimeoutWakeup = Long.MAX_VALUE;
		
		PeerNode[] peers;
		synchronized(this) {
			peers = connectedPeers;
		}
		if(!node.enablePerNodeFailureTables)
			key = null;
		if(logMINOR)
			Logger.minor(this, "Choosing closest peer: connectedPeers=" + peers.length+" key "+key);
		
		double myLoc = node.getLocation();
		
		double maxDiff = Double.MAX_VALUE;
		if(!ignoreSelf)
			maxDiff = Location.distance(myLoc, target);
		
		double prevLoc = -1.0;
		if(pn != null) prevLoc = pn.getLocation();

		/**
		 * Routing order:
		 * - Non-timed-out non-backed-off peers, in order of closeness to the target.
		 * - Timed-out, non-backed-off peers, least recently timed out first.
		 * - Non-timed-out backed-off peers, in order of closeness to the target.
		 * - Timed out, backed-off peers, least recently timed out first.
		 * - 
		 */
		PeerNode closest = null;
		double closestDistance = Double.MAX_VALUE;
		// If closestDistance is FOAF, this is the real distance.
		// Reset every time closestDistance is.
		double closestRealDistance = Double.MAX_VALUE;

		PeerNode closestBackedOff = null;
		double closestBackedOffDistance = Double.MAX_VALUE;
		double closestRealBackedOffDistance = Double.MAX_VALUE;

		PeerNode closestNotBackedOff = null;
		double closestNotBackedOffDistance = Double.MAX_VALUE;
		double closestRealNotBackedOffDistance = Double.MAX_VALUE;

		PeerNode leastRecentlyTimedOut = null;
		long timeLeastRecentlyTimedOut = Long.MAX_VALUE;
		double leastRecentlyTimedOutDistance = Double.MAX_VALUE;

		PeerNode leastRecentlyTimedOutBackedOff = null;
		long timeLeastRecentlyTimedOutBackedOff = Long.MAX_VALUE;
		double leastRecentlyTimedOutBackedOffDistance = Double.MAX_VALUE;
		
		TimedOutNodesList entry = null;

		if(key != null)
			entry = node.failureTable.getTimedOutNodesList(key);

		int count = 0;
		
		double[] selectionRates = new double[peers.length];
		double totalSelectionRate = 0.0;
		for(int i=0;i<peers.length;i++) {
			selectionRates[i] = peers[i].selectionRate();
			totalSelectionRate += selectionRates[i];
		}
		boolean enableFOAFMitigationHack = (peers.length >= PeerNode.SELECTION_MIN_PEERS) && (totalSelectionRate > 0.0);
		for(int i = 0; i < peers.length; i++) {
			PeerNode p = peers[i];
			if(routedTo.contains(p)) {
				if(logMINOR)
					Logger.minor(this, "Skipping (already routed to): " + p.getPeer());
				continue;
			}
			if(p == pn) {
				if(logMINOR)
					Logger.minor(this, "Skipping (req came from): " + p.getPeer());
				continue;
			}
			if(!p.isRoutable()) {
				if(logMINOR)
					Logger.minor(this, "Skipping (not connected): " + p.getPeer());
				continue;
			}
			if(p.isDisconnecting()) {
				if(logMINOR)
					Logger.minor(this, "Skipping (disconnecting): "+p.getPeer());
				continue;
			}
			if(minVersion > 0 && Version.getArbitraryBuildNumber(p.getVersion(), -1) < minVersion) {
				if(logMINOR)
					Logger.minor(this, "Skipping old version: " + p.getPeer());
				continue;
			}
			if(enableFOAFMitigationHack) {
				double selectionRate = selectionRates[i];
				double selectionSamplesPercentage = selectionRate / totalSelectionRate;
				if(PeerNode.SELECTION_PERCENTAGE_WARNING < selectionSamplesPercentage) {
					if(logMINOR)
						Logger.minor(this, "Skipping over-selectionned peer(" + selectionSamplesPercentage + "%): " + p.getPeer());
					continue;
				}
			}
			
			/** For RecentlyFailed i.e. request quenching */
			long timeoutRF = -1;
			/** For per-node failure tables i.e. routing */
			long timeoutFT = -1;
			if(entry != null && !ignoreTimeout) {
				timeoutFT = entry.getTimeoutTime(p, outgoingHTL, now, true);
				timeoutRF = entry.getTimeoutTime(p, outgoingHTL, now, false);
				if(timeoutRF > now)
					soonestTimeoutWakeup = Math.min(soonestTimeoutWakeup, timeoutRF);
			}
			boolean timedOut = timeoutFT > now;
			if(timedOut) countWaiting++;
			//To help avoid odd race conditions, get the location only once and use it for all calculations.
			double loc = p.getLocation();
			boolean direct = true;
			double realDiff = Location.distance(loc, target);
			double diff = realDiff;
			
			double[] peersLocation = p.getPeersLocation();
			if((peersLocation != null) && (p.shallWeRouteAccordingToOurPeersLocation())) {
				for(double l : peersLocation) {
					boolean ignoreLoc = false; // Because we've already been there
					if(Math.abs(l - myLoc) < Double.MIN_VALUE * 2 ||
							Math.abs(l - prevLoc) < Double.MIN_VALUE * 2)
						ignoreLoc = true;
					else {
						for(PeerNode cmpPN : routedTo)
							if(Math.abs(l - cmpPN.getLocation()) < Double.MIN_VALUE * 2) {
								ignoreLoc = true;
								break;
							}
					}
					if(ignoreLoc) continue;
					double newDiff = Location.distance(l, target);
					if(newDiff < diff) {
						loc = l;
						diff = newDiff;
						direct = false;
					}
				}
				if(logMINOR)
					Logger.minor(this, "The peer "+p+" has published his peer's locations and the closest we have found to the target is "+diff+" away.");
			}
			
			if(diff > maxDistance)
				continue;
			if((!ignoreSelf) && (diff > maxDiff)) {
				if(logMINOR)
					Logger.minor(this, "Ignoring, further than self >maxDiff=" + maxDiff);
				continue;
			}
			count++;
			if(logMINOR)
				Logger.minor(this, "p.loc=" + loc + ", target=" + target + ", d=" + Location.distance(loc, target) + " usedD=" + diff + " timedOut=" + timedOut + " for " + p.getPeer());
			boolean chosen = false;
			if(diff < closestDistance || (Math.abs(diff - closestDistance) < Double.MIN_VALUE*2 && (direct || realDiff < closestRealDistance))) {
				closestDistance = diff;
				closest = p;
				chosen = true;
				closestRealDistance = realDiff;
				if(logMINOR)
					Logger.minor(this, "New best: " + diff + " (" + loc + " for " + p.getPeer());
			}
			boolean backedOff = p.isRoutingBackedOff(ignoreBackoffUnder, realTime);
			if(backedOff && (diff < closestBackedOffDistance || (Math.abs(diff - closestBackedOffDistance) < Double.MIN_VALUE*2 && (direct || realDiff < closestRealBackedOffDistance))) && !timedOut) {
				closestBackedOffDistance = diff;
				closestBackedOff = p;
				chosen = true;
				closestRealBackedOffDistance = realDiff;
				if(logMINOR)
					Logger.minor(this, "New best-backed-off: " + diff + " (" + loc + " for " + p.getPeer());
			}
			if(!backedOff && (diff < closestNotBackedOffDistance || (Math.abs(diff - closestNotBackedOffDistance) < Double.MIN_VALUE*2 && (direct || realDiff < closestRealNotBackedOffDistance))) && !timedOut) {
				closestNotBackedOffDistance = diff;
				closestNotBackedOff = p;
				chosen = true;
				closestRealNotBackedOffDistance = realDiff;
				if(logMINOR)
					Logger.minor(this, "New best-not-backed-off: " + diff + " (" + loc + " for " + p.getPeer());
			}
			if(timedOut)
				if(!backedOff) {
					if(timeoutFT < timeLeastRecentlyTimedOut) {
						timeLeastRecentlyTimedOut = timeoutFT;
						leastRecentlyTimedOut = p;
						leastRecentlyTimedOutDistance = diff;
					}
				} else
					if(timeoutFT < timeLeastRecentlyTimedOutBackedOff) {
						timeLeastRecentlyTimedOutBackedOff = timeoutFT;
						leastRecentlyTimedOutBackedOff = p;
						leastRecentlyTimedOutBackedOffDistance = diff;
					}
			if(addUnpickedLocsTo != null && !chosen) {
				Double d = new Double(loc);
				// Here we can directly compare double's because they aren't processed in any way, and are finite and (probably) nonzero.
				if(!addUnpickedLocsTo.contains(d))
					addUnpickedLocsTo.add(d);
			}
		}

		PeerNode best = closestNotBackedOff;
		double bestDistance = closestNotBackedOffDistance;
		
		/**
		 * Various things are "advisory" i.e. they are taken into account but do not cause a request not to be routed at all:
		 * - Backoff: A node is backed off for a period after it rejects a request; 
		 * this is randomised and increases exponentially if no requests are accepted; 
		 * a longer period is imposed for timeouts after a request has been accepted 
		 * and transfer failures.
		 * - Recent failures: After various kinds of failures we impose a timeout, 
		 * until when we will try to avoid sending the same key to that node. This is 
		 * part of per-node failure tables.
		 * Combining these:
		 * - If there are nodes which are both not backed off and not timed out, we 
		 * route to whichever of those nodes is closest to the target location. If we 
		 * are still here, all nodes are either backed off or timed out.
		 * - If there are nodes which are timed out but not backed off, choose the node
		 * whose timeout expires soonest. Hence if a single key is requested 
		 * continually, we round-robin between nodes. If we still don't have a winner,
		 * we know all nodes are backed off.
		 * - If there are nodes which are backed off but not timed out, choose the node
		 * which is closest to the target but is not backed off. If we still don't have
		 * a winner, all nodes are backed off AND timed out.
		 * - Choose the backed off node whose timeout expires soonest.
		 */
		if(best == null) {
			if(leastRecentlyTimedOut != null) {
				// FIXME downgrade to DEBUG
				best = leastRecentlyTimedOut;
				bestDistance = leastRecentlyTimedOutDistance;
				if(logMINOR)
					Logger.minor(this, "Using least recently failed in-timeout-period peer for key: " + best.shortToString() + " for " + key);
			} else if(closestBackedOff != null) {
				best = closestBackedOff;
				bestDistance = closestBackedOffDistance;
				if(logMINOR)
					Logger.minor(this, "Using best backed-off peer for key: " + best.shortToString());
			} else if(leastRecentlyTimedOutBackedOff != null) {
				best = leastRecentlyTimedOutBackedOff;
				bestDistance = leastRecentlyTimedOutBackedOffDistance;
				if(logMINOR)
					Logger.minor(this, "Using least recently failed in-timeout-period backed-off peer for key: " + best.shortToString() + " for " + key);
			}
		}
		
		if(recentlyFailed != null && logMINOR)
			Logger.minor(this, "Count waiting: "+countWaiting);
		if(recentlyFailed != null && countWaiting >= 3) {
			// Recently failed is possible.
			// Route twice, each time ignoring timeout.
			// If both return a node which is in timeout, we should do RecentlyFailed.
			PeerNode first = closerPeer(pn, routedTo, target, ignoreSelf, false, minVersion, null, maxDistance, key, outgoingHTL, ignoreBackoffUnder, isLocal, realTime, null, true, now);
			if(first != null) {
				long firstTime;
				long secondTime;
				if((firstTime = entry.getTimeoutTime(first, outgoingHTL, now, false)) > now) {
					if(logMINOR) Logger.minor(this, "First choice is past now");
					HashSet<PeerNode> newRoutedTo = new HashSet<PeerNode>(routedTo);
					newRoutedTo.add(first);
					PeerNode second = closerPeer(pn, newRoutedTo, target, ignoreSelf, false, minVersion, null, maxDistance, key, outgoingHTL, ignoreBackoffUnder, isLocal, realTime, null, true, now);
					if(second != null) {
						if((secondTime = entry.getTimeoutTime(first, outgoingHTL, now, false)) > now) {
							if(logMINOR) Logger.minor(this, "Second choice is past now");
							// Recently failed!
							// Return the time at which this will change.
							// This is the sooner of the two top nodes' timeouts.
							// We also take into account the sooner of any timed out node, IF there are exactly 3 nodes waiting.
							long until = Math.min(secondTime, firstTime);
							if(countWaiting == 3) {
								// Count the others as well if there are only 3.
								// If there are more than that they won't matter.
								until = Math.min(until, soonestTimeoutWakeup);
								if(logMINOR) Logger.minor(this, "Recently failed: "+(int)Math.min(Integer.MAX_VALUE, (soonestTimeoutWakeup - now))+"ms");
							}
							
							long check;
							if(best == closestNotBackedOff)
								// We are routing to the perfect node, so no node coming out of backoff/FailureTable will make any difference; don't check.
								check = Long.MAX_VALUE;
							else
								// A node waking up from backoff or FailureTable might well change the decision, which limits the length of a RecentlyFailed.
								check = checkBackoffsForRecentlyFailed(peers, best, target, bestDistance, myLoc, prevLoc, now, entry, outgoingHTL);
							if(check > now + MIN_DELTA) {
								if(check < until) {
									if(logMINOR) Logger.minor(this, "Reducing RecentlyFailed from "+(until-now)+"ms to "+(check-now)+"ms because of check for peers to wakeup");
									until = check;
								}
								recentlyFailed.fail(countWaiting, soonestTimeoutWakeup);
								return null;
							} else {
								// Waking up too soon. Don't RecentlyFailed.
								if(logMINOR) Logger.minor(this, "Not sending RecentlyFailed because will wake up in "+(check-now)+"ms");
							}
						}
					} else {
						if(logMINOR) Logger.minor(this, "Second choice is not in timeout (for recentlyfailed): "+second);
					}
				} else {
					if(logMINOR) Logger.minor(this, "First choice is not in timeout (for recentlyfailed): "+first);
				}
			}
		}

		// DO NOT PUT A ELSE HERE: we need to re-check the value!
		if(best != null) {
			//racy... getLocation() could have changed
			if(calculateMisrouting) {
				node.nodeStats.routingMissDistanceOverall.report(Location.distance(best, closest.getLocation()));
				(isLocal ? node.nodeStats.routingMissDistanceLocal : node.nodeStats.routingMissDistanceRemote).report(Location.distance(best, closest.getLocation()));
				int numberOfConnected = getPeerNodeStatusSize(PEER_NODE_STATUS_CONNECTED, false);
				int numberOfRoutingBackedOff = getPeerNodeStatusSize(PEER_NODE_STATUS_ROUTING_BACKED_OFF, false);
				if(numberOfRoutingBackedOff + numberOfConnected > 0)
					node.nodeStats.backedOffPercent.report((double) numberOfRoutingBackedOff / (double) (numberOfRoutingBackedOff + numberOfConnected));
			}

			//racy... getLocation() could have changed
			if(addUnpickedLocsTo != null)
				//Add the location which we did not pick, if it exists.
				if(closestNotBackedOff != null && closestBackedOff != null)
					addUnpickedLocsTo.add(new Double(closestBackedOff.getLocation()));
					
			incrementSelectionSamples(now, best);
		}
		
		return best;
	}

	static final int MIN_DELTA = 2000;
	
	/** Check whether the routing situation will change soon because of a node coming out of backoff or of
	 * a FailureTable timeout.
	 * 
	 * If we have routed to a backed off node, or a node due to a failure-table timeout, there is a good
	 * chance that the ideal node will change shortly.
	 * 
	 * @return The time at which there will be a different best location to route to for this key, or
	 * Long.MAX_VALUE if we cannot predict a better peer after any amount of time.
	 */
	private long checkBackoffsForRecentlyFailed(PeerNode[] peers, PeerNode best, double target, double bestDistance, double myLoc, double prevLoc, long now, TimedOutNodesList entry, short outgoingHTL) {
		long overallWakeup = Long.MAX_VALUE;
		for(PeerNode p : peers) {
			if(p == best) continue;
			if(!p.isRoutable()) continue;
			
			// Is it further from the target than what we've chosen?
			// It probably is, but if there is backoff or failure tables involved it might not be.
			
			double loc = p.getLocation();
			double realDiff = Location.distance(loc, target);
			double diff = realDiff;
			
			double[] peersLocation = p.getPeersLocation();
			if((peersLocation != null) && (p.shallWeRouteAccordingToOurPeersLocation())) {
				for(double l : peersLocation) {
					boolean ignoreLoc = false; // Because we've already been there
					if(Math.abs(l - myLoc) < Double.MIN_VALUE * 2 ||
							Math.abs(l - prevLoc) < Double.MIN_VALUE * 2)
						continue;
					// For purposes of recently failed, we haven't routed anywhere else.
					// However we do need to check for our location, and the source's location.
					if(ignoreLoc) continue;
					double newDiff = Location.distance(l, target);
					if(newDiff < diff) {
						loc = l;
						diff = newDiff;
					}
				}
				if(logMINOR)
					Logger.minor(this, "The peer "+p+" has published his peer's locations and the closest we have found to the target is "+diff+" away.");
			}
			
			if(diff >= bestDistance) continue;
			
			// The peer is of interest.
			// It will be relevant to routing at max(wakeup from backoff, failure table timeout, recentlyfailed timeout).
			
			long wakeup = 0;
			
			long timeoutFT = entry.getTimeoutTime(p, outgoingHTL, now, true);
			long timeoutRF = entry.getTimeoutTime(p, outgoingHTL, now, false);
			
			if(timeoutFT > now)
				wakeup = Math.max(wakeup, timeoutFT);
			if(timeoutRF > now)
				wakeup = Math.max(wakeup, timeoutRF);
			
			long bulkBackoff = p.getRoutingBackedOffUntilBulk();
			long rtBackoff = p.getRoutingBackedOffUntilRT();
			
			// Whichever backoff is sooner, but ignore if not backed off.
			
			if(bulkBackoff > now && rtBackoff <= now)
				wakeup = Math.max(wakeup, bulkBackoff);
			else if(bulkBackoff <= now && rtBackoff > now)
				wakeup = Math.max(wakeup, rtBackoff);
			else if(bulkBackoff > now && rtBackoff > now)
				wakeup = Math.max(wakeup, Math.min(bulkBackoff, rtBackoff));
			if(wakeup > now) {
				if(logMINOR) Logger.minor(this, "Peer "+p+" will wake up from backoff, failure table and recentlyfailed in "+(wakeup-now)+"ms");
				overallWakeup = Math.min(overallWakeup, wakeup);
			} else {
				// Race condition??? Just come out of backoff and we used the other one?
				// Don't take it into account.
				if(logMINOR) Logger.minor(this, "Better node in check backoffs for RecentlyFailed??? "+p);
			}
		}
		return overallWakeup;
	}

	/**
	 * @return Some status information
	 */
	public String getStatus() {
		StringBuilder sb = new StringBuilder();
		PeerNode[] peers;
		synchronized(this) {
			peers = myPeers;
		}
		String[] status = new String[peers.length];
		for(int i = 0; i < peers.length; i++) {
			PeerNode pn = peers[i];
			status[i] = pn.getStatus(true).toString();
		}
		Arrays.sort(status);
		for(int i = 0; i < status.length; i++) {
			sb.append(status[i]);
			sb.append('\n');
		}
		return sb.toString();
	}

	/**
	 * @return TMCI peer list
	 */
	public String getTMCIPeerList() {
		StringBuilder sb = new StringBuilder();
		PeerNode[] peers;
		synchronized(this) {
			peers = myPeers;
		}
		String[] peerList = new String[peers.length];
		for(int i = 0; i < peers.length; i++) {
			PeerNode pn = peers[i];
			peerList[i] = pn.getTMCIPeerInfo();
		}
		Arrays.sort(peerList);
		for(int i = 0; i < peerList.length; i++) {
			sb.append(peerList[i]);
			sb.append('\n');
		}
		return sb.toString();
	}

	public String getFreevizOutput() {
		StringBuilder sb = new StringBuilder();
		PeerNode[] peers;
		synchronized(this) {
			peers = myPeers;
		}
		String[] identity = new String[peers.length];
		for(int i = 0; i < peers.length; i++) {
			PeerNode pn = peers[i];
			identity[i] = pn.getFreevizOutput();
		}
		Arrays.sort(identity);
		for(int i = 0; i < identity.length; i++) {
			sb.append(identity[i]);
			sb.append('\n');
		}
		return sb.toString();
	}
	private final Object writePeersSync = new Object();
	private final Object writePeerFileSync = new Object();

	void writePeers() {
		shouldWritePeers = true;
	}
	
	protected String getDarknetPeersString() {
		StringBuilder sb = new StringBuilder();
		PeerNode[] peers;
		synchronized(this) {
			peers = myPeers;
		}
		for(PeerNode pn : peers) {
			if(pn instanceof DarknetPeerNode)
				sb.append(pn.exportDiskFieldSet().toOrderedString());
		}
		
		return sb.toString();
	}
	
	protected String getOpennetPeersString() {
		StringBuilder sb = new StringBuilder();
		PeerNode[] peers;
		synchronized(this) {
			peers = myPeers;
		}
		for(PeerNode pn : peers) {
			if(pn instanceof OpennetPeerNode)
				sb.append(pn.exportDiskFieldSet().toOrderedString());
		}
		
		return sb.toString();
	}
	
	protected String getOldOpennetPeersString(OpennetManager om) {
		StringBuilder sb = new StringBuilder();
		for(PeerNode pn : om.getOldPeers()) {
			if(pn instanceof OpennetPeerNode)
				sb.append(pn.exportDiskFieldSet().toOrderedString());
		}
		
		return sb.toString();
	}

	private void writePeersInner() {
                String newDarknetPeersString = null,
                        newOpennetPeersString = null,
                        newOldOpennetPeersString =null;
		
		synchronized(writePeersSync) {
			if(darkFilename != null)
				newDarknetPeersString = getDarknetPeersString();
			OpennetManager om = node.getOpennet();
			if(om != null) {
				if(openFilename != null)
					newOpennetPeersString = getOpennetPeersString();
				oldOpennetPeersFilename = om.getOldPeersFilename();
				newOldOpennetPeersString = getOldOpennetPeersString(om);
			}
		}

		synchronized(writePeerFileSync) {
			if(newDarknetPeersString != null && !newDarknetPeersString.equals(darknetPeersStringCache))
				writePeersInner(darkFilename, darknetPeersStringCache = newDarknetPeersString);
			if(newOldOpennetPeersString != null && !newOldOpennetPeersString.equals(oldOpennetPeersStringCache)) {
				writePeersInner(oldOpennetPeersFilename, oldOpennetPeersStringCache = newOldOpennetPeersString);
			}
			if(newOpennetPeersString != null && !newOpennetPeersString.equals(opennetPeersStringCache)) {
				writePeersInner(openFilename, opennetPeersStringCache = newOpennetPeersString);
			}
		}
	}

	/**
	 * Write the peers file to disk
	 */
	private void writePeersInner(String filename, String sb) {
		synchronized(writePeerFileSync) {
			FileOutputStream fos = null;
			String f = filename + ".bak";
			try {
				fos = new FileOutputStream(f);
			} catch(FileNotFoundException e2) {
				Logger.error(this, "Cannot write peers to disk: Cannot create " + f + " - " + e2, e2);
				Closer.close(fos);
				return;
			}
			OutputStreamWriter w = null;
			try {
				w = new OutputStreamWriter(fos, "UTF-8");
			} catch(UnsupportedEncodingException e2) {
				Closer.close(w);
				throw new Error("Impossible: JVM doesn't support UTF-8: " + e2, e2);
			}
			try {
				w.write(sb);
				w.flush();
				fos.getFD().sync();
				w.close();
				w = null;

				File fnam = new File(filename);
				FileUtil.renameTo(new File(f), fnam);
			} catch(IOException e) {
				try {
					fos.close();
				} catch(IOException e1) {
					Logger.error(this, "Cannot close peers file: " + e, e);
				}
				Logger.error(this, "Cannot write file: " + e, e);
				return; // don't overwrite old file!
			} finally {
				Closer.close(w);
				Closer.close(fos);
			}
		}
	}

	/**
	 * Update the numbers needed by our PeerManagerUserAlert on the UAM.
	 * Also run the node's onConnectedPeers() method if applicable
	 */
	public void updatePMUserAlert() {
		if(ua == null)
			return;
		int peers, darknetPeers, opennetPeers;
		synchronized(this) {
			darknetPeers = this.getDarknetPeers().length;
			opennetPeers = this.getOpennetPeers().length;
			peers = darknetPeers + opennetPeers; // Seednodes don't count.
		}
		OpennetManager om = node.getOpennet();

		boolean opennetDefinitelyPortForwarded;
		boolean opennetEnabled;
		boolean opennetAssumeNAT;
		if(om != null) {
			opennetEnabled = true;
			opennetDefinitelyPortForwarded = om.crypto.definitelyPortForwarded();
			opennetAssumeNAT = om.crypto.config.alwaysHandshakeAggressively();
		} else {
			opennetEnabled = false;
			opennetDefinitelyPortForwarded = false;
			opennetAssumeNAT = false;
		}
		boolean darknetDefinitelyPortForwarded = node.darknetDefinitelyPortForwarded();
		boolean darknetAssumeNAT = node.darknetCrypto.config.alwaysHandshakeAggressively();
		synchronized(ua) {
			ua.opennetDefinitelyPortForwarded = opennetDefinitelyPortForwarded;
			ua.darknetDefinitelyPortForwarded = darknetDefinitelyPortForwarded;
			ua.opennetAssumeNAT = opennetAssumeNAT;
			ua.darknetAssumeNAT = darknetAssumeNAT;
			ua.darknetConns = getPeerNodeStatusSize(PEER_NODE_STATUS_CONNECTED, true) +
				getPeerNodeStatusSize(PEER_NODE_STATUS_ROUTING_BACKED_OFF, true);
			ua.conns = getPeerNodeStatusSize(PEER_NODE_STATUS_CONNECTED, false) +
				getPeerNodeStatusSize(PEER_NODE_STATUS_ROUTING_BACKED_OFF, false);
			ua.darknetPeers = darknetPeers;
			ua.disconnDarknetPeers = darknetPeers - ua.darknetConns;
			ua.peers = peers;
			ua.neverConn = getPeerNodeStatusSize(PEER_NODE_STATUS_NEVER_CONNECTED, true);
			ua.clockProblem = getPeerNodeStatusSize(PEER_NODE_STATUS_CLOCK_PROBLEM, false);
			ua.connError = getPeerNodeStatusSize(PEER_NODE_STATUS_CONN_ERROR, true);
			ua.isOpennetEnabled = opennetEnabled;
		}
		if(anyConnectedPeers())
			node.onConnectedPeer();
	}

	public boolean anyConnectedPeers() {
		PeerNode[] conns;
		synchronized(this) {
			conns = connectedPeers;
		}
		for(int i = 0; i < conns.length; i++) {
			if(conns[i].isRoutable())
				return true;
		}
		return false;
	}
	
	public boolean anyDarknetPeers() {
		PeerNode[] conns;
		synchronized(this) {
			conns = connectedPeers;
		}
		for(PeerNode p : conns)
			if(p.isDarknet())
				return true;
		return false;
	}

	/**
	 * Ask each PeerNode to read in it's extra peer data
	 */
	public void readExtraPeerData() {
		DarknetPeerNode[] peers = getDarknetPeers();
		for(int i = 0; i < peers.length; i++) {
			try {
				peers[i].readExtraPeerData();
			} catch(Exception e) {
				Logger.error(this, "Got exception while reading extra peer data", e);
			}
		}
		String msg = "Extra peer data reading and processing completed";
		Logger.normal(this, msg);
		System.out.println(msg);
	}

	public void start() {
		ua = new PeerManagerUserAlert(node.nodeStats);
		updatePMUserAlert();
		node.clientCore.alerts.register(ua);
		node.getTicker().queueTimedJob(writePeersRunnable, 0);
	}

	public int countNonBackedOffPeers(boolean realTime) {
		PeerNode[] peers;
		synchronized(this) {
			peers = connectedPeers; // even if myPeers peers are connected they won't be routed to
		}
		int countNoBackoff = 0;
		for(int i = 0; i < peers.length; i++) {
			if(peers[i].isRoutable())
				if(!peers[i].isRoutingBackedOff(realTime))
					countNoBackoff++;
		}
		return countNoBackoff;
	}
	// Stats stuff
	/**
	 * Update oldestNeverConnectedPeerAge if the timer has expired
	 */
	public void maybeUpdateOldestNeverConnectedDarknetPeerAge(long now) {
		synchronized(this) {
			if(now <= nextOldestNeverConnectedDarknetPeerAgeUpdateTime)
				return;
			nextOldestNeverConnectedDarknetPeerAgeUpdateTime = now + oldestNeverConnectedPeerAgeUpdateInterval;
		}
		oldestNeverConnectedDarknetPeerAge = 0;
		PeerNode[] peerList = myPeers;
		for(int i = 0; i < peerList.length; i++) {
			PeerNode pn = peerList[i];
			if(!pn.isDarknet()) continue;
			if(pn.getPeerNodeStatus() == PEER_NODE_STATUS_NEVER_CONNECTED)
				if((now - pn.getPeerAddedTime()) > oldestNeverConnectedDarknetPeerAge)
					oldestNeverConnectedDarknetPeerAge = now - pn.getPeerAddedTime();
		}
		if(oldestNeverConnectedDarknetPeerAge > 0 && logMINOR)
			Logger.minor(this, "Oldest never connected peer is " + oldestNeverConnectedDarknetPeerAge + "ms old");
		nextOldestNeverConnectedDarknetPeerAgeUpdateTime = now + oldestNeverConnectedPeerAgeUpdateInterval;
	}

	public long getOldestNeverConnectedDarknetPeerAge() {
		return oldestNeverConnectedDarknetPeerAge;
	}

	/**
	 * Log the current PeerNode status summary if the timer has expired
	 */
	public void maybeLogPeerNodeStatusSummary(long now) {
		if(now > nextPeerNodeStatusLogTime) {
			if((now - nextPeerNodeStatusLogTime) > (10 * 1000) && nextPeerNodeStatusLogTime > 0)
				Logger.error(this, "maybeLogPeerNodeStatusSummary() not called for more than 10 seconds (" + (now - nextPeerNodeStatusLogTime) + ").  PacketSender getting bogged down or something?");

			int numberOfConnected = 0;
			int numberOfRoutingBackedOff = 0;
			int numberOfTooNew = 0;
			int numberOfTooOld = 0;
			int numberOfDisconnected = 0;
			int numberOfNeverConnected = 0;
			int numberOfDisabled = 0;
			int numberOfListenOnly = 0;
			int numberOfListening = 0;
			int numberOfBursting = 0;
			int numberOfClockProblem = 0;
			int numberOfConnError = 0;
			int numberOfDisconnecting = 0;
			int numberOfRoutingDisabled = 0;

			PeerNode[] peers = this.myPeers;
			
			for(int i = 0; i < peers.length; i++) {
				if(peers[i] == null) {
					Logger.error(this, "getPeerNodeStatuses(true)[" + i + "] == null!");
					continue;
				}
				int status = peers[i].getPeerNodeStatus();
				switch(status) {
					case PEER_NODE_STATUS_CONNECTED:
						numberOfConnected++;
						break;
					case PEER_NODE_STATUS_ROUTING_BACKED_OFF:
						numberOfRoutingBackedOff++;
						break;
					case PEER_NODE_STATUS_TOO_NEW:
						numberOfTooNew++;
						break;
					case PEER_NODE_STATUS_TOO_OLD:
						numberOfTooOld++;
						break;
					case PEER_NODE_STATUS_DISCONNECTED:
						numberOfDisconnected++;
						break;
					case PEER_NODE_STATUS_NEVER_CONNECTED:
						numberOfNeverConnected++;
						break;
					case PEER_NODE_STATUS_DISABLED:
						numberOfDisabled++;
						break;
					case PEER_NODE_STATUS_LISTEN_ONLY:
						numberOfListenOnly++;
						break;
					case PEER_NODE_STATUS_LISTENING:
						numberOfListening++;
						break;
					case PEER_NODE_STATUS_BURSTING:
						numberOfBursting++;
						break;
					case PEER_NODE_STATUS_CLOCK_PROBLEM:
						numberOfClockProblem++;
						break;
					case PEER_NODE_STATUS_CONN_ERROR:
						numberOfConnError++;
						break;
					case PEER_NODE_STATUS_DISCONNECTING:
						numberOfDisconnecting++;
						break;
					case PEER_NODE_STATUS_ROUTING_DISABLED:
						numberOfRoutingDisabled++;
						break;
					default:
						Logger.error(this, "Unknown peer status value : " + status);
						break;
				}
			}
			Logger.normal(this, "Connected: " + numberOfConnected + "  Routing Backed Off: " + numberOfRoutingBackedOff + "  Too New: " + numberOfTooNew + "  Too Old: " + numberOfTooOld + "  Disconnected: " + numberOfDisconnected + "  Never Connected: " + numberOfNeverConnected + "  Disabled: " + numberOfDisabled + "  Bursting: " + numberOfBursting + "  Listening: " + numberOfListening + "  Listen Only: " + numberOfListenOnly + "  Clock Problem: " + numberOfClockProblem + "  Connection Problem: " + numberOfConnError + "  Disconnecting: " + numberOfDisconnecting);
			nextPeerNodeStatusLogTime = now + peerNodeStatusLogInterval;
		}
	}

	/**
	 * Add a PeerNode status to the map
	 */
	public void addPeerNodeStatus(int pnStatus, PeerNode peerNode, boolean noLog) {
		Integer peerNodeStatus = Integer.valueOf(pnStatus);
		addPeerNodeStatuses(pnStatus, peerNode, peerNodeStatus, peerNodeStatuses, noLog);
		if(!peerNode.isOpennet())
			addPeerNodeStatuses(pnStatus, peerNode, peerNodeStatus, peerNodeStatusesDarknet, noLog);
	}

	private void addPeerNodeStatuses(int pnStatus, PeerNode peerNode, Integer peerNodeStatus, HashMap<Integer, HashSet<PeerNode>> statuses, boolean noLog) {
		HashSet<PeerNode> statusSet = null;
		synchronized(statuses) {
			if(statuses.containsKey(peerNodeStatus)) {
				statusSet = statuses.get(peerNodeStatus);
				if(statusSet.contains(peerNode)) {
					if(!noLog)
						Logger.error(this, "addPeerNodeStatus(): node already in peerNodeStatuses: " + peerNode + " status " + PeerNode.getPeerNodeStatusString(peerNodeStatus.intValue()), new Exception("debug"));
					return;
				}
				statuses.remove(peerNodeStatus);
			} else
				statusSet = new HashSet<PeerNode>();
			if(logMINOR)
				Logger.minor(this, "addPeerNodeStatus(): adding PeerNode for '" + peerNode.getIdentityString() + "' with status '" + PeerNode.getPeerNodeStatusString(peerNodeStatus.intValue()) + "'");
			statusSet.add(peerNode);
			statuses.put(peerNodeStatus, statusSet);
		}
	}

	/**
	 * How many PeerNodes have a particular status?
	 * @param darknet If true, only count darknet nodes, if false, count all nodes.
	 */
	public int getPeerNodeStatusSize(int pnStatus, boolean darknet) {
		Integer peerNodeStatus = Integer.valueOf(pnStatus);
		HashSet<PeerNode> statusSet = null;
		HashMap<Integer, HashSet<PeerNode>> statuses = darknet ? peerNodeStatusesDarknet : this.peerNodeStatuses;
		synchronized(statuses) {
			if(statuses.containsKey(peerNodeStatus))
				statusSet = statuses.get(peerNodeStatus);
			else
				statusSet = new HashSet<PeerNode>();
			return statusSet.size();
		}
	}

	/**
	 * Remove a PeerNode status from the map
	 * @param isInPeers If true, complain if the node is not in the peers list; if false, complain if it is.
	 */
	public void removePeerNodeStatus(int pnStatus, PeerNode peerNode, boolean noLog) {
		Integer peerNodeStatus = Integer.valueOf(pnStatus);
		removePeerNodeStatus(pnStatus, peerNodeStatus, peerNode, peerNodeStatuses, noLog);
		if(!peerNode.isOpennet())
			removePeerNodeStatus(pnStatus, peerNodeStatus, peerNode, peerNodeStatusesDarknet, noLog);
	}

	private void removePeerNodeStatus(int pnStatus, Integer peerNodeStatus, PeerNode peerNode, HashMap<Integer, HashSet<PeerNode>> statuses, boolean noLog) {
		HashSet<PeerNode> statusSet = null;
		synchronized(statuses) {
			if(statuses.containsKey(peerNodeStatus)) {
				statusSet = statuses.get(peerNodeStatus);
				if(!statusSet.contains(peerNode)) {
					if(!noLog)
						Logger.error(this, "removePeerNodeStatus(): identity '" + peerNode.getIdentityString() + " for " + peerNode.shortToString() + "' not in peerNodeStatuses with status '" + PeerNode.getPeerNodeStatusString(peerNodeStatus.intValue()) + "'", new Exception("debug"));
					return;
				}
				if(statuses.isEmpty())
					statuses.remove(peerNodeStatus);
			} else
				statusSet = new HashSet<PeerNode>();
			if(logMINOR)
				Logger.minor(this, "removePeerNodeStatus(): removing PeerNode for '" + peerNode.getIdentityString() + "' with status '" + PeerNode.getPeerNodeStatusString(peerNodeStatus.intValue()) + "'");
			if(statusSet.contains(peerNode))
				statusSet.remove(peerNode);
		}
	}

	/**
	 * Add a PeerNode routing backoff reason to the map
	 */
	public void addPeerNodeRoutingBackoffReason(String peerNodeRoutingBackoffReason, PeerNode peerNode, boolean realTime) {
		if(peerNodeRoutingBackoffReason == null) {
			Logger.error(this, "Impossible backoff reason null on "+peerNode+" realtime="+realTime, new Exception("error"));
			return;
		}
		HashMap<String, HashSet<PeerNode>> peerNodeRoutingBackoffReasons =
			realTime ? peerNodeRoutingBackoffReasonsRT : peerNodeRoutingBackoffReasonsBulk;
		synchronized(peerNodeRoutingBackoffReasons) {
			HashSet<PeerNode> reasonSet = null;
			if(peerNodeRoutingBackoffReasons.containsKey(peerNodeRoutingBackoffReason)) {
				reasonSet = peerNodeRoutingBackoffReasons.get(peerNodeRoutingBackoffReason);
				if(reasonSet.contains(peerNode)) {
					Logger.error(this, "addPeerNodeRoutingBackoffReason(): identity '" + peerNode.getIdentityString() + "' already in peerNodeRoutingBackoffReasons as " + peerNode.getPeer() + " with status code " + peerNodeRoutingBackoffReason);
					return;
				}
				peerNodeRoutingBackoffReasons.remove(peerNodeRoutingBackoffReason);
			} else
				reasonSet = new HashSet<PeerNode>();
			if(logMINOR)
				Logger.minor(this, "addPeerNodeRoutingBackoffReason(): adding PeerNode for '" + peerNode.getIdentityString() + "' with status code " + peerNodeRoutingBackoffReason);
			reasonSet.add(peerNode);
			peerNodeRoutingBackoffReasons.put(peerNodeRoutingBackoffReason, reasonSet);
		}
	}

	/**
	 * What are the currently tracked PeerNode routing backoff reasons?
	 */
	public String[] getPeerNodeRoutingBackoffReasons(boolean realTime) {
		HashMap<String, HashSet<PeerNode>> peerNodeRoutingBackoffReasons =
			realTime ? peerNodeRoutingBackoffReasonsRT : peerNodeRoutingBackoffReasonsBulk;
		String[] reasonStrings;
		synchronized(peerNodeRoutingBackoffReasons) {
			reasonStrings = peerNodeRoutingBackoffReasons.keySet().toArray(new String[peerNodeRoutingBackoffReasons.size()]);
		}
		Arrays.sort(reasonStrings);
		return reasonStrings;
	}

	/**
	 * How many PeerNodes have a particular routing backoff reason?
	 */
	public int getPeerNodeRoutingBackoffReasonSize(String peerNodeRoutingBackoffReason, boolean realTime) {
		HashMap<String, HashSet<PeerNode>> peerNodeRoutingBackoffReasons =
			realTime ? peerNodeRoutingBackoffReasonsRT : peerNodeRoutingBackoffReasonsBulk;
		HashSet<PeerNode> reasonSet = null;
		synchronized(peerNodeRoutingBackoffReasons) {
			if(peerNodeRoutingBackoffReasons.containsKey(peerNodeRoutingBackoffReason)) {
				reasonSet = peerNodeRoutingBackoffReasons.get(peerNodeRoutingBackoffReason);
				return reasonSet.size();
			} else
				return 0;
		}
	}

	/**
	 * Remove a PeerNode routing backoff reason from the map
	 */
	public void removePeerNodeRoutingBackoffReason(String peerNodeRoutingBackoffReason, PeerNode peerNode, boolean realTime) {
		HashSet<PeerNode> reasonSet = null;
		HashMap<String, HashSet<PeerNode>> peerNodeRoutingBackoffReasons =
			realTime ? peerNodeRoutingBackoffReasonsRT : peerNodeRoutingBackoffReasonsBulk;
		synchronized(peerNodeRoutingBackoffReasons) {
			if(peerNodeRoutingBackoffReasons.containsKey(peerNodeRoutingBackoffReason)) {
				reasonSet = peerNodeRoutingBackoffReasons.get(peerNodeRoutingBackoffReason);
				if(!reasonSet.contains(peerNode)) {
					Logger.error(this, "removePeerNodeRoutingBackoffReason(): identity '" + peerNode.getIdentityString() + "' not in peerNodeRoutingBackoffReasons with status code " + peerNodeRoutingBackoffReason, new Exception("debug"));
					return;
				}
				peerNodeRoutingBackoffReasons.remove(peerNodeRoutingBackoffReason);
			} else
				reasonSet = new HashSet<PeerNode>();
			if(logMINOR)
				Logger.minor(this, "removePeerNodeRoutingBackoffReason(): removing PeerNode for '" + peerNode.getIdentityString() + "' with status code " + peerNodeRoutingBackoffReason);
			if(reasonSet.contains(peerNode))
				reasonSet.remove(peerNode);
			if(reasonSet.size() > 0)
				peerNodeRoutingBackoffReasons.put(peerNodeRoutingBackoffReason, reasonSet);
		}
	}

	public PeerNodeStatus[] getPeerNodeStatuses(boolean noHeavy) {
		PeerNode[] peers;
		synchronized(this) {
			peers = myPeers;
		}
		PeerNodeStatus[] _peerNodeStatuses = new PeerNodeStatus[peers.length];
		for(int peerIndex = 0, peerCount = peers.length; peerIndex < peerCount; peerIndex++) {
			_peerNodeStatuses[peerIndex] = peers[peerIndex].getStatus(noHeavy);
		}
		return _peerNodeStatuses;
	}

	public DarknetPeerNodeStatus[] getDarknetPeerNodeStatuses(boolean noHeavy) {
		DarknetPeerNode[] peers = getDarknetPeers();
		DarknetPeerNodeStatus[] _peerNodeStatuses = new DarknetPeerNodeStatus[peers.length];
		for(int peerIndex = 0, peerCount = peers.length; peerIndex < peerCount; peerIndex++) {
			_peerNodeStatuses[peerIndex] = (DarknetPeerNodeStatus) peers[peerIndex].getStatus(noHeavy);
		}
		return _peerNodeStatuses;
	}

	public OpennetPeerNodeStatus[] getOpennetPeerNodeStatuses(boolean noHeavy) {
		OpennetPeerNode[] peers = getOpennetPeers();
		OpennetPeerNodeStatus[] _peerNodeStatuses = new OpennetPeerNodeStatus[peers.length];
		for(int peerIndex = 0, peerCount = peers.length; peerIndex < peerCount; peerIndex++) {
			_peerNodeStatuses[peerIndex] = (OpennetPeerNodeStatus) peers[peerIndex].getStatus(noHeavy);
		}
		return _peerNodeStatuses;
	}

	/**
	 * Update hadRoutableConnectionCount/routableConnectionCheckCount on peers if the timer has expired
	 */
	public void maybeUpdatePeerNodeRoutableConnectionStats(long now) {
		synchronized(this) {
			if(now <= nextRoutableConnectionStatsUpdateTime)
				return;
			nextRoutableConnectionStatsUpdateTime = now + routableConnectionStatsUpdateInterval;
		}
		if(-1 != nextRoutableConnectionStatsUpdateTime) {
			PeerNode[] peerList = myPeers;
			for(int i = 0; i < peerList.length; i++) {
				PeerNode pn = peerList[i];
				pn.checkRoutableConnectionStatus();
			}
		}
	}

	/**
	 * Get the darknet peers list.
	 * FIXME: optimise
	 */
	public DarknetPeerNode[] getDarknetPeers() {
		PeerNode[] peers;
		synchronized(this) {
			peers = myPeers;
		}
		// FIXME optimise! Maybe maintain as a separate list?
		Vector<PeerNode> v = new Vector<PeerNode>(myPeers.length);
		for(int i = 0; i < peers.length; i++) {
			if(peers[i] instanceof DarknetPeerNode)
				v.add(peers[i]);
		}
		return v.toArray(new DarknetPeerNode[v.size()]);
	}

	public Vector<SeedServerPeerNode> getConnectedSeedServerPeersVector(HashSet<ByteArrayWrapper> exclude) {
		PeerNode[] peers;
		synchronized(this) {
			peers = myPeers;
		}
		// FIXME optimise! Maybe maintain as a separate list?
		Vector<SeedServerPeerNode> v = new Vector<SeedServerPeerNode>(myPeers.length);
		for(PeerNode p : peers) {
			if(p instanceof SeedServerPeerNode) {
				SeedServerPeerNode sspn = (SeedServerPeerNode) p;
				if(exclude != null && exclude.contains(new ByteArrayWrapper(sspn.getIdentity()))) {
					if(logMINOR)
						Logger.minor(this, "Not including in getConnectedSeedServerPeersVector() as in exclude set: " + sspn.userToString());
					continue;
				}
				if(!sspn.isConnected()) {
					if(logMINOR)
						Logger.minor(this, "Not including in getConnectedSeedServerPeersVector() as disconnected: " + sspn.userToString());
					continue;
				}
				v.add(sspn);
			}
		}
		return v;
	}

	public List<SeedServerPeerNode> getSeedServerPeersVector() {
		PeerNode[] peers;
		synchronized(this) {
			peers = myPeers;
		}
		// FIXME optimise! Maybe maintain as a separate list?
		List<SeedServerPeerNode> v = new ArrayList<SeedServerPeerNode>(myPeers.length);
		for(PeerNode peer : peers) {
			if(peer instanceof SeedServerPeerNode)
				v.add((SeedServerPeerNode)peer);
		}
		return v;
	}

	/**
	 * Get the opennet peers list.
	 */
	public OpennetPeerNode[] getOpennetPeers() {
		PeerNode[] peers;
		synchronized(this) {
			peers = myPeers;
		}
		// FIXME optimise! Maybe maintain as a separate list?
		Vector<PeerNode> v = new Vector<PeerNode>(myPeers.length);
		for(int i = 0; i < peers.length; i++) {
			if(peers[i] instanceof OpennetPeerNode)
				v.add(peers[i]);
		}
		return v.toArray(new OpennetPeerNode[v.size()]);
	}
	
	public PeerNode[] getOpennetAndSeedServerPeers() {
		PeerNode[] peers;
		synchronized(this) {
			peers = myPeers;
		}
		// FIXME optimise! Maybe maintain as a separate list?
		Vector<PeerNode> v = new Vector<PeerNode>(myPeers.length);
		for(int i = 0; i < peers.length; i++) {
			if(peers[i] instanceof OpennetPeerNode)
				v.add(peers[i]);
			else if(peers[i] instanceof SeedServerPeerNode)
				v.add(peers[i]);
		}
		return v.toArray(new PeerNode[v.size()]);
	}

	public boolean anyConnectedPeerHasAddress(FreenetInetAddress addr, PeerNode pn) {
		PeerNode[] peers;
		synchronized(this) {
			peers = myPeers;
		}
		for(int i = 0; i < peers.length; i++) {
			if(peers[i] == pn)
				continue;
			if(!peers[i].isConnected())
				continue;
			if(!peers[i].isRealConnection())
				continue; // Ignore non-searchable peers i.e. bootstrapping peers
			// If getPeer() is null then presumably !isConnected().
			if(peers[i].getPeer().getFreenetAddress().equals(addr))
				return true;
		}
		return false;
	}

	public void removeOpennetPeers() {
		synchronized(this) {
			ArrayList<PeerNode> keep = new ArrayList<PeerNode>();
			ArrayList<PeerNode> conn = new ArrayList<PeerNode>();
			for(PeerNode pn : myPeers) {
				if(pn instanceof OpennetPeerNode)
					continue;
				keep.add(pn);
				if(pn.isConnected())
					conn.add(pn);
			}
			myPeers = keep.toArray(new PeerNode[keep.size()]);
			connectedPeers = keep.toArray(new PeerNode[conn.size()]);
		}
		updatePMUserAlert();
		notifyPeerStatusChangeListeners();
	}

	public PeerNode containsPeer(PeerNode pn) {
		PeerNode[] peers = pn.isOpennet() ? ((PeerNode[]) getOpennetAndSeedServerPeers()) : ((PeerNode[]) getDarknetPeers());

		for(int i = 0; i < peers.length; i++)
			if(Arrays.equals(pn.getIdentity(), peers[i].getIdentity()))
				return peers[i];

		return null;
	}

	public int quickCountConnectedPeers() {
		PeerNode[] conns = connectedPeers;
		if(conns == null)
			return 0;
		return conns.length;
	}

	public int countConnectedDarknetPeers() {
		int count = 0;
		PeerNode[] peers = myPeers;
		for(int i = 0; i < peers.length; i++) {
			if(peers[i] == null)
				continue;
			if(!(peers[i] instanceof DarknetPeerNode))
				continue;
			if(peers[i].isOpennet())
				continue;
			if(!peers[i].isRoutable())
				continue;
			count++;
		}
		if(logMINOR) Logger.minor(this, "countConnectedDarknetPeers() returning "+count);
		return count;
	}

	public int countConnectedPeers() {
		int count = 0;
		PeerNode[] peers = myPeers;
		for(int i = 0; i < peers.length; i++) {
			if(peers[i] == null)
				continue;
			if(!peers[i].isRoutable())
				continue;
			count++;
		}
		return count;
	}

	public int countAlmostConnectedDarknetPeers() {
		int count = 0;
		PeerNode[] peers = myPeers;
		for(int i = 0; i < peers.length; i++) {
			if(peers[i] == null)
				continue;
			if(!(peers[i] instanceof DarknetPeerNode))
				continue;
			if(peers[i].isOpennet())
				continue;
			if(!peers[i].isConnected())
				continue;
			count++;
		}
		return count;
	}

	public int countCompatibleDarknetPeers() {
		int count = 0;
		PeerNode[] peers = myPeers;
		for(int i = 0; i < peers.length; i++) {
			if(peers[i] == null)
				continue;
			if(!(peers[i] instanceof DarknetPeerNode))
				continue;
			if(peers[i].isOpennet())
				continue;
			if(!peers[i].isConnected())
				continue;
			if(!peers[i].isRoutingCompatible())
				continue;
			count++;
		}
		return count;
	}

	public int countConnectedOpennetPeers() {
		int count = 0;
		PeerNode[] peers = connectedPeers;
		for(int i = 0; i < peers.length; i++) {
			if(peers[i] == null)
				continue;
			if(!(peers[i] instanceof OpennetPeerNode))
				continue;
			if(!peers[i].isRoutable())
				continue;
			count++;
		}
		return count;
	}

	/**
	 * How many peers do we have that actually may connect? Don't include seednodes, disabled nodes, etc.
	 */
	public int countValidPeers() {
		PeerNode[] peers = myPeers;
		int count = 0;
		for(int i = 0; i < peers.length; i++) {
			if(!peers[i].isRealConnection())
				continue;
			if(peers[i].isDisabled())
				continue;
			count++;
		}
		return count;
	}
	
	public int countSeednodes() {
		int count = 0;
		for(PeerNode peer : myPeers) {
			if(peer instanceof SeedServerPeerNode || 
					peer instanceof SeedClientPeerNode)
				count++;
		}
		return count;
	}
	
	public int countBackedOffPeersEither() {
		PeerNode[] peers = myPeers;
		int count = 0;
		for(int i = 0; i < peers.length; i++) {
			if(!peers[i].isRealConnection())
				continue;
			if(peers[i].isDisabled())
				continue;
			if(peers[i].isRoutingBackedOffEither())
				count++;
		}
		return count;
	}

	public int countBackedOffPeers(boolean realTime) {
		PeerNode[] peers = myPeers;
		int count = 0;
		for(int i = 0; i < peers.length; i++) {
			if(!peers[i].isRealConnection())
				continue;
			if(peers[i].isDisabled())
				continue;
			if(peers[i].isRoutingBackedOff(realTime))
				count++;
		}
		return count;
	}

	public PeerNode getByIdentity(byte[] identity) {
		PeerNode[] peers = myPeers;
		for(int i = 0; i < peers.length; i++) {
			if(Arrays.equals(peers[i].getIdentity(), identity))
				return peers[i];
		}
		return null;
	}
	
	private void incrementSelectionSamples(long now, PeerNode pn) {
		// TODO: reimplement with a bit field to spare memory
		pn.incrementNumberOfSelections(now);
	}
	
	/** Notifies the listeners about status change*/
	private void notifyPeerStatusChangeListeners(){
		for(PeerStatusChangeListener l:listeners){
			l.onPeerStatusChange();
			for(PeerNode pn:myPeers){
				pn.registerPeerNodeStatusChangeListener(l);
			}
		}
	}
	
	/** Registers a listener to be notified when peers' statuses changes
	 * @param listener - the listener to be registered*/
	public void addPeerStatusChangeListener(PeerStatusChangeListener listener){
		listeners.add(listener);
		for(PeerNode pn:myPeers){
			pn.registerPeerNodeStatusChangeListener(listener);
		}
	}
	
	/** Removes a listener
	 * @param listener - The listener to be removed*/
	public void removePeerStatusChangeListener(PeerStatusChangeListener listener){
		listeners.remove(listener);
	}
	
	/** A listener interface that can be used to be notified about peer status change events*/
	public static interface PeerStatusChangeListener{
		/** Peers status have changed*/
		public void onPeerStatusChange();
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 *  * Public License, version 2 (or at your option any later version). See
 *   * http://www.gnu.org/ for further details of the GPL. */
package freenet.support;

public class LogThresholdCallback {

	public LogThresholdCallback() {
	}

	public void shouldUpdate(){}; 
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.pluginmanager;

import java.util.HashMap;

import com.db4o.Db4o;
import com.db4o.ObjectContainer;
import com.db4o.ObjectSet;
import com.db4o.config.Configuration;
import com.db4o.io.MemoryIoAdapter;

/**
 * This is a PluginStore. Plugins can use that to store all kinds of primary
 * data types with as many recursion level as needed.
 * @author Artefact2
 */
public class PluginStore {
	public HashMap<String, PluginStore> subStores = new HashMap<String, PluginStore>();
	public HashMap<String, Long> longs = new HashMap<String, Long>();
	public HashMap<String, long[]> longsArrays = new HashMap<String, long[]>();
	public HashMap<String, Integer> integers = new HashMap<String, Integer>();
	public HashMap<String, int[]> integersArrays = new HashMap<String, int[]>();
	public HashMap<String, Short> shorts = new HashMap<String, Short>();
	public HashMap<String, short[]> shortsArrays = new HashMap<String, short[]>();
	public HashMap<String, Boolean> booleans = new HashMap<String, Boolean>();
	public HashMap<String, boolean[]> booleansArrays = new HashMap<String, boolean[]>();
	public HashMap<String, Byte> bytes = new HashMap<String, Byte>();
	public HashMap<String, byte[]> bytesArrays = new HashMap<String, byte[]>();
	public HashMap<String, String> strings = new HashMap<String, String>();
	public HashMap<String, String[]> stringsArrays = new HashMap<String, String[]>();

	public byte[] exportStore() {
		Configuration conf = Db4o.newConfiguration();
		MemoryIoAdapter mia = new MemoryIoAdapter();
		conf.io(mia);
		ObjectContainer o = Db4o.openFile(conf, "Export");
		PluginStoreContainer psc = new PluginStoreContainer();
		psc.pluginStore = this;
		o.ext().store(psc, Integer.MAX_VALUE);
		o.commit();
		o.close();
		return mia.get("Export");
	}

	public static PluginStore importStore(byte[] exportedStore) {
		Configuration conf = Db4o.newConfiguration();
		MemoryIoAdapter mia = new MemoryIoAdapter();
		conf.io(mia);
		mia.put("Import", exportedStore);
		ObjectContainer o = Db4o.openFile(conf, "Import");
		ObjectSet query = o.query(PluginStoreContainer.class);
		if(query.size() > 0) {
			o.activate(query.get(0), Integer.MAX_VALUE);
			PluginStore ret = ((PluginStoreContainer) query.get(0)).pluginStore;
			o.close();
			return ret;
		} else {
			o.close();
			return null;
		}
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.clients.http;

import java.util.Hashtable;
import java.util.Set;

import freenet.client.HighLevelSimpleClient;
import freenet.node.NodeClientCore;
import freenet.support.HTMLNode;

public class LocalFileN2NMToadlet extends LocalFileBrowserToadlet {
	
	public String path() {
		return "/n2nm-browse/";
	}
	
	protected String postTo(){
		return "/send_n2ntm/";
	}
	
	public LocalFileN2NMToadlet(NodeClientCore core, HighLevelSimpleClient highLevelSimpleClient) {
		super(core, highLevelSimpleClient);
	}
	
	protected void createInsertDirectoryButton(HTMLNode fileRow, String path, ToadletContext ctx, Hashtable<String, String> fieldPairs) {
		fileRow.addChild("td");
	}
	
	protected Hashtable<String, String> persistanceFields(Hashtable<String, String> set){
		Hashtable<String, String> fieldPairs = new Hashtable<String, String>();
		String message = set.get("message");
		if(message != null) fieldPairs.put("message", message);
		Set<String> keys = set.keySet();
		for(String key : keys)
		{
			if(key.startsWith("node_"))
			{
				fieldPairs.put(key, "1");
			}
		}
		return fieldPairs;
	}
}
/*
 * Dijjer - A Peer to Peer HTTP Cache
 * Copyright (C) 2004,2005 Change.Tv, Inc
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */

package freenet.support;

import java.io.DataInput;
import java.io.DataOutputStream;
import java.io.IOException;

import freenet.io.WritableToDataOutputStream;

public class BitArray implements WritableToDataOutputStream {

    public static final String VERSION = "$Id: BitArray.java,v 1.2 2005/08/25 17:28:19 amphibian Exp $";

	private int _size;
	private byte[] _bits;

	public BitArray(byte[] data) {
		_bits = data;
		_size = data.length*8;
	}
	
	public BitArray copy() {
		return new BitArray(this);
	}
	
	/**
	 * This constructor does not check for unacceptable sizes, and should only be used on trusted data.
	 */
	public BitArray(DataInput dis) throws IOException {
		_size = dis.readInt();
		_bits = new byte[(_size / 8) + (_size % 8 == 0 ? 0 : 1)];
		dis.readFully(_bits);
	}
	
	public BitArray(DataInput dis, int maxSize) throws IOException {
		_size = dis.readInt();
		if (_size<=0 || _size>maxSize)
			throw new IOException("Unacceptable bitarray size: "+_size);
		_bits = new byte[(_size / 8) + (_size % 8 == 0 ? 0 : 1)];
		dis.readFully(_bits);
	}

	public BitArray(int size) {
		_size = size;
		_bits = new byte[(size / 8) + (size % 8 == 0 ? 0 : 1)];
	}

	public BitArray(BitArray src) {
		this._size = src._size;
		this._bits = new byte[src._bits.length];
		System.arraycopy(src._bits, 0, _bits, 0, src._bits.length);
	}
	
	public void setBit(int pos, boolean f) {
		if(pos > _size) throw new ArrayIndexOutOfBoundsException();
		int b = unsignedByteToInt(_bits[pos / 8]);
		int mask = (1 << (pos % 8));
		if (f) {
			_bits[pos / 8] = (byte) (b | mask);
		} else {
			_bits[pos / 8] = (byte) (b & (~mask));
		}
	}

	public boolean bitAt(int pos) {
		if(pos > _size) throw new ArrayIndexOutOfBoundsException();
		int b = unsignedByteToInt(_bits[pos / 8]);
		int mask = (1 << (pos % 8));
		return (b & mask) != 0;
	}

	public static int unsignedByteToInt(byte b) {
		return b & 0xFF;
	}

	@Override
	public String toString() {
		StringBuilder sb = new StringBuilder(this._size);
		for (int x=0; x<_size; x++) {
			if (bitAt(x)) {
				sb.append('1');
			} else {
				sb.append('0');
			}
		}
		return sb.toString();
	}

	public void writeToDataOutputStream(DataOutputStream dos) throws IOException {
		dos.writeInt(_size);
		dos.write(_bits);
	}

	public static int serializedLength(int size) {
		return ((size / 8) + (size % 8 == 0 ? 0 : 1)) + 4;
	}

	public int getSize() {
		return _size;
	}
	
	@Override
	public boolean equals(Object o) {
		if (!(o instanceof BitArray)) {
			return false;
		}
		BitArray ba = (BitArray) o;
		if (ba.getSize() != getSize()) {
			return false;
		}
		for (int x=0; x<getSize(); x++) {
			if (ba.bitAt(x) != bitAt(x)) {
				return false;
			}
		}
		return true;
	}
	
	@Override
	public int hashCode() {
	    return Fields.hashCode(_bits);
	}

	public void setAllOnes() {
		for(int i=0;i<_bits.length;i++)
			_bits[i] = (byte)0xFF;
	}

	public int firstOne(int start) {
		int startByte = start/8;
		int startBit = start%8;
		for(int i=startByte;i<_bits.length;i++) {
			byte b = _bits[i];
			if(b == 0) continue;
			for(int j=startBit;j<8;j++) {
				int mask = (1 << j);
				if((b & mask) != 0) {
					int x = i*8+j;
					if(x >= _size) return -1;
					return x;
				}
			}
			startBit = 0;
		}
		return -1;
	}
	
	public int firstOne() {
		return firstOne(0);
	}

	public int firstZero(int start) {
		int startByte = start/8;
		int startBit = start%8;
		for(int i=startByte;i<_bits.length;i++) {
			byte b = _bits[i];
			if(b == (byte)255) continue;
			for(int j=startBit;j<8;j++) {
				int mask = (1 << j);
				if((b & mask) == 0) {
					int x = i*8+j;
					if(x >= _size) return -1;
					return x;
				}
			}
			startBit = 0;
		}
		return -1;
	}

	public void setSize(int size) {
		if(_size == size) return;
		int oldSize = _size;
		_size = size;
		int bytes = (size / 8) + (size % 8 == 0 ? 0 : 1);
		if(_bits.length != bytes) {
			byte[] newBuff = new byte[bytes];
			System.arraycopy(_bits, 0, newBuff, 0, Math.min(_bits.length, newBuff.length));
			_bits = newBuff;
		}
		if(oldSize < _size && oldSize % 8 != 0) {
			for(int i=oldSize;i<Math.min(_size, oldSize - oldSize % 8 + 8);i++) {
				setBit(i, false);
			}
		}
	}

	public int lastOne(int start) {
		if(start >= _size) start = _size-1;
		int startByte = start/8;
		int startBit = start%8;
		if(startByte >= _bits.length) {
			System.err.println("Start byte is "+startByte+" _bits.length is "+_bits.length);
			assert(false);
		}
		for(int i=startByte;i>=0;i--,startBit=7) {
			byte b = _bits[i];
			if(b == (byte)0) continue;
			for(int j=startBit;j>=0;j--) {
				int mask = (1 << j);
				if((b & mask) != 0) {
					int x = i*8+j;
					return x;
				}
			}
		}
		return -1;
	}
}package freenet.client.async;

import freenet.keys.KeyVerifyException;

public class BinaryBlobFormatException extends Exception {

	/**
	 * 
	 */
	private static final long serialVersionUID = 1L;

	public BinaryBlobFormatException(String message) {
		super(message);
	}

	public BinaryBlobFormatException(String message, KeyVerifyException e) {
		super(message, e);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.simulator;

import freenet.crypt.RandomSource;
import freenet.io.comm.NotConnectedException;
import freenet.io.comm.PeerParseException;
import freenet.io.comm.ReferenceSignatureVerificationException;
import freenet.node.FSParseException;
import freenet.node.Node;
import freenet.node.NodeInitException;
import freenet.node.NodeStarter;
import freenet.node.PeerNode;
import freenet.node.DarknetPeerNode.FRIEND_TRUST;
import freenet.support.Executor;
import freenet.support.Logger;
import freenet.support.PooledExecutor;
import freenet.support.Logger.LogLevel;
import freenet.support.LoggerHook.InvalidThresholdException;

/**
 * @author amphibian
 * 
 * When the code is invoked via this class, it:
 * - Creates two nodes.
 * - Connects them to each other
 * - Sends pings from the first node to the second node.
 * - Prints on the logger when packets are sent, when they are
 *   received, (by both sides), and their sequence numbers.
 */
public class RealNodePingTest {
	
	public static final int DARKNET_PORT1 = RealNodeNetworkColoringTest.DARKNET_PORT_END;
	public static final int DARKNET_PORT2 = RealNodeNetworkColoringTest.DARKNET_PORT_END+1;
	
	static final FRIEND_TRUST trust = FRIEND_TRUST.LOW;

    public static void main(String[] args) throws FSParseException, PeerParseException, InterruptedException, ReferenceSignatureVerificationException, NodeInitException, InvalidThresholdException {
        RandomSource random = NodeStarter.globalTestInit("pingtest", false, LogLevel.ERROR, "", true);
        // Create 2 nodes
        Executor executor = new PooledExecutor();
        Node node1 = NodeStarter.createTestNode(DARKNET_PORT1, 0, "pingtest", true, Node.DEFAULT_MAX_HTL, 0, random, executor, 1000, 65536, true, false, false, false, false, false, true, 0, false, false, true, false, null);
        Node node2 = NodeStarter.createTestNode(DARKNET_PORT2, 0, "pingtest", true, Node.DEFAULT_MAX_HTL, 0, random, executor, 1000, 65536, true, false, false, false, false, false, true, 0, false, false, true, false, null);
        // Connect
        node1.connect(node2, trust);
        node2.connect(node1, trust);
        // No swapping
        node1.start(true);
        node2.start(true);
        // Ping
        PeerNode pn = node1.getPeerNodes()[0];
        int pingID = 0;
        Thread.sleep(20000);
        //node1.usm.setDropProbability(4);
        while(true) {
            Logger.minor(RealNodePingTest.class, "Sending PING "+pingID);
            boolean success;
            try {
                success = pn.ping(pingID);
            } catch (NotConnectedException e1) {
                Logger.normal(RealNodePingTest.class, "Not connected");
                continue;
            }
            if(success)
                Logger.normal(RealNodePingTest.class, "PING "+pingID+" successful");
            else
                Logger.normal(RealNodePingTest.class, "PING FAILED: "+pingID);
            try {
                Thread.sleep(2000);
            } catch (InterruptedException e) {
                // Shouldn't happen
            }
            pingID++;
        }
    }
}
package freenet.client.async;

import com.db4o.ObjectContainer;

import freenet.keys.USK;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;

/** Proxy class to only pass through latest-slot updates after an onRoundFinished().
 * Note that it completely ignores last-known-good updates.
 * @author toad
 */
public class USKSparseProxyCallback implements USKProgressCallback {

	final USKCallback target;
	final USK key;

	private long lastEdition;
	private long lastSent;
	private boolean lastMetadata;
	private short lastCodec;
	private byte[] lastData;
	private boolean lastWasKnownGoodToo;
	private boolean roundFinished;
	
    private static volatile boolean logMINOR;
	static {
		Logger.registerClass(USKSparseProxyCallback.class);
	}

	public USKSparseProxyCallback(USKCallback cb, USK key) {
		target = cb;
		lastEdition = -1; // So we see the first one even if it's 0
		lastSent = -1;
		this.key = key;
		if(logMINOR) Logger.minor(this, "Creating sparse proxy callback "+this+" for "+cb+" for "+key);
	}

	public void onFoundEdition(long l, USK key, ObjectContainer container,
			ClientContext context, boolean metadata, short codec, byte[] data,
			boolean newKnownGood, boolean newSlotToo) {
		synchronized(this) {
			if(l < lastEdition) {
				if(!roundFinished) return;
				if(!newKnownGood) return;
			} else if(l == lastEdition) {
				if(newKnownGood) lastWasKnownGoodToo = true;
			} else {
				lastEdition = l;
				lastMetadata = metadata;
				lastCodec = codec;
				lastData = data;
				lastWasKnownGoodToo = newKnownGood;
			}
			if(!roundFinished) return;
		}
		target.onFoundEdition(l, key, null, context, metadata, codec, data, newKnownGood, newSlotToo);
	}

	public short getPollingPriorityNormal() {
		return target.getPollingPriorityNormal();
	}

	public short getPollingPriorityProgress() {
		return target.getPollingPriorityProgress();
	}

	public void onSendingToNetwork(ClientContext context) {
		innerRoundFinished(context, false);
	}

	public void onRoundFinished(ClientContext context) {
		innerRoundFinished(context, true);
	}
	
	private void innerRoundFinished(ClientContext context, boolean finishedRound) {
		long ed;
		boolean meta;
		short codec;
		byte[] data;
		boolean wasKnownGood;
		synchronized(this) {
			if(finishedRound)
				roundFinished = true;
			if(lastSent == lastEdition) return;
			lastSent = ed = lastEdition;
			meta = lastMetadata;
			codec = lastCodec;
			data = lastData;
			wasKnownGood = lastWasKnownGoodToo;
		}
		if(ed == -1) {
			ed = context.uskManager.lookupLatestSlot(key);
			if(ed == -1) return;
			meta = false;
			codec = -1;
			data = null;
			wasKnownGood = false;
		}
		if(ed == -1) return;
		target.onFoundEdition(ed, key, null, context, meta, codec, data, wasKnownGood, wasKnownGood);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.l10n;

import freenet.l10n.BaseL10n.LANGUAGE;
import java.io.File;

/**
 * This is the interface used to localize the Node. It just wraps a BaseL10n
 * with correct parameters so that this class can entierly be used using
 * static methods, which is much more practical to use than non-static methods.
 *
 * Why doesn't that class extends BaseL10n ? Because it has to be static.
 *
 * How to change the language ? Simply create a new NodeL1On object. You don't have
 * to stock the created object because you can access it using static methods.
 * @author Artefact2
 */
public class NodeL10n {

	private static BaseL10n b;

	/**
	 * Initialize the Node localization with the node's default language, and
	 * overrides in the working directory.
	 */
	public NodeL10n() {
		this(LANGUAGE.getDefault(), new File("."));
	}

	/**
	 * Initialize the Node localization. You must also call that constructor
	 * if you want to change the language.
	 * @param lang Language to use.
	 * @see LANGUAGE.mapToLanguage(String)
	 */
	public NodeL10n(final LANGUAGE lang, File overrideDir) {
		NodeL10n.b = new BaseL10n("freenet/l10n/", "freenet.l10n.${lang}.properties",
		  overrideDir.getPath()+File.separator+"freenet.l10n.${lang}.override.properties", lang);
	}

	/**
	 * Get the BaseL10n used to localize the node.
	 * @return BaseL10n.
	 * @see BaseL10n
	 */
	public static BaseL10n getBase() {
		if(b==null){
			new NodeL10n();
		}
		return b;
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.io.xfer;

/**
 * Thrown when a throttle is deprecated.
 * @author toad
 */
public class ThrottleDeprecatedException extends Exception {

	private static final long serialVersionUID = -4542976419025644806L;

	ThrottleDeprecatedException(PacketThrottle target) {
		this.target = target;
	}
	
	public final PacketThrottle target;

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.client.filter;

import java.net.URI;

import freenet.keys.FreenetURI;

public interface FoundURICallback {

	/**
	 * Called when a Freenet URI is found.
	 * @param uri The URI.
	 * FIXME: Indicate the type of the link e.g. inline image, hyperlink, etc??
	 */
	public void foundURI(FreenetURI uri);

	/**
	 * Called when a Freenet URI is found.
	 * @param uri The URI.
	 * FIXME: Indicate the type of the link e.g. inline image, hyperlink, etc??
	 */
	public void foundURI(FreenetURI uri, boolean inline);

	/**
	 * Called when some plain text is processed. This is used typically by
	 * spiders to index pages by their content.
	 * @param text The text. Will already have been fed through whatever decoding
	 * is necessary depending on the type of the source document e.g. HTMLDecoder.
	 * Will need to be re-encoded before being sent to e.g. a browser.
	 * @param type Can be null, or may be for example the name of the HTML tag
	 * directly surrounding the text. E.g. "title" lets you find page titles.
	 * @param baseURI The current base URI for this page. The base URI is not
	 * necessarily the URI of the page. It's the URI against which URIs on the
	 * page are resolved. It defaults to the URI of the page but can be overridden
	 * by base href in html, for example.	 */
	public void onText(String text, String type, URI baseURI);

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.client.async;

import java.io.DataInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Random;
import java.util.Vector;

import com.db4o.ObjectContainer;

import freenet.client.ArchiveContext;
import freenet.client.FECCallback;
import freenet.client.FECCodec;
import freenet.client.FECJob;
import freenet.client.FECQueue;
import freenet.client.FailureCodeTracker;
import freenet.client.FetchContext;
import freenet.client.FetchException;
import freenet.client.Metadata;
import freenet.client.MetadataParseException;
import freenet.client.SplitfileBlock;
import freenet.keys.CHKBlock;
import freenet.keys.CHKEncodeException;
import freenet.keys.CHKVerifyException;
import freenet.keys.ClientCHK;
import freenet.keys.ClientCHKBlock;
import freenet.keys.ClientKeyBlock;
import freenet.keys.Key;
import freenet.keys.KeyBlock;
import freenet.keys.KeyDecodeException;
import freenet.keys.NodeCHK;
import freenet.keys.TooBigException;
import freenet.node.KeysFetchingLocally;
import freenet.node.RequestScheduler;
import freenet.node.SendableGet;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;
import freenet.support.io.BucketTools;
import freenet.support.io.MultiReaderBucket;

/**
 * A single segment within a SplitFileFetcher.
 * This in turn controls a large number of SplitFileFetcherSubSegment's, which are registered on the ClientRequestScheduler.
 */
public class SplitFileFetcherSegment implements FECCallback, HasCooldownTrackerItem {

	private static volatile boolean logMINOR;
	
	private static final boolean FORCE_CHECK_FEC_KEYS = true;
	
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback() {
			
			@Override
			public void shouldUpdate() {
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}
	
	final short splitfileType;
	SplitFileSegmentKeys keys;
	boolean[] foundKeys;
	// FIXME remove eventually, needed for back compat for now
	ClientCHK[] dataKeys;
	ClientCHK[] checkKeys;
	final MinimalSplitfileBlock[] dataBuckets;
	final MinimalSplitfileBlock[] checkBuckets;
	final int[] dataRetries;
	final int[] checkRetries;
	// FIXME remove eventually, needed for back compat for now
	final Vector<SplitFileFetcherSubSegment> subSegments;
	private SplitFileFetcherSegmentGet getter;
	final int minFetched;
	final SplitFileFetcher parentFetcher;
	final ClientRequester parent;
	final ArchiveContext archiveContext;
	final long maxBlockLength;
	/** Has the segment finished processing? Irreversible. */
	private volatile boolean finished;
	private boolean startedDecode;
	/** Bucket to store the data retrieved, after it has been decoded.
	 * FIXME remove eventually, needed for back compat for now. */
	private Bucket decodedData;
	/** Fetch context for block fetches */
	final FetchContext blockFetchContext;
	/** Recursion level */
	final int recursionLevel;
	private FetchException failureException;
	/**
	 * If true, the last data block has bad padding and cannot be involved in FEC decoding.
	 */
	private final boolean ignoreLastDataBlock;
	private int fatallyFailedBlocks;
	private int failedBlocks;
	/**
	 * The number of blocks fetched. If ignoreLastDataBlock is set, fetchedBlocks does not
	 * include the last data block.
	 */
	private int fetchedBlocks;
	/**
	 * The number of data blocks (NOT check blocks) fetched. On a small splitfile, sometimes
	 * we will manage to fetch all the data blocks. If so, we can complete the segment, even
	 * if we can't do a FEC decode because of ignoreLastDataBlock.
	 */
	private int fetchedDataBlocks;
	final FailureCodeTracker errors;
	private boolean finishing;
	private boolean scheduled;
	private final boolean persistent;
	final int segNum;
	final boolean pre1254;
	final byte[] forceCryptoKey;
	final byte cryptoAlgorithm;
	
	private int maxRetries;
	
	// A persistent hashCode is helpful in debugging, and also means we can put
	// these objects into sets etc when we need to.
	
	private final int hashCode;

	// After the fetcher has finished with the segment, *and* we have encoded and started healing inserts,
	// we can removeFrom(). Note that encodes are queued to the database.
	private boolean fetcherFinished = false;
	private boolean fetcherHalfFinished = false;
	private boolean encoderFinished = false;
	
	/** The number of cross-check blocks at the end of the data blocks. These count
	 * as data blocks for most purposes but they are not included in the final data.
	 * They are the extra redundancy created for SplitFileFetcherCrossSegment's. */
	final int crossCheckBlocks;
	/** The cross-segment for each data or cross-check block. */
	private final SplitFileFetcherCrossSegment[] crossSegmentsByBlock;

	// Only used in initial allocation
	private transient int crossDataBlocksAllocated;
	private transient int crossCheckBlocksAllocated;
	
	private final boolean realTimeFlag;
	
	private int cachedCooldownTries;
	private long cachedCooldownTime;
	
	@Override
	public int hashCode() {
		return hashCode;
	}
	
	private transient FECCodec codec;
	
	public SplitFileFetcherSegment(short splitfileType, SplitFileSegmentKeys keys, SplitFileFetcher fetcher, ArchiveContext archiveContext, FetchContext blockFetchContext, long maxTempLength, int recursionLevel, ClientRequester requester, int segNum, boolean ignoreLastDataBlock, boolean pre1254, int crossCheckBlocks, byte cryptoAlgorithm, byte[] forceCryptoKey, int maxRetries, boolean realTimeFlag) throws MetadataParseException, FetchException {
		this.crossCheckBlocks = crossCheckBlocks;
		this.keys = keys;
		this.realTimeFlag = realTimeFlag;
		int dataBlocks = keys.getDataBlocks();
		int checkBlocks = keys.getCheckBlocks();
		foundKeys = new boolean[dataBlocks + checkBlocks];
		this.crossSegmentsByBlock = new SplitFileFetcherCrossSegment[dataBlocks];
		this.segNum = segNum;
		this.hashCode = super.hashCode();
		this.persistent = fetcher.persistent;
		this.parentFetcher = fetcher;
		this.ignoreLastDataBlock = ignoreLastDataBlock;
		this.errors = new FailureCodeTracker(false);
		this.archiveContext = archiveContext;
		this.splitfileType = splitfileType;
		this.maxRetries = maxRetries;
		this.parent = requester;
		dataKeys = null;
		checkKeys = null;
		if(splitfileType == Metadata.SPLITFILE_NONREDUNDANT) {
			minFetched = dataBlocks;
		} else if(splitfileType == Metadata.SPLITFILE_ONION_STANDARD) {
			minFetched = dataBlocks;
		} else throw new MetadataParseException("Unknown splitfile type"+splitfileType);
		finished = false;
		decodedData = null;
		dataBuckets = new MinimalSplitfileBlock[dataBlocks];
		checkBuckets = new MinimalSplitfileBlock[checkBlocks];
		for(int i=0;i<dataBuckets.length;i++) {
			dataBuckets[i] = new MinimalSplitfileBlock(i);
		}
		for(int i=0;i<checkBuckets.length;i++)
			checkBuckets[i] = new MinimalSplitfileBlock(i+dataBuckets.length);
		if(maxRetries != -1) {
			dataRetries = new int[dataBlocks];
			checkRetries = new int[checkBlocks];
		} else {
			dataRetries = null;
			checkRetries = null;
		}
		subSegments = null;
		getter = new SplitFileFetcherSegmentGet(parent, this, realTimeFlag);
		maxBlockLength = maxTempLength;
		this.blockFetchContext = blockFetchContext;
		this.recursionLevel = 0;
		if(logMINOR) Logger.minor(this, "Created "+this+" for "+parentFetcher+" : "+dataBuckets.length+" data blocks "+checkBuckets.length+" check blocks");
		this.pre1254 = pre1254;
		this.cryptoAlgorithm = cryptoAlgorithm;
		this.forceCryptoKey = forceCryptoKey;
	}

	public synchronized boolean isFinished(ObjectContainer container) {
		if(finished) return true;
		// Deactivating parent is a *bad* side-effect, so avoid it.
		boolean deactivateParent = false;
		if(persistent) {
			deactivateParent = !container.ext().isActive(parent);
			if(deactivateParent) container.activate(parent, 1);
		}
		boolean ret = parent.isCancelled();
		if(deactivateParent)
			container.deactivate(parent, 1);
		return ret;
	}
	
	public synchronized boolean succeeded() {
		return finished;
	}

	public synchronized boolean isFinishing(ObjectContainer container) {
		return isFinished(container) || finishing;
	}
	
	/** Throw a FetchException, if we have one. Else do nothing. 
	 * @param container */
	public synchronized void throwError(ObjectContainer container) throws FetchException {
		if(failureException != null) {
			if(persistent) container.activate(failureException, 5);
			if(persistent)
				throw failureException.clone(); // We will remove, caller is responsible for clone
			else
				throw failureException;
		}
	}
	
	/** Decoded length? 
	 * @param container */
	public long decodedLength(ObjectContainer container) {
		if(decodedData != null) {
			if(persistent)
				container.activate(decodedData, 1);
			return decodedData.size();
		} else
			return (this.dataBuckets.length - crossCheckBlocks) * CHKBlock.DATA_LENGTH;
	}

	/** Write the decoded segment's data to an OutputStream */
	public long writeDecodedDataTo(OutputStream os, long truncateLength, ObjectContainer container) throws IOException {
		if(logMINOR)
			Logger.minor(this, "Writing decoded data on "+this);
		if(decodedData != null) {
			if(persistent) container.activate(decodedData, Integer.MAX_VALUE);
			long len = decodedData.size();
			if((truncateLength >= 0) && (truncateLength < len))
				len = truncateLength;
			BucketTools.copyTo(decodedData, os, Math.min(truncateLength, decodedData.size()));
			return len;
		} else {
			long totalCopied = 0;
			byte[] buf = new byte[CHKBlock.DATA_LENGTH];
			for(int i=0;i<dataBuckets.length-crossCheckBlocks;i++) {
				if(logMINOR) Logger.minor(this, "Copying data from block "+i);
				SplitfileBlock status = dataBuckets[i];
				if(status == null) throw new NullPointerException();
				boolean blockActive = true;
				if(persistent) {
					blockActive = container.ext().isActive(status);
					if(!blockActive)
						container.activate(status, Integer.MAX_VALUE);
				}
				Bucket data = status.getData();
				if(data == null) 
					throw new NullPointerException("Data bucket "+i+" of "+dataBuckets.length+" is null in writeDecodedData on "+this+" status = "+status+" number "+status.getNumber()+" data "+status.getData()+" persistence = "+persistent+(persistent ? (" (block active = "+container.ext().isActive(status)+" block ID = "+container.ext().getID(status)+" seg active="+container.ext().isActive(this)+")"):""));
				if(persistent) container.activate(data, 1);
				long copy;
				if(truncateLength < 0)
					copy = Long.MAX_VALUE;
				else
					copy = truncateLength - totalCopied;
				if(copy < ((long)CHKBlock.DATA_LENGTH))
					buf = new byte[(int)copy];
				InputStream is = data.getInputStream();
				DataInputStream dis = new DataInputStream(is);
				dis.readFully(buf);
				is.close();
				os.write(buf);
				totalCopied += buf.length;
				if(!blockActive) container.deactivate(status, 1);
			}
			if(logMINOR) Logger.minor(this, "Copied data ("+totalCopied+")");
			return totalCopied;
		}
	}

	/** How many blocks have failed due to running out of retries? */
	public synchronized int failedBlocks() {
		return failedBlocks;
	}
	
	/** How many blocks have been successfully fetched? */
	public synchronized int fetchedBlocks() {
		return fetchedBlocks;
	}

	/** How many blocks failed permanently due to fatal errors? */
	public synchronized int fatallyFailedBlocks() {
		return fatallyFailedBlocks;
	}

	private short onSuccessInner(Bucket data, int blockNo, ObjectContainer container, ClientContext context) {
		SplitFileFetcherCrossSegment crossSegment = null;
		short res = 0;
		synchronized(this) {
			boolean dontNotify;
			boolean allFailed = false;
			boolean decodeNow = false;
			boolean wasDataBlock = false;
			if(finished) {
				// Happens sometimes, don't complain about it...
				// What this means is simply that there were a bunch of requests
				// running, one of them completed, the whole segment went into
				// decode, and now the extra requests are surplus to requirements.
				// It's a slight overhead, but the alternative is worse.
				if(logMINOR)
					Logger.minor(this, "onSuccess() when already finished for "+this);
				data.free();
				if(persistent) data.removeFrom(container);
				return -1;
			}
			// We accept blocks after startedDecode. So the FEC code will drop the surplus blocks.
			// This is important for cross-segment encoding:
			// Cross-segment decodes a block. This triggers a decode here, and also an encode on the cross-segment.
			// If we ignore the block here, the cross-segment encode will fail because a block is missing.
			if(blockNo < dataBuckets.length) {
				wasDataBlock = true;
				if(haveFoundKey(blockNo, container)) {
					if(!startedDecode) {
						// This can happen.
						// We queue a persistent download, we queue a transient.
						// The transient goes through DatastoreChecker first,
						// and feeds the block to us. We don't finish, because
						// we need more blocks. Then the persistent goes through 
						// the DatastoreChecker, and calls us again with the same
						// block.
						if(logMINOR)
							Logger.minor(this, "Block already finished: "+blockNo);
					}
					data.free();
					if(persistent) data.removeFrom(container);
					return -1;
				}
				if(persistent)
					container.activate(dataBuckets[blockNo], 1);
				Bucket existingBlock = dataBuckets[blockNo].trySetData(data);
				if(existingBlock != null) {
					if(logMINOR)
						Logger.minor(this, "Already have data for data block "+blockNo);
					if(existingBlock != data) {
						data.free();
						if(persistent) data.removeFrom(container);
					}
					return -1;
				}
				setFoundKey(blockNo, container, context);
				if(persistent) {
					data.storeTo(container);
					container.store(dataBuckets[blockNo]);
					container.store(this); // We could return -1, so we need to store(this) here
				}
				if(crossCheckBlocks != 0)
					crossSegment = crossSegmentsByBlock[blockNo];
			} else if(blockNo < checkBuckets.length + dataBuckets.length) {
				int checkNo = blockNo - dataBuckets.length;
				if(haveFoundKey(blockNo, container)) {
					if(!startedDecode) {
						if(logMINOR)
							Logger.minor(this, "Check block already finished: "+checkNo);
					}
					data.free();
					if(persistent) data.removeFrom(container);
					return -1;
				}
				if(persistent)
					container.activate(checkBuckets[checkNo], 1);
				Bucket existingBlock = checkBuckets[checkNo].trySetData(data);
				if(existingBlock != null) {
					if(logMINOR)
						Logger.minor(this, "Already have data for check block "+checkNo);
					if(existingBlock != data) {
						data.free();
						if(persistent) data.removeFrom(container);
					}
					return -1;
				}
				setFoundKey(blockNo, container, context);
				if(persistent) {
					data.storeTo(container);
					container.store(checkBuckets[checkNo]);
					container.store(this); // We could return -1, so we need to store(this) here
				}
			} else
				Logger.error(this, "Unrecognized block number: "+blockNo, new Exception("error"));
			if(startedDecode) {
				return -1;
			} else {
				boolean tooSmall = data.size() < CHKBlock.DATA_LENGTH;
				// Don't count the last data block, since we can't use it in FEC decoding.
				if(tooSmall && ((!ignoreLastDataBlock) || (blockNo != dataBuckets.length - 1))) {
					fail(new FetchException(FetchException.INVALID_METADATA, "Block too small in splitfile: block "+blockNo+" of "+dataBuckets.length+" data keys, "+checkBuckets.length+" check keys"), container, context, true);
					return -1;
				}
				if(!(ignoreLastDataBlock && blockNo == dataBuckets.length - 1 && tooSmall))
					fetchedBlocks++;
				else
					// This block is not going to be fetched, and because of the insertion format. 
					// Thus it is a fatal failure. We need to track it, because it is quite possible
					// to fetch the last block, not complete because it's the last block, and hang.
					fatallyFailedBlocks++;
				// However, if we manage to get EVERY data block (common on a small splitfile),
				// we don't need to FEC decode.
				if(wasDataBlock)
					fetchedDataBlocks++;
				if(logMINOR) Logger.minor(this, "Fetched "+fetchedBlocks+" blocks in onSuccess("+blockNo+")");
				boolean haveDataBlocks = fetchedDataBlocks == dataBuckets.length;
				decodeNow = (!startedDecode) && (fetchedBlocks >= minFetched || haveDataBlocks);
				if(decodeNow) {
					decodeNow = checkAndDecodeNow(container, blockNo, tooSmall, haveDataBlocks);
				}
				if(!decodeNow) {
					// Avoid hanging when we have n-1 check blocks, we succeed on the last data block,
					// we don't have the other data blocks, and we have nothing else fetching.
					allFailed = failedBlocks + fatallyFailedBlocks > (dataBuckets.length + checkBuckets.length - minFetched);
				}
			}
			dontNotify = !scheduled;
			if(dontNotify) res |= ON_SUCCESS_DONT_NOTIFY;
			if(allFailed) res |= ON_SUCCESS_ALL_FAILED;
			if(decodeNow) res |= ON_SUCCESS_DECODE_NOW;
		}
		if(persistent) container.store(this);
		if(crossSegment != null) {
			boolean active = true;
			if(persistent) {
				active = container.ext().isActive(crossSegment);
				if(!active) container.activate(crossSegment, 1);
			}
			crossSegment.onFetched(this, blockNo, container, context);
			if(!active) container.deactivate(crossSegment, 1);
		}
		return res;
	}
	
	private void setFoundKey(int blockNo, ObjectContainer container, ClientContext context) {
		if(keys == null) migrateToKeys(container);
		else {
			if(persistent) container.activate(keys, 1);
		}
		synchronized(this) {
			if(foundKeys[blockNo]) return;
			foundKeys[blockNo] = true;
			// Don't remove from KeyListener if we have already removed this segment.
			if(startedDecode) return;
		}
		if(persistent) container.store(this);
		SplitFileFetcherKeyListener listener = parentFetcher.getListener();
		if(listener == null)
			Logger.error(this, "NO LISTENER FOR "+this, new Exception("error"));
		else
			listener.removeKey(keys.getNodeKey(blockNo, null, false), this, container, context);
	}

	private boolean haveFoundKey(int blockNo, ObjectContainer container) {
		if(keys == null) migrateToKeys(container);
		return foundKeys[blockNo];
	}

	private synchronized void migrateToKeys(ObjectContainer container) {
		if(logMINOR) Logger.minor(this, "Migrating keys on "+this);
		keys = new SplitFileSegmentKeys(dataKeys.length, checkBuckets.length, forceCryptoKey, cryptoAlgorithm);
		foundKeys = new boolean[dataKeys.length + checkBuckets.length];
		for(int i=0;i<dataKeys.length;i++) {
			ClientCHK key = dataKeys[i];
			if(key == null) {
				foundKeys[i] = true;
			} else {
				if(persistent) container.activate(key, 5);
				keys.setKey(i, key);
				key.removeFrom(container);
			}
		}
		for(int i=0;i<checkBuckets.length;i++) {
			ClientCHK key = checkKeys[i];
			if(key == null) {
				foundKeys[i+dataKeys.length] = true;
			} else {
				if(persistent) container.activate(key, 5);
				keys.setKey(i+dataKeys.length, key);
				key.removeFrom(container);
			}
		}
		dataKeys = null;
		checkKeys = null;
		if(persistent) {
			container.store(keys);
			container.store(this);
		}
	}

	private synchronized boolean checkAndDecodeNow(ObjectContainer container, int blockNo, boolean tooSmall, boolean haveDataBlocks) {
		if(startedDecode) return false; // Caller checks anyway but worth checking.
		boolean decodeNow = true;
		// Double-check...
		// This is somewhat defensive, but these things have happened, and caused stalls.
		// And this may help recover from persistent damage caused by previous bugs ...
		int count = 0;
		boolean lastBlockTruncated = false;
		for(int i=0;i<dataBuckets.length;i++) {
			boolean active = true;
			if(persistent) {
				active = container.ext().isActive(dataBuckets[i]);
				if(!active) container.activate(dataBuckets[i], 1);
			}
			Bucket d = dataBuckets[i].getData();
			if(d != null) {
				count++;
				if(i == dataBuckets.length-1) {
					if(ignoreLastDataBlock) {
						if(blockNo == dataBuckets.length && tooSmall) {
							// Too small.
							lastBlockTruncated = true;
						} else if(blockNo == dataBuckets.length && !tooSmall) {
							// Not too small. Cool.
							//lastBlockTruncated = false;
						} else {
							boolean blockActive = true;
							if(persistent) {
								blockActive = container.ext().isActive(d);
								if(!blockActive) container.activate(d, 1);
							}
							lastBlockTruncated = d.size() < CHKBlock.DATA_LENGTH;
							if(!blockActive) container.deactivate(d, 1);
						}
//					} else {
//						lastBlockTruncated = false;
					}
				}
			}
			if(!active) container.deactivate(dataBuckets[i], 1);
		}
		if(haveDataBlocks && count < dataBuckets.length) {
			Logger.error(this, "haveDataBlocks is wrong: count is "+count);
		} else if(haveDataBlocks && count >= dataBuckets.length) {
			startedDecode = true;
			return true;
		}
		if(lastBlockTruncated) count--;
		for(int i=0;i<checkBuckets.length;i++) {
			boolean active = true;
			if(persistent) {
				active = container.ext().isActive(checkBuckets[i]);
				if(!active) container.activate(checkBuckets[i], 1);
			}
			if(checkBuckets[i].getData() != null) {
				count++;
			}
			if(!active) container.deactivate(checkBuckets[i], 1);
		}
		if(count < dataBuckets.length) {
			Logger.error(this, "Attempting to decode but only "+count+" of "+dataBuckets.length+" blocks available!", new Exception("error"));
			decodeNow = false;
			fetchedDataBlocks = count;
		} else {
			startedDecode = true;
			finishing = true;
		}
		return decodeNow;
	}

	private static final short ON_SUCCESS_DONT_NOTIFY = 1;
	private static final short ON_SUCCESS_ALL_FAILED = 2;
	private static final short ON_SUCCESS_DECODE_NOW = 4;
	
	/**
	 * 
	 * @param data Will be freed if not used.
	 * @param blockNo
	 * @param block
	 * @param container
	 * @param context
	 * @param sub
	 * @return
	 */
	public boolean onSuccess(Bucket data, int blockNo, ClientCHKBlock block, ObjectContainer container, ClientContext context, SplitFileFetcherSubSegment sub) {
		if(persistent)
			container.activate(this, 1);
		if(data == null) throw new NullPointerException();
		// FIXME RECONSTRUCT BLOCK FOR BINARY BLOB.
		// Also can serve as an integrity check - if the key generated is wrong something is busted.
		// Probably only worth the effort if we are actually adding to a binary blob???
//		if(block == null) {
//			if(logMINOR) Logger.minor(this, "Reconstructing block from cross-segment decode");
//			block = encode(data, blockNo);
//		}
		if(logMINOR) Logger.minor(this, "Fetched block "+blockNo+" in "+this+" data="+dataBuckets.length+" check="+checkBuckets.length);
		try {
			if(!maybeAddToBinaryBlob(data, block, blockNo, container, context, block == null ? "CROSS-SEGMENT FEC" : "UNKNOWN")) {
				if((ignoreLastDataBlock && blockNo == dataBuckets.length-1) || (ignoreLastDataBlock && fetchedDataBlocks == dataBuckets.length)) {
					// Ignore
				} else if(block == null) {
					// Cross-segment, just return false.
					Logger.error(this, "CROSS-SEGMENT DECODED/ENCODED BLOCK INVALID: "+blockNo, new Exception("error"));
					onFatalFailure(new FetchException(FetchException.INTERNAL_ERROR, "Invalid block from cross-segment decode"), blockNo, container, context);
					data.free();
					if(persistent) data.removeFrom(container);
					return false;
				} else {
					Logger.error(this, "DATA BLOCK INVALID: "+blockNo, new Exception("error"));
					onFatalFailure(new FetchException(FetchException.INTERNAL_ERROR, "Invalid block"), blockNo, container, context);
					data.free();
					if(persistent) data.removeFrom(container);
					return false;
				}
			}
		} catch (FetchException e) {
			fail(e, container, context, false);
			data.free();
			if(persistent) data.removeFrom(container);
			return false;
		}
		// No need to unregister key, because it will be cleared in tripPendingKey().
		short result = onSuccessInner(data, blockNo, container, context);
		if(result == (short)-1) return false;
		finishOnSuccess(result, container, context);
		return true;
	}

	private void finishOnSuccess(short result, ObjectContainer container, ClientContext context) {
		boolean dontNotify = (result & ON_SUCCESS_DONT_NOTIFY) == ON_SUCCESS_DONT_NOTIFY;
		boolean allFailed = (result & ON_SUCCESS_ALL_FAILED) == ON_SUCCESS_ALL_FAILED;
		boolean decodeNow = (result & ON_SUCCESS_DECODE_NOW) == ON_SUCCESS_DECODE_NOW;
		if(logMINOR) Logger.minor(this, "finishOnSuccess: result = "+result+" dontNotify="+dontNotify+" allFailed="+allFailed+" decodeNow="+decodeNow);
		if(persistent) {
			container.store(this);
			container.activate(parent, 1);
		}
		parent.completedBlock(dontNotify, container, context);
		if(decodeNow) {
			if(persistent)
				container.activate(parentFetcher, 1);
			parentFetcher.removeMyPendingKeys(this, container, context);
			if(persistent)
				container.deactivate(parentFetcher, 1);
			removeSubSegments(container, context, false);
			decode(container, context);
		} else if(allFailed) {
			fail(new FetchException(FetchException.SPLITFILE_ERROR, errors), container, context, true);
		}
		if(persistent) {
			container.deactivate(parent, 1);
		}
	}

	public void decode(ObjectContainer container, ClientContext context) {
		// Concurrency issue (transient only): Only one thread can call decode(), because 
		// only on one thread will checkAndDecodeNow return true. However, it is possible 
		// for another thread to fail the download and remove the blocks in the meantime.
		if(persistent)
			container.activate(this, 1);
		// Now decode
		if(logMINOR) Logger.minor(this, "Decoding "+SplitFileFetcherSegment.this);

		// Determine this early on so it is before segmentFinished or encoderFinished - maximum chance of still having parentFetcher.
		createdBeforeRestart = createdBeforeRestart(container);
		
		if(persistent)
			container.store(this);
		
		// Activate buckets
		if(persistent) {
			for(int i=0;i<dataBuckets.length;i++) {
				container.activate(dataBuckets[i], 1);
			}
			for(int i=0;i<checkBuckets.length;i++)
				container.activate(checkBuckets[i], 1);
		}
		int data = 0;
		synchronized(this) {
			if(finished || encoderFinished) return;
			for(int i=0;i<dataBuckets.length;i++) {
				if(dataBuckets[i].getData() != null) {
					data++;
				} else {
					// Use flag to indicate that it the block needed to be decoded.
					dataBuckets[i].flag = true;
					if(persistent) dataBuckets[i].storeTo(container);
				}
			}
		}
		if(getter != null) {
			if(persistent) container.activate(getter, 1);
			getter.unregister(container, context, getPriorityClass(container));
			if(persistent) getter.removeFrom(container);
			getter = null;
			if(persistent) container.store(this);
		}
		if(data == dataBuckets.length) {
			if(logMINOR)
				Logger.minor(this, "Already decoded");
			if(persistent) {
				for(int i=0;i<dataBuckets.length;i++) {
					container.activate(dataBuckets[i].getData(), 1);
				}
			}
			synchronized(this) {
				startedDecode = true;
			}
			onDecodedSegment(container, context, null, null, null, dataBuckets, checkBuckets);
			return;
		}
		
		if(splitfileType != Metadata.SPLITFILE_NONREDUNDANT) {
			FECQueue queue = context.fecQueue;
			int count = 0;
			synchronized(this) {
				if(finished || encoderFinished) return;
				// Double-check...
				for(int i=0;i<dataBuckets.length;i++) {
					Bucket d = dataBuckets[i].getData();
					if(d != null) {
						boolean valid = false;
						if(i == dataBuckets.length-1) {
							if(ignoreLastDataBlock) {
								boolean blockActive = true;
								if(persistent) {
									blockActive = container.ext().isActive(d);
									if(!blockActive) container.activate(d, 1);
								}
								if(d.size() >= CHKBlock.DATA_LENGTH)
									valid = true;
								if(!blockActive) container.deactivate(d, 1);
							} else {
								valid = true;
							}
						} else {
							valid = true;
						}
						if(valid)
							count++;
					}
				}
				for(int i=0;i<checkBuckets.length;i++) {
					if(checkBuckets[i].getData() != null)
						count++;
				}
			}
			if(count < dataBuckets.length) {
				Logger.error(this, "Attempting to decode but only "+count+" of "+dataBuckets.length+" blocks available!", new Exception("error"));
				// startedDecode and finishing are already set, so we can't recover.
				fail(new FetchException(FetchException.INTERNAL_ERROR, "Not enough blocks to decode but decoding anyway?!"), container, context, true);
				return;
			}
			if(persistent)
				container.activate(parent, 1);
			MinimalSplitfileBlock block = dataBuckets[dataBuckets.length-1];
			if(block == null) {
				synchronized(this) {
					if(!finished || encoderFinished)
						Logger.error(this, "Last block wrapper is null yet not finished?!");
					return;
				}
			}
			Bucket lastBlock = block.getData();
			if(lastBlock != null) {
				if(persistent)
					container.activate(lastBlock, 1);
				if(ignoreLastDataBlock && lastBlock.size() < CHKBlock.DATA_LENGTH) {
					lastBlock.free();
					if(persistent)
						lastBlock.removeFrom(container);
					dataBuckets[dataBuckets.length-1].clearData();
					if(persistent)
						container.store(dataBuckets[dataBuckets.length-1]);
					// It will be decoded by the FEC job.
				} else if(lastBlock.size() != CHKBlock.DATA_LENGTH) {
					// All new inserts will have the last block padded. If it was an old insert, ignoreLastDataBlock
					// would be set. Another way we can get here is if the last data block of a segment other than
					// the last data block is too short.
					fail(new FetchException(FetchException.INVALID_METADATA, "Last data block is not the standard size"), container, context, true);
				}
			}
			synchronized(this) {
				if(finished || encoderFinished) return;
			}
			if(codec == null)
				codec = FECCodec.getCodec(splitfileType, dataBuckets.length, checkBuckets.length);
			FECJob job = new FECJob(codec, queue, dataBuckets, checkBuckets, CHKBlock.DATA_LENGTH, context.getBucketFactory(persistent), this, true, parent.getPriorityClass(), persistent);
			codec.addToQueue(job, 
					queue, container);
			if(logMINOR)
				Logger.minor(this, "Queued FEC job: "+job);
			if(persistent)
				container.deactivate(parent, 1);
			// Now have all the data blocks (not necessarily all the check blocks)
		} else {
			Logger.error(this, "SPLITFILE_NONREDUNDANT !!");
			synchronized(this) {
				startedDecode = true;
			}
			onDecodedSegment(container, context, null, null, null, null, null);
		}
	}
	
	public void onDecodedSegment(ObjectContainer container, ClientContext context, FECJob job, Bucket[] dataBuckets2, Bucket[] checkBuckets2, SplitfileBlock[] dataBlockStatus, SplitfileBlock[] checkBlockStatus) {
		if(persistent) {
			container.activate(parent, 1);
			container.activate(context, 1);
			container.activate(blockFetchContext, 1);
		}
		synchronized(this) {
			if(encoderFinished)
				Logger.error(this, "Decoded segment after encoder finished");
		}
		if(codec == null)
			codec = FECCodec.getCodec(splitfileType, dataBuckets.length, checkBuckets.length);
		// Because we use SplitfileBlock, we DON'T have to copy here.
		// See FECCallback comments for explanation.
		if(persistent) {
			for(int i=0;i<dataBuckets.length;i++) {
				// The FECCodec won't set them.
				// But they should be active.
				if(dataBlockStatus[i] != dataBuckets[i]) {
					long theirID = container.ext().getID(dataBlockStatus[i]);
					long ourID = container.ext().getID(dataBuckets[i]);
					if(theirID == ourID) {
						Logger.error(this, "DB4O BUG DETECTED IN DECODED SEGMENT!: our block: "+dataBuckets[i]+" block from decode "+dataBlockStatus[i]+" both have ID "+ourID+" = "+theirID);
						dataBuckets[i] = (MinimalSplitfileBlock) dataBlockStatus[i];
					}
				}
				if(logMINOR)
					Logger.minor(this, "Data block "+i+" is "+dataBuckets[i]);
				if(!container.ext().isStored(dataBuckets[i]))
					Logger.error(this, "Data block "+i+" is not stored!");
				else if(!container.ext().isActive(dataBuckets[i]))
					Logger.error(this, "Data block "+i+" is inactive! : "+dataBuckets[i]);
				if(dataBuckets[i] == null)
					Logger.error(this, "Data block "+i+" is null!");
				else if(dataBuckets[i].getData() == null)
					Logger.error(this, "Data block "+i+" has null data!");
				else
					dataBuckets[i].getData().storeTo(container);
				container.store(dataBuckets[i]);
			}
		}
		if(crossCheckBlocks != 0) {
			for(int i=0;i<dataBuckets.length;i++) {
				if(persistent) container.activate(dataBuckets[i], 1); // onFetched might deactivate blocks.
				if(dataBuckets[i].flag) {
					// New block. Might allow a cross-segment decode.
					boolean active = true;
					if(persistent) {
						active = container.ext().isActive(crossSegmentsByBlock[i]);
						if(!active) container.activate(crossSegmentsByBlock[i], 1);
					}
					crossSegmentsByBlock[i].onFetched(this, i, container, context);
					if(!active) container.deactivate(crossSegmentsByBlock[i], 1);
				}
			}
		}
		
		int[] dataRetries = this.dataRetries;
		MyCooldownTrackerItem tracker = makeCooldownTrackerItem(container, context);
		if(getMaxRetries(container) == -1) {
			// Cooldown and retry counts entirely kept in RAM.
			dataRetries = tracker.dataRetries;
		}
		
		boolean allDecodedCorrectly = true;
		boolean allFromStore = !parent.sentToNetwork;
		for(int i=0;i<dataBuckets.length;i++) {
			if(persistent && crossCheckBlocks != 0) {
				// onFetched might deactivate blocks.
				container.activate(dataBuckets[i], 1);
			}
			Bucket data = dataBlockStatus[i].getData();
			if(data == null) 
				throw new NullPointerException("Data bucket "+i+" of "+dataBuckets.length+" is null in onDecodedSegment");
			boolean heal = dataBuckets[i].flag;
			if(allFromStore) heal = false;
			try {
				if(persistent && crossCheckBlocks != 0) {
					// onFetched might deactivate blocks.
					container.activate(data, Integer.MAX_VALUE);
				}
				if(!maybeAddToBinaryBlob(data, null, i, container, context, "FEC DECODE")) {
					if(ignoreLastDataBlock && i == dataBuckets.length-1) {
						// Padding issue: It was inserted un-padded and we decoded it padded, or similar situations.
						// Does not corrupt the result, or at least, corruption is undetectable if it's only on the last block.
						Logger.normal(this, "Last block padding issue on decode on "+this);
					} else {
						// Most likely the data was corrupt as inserted.
						Logger.error(this, "Data block "+i+" FAILED TO DECODE CORRECTLY");
						fail(new FetchException(FetchException.SPLITFILE_DECODE_ERROR), container, context, false);
						return;
					}
					// Disable healing.
					heal = false;
					allDecodedCorrectly = false;
				}
			} catch (FetchException e) {
				fail(e, container, context, false);
				return;
			}
			if(heal) {
				// 100% chance if we had to retry since startup, 5% chance otherwise.
				if(dataRetries[i] == 0) {
					int odds = createdBeforeRestart ? 10 : 20;
					if(context.fastWeakRandom.nextInt(odds) != 0)
						heal = false;
				}
			}
			if(heal) {
				Bucket wrapper = queueHeal(data, container, context);
				if(wrapper != data) {
					assert(!persistent);
					dataBuckets[i].replaceData(wrapper);
				}
			}
		}
		if(allDecodedCorrectly && logMINOR) Logger.minor(this, "All decoded correctly on "+this);
		if(persistent) container.store(this);
		if(persistent) {
			boolean fin;
			synchronized(this) {
				fin = fetcherFinished;
			}
			if(fin) {
				encoderFinished(container, context);
				return;
			}
		}
		boolean finishNow = splitfileType == Metadata.SPLITFILE_NONREDUNDANT || !isCollectingBinaryBlob();
		if(finishNow) {
			// Must set finished BEFORE calling parentFetcher.
			// Otherwise a race is possible that might result in it not seeing our finishing.
			synchronized(this) {
				finished = true;
			}
			if(persistent) container.store(this);
			if(persistent) container.activate(parentFetcher, 1);
			parentFetcher.segmentFinished(SplitFileFetcherSegment.this, container, context);
			if(persistent) container.deactivate(parentFetcher, 1);
		}
		// Leave active before queueing

		if(splitfileType == Metadata.SPLITFILE_NONREDUNDANT) {
			if(persistent) {
				container.deactivate(parent, 1);
				container.deactivate(context, 1);
			}
			if(persistent)
				encoderFinished(container, context);
			return;
		}
		
		// Now heal

		/** Splitfile healing:
		 * Any block which we have tried and failed to download should be 
		 * reconstructed and reinserted.
		 */

		Bucket lastBlock = dataBuckets[dataBuckets.length-1].getData();
		if(lastBlock != null) {
			if(persistent)
				container.activate(lastBlock, 1);
			if(ignoreLastDataBlock && lastBlock.size() != CHKBlock.DATA_LENGTH) {
				if(!finishNow) {
					synchronized(this) {
						finished = true;
					}
					if(persistent) container.store(this);
					if(persistent) container.activate(parentFetcher, 1);
					parentFetcher.segmentFinished(SplitFileFetcherSegment.this, container, context);
					if(persistent) container.deactivate(parentFetcher, 1);
				}
				if(persistent) {
					container.deactivate(parent, 1);
					container.deactivate(context, 1);
					encoderFinished(container, context);
				}
				return;
			}
		}
		
		// Encode any check blocks we don't have
		for(int i=0;i<checkBuckets.length;i++) {
			if(checkBuckets[i].getData() == null) {
				// Use flag to indicate that it the block needed to be decoded.
				checkBuckets[i].flag = true;
				if(persistent) checkBuckets[i].storeTo(container);
			}
		}

		try {
			synchronized(this) {
				if(encoderFinished) {
					Logger.error(this, "Encoder finished in onDecodedSegment at end??? on "+this);
					return; // Calling addToQueue now will NPE.
				}
			}
			codec.addToQueue(new FECJob(codec, context.fecQueue, dataBuckets, checkBuckets, 32768, context.getBucketFactory(persistent), this, false, parent.getPriorityClass(), persistent),
					context.fecQueue, container);
			if(persistent) {
				container.deactivate(parent, 1);
				container.deactivate(context, 1);
			}
		} catch (Throwable t) {
			Logger.error(this, "Caught "+t, t);
			onFailed(t, container, context);
			if(persistent)
				encoderFinished(container, context);
		}
	}

	// Set at decode time, before calling segmentFinished or encoderFinished.
	private boolean createdBeforeRestart;

	public void onEncodedSegment(ObjectContainer container, ClientContext context, FECJob job, Bucket[] dataBuckets2, Bucket[] checkBuckets2, SplitfileBlock[] dataBlockStatus, SplitfileBlock[] checkBlockStatus) {
		try {
		if(persistent) {
			container.activate(parent, 1);
		}
		if(logMINOR)
			Logger.minor(this, "Encoded "+this);
		// Because we use SplitfileBlock, we DON'T have to copy here.
		// See FECCallback comments for explanation.
		synchronized(this) {
			if(encoderFinished)
				Logger.error(this, "Decoded segment after encoder finished");
			// Now insert *ALL* blocks on which we had at least one failure, and didn't eventually succeed
			for(int i=0;i<dataBuckets.length;i++) {
				if(dataBuckets[i] == null) {
					Logger.error(this, "Data bucket "+i+" is null in onEncodedSegment on "+this);
					continue;
				}
				if(dataBuckets[i] != dataBlockStatus[i]) {
					Logger.error(this, "Data block "+i+" : ours is "+dataBuckets[i]+" codec's is "+dataBlockStatus[i]);
					if(persistent) {
						if(container.ext().getID(dataBuckets[i]) == container.ext().getID(dataBlockStatus[i]))
							Logger.error(this, "DB4O BUG DETECTED: SAME UID FOR TWO OBJECTS: "+dataBuckets[i]+"="+container.ext().getID(dataBuckets[i])+" and "+dataBlockStatus[i]+"="+container.ext().getID(dataBlockStatus[i])+" ... attempting workaround ...");
						Logger.error(this, "Ours is "+(container.ext().isStored(dataBuckets[i])?"stored ":"")+(container.ext().isActive(dataBuckets[i])?"active ":"")+" UUID "+container.ext().getID(dataBuckets[i]));
						Logger.error(this, "Theirs is "+(container.ext().isStored(dataBlockStatus[i])?"stored ":"")+(container.ext().isActive(dataBlockStatus[i])?"active ":"")+" UUID "+container.ext().getID(dataBlockStatus[i]));
					}
					dataBuckets[i] = (MinimalSplitfileBlock) dataBlockStatus[i];
				}
				Bucket data = dataBuckets[i].getData();
				if(data == null) {
					Logger.error(this, "Data bucket "+i+" has null contents in onEncodedSegment on "+this+" for block "+dataBuckets[i]);
					if(persistent) {
						if(!container.ext().isStored(dataBuckets[i]))
							Logger.error(this, "Splitfile block appears not to be stored");
						else if(!container.ext().isActive(dataBuckets[i]))
							Logger.error(this, "Splitfile block appears not to be active");
					}
					continue;
				}
				
			}
			int[] checkRetries = this.checkRetries;
			MyCooldownTrackerItem tracker = makeCooldownTrackerItem(container, context);
			if(getMaxRetries(container) == -1) {
				// Cooldown and retry counts entirely kept in RAM.
				checkRetries = tracker.checkRetries;
			}
			boolean allEncodedCorrectly = true;
			boolean allFromStore = false;
			synchronized(this) {
				if(parent == null) {
					allFromStore = false;
					if(!fetcherFinished) {
						Logger.error(this, "Parent is null on "+this+" but fetcher is not finished");
					} else {
						allFromStore = !parent.sentToNetwork;
					}
				}
			}
			for(int i=0;i<checkBuckets.length;i++) {
				boolean heal = false;
				// Check buckets will already be active because the FEC codec
				// has been using them.
				if(checkBuckets[i] == null) {
					Logger.error(this, "Check bucket "+i+" is null in onEncodedSegment on "+this);
					allEncodedCorrectly = false;
					continue;
				}
				if(checkBuckets[i] != checkBlockStatus[i]) {
					Logger.error(this, "Check block "+i+" : ours is "+checkBuckets[i]+" codec's is "+checkBlockStatus[i]);
					if(persistent) {
						if(container.ext().getID(checkBuckets[i]) == container.ext().getID(checkBlockStatus[i]))
							Logger.error(this, "DB4O BUG DETECTED: SAME UID FOR TWO OBJECTS: "+checkBuckets[i]+"="+container.ext().getID(checkBuckets[i])+" and "+checkBlockStatus[i]+"="+container.ext().getID(checkBlockStatus[i])+" ... attempting workaround ...");
						Logger.error(this, "Ours is "+(container.ext().isStored(checkBuckets[i])?"stored ":"")+(container.ext().isActive(checkBuckets[i])?"active ":"")+" UUID "+container.ext().getID(checkBuckets[i]));
						Logger.error(this, "Theirs is "+(container.ext().isStored(checkBlockStatus[i])?"stored ":"")+(container.ext().isActive(checkBlockStatus[i])?"active ":"")+" UUID "+container.ext().getID(checkBlockStatus[i]));
					}
					checkBuckets[i] = (MinimalSplitfileBlock) checkBlockStatus[i];
				}
				Bucket data = checkBuckets[i].getData();
				if(data == null) {
					Logger.error(this, "Check bucket "+i+" has null contents in onEncodedSegment on "+this+" for block "+checkBuckets[i]);
					if(persistent) {
						if(!container.ext().isStored(dataBuckets[i]))
							Logger.error(this, "Splitfile block appears not to be stored");
						else if(!container.ext().isActive(dataBuckets[i]))
							Logger.error(this, "Splitfile block appears not to be active");
					}
					continue;
				}
				heal = checkBuckets[i].flag;
				if(allFromStore) heal = false;
				try {
					if(!maybeAddToBinaryBlob(data, null, i+dataBuckets.length, container, context, "FEC ENCODE")) {
						heal = false;
						if(!(ignoreLastDataBlock && fetchedDataBlocks == dataBuckets.length))
							Logger.error(this, "FAILED TO ENCODE CORRECTLY so not healing check block "+i);
						allEncodedCorrectly = false;
					}
				} catch (FetchException e) {
					fail(e, container, context, false);
					return;
				}
				if(heal) {
					// 100% chance if we had to retry since startup, 5% chance otherwise.
					if(checkRetries[i] == 0) {
						int odds = createdBeforeRestart ? 10 : 20;
						if(context.fastWeakRandom.nextInt(odds) != 0)
							heal = false;
					}
				}
				if(heal) {
					Bucket wrapper = queueHeal(data, container, context);
					if(wrapper != data) {
						assert(!persistent);
						wrapper.free();
					}
					data.free();
					if(persistent) data.removeFrom(container);
					checkBuckets[i].clearData();
				} else {
					data.free();
				}
				if(persistent)
					checkBuckets[i].removeFrom(container);
				checkBuckets[i] = null;
				setFoundKey(i+dataBuckets.length, container, context);
			}
			if(logMINOR) {
				if(allEncodedCorrectly) Logger.minor(this, "All encoded correctly on "+this);
				else Logger.minor(this, "Not encoded correctly on "+this);
			}
			finished = true;
			if(persistent && !fetcherFinished) {
				container.store(this);
			}
		}
		if(logMINOR) Logger.minor(this, "Checked blocks.");
		// Defer the completion until we have generated healing blocks if we are collecting binary blobs.
		if(!(splitfileType == Metadata.SPLITFILE_NONREDUNDANT || !isCollectingBinaryBlob())) {
			if(persistent)
				container.activate(parentFetcher, 1);
			parentFetcher.segmentFinished(SplitFileFetcherSegment.this, container, context);
			if(persistent)
				container.deactivate(parentFetcher, 1);
		}
		} finally {
			if(persistent)
				encoderFinished(container, context);
		}
	}

	private boolean createdBeforeRestart(ObjectContainer container) {
		SplitFileFetcher f = parentFetcher;
		if(f == null) {
			Logger.error(this, "Created before restart returning false because parent fetcher already gone");
			return false;
		}
		boolean active = true;
		if(persistent) {
			active = container.ext().isActive(parentFetcher);
			if(!active) container.activate(f, 1);
		}
		SplitFileFetcherKeyListener listener = f.getListener();
		if(!active) container.deactivate(f, 1);
		if(listener == null) {
			Logger.error(this, "Created before restart return false because no listener");
			return false;
		}
		return listener.loadedOnStartup;
	}

	boolean isCollectingBinaryBlob() {
		if(parent instanceof ClientGetter) {
			ClientGetter getter = (ClientGetter) (parent);
			return getter.collectingBinaryBlob();
		} else return false;
	}
	
	private boolean maybeAddToBinaryBlob(Bucket data, ClientCHKBlock block, int blockNo, ObjectContainer container, ClientContext context, String dataSource) throws FetchException {
		if(FORCE_CHECK_FEC_KEYS || parent instanceof ClientGetter) {
			if(FORCE_CHECK_FEC_KEYS || ((ClientGetter)parent).collectingBinaryBlob()) {
				try {
					// Note: dontCompress is true. if false we need to know the codec it was compresssed to get a proper blob
					byte[] buf = BucketTools.toByteArray(data);
					if(!(buf.length == CHKBlock.DATA_LENGTH)) {
						// All new splitfile inserts insert only complete blocks even at the end.
						if((!ignoreLastDataBlock) || (blockNo != dataBuckets.length-1))
							Logger.error(this, "Block is too small: "+buf.length);
						return false;
					}
					if(block == null) {
						block = 
							ClientCHKBlock.encodeSplitfileBlock(buf, forceCryptoKey, cryptoAlgorithm);
					}
					ClientCHK key = getBlockKey(blockNo, container);
					if(key != null) {
						if(!(key.equals(block.getClientKey()))) {
							if(ignoreLastDataBlock && blockNo == dataBuckets.length-1 && dataSource.equals("FEC DECODE")) {
								if(logMINOR) Logger.minor(this, "Last block wrong key, ignored because expected due to padding issues");
							} else if(ignoreLastDataBlock && fetchedDataBlocks == dataBuckets.length && dataSource.equals("FEC ENCODE")) {
								// We padded the last block. The inserter might have used a different padding algorithm.
								if(logMINOR) Logger.minor(this, "Wrong key, might be due to padding issues");
							} else {
								Logger.error(this, "INVALID KEY FROM "+dataSource+": Block "+blockNo+" (data "+dataBuckets.length+" check "+checkBuckets.length+" ignore last block="+ignoreLastDataBlock+") : key "+block.getClientKey().getURI()+" should be "+key.getURI(), new Exception("error"));
							}
							return false;
						} else {
							if(logMINOR) Logger.minor(this, "Verified key for block "+blockNo+" from "+dataSource);
						}
					} else {
						if((dataSource.equals("FEC ENCODE") || dataSource.equals("FEC DECODE")
								|| dataSource.equals("CROSS-SEGMENT FEC")) && haveBlock(blockNo, container)) {
							// Ignore. FIXME Probably we should not delete the keys until after the encode??? Back compatibility issues maybe though...
							if(logMINOR) Logger.minor(this, "Key is null for block "+blockNo+" when checking key / adding to binary blob, key source is "+dataSource, new Exception("error"));
						} else {
							Logger.error(this, "Key is null for block "+blockNo+" when checking key / adding to binary blob, key source is "+dataSource, new Exception("error"));
						}
					}
					if(parent instanceof ClientGetter) {
						((ClientGetter)parent).addKeyToBinaryBlob(block, container, context);
					}
					return true;
				} catch (CHKEncodeException e) {
					Logger.error(this, "Failed to encode (collecting binary blob) block "+blockNo+": "+e, e);
					throw new FetchException(FetchException.INTERNAL_ERROR, "Failed to encode for binary blob: "+e);
				} catch (IOException e) {
					throw new FetchException(FetchException.BUCKET_ERROR, "Failed to encode for binary blob: "+e);
				}
			}
		}
		return true; // Assume it is encoded correctly.
	}

	/**
	 * Queue the data for a healing insert. If the data is persistent, we copy it; the caller must free the 
	 * original data when it is finished with it, the healing queue will free the copied data. If the data is 
	 * not persistent, we create a MultiReaderBucket wrapper, so that the data will be freed when both the caller
	 * and the healing queue are finished with it; the caller must accept the returned bucket, and free it when it
	 * is finished with it. 
	 */
	private Bucket queueHeal(Bucket data, ObjectContainer container, ClientContext context) {
		Bucket copy;
		if(persistent) {
			try {
				copy = context.tempBucketFactory.makeBucket(data.size());
				BucketTools.copy(data, copy);
			} catch (IOException e) {
				Logger.normal(this, "Failed to copy data for healing: "+e, e);
				return data;
			}
		} else {
			MultiReaderBucket wrapper = new MultiReaderBucket(data);
			copy = wrapper.getReaderBucket();
			data = wrapper.getReaderBucket();
		}
		if(logMINOR) Logger.minor(this, "Queueing healing insert for "+data+" on "+this);
		context.healingQueue.queue(copy, forceCryptoKey, cryptoAlgorithm, context);
		return data;
	}
	
	/** This is after any retries and therefore is either out-of-retries or fatal 
	 * @param container */
	public void onFatalFailure(FetchException e, int blockNo, ObjectContainer container, ClientContext context) {
		if(persistent)
			container.activate(this, 1);
		if(logMINOR) Logger.minor(this, "Permanently failed block: "+blockNo+" on "+this+" : "+e, e);
		boolean allFailed;
		// Since we can't keep the key, we need to unregister for it at this point to avoid a memory leak
		synchronized(this) {
			if(isFinishing(container)) return; // this failure is now irrelevant, and cleanup will occur on the decoder thread
			if(haveFoundKey(blockNo, container)) {
				Logger.error(this, "Block already finished: "+blockNo);
				return;
			}
			setFoundKey(blockNo, container, context);
			// :(
			boolean deactivateParent = false; // can get called from wierd places, don't deactivate parent if not necessary
			if(persistent) {
				deactivateParent = !container.ext().isActive(parent);
				if(deactivateParent) container.activate(parent, 1);
			}
			if(e.isFatal()) {
				fatallyFailedBlocks++;
				parent.fatallyFailedBlock(container, context);
			} else {
				failedBlocks++;
				parent.failedBlock(container, context);
			}
			if(deactivateParent)
				container.deactivate(parent, 1);
			// Once it is no longer possible to have a successful fetch, fail...
			allFailed = failedBlocks + fatallyFailedBlocks > (dataBuckets.length + checkBuckets.length - minFetched);
		}
		if(persistent)
			container.store(this);
		if(allFailed) {
			if(persistent) container.activate(errors, Integer.MAX_VALUE);
			fail(new FetchException(FetchException.SPLITFILE_ERROR, errors), container, context, false);
		}
	}
	/** A request has failed non-fatally, so the block may be retried.
	 * Caller must update errors.
	 * @param container */
	public void onNonFatalFailure(FetchException e, int blockNo, ObjectContainer container, ClientContext context) {
		onNonFatalFailure(e, blockNo, container, context, true);
	}
	
	private void onNonFatalFailure(FetchException e, int blockNo, ObjectContainer container, ClientContext context, boolean callStore) {
		if(persistent) {
			container.activate(blockFetchContext, 1);
		}
		int maxTries = blockFetchContext.maxNonSplitfileRetries;
		RequestScheduler sched = context.getFetchScheduler(false, realTimeFlag);
		if(onNonFatalFailure(e, blockNo, container, context, sched, maxTries, callStore)) {
			// At least one request was rescheduled, so we have requests to send.
			// Clear our cooldown cache entry and those of our parents.
			makeGetter(container, context);
			if(getter != null) {
				rescheduleGetter(container, context);
			}
		}
	}
	
	SplitFileFetcherSegmentGet rescheduleGetter(ObjectContainer container, ClientContext context) {
		SplitFileFetcherSegmentGet getter = makeGetter(container, context);
		if(getter == null) return null;
		boolean getterActive = true;
		if(persistent) {
			getterActive = container.ext().isActive(getter);
			if(!getterActive) container.activate(getter, 1);
		}
		getter.reschedule(container, context);
		getter.clearCooldown(container, context, true);
		if(!getterActive) container.deactivate(getter, 1);
		return getter;
	}

	public void onNonFatalFailure(FetchException[] failures, int[] blockNos, ObjectContainer container, ClientContext context) {
		if(persistent) {
			container.activate(blockFetchContext, 1);
		}
		int maxTries = blockFetchContext.maxNonSplitfileRetries;
		RequestScheduler sched = context.getFetchScheduler(false, realTimeFlag);
		boolean reschedule = false;
		for(int i=0;i<failures.length;i++) {
			if(onNonFatalFailure(failures[i], blockNos[i], container, context, sched, maxTries, false))
				reschedule = true;
		}
		if(persistent) container.store(this); // We don't call container.store(this) in each onNonFatalFailure because it takes much CPU time.
		if(reschedule) {
			// At least one request was rescheduled, so we have requests to send.
			// Clear our cooldown cache entry and those of our parents.
			makeGetter(container, context);
			if(getter != null) {
				rescheduleGetter(container, context);
			}
		}
	}

	static class MyCooldownTrackerItem implements CooldownTrackerItem {
		MyCooldownTrackerItem(int data, int check) {
			dataRetries = new int[data];
			checkRetries = new int[check];
			dataCooldownTimes = new long[data];
			checkCooldownTimes = new long[check];
		}
		final int[] dataRetries;
		final int[] checkRetries;
		final long[] dataCooldownTimes;
		final long[] checkCooldownTimes;
	}
	
	/**
	 * Caller must set(this) iff returns true.
	 * @return True if the getter should be rescheduled.
	 */
	private boolean onNonFatalFailure(FetchException e, int blockNo, ObjectContainer container, ClientContext context, RequestScheduler sched, int maxTries, boolean callStore) {
		if(logMINOR) Logger.minor(this, "Calling onNonFatalFailure for block "+blockNo+" on "+this);
		int tries;
		boolean failed = false;
		boolean cooldown = false;
		ClientCHK key;
		int[] dataRetries = this.dataRetries;
		int[] checkRetries = this.checkRetries;
		MyCooldownTrackerItem tracker = makeCooldownTrackerItem(container, context);
		long[] dataCooldownTimes = tracker.dataCooldownTimes;
		long[] checkCooldownTimes = tracker.checkCooldownTimes;
		if(maxTries == -1) {
			// Cooldown and retry counts entirely kept in RAM.
			dataRetries = tracker.dataRetries;
			checkRetries = tracker.checkRetries;
			callStore = false;
		}
		checkCachedCooldownData(container);
		synchronized(this) {
			if(isFinished(container)) return false;
			if(blockNo < dataBuckets.length) {
				key = this.getBlockKey(blockNo, container);
				tries = ++dataRetries[blockNo];
				if(tries > maxTries && maxTries >= 0) failed = true;
				else {
					if(cachedCooldownTries == 0 ||
							tries % cachedCooldownTries == 0) {
						long now = System.currentTimeMillis();
						if(dataCooldownTimes[blockNo] > now)
							Logger.error(this, "Already on the cooldown queue! for "+this+" data block no "+blockNo, new Exception("error"));
						else {
							SplitFileFetcherSegmentGet getter = makeGetter(container, context);
							if(getter != null) {
								dataCooldownTimes[blockNo] = now + cachedCooldownTime;
								if(logMINOR) Logger.minor(this, "Putting data block "+blockNo+" into cooldown until "+(dataCooldownTimes[blockNo]-now));
							}
						}
						cooldown = true;
					}
				}
			} else {
				int checkNo = blockNo - dataBuckets.length;
				key = this.getBlockKey(blockNo, container);
				if(persistent)
					container.activate(key, 5);
				tries = ++checkRetries[checkNo];
				if(tries > maxTries && maxTries >= 0) failed = true;
				else {
					if(cachedCooldownTries == 0 ||
							tries % cachedCooldownTries == 0) {
						long now = System.currentTimeMillis();
						if(checkCooldownTimes[checkNo] > now)
							Logger.error(this, "Already on the cooldown queue! for "+this+" check block no "+blockNo, new Exception("error"));
						else {
							SplitFileFetcherSegmentGet getter = makeGetter(container, context);
							if(getter != null) {
								checkCooldownTimes[checkNo] = now + cachedCooldownTime;
								if(logMINOR) Logger.minor(this, "Putting check block "+blockNo+" into cooldown until "+(checkCooldownTimes[checkNo]-now));
							}
						}
						cooldown = true;
					}
				}
			}
		}
		if(failed) {
			onFatalFailure(e, blockNo, container, context);
			if(logMINOR)
				Logger.minor(this, "Not retrying block "+blockNo+" on "+this+" : tries="+tries+"/"+maxTries);
			return false;
		}
		boolean mustSchedule = false;
		if(cooldown) {
			// Registered to cooldown queue
			if(logMINOR)
				Logger.minor(this, "Added to cooldown queue: "+key+" for "+this);
		} else {
			// If we are here we are going to retry
			mustSchedule = true;
			if(logMINOR)
				Logger.minor(this, "Retrying block "+blockNo+" on "+this+" : tries="+tries+"/"+maxTries);
		}
		if(persistent) {
			if(callStore) container.store(this);
			container.deactivate(key, 5);
		}
		return mustSchedule;
	}
	
	private void checkCachedCooldownData(ObjectContainer container) {
		// 0/0 is illegal, and it's also the default, so use it to indicate we haven't fetched them.
		if(!(cachedCooldownTime == 0 && cachedCooldownTries == 0)) {
			// Okay, we have already got them.
			return;
		}
		innerCheckCachedCooldownData(container);
	}
	
	private void innerCheckCachedCooldownData(ObjectContainer container) {
		boolean active = true;
		if(persistent) {
			active = container.ext().isActive(blockFetchContext);
			container.activate(blockFetchContext, 1);
		}
		cachedCooldownTries = blockFetchContext.getCooldownRetries();
		cachedCooldownTime = blockFetchContext.getCooldownTime();
		if(!active) container.deactivate(blockFetchContext, 1);
	}

	private MyCooldownTrackerItem makeCooldownTrackerItem(
			ObjectContainer container, ClientContext context) {
		return (MyCooldownTrackerItem) context.cooldownTracker.make(this, persistent, container);
	}

	/** Called when localRequestOnly is set and we check the datastore and find nothing.
	 * We should fail() unless we are already decoding. */
	void failCheckingDatastore(ObjectContainer container, ClientContext context) {
		fail(null, container, context, false, true);
	}
	
	void fail(FetchException e, ObjectContainer container, ClientContext context, boolean dontDeactivateParent) {
		fail(e, container, context, dontDeactivateParent, false);
	}
	
	private void fail(FetchException e, ObjectContainer container, ClientContext context, boolean dontDeactivateParent, boolean checkingStoreOnly) {
		if(logMINOR) Logger.minor(this, "Failing segment "+this, e);
		boolean alreadyDecoding = false;
		synchronized(this) {
			if(finished) return;
			if(startedDecode && checkingStoreOnly) return;
			if(checkingStoreOnly) e = new FetchException(FetchException.DATA_NOT_FOUND);
			finished = true;
			alreadyDecoding = startedDecode;
			this.failureException = e;
			// Failure in decode is possible.
			for(int i=0;i<checkBuckets.length;i++) {
				MinimalSplitfileBlock b = checkBuckets[i];
				if(persistent)
					container.activate(b, 2);
				if(b != null) {
					Bucket d = b.getData();
					if(d != null) d.free();
				}
				if(persistent)
					b.removeFrom(container);
				checkBuckets[i] = null;
			}
		}
		if(getter != null) {
			if(persistent) container.activate(getter, 1);
			getter.unregister(container, context, getPriorityClass(container));
			if(persistent) getter.removeFrom(container);
			getter = null;
			if(persistent) container.store(this);
		}
		encoderFinished(container, context);
		removeSubSegments(container, context, false);
		if(persistent) {
			container.store(this);
			container.activate(parentFetcher, 1);
		}
		if(!alreadyDecoding)
			parentFetcher.removeMyPendingKeys(this, container, context);
		parentFetcher.segmentFinished(this, container, context);
		if(persistent && !dontDeactivateParent)
			container.deactivate(parentFetcher, 1);
	}

	public SendableGet schedule(ObjectContainer container, ClientContext context) {
		if(persistent) {
			container.activate(this, 1);
		}
		SplitFileFetcherSegmentGet get = makeGetter(container, context);
		synchronized(this) {
			scheduled = true;
		}
		if(persistent)
			container.store(this);
		if(persistent) container.activate(get, 1);
		return get;
	}

	public void cancel(ObjectContainer container, ClientContext context) {
		fail(new FetchException(FetchException.CANCELLED), container, context, true);
	}

	public void onBlockSetFinished(ClientGetState state) {
		// Ignore; irrelevant
	}

	public void onTransition(ClientGetState oldState, ClientGetState newState) {
		// Ignore
	}

	public synchronized ClientCHK getBlockKey(int blockNum, ObjectContainer container) {
		if(keys == null) migrateToKeys(container);
		else {
			if(persistent) container.activate(keys, 1);
		}
		return keys.getKey(blockNum, foundKeys, persistent);
	}
	
	public NodeCHK getBlockNodeKey(int blockNum, ObjectContainer container) {
		ClientCHK key = getBlockKey(blockNum, container);
		if(key != null) return key.getNodeCHK();
		else return null;
	}

	private void removeSubSegments(ObjectContainer container, ClientContext context, boolean finishing) {
		if(subSegments == null) return;
		if(persistent)
			container.activate(subSegments, 1);
		SplitFileFetcherSubSegment[] deadSegs;
		synchronized(this) {
			deadSegs = subSegments.toArray(new SplitFileFetcherSubSegment[subSegments.size()]);
			subSegments.clear();
		}
		if(persistent && deadSegs.length > 0)
			container.store(this);
		for(int i=0;i<deadSegs.length;i++) {
			if(deadSegs[i] == null) {
				Logger.error(this, "Subsegment "+i+" of "+deadSegs.length+" on "+this+" is null!");
				continue;
			}
			if(persistent)
				container.activate(deadSegs[i], 1);
			deadSegs[i].kill(container, context, true, false);
			context.getChkFetchScheduler(realTimeFlag).removeFromStarterQueue(deadSegs[i], container, true);
			if(persistent)
				container.deactivate(deadSegs[i], 1);
		}
		if(persistent && !finishing) {
			container.store(this);
			container.store(subSegments);
		}
	}

	public synchronized long getCooldownWakeup(int blockNum, int maxTries, ObjectContainer container, ClientContext context) {
		MyCooldownTrackerItem tracker = makeCooldownTrackerItem(container, context);
		long[] dataCooldownTimes = tracker.dataCooldownTimes;
		long[] checkCooldownTimes = tracker.checkCooldownTimes;
		if(blockNum < dataBuckets.length)
			return dataCooldownTimes[blockNum];
		else
			return checkCooldownTimes[blockNum - dataBuckets.length];
	}
	
	synchronized void setMaxCooldownWakeup(long until, int blockNum, int maxTries, ObjectContainer container, ClientContext context) {
		MyCooldownTrackerItem tracker = makeCooldownTrackerItem(container, context);
		long[] dataCooldownTimes = tracker.dataCooldownTimes;
		long[] checkCooldownTimes = tracker.checkCooldownTimes;
		if(blockNum < dataBuckets.length)
			dataCooldownTimes[blockNum] = Math.max(dataCooldownTimes[blockNum], until);
		else
			checkCooldownTimes[blockNum - dataBuckets.length] = Math.max(checkCooldownTimes[blockNum - dataBuckets.length], until);
	}
	
	private int getRetries(int blockNum, ObjectContainer container, ClientContext context) {
		return getRetries(blockNum, getMaxRetries(container), container, context);
	}
	
	private int getRetries(int blockNum, int maxTries, ObjectContainer container, ClientContext context) {
		int[] dataRetries = this.dataRetries;
		int[] checkRetries = this.checkRetries;
		if(maxTries == -1) {
			// Cooldown and retry counts entirely kept in RAM.
			MyCooldownTrackerItem tracker = makeCooldownTrackerItem(container, context);
			dataRetries = tracker.dataRetries;
			checkRetries = tracker.checkRetries;
		}
		if(blockNum < dataBuckets.length)
			return dataRetries[blockNum];
		else
			return checkRetries[blockNum - dataBuckets.length];
	}

	/**
	 * @return True if the key was wanted, false otherwise. 
	 */
	public boolean requeueAfterCooldown(Key key, long time, ObjectContainer container, ClientContext context, SplitFileFetcherSubSegment dontDeactivate) {
		if(persistent)
			container.activate(this, 1);
		boolean notFound = true;
		synchronized(this) {
		if(isFinishing(container)) return false;
		// FIXME need a more efficient way to get maxTries!
		if(persistent) {
			container.activate(blockFetchContext, 1);
		}
		int maxTries = blockFetchContext.maxNonSplitfileRetries;
		if(keys == null) 
			migrateToKeys(container);
		else {
			if(persistent) container.activate(keys, 1);
		}
		int[] matches = keys.getBlockNumbers((NodeCHK)key, foundKeys);
		for(int i : matches) {
			ClientCHK k = keys.getKey(i, foundKeys, persistent);
			if(k.getNodeKey(false).equals(key)) {
				if(getCooldownWakeup(i, maxTries, container, context) > time) {
					if(logMINOR)
						Logger.minor(this, "Not retrying after cooldown for data block "+i+" as deadline has not passed yet on "+this+" remaining time: "+(getCooldownWakeup(i, maxTries, container, context)-time)+"ms");
					return false;
				}
				if(foundKeys[i]) continue;
				if(logMINOR)
					Logger.minor(this, "Retrying after cooldown on "+this+": block "+i+" on "+this+" : tries="+getRetries(i, container, context)+"/"+maxTries);
				
				notFound = false;
			} else {
				if(persistent)
					container.deactivate(k, 5);
			}
		}
		}
		if(notFound) {
			Logger.error(this, "requeueAfterCooldown: Key not found!: "+key+" on "+this);
		} else {
			rescheduleGetter(container, context);
		}
		return true;
	}

	public synchronized long getCooldownWakeupByKey(Key key, ObjectContainer container, ClientContext context) {
		if(keys == null) 
			migrateToKeys(container);
		else {
			if(persistent) container.activate(keys, 1);
		}
		int blockNo = keys.getBlockNumber((NodeCHK)key, foundKeys);
		if(blockNo == -1) return -1;
		return getCooldownWakeup(blockNo, getMaxRetries(container), container, context);
	}

	public synchronized int getBlockNumber(Key key, ObjectContainer container) {
		if(keys == null) 
			migrateToKeys(container);
		else {
			if(persistent) container.activate(keys, 1);
		}
		return keys.getBlockNumber((NodeCHK)key, foundKeys);
	}

	public synchronized Integer[] getKeyNumbersAtRetryLevel(int retryCount, ObjectContainer container, ClientContext context) {
		Vector<Integer> v = new Vector<Integer>();
		if(keys == null) 
			migrateToKeys(container);
		else {
			if(persistent) container.activate(keys, 1);
		}
		int maxTries = getMaxRetries(container);
		int[] dataRetries = this.dataRetries;
		int[] checkRetries = this.checkRetries;
		if(maxTries == -1) {
			// Cooldown and retry counts entirely kept in RAM.
			MyCooldownTrackerItem tracker = makeCooldownTrackerItem(container, context);
			dataRetries = tracker.dataRetries;
			checkRetries = tracker.checkRetries;
		}
		for(int i=0;i<dataRetries.length;i++) {
			if(foundKeys[i]) continue;
			if(dataRetries[i] == retryCount)
				v.add(Integer.valueOf(i));
		}
		for(int i=0;i<checkRetries.length;i++) {
			if(foundKeys[i+dataBuckets.length]) continue;
			if(checkRetries[i] == retryCount)
				v.add(Integer.valueOf(i+dataBuckets.length));
		}
		return v.toArray(new Integer[v.size()]);
	}

	public void onFailed(Throwable t, ObjectContainer container, ClientContext context) {
		synchronized(this) {
			if(finished) {
				Logger.error(this, "FEC decode or encode failed but already finished: "+t, t);
				return;
			}
			finished = true;
		}
		if(persistent)
			container.store(this);
		this.fail(new FetchException(FetchException.INTERNAL_ERROR, "FEC failure: "+t, t), container, context, false);
	}

	public synchronized boolean haveBlock(int blockNo, ObjectContainer container) {
		if(blockNo < dataBuckets.length) {
			boolean wasActive = false;
			if(dataBuckets[blockNo] == null) return false;
			if(persistent) {
				wasActive = container.ext().isActive(dataBuckets[blockNo]);
				if(!wasActive)
					container.activate(dataBuckets[blockNo], 1);
			}
			boolean retval = dataBuckets[blockNo].hasData();
			if(persistent && !wasActive)
				container.deactivate(dataBuckets[blockNo], 1);
			return retval;
		} else {
			boolean wasActive = false;
			blockNo -= dataBuckets.length;
			if(checkBuckets[blockNo] == null) return false;
			if(persistent) {
				wasActive = container.ext().isActive(checkBuckets[blockNo]);
				if(!wasActive)
					container.activate(checkBuckets[blockNo], 1);
			}
			boolean retval = checkBuckets[blockNo].hasData();
			if(persistent && !wasActive)
				container.deactivate(checkBuckets[blockNo], 1);
			return retval;
		}
	}

	public short getPriorityClass(ObjectContainer container) {
		if(persistent)
			container.activate(parent, 1);
		return parent.priorityClass;
	}

	public boolean isCancelled(ObjectContainer container) {
		return isFinishing(container);
	}

	public Key[] listKeys(ObjectContainer container) {
		if(keys == null) 
			migrateToKeys(container);
		else {
			if(persistent) container.activate(keys, 1);
		}
		return keys.listNodeKeys(foundKeys, persistent);
	}

	/**
	 * @return True if we fetched a block.
	 * Hold the lock for the whole duration of this method. If a transient request
	 * has two copies of onGotKey() run in parallel, we want only one of them to
	 * return true, otherwise SFFKL will remove the keys from the main bloom
	 * filter twice, resulting in collateral damage to other overlapping keys,
	 * and then "NOT IN BLOOM FILTER" errors, or worse, false negatives.
	 */
	public boolean onGotKey(Key key, KeyBlock block, ObjectContainer container, ClientContext context) {
		ClientCHKBlock cb = null;
		int blockNum;
		Bucket data = null;
		short onSuccessResult = (short) -1;
		FetchException fatal = null;
		synchronized(this) {
			blockNum = this.getBlockNumber(key, container);
			if(blockNum < 0) {
				if(logMINOR) Logger.minor(this, "Rejecting block because not found");
				return false;
			}
			if(finished || startedDecode || fetcherFinished) {
				if(logMINOR) Logger.minor(this, "Rejecting block because "+(finished?"finished ":"")+(startedDecode?"started decode ":"")+(fetcherFinished?"fetcher finished ":""));
				return false; // The block was present but we didn't want it.
			}
			if(logMINOR)
				Logger.minor(this, "Found key for block "+blockNum+" on "+this+" in onGotKey() for "+key);
			ClientCHK ckey = this.getBlockKey(blockNum, container);
			try {
				cb = new ClientCHKBlock((CHKBlock)block, ckey);
			} catch (CHKVerifyException e) {
				fatal = new FetchException(FetchException.BLOCK_DECODE_ERROR, e);
			}
			if(cb != null) {
				data = extract(cb, blockNum, container, context);
				if(data == null) {
					if(logMINOR)
						Logger.minor(this, "Extract failed");
					return false;
				} else {
					// This can be done safely inside the lock.
					if(parent instanceof ClientGetter)
						((ClientGetter)parent).addKeyToBinaryBlob(cb, container, context);
					if(!cb.isMetadata()) {
						// We MUST remove the keys before we exit the synchronized block,
						// thus ensuring that the next call will return FALSE, and the keys
						// will only be removed from the Bloom filter ONCE!
						onSuccessResult = onSuccessInner(data, blockNum, container, context);
					}
				}
			}
		}
		if(fatal != null) {
			if(persistent)
				container.activate(errors, 1);
			errors.inc(fatal.mode);
			if(persistent)
				errors.storeTo(container);
			this.onFatalFailure(fatal, blockNum, container, context);
			return false;
		} else if(data == null) {
			return false; // Extract failed
		} else { // cb != null
			if(!cb.isMetadata()) {
				if(onSuccessResult != (short) -1)
					finishOnSuccess(onSuccessResult, container, context);
				return true;
			} else {
				onFatalFailure(new FetchException(FetchException.INVALID_METADATA, "Metadata where expected data"), blockNum, container, context);
				return true;
			}
		}
	}
	
	private synchronized MinimalSplitfileBlock getBlock(int blockNum) {
		if(blockNum < dataBuckets.length) {
			return dataBuckets[blockNum];
		}
		blockNum -= dataBuckets.length;
		return checkBuckets[blockNum];
	}
	
	public Bucket getBlockBucket(int blockNum, ObjectContainer container) {
		MinimalSplitfileBlock block = getBlock(blockNum);
		boolean active = true;
		if(block != null && persistent) {
			active = container.ext().isActive(block);
			if(!active) container.activate(block, 1);
		}
		if(block == null) {
			Logger.error(this, "Block is null: "+blockNum+" on "+this+" activated = "+container.ext().isActive(this)+" finished = "+finished+" encoder finished = "+encoderFinished+" fetcher finished = "+fetcherFinished);
			return null;
		}
		Bucket ret = block.getData();
		if(ret == null && logMINOR) Logger.minor(this, "Bucket is null: "+blockNum+" on "+this+" for "+block);
		if(!active)
			container.deactivate(block, 1);
		return ret;
	}
	
	public boolean hasBlockWrapper(int i) {
		return getBlock(i) != null;
	}

	/** Convert a ClientKeyBlock to a Bucket. If an error occurs, report it via onFailure
	 * and return null.
	 */
	protected Bucket extract(ClientKeyBlock block, int blockNum, ObjectContainer container, ClientContext context) {
		Bucket data;
		try {
			data = block.decode(context.getBucketFactory(persistent), (int)(Math.min(this.blockFetchContext.maxOutputLength, Integer.MAX_VALUE)), false);
		} catch (KeyDecodeException e1) {
			if(logMINOR)
				Logger.minor(this, "Decode failure: "+e1, e1);
			// All other callers to onFatalFailure increment the error counter in SplitFileFetcherSubSegment.
			// Therefore we must do so here.
			if(persistent)
				container.activate(errors, 1);
			errors.inc(FetchException.BLOCK_DECODE_ERROR);
			if(persistent)
				errors.storeTo(container);
			this.onFatalFailure(new FetchException(FetchException.BLOCK_DECODE_ERROR, e1.getMessage()), blockNum, container, context);
			return null;
		} catch (TooBigException e) {
			if(persistent)
				container.activate(errors, 1);
			errors.inc(FetchException.TOO_BIG);
			if(persistent)
				errors.storeTo(container);
			this.onFatalFailure(new FetchException(FetchException.TOO_BIG, e.getMessage()), blockNum, container, context);
			return null;
		} catch (IOException e) {
			Logger.error(this, "Could not capture data - disk full?: "+e, e);
			if(persistent)
				container.activate(errors, 1);
			errors.inc(FetchException.BUCKET_ERROR);
			if(persistent)
				errors.storeTo(container);
			this.onFatalFailure(new FetchException(FetchException.BUCKET_ERROR, e), blockNum, container, context);
			return null;
		}
		if(logMINOR)
			Logger.minor(this, data == null ? "Could not decode: null" : ("Decoded "+data.size()+" bytes"));
		return data;
	}


	public boolean persistent() {
		return persistent;
	}

	public void deactivateKeys(ObjectContainer container) {
		container.deactivate(keys, 1);
	}

	public void freeDecodedData(ObjectContainer container, boolean noStore) {
		synchronized(this) {
			if(!encoderFinished) return;
			if(!fetcherHalfFinished) return;
		}
		if(logMINOR) Logger.minor(this, "Freeing decoded data on segment "+this);
		if(decodedData != null) {
			if(persistent)
				container.activate(decodedData, 1);
			decodedData.free();
			if(persistent)
				decodedData.removeFrom(container);
			decodedData = null;
		}
		for(int i=0;i<dataBuckets.length;i++) {
			MinimalSplitfileBlock block = dataBuckets[i];
			if(block == null) continue;
			if(persistent) container.activate(block, 1);
			Bucket data = block.getData();
			if(data != null) {
				// We only free the data blocks at the last minute.
				if(persistent) container.activate(data, 1);
				data.free();
			}
			if(persistent) block.removeFrom(container);
			dataBuckets[i] = null;
		}
		for(int i=0;i<checkBuckets.length;i++) {
			MinimalSplitfileBlock block = checkBuckets[i];
			if(block == null) continue;
			if(persistent) container.activate(block, 1);
			Bucket data = block.getData();
			if(data != null) {
				if(persistent) container.activate(data, 1);
				data.free();
			}
			if(persistent) block.removeFrom(container);
			checkBuckets[i] = null;
		}
		if(persistent && !noStore)
			container.store(this);
	}

	public void removeFrom(ObjectContainer container, ClientContext context) {
		if(!finished) {
			Logger.error(this, "Removing "+this+" but not finished, fetcher finished "+fetcherFinished+" fetcher half finished "+fetcherHalfFinished+" encoder finished "+encoderFinished);
		}
		if(logMINOR) Logger.minor(this, "removing "+this);
		context.cooldownTracker.remove(this, true, container);
		freeDecodedData(container, true);
		removeSubSegments(container, context, true);
		if(subSegments != null) container.delete(subSegments);
		if(dataKeys != null) {
			for(int i=0;i<dataKeys.length;i++) {
				if(dataKeys[i] != null) dataKeys[i].removeFrom(container);
				dataKeys[i] = null;
			}
		}
		if(checkKeys != null) {
			for(int i=0;i<checkKeys.length;i++) {
				if(checkKeys[i] != null) checkKeys[i].removeFrom(container);
				checkKeys[i] = null;
			}
		}
		container.activate(errors, 1);
		errors.removeFrom(container);
		if(failureException != null) {
			container.activate(failureException, 5);
			failureException.removeFrom(container);
		}
		if(keys != null) {
			container.activate(keys, 1);
			keys.removeFrom(container);
		}
		if(getter != null) {
			container.activate(getter, 1);
			Logger.error(this, "Getter still exists: "+getter+" for "+this);
			// Unable to unregister because parent does not exist so we don't know the priority.
			getter.removeFrom(container);
		}
		container.delete(this);
	}

	/** Free the data blocks but only if the encoder has finished with them. */
	public void fetcherHalfFinished(ObjectContainer container) {
		boolean finish = false;
		synchronized(this) {
			if(fetcherHalfFinished) return;
			fetcherHalfFinished = true;
			finish = encoderFinished;
		}
		if(finish) {
			if(crossCheckBlocks == 0) 
				freeDecodedData(container, false);
			// Else wait for the whole splitfile to complete in fetcherFinished(), and then free decoded data in removeFrom().
		} else {
			if(logMINOR) Logger.minor(this, "Fetcher half-finished but fetcher not finished on "+this);
		}
		if(persistent) container.store(this);
		
	}
	
	public void fetcherFinished(ObjectContainer container, ClientContext context) {
		context.cooldownTracker.remove(this, persistent, container);
		synchronized(this) {
			if(fetcherFinished) return;
			fetcherFinished = true;
			fetcherHalfFinished = true;
			if(!encoderFinished) {
				if(!startedDecode) {
					if(logMINOR) Logger.minor(this, "Never started decode, completing immediately on "+this);
					encoderFinished = true;
					if(persistent) container.store(this);
				} else {
					if(persistent) container.store(this);
					if(logMINOR) Logger.minor(this, "Fetcher finished but encoder not finished on "+this);
					return;
				}
			}
		}
		if(persistent) removeFrom(container, context);
		else freeDecodedData(container, true);
	}
	
	private void encoderFinished(ObjectContainer container, ClientContext context) {
		context.cooldownTracker.remove(this, persistent, container);
		boolean finish = false;
		boolean half = false;
		synchronized(this) {
			encoderFinished = true;
			finish = fetcherFinished;
			half = fetcherHalfFinished;
		}
		if(finish) {
			if(persistent) removeFrom(container, context);
		} else if(half) {
			if(crossCheckBlocks == 0)
				freeDecodedData(container, false);
			// Else wait for the whole splitfile to complete in fetcherFinished(), and then free decoded data in removeFrom().
			if(persistent) container.store(this);
			if(logMINOR) Logger.minor(this, "Encoder finished but fetcher not finished on "+this);
		} else {
			if(persistent) container.store(this);
		}
	}
	
	public int allocateCrossDataBlock(SplitFileFetcherCrossSegment seg, Random random) {
		int size = realDataBlocks();
		if(crossDataBlocksAllocated == size) return -1;
		int x = 0;
		for(int i=0;i<10;i++) {
			x = random.nextInt(size);
			if(crossSegmentsByBlock[x] == null) {
				crossSegmentsByBlock[x] = seg;
				crossDataBlocksAllocated++;
				return x;
			}
		}
		for(int i=0;i<size;i++) {
			x++;
			if(x == size) x = 0;
			if(crossSegmentsByBlock[x] == null) {
				crossSegmentsByBlock[x] = seg;
				crossDataBlocksAllocated++;
				return x;
			}
		}
		throw new IllegalStateException("Unable to allocate cross data block even though have not used all slots up???");
	}

	public int allocateCrossCheckBlock(SplitFileFetcherCrossSegment seg, Random random) {
		if(crossCheckBlocksAllocated == crossCheckBlocks) return -1;
		int x = dataBuckets.length - (1 + random.nextInt(crossCheckBlocks));
		for(int i=0;i<crossCheckBlocks;i++) {
			x++;
			if(x == dataBuckets.length) x = dataBuckets.length - crossCheckBlocks;
			if(crossSegmentsByBlock[x] == null) {
				crossSegmentsByBlock[x] = seg;
				crossCheckBlocksAllocated++;
				return x;
			}
		}
		throw new IllegalStateException("Unable to allocate cross check block even though have not used all slots up???");
	}
	
	public final int realDataBlocks() {
		return dataBuckets.length - crossCheckBlocks;
	}

	/**
	 * 
	 * @param fetching
	 * @param onlyLowestTries If true, only return blocks of the lowest currently valid 
	 * retry count. Otherwise return all blocks that we haven't either found or given up on.
	 * @param container
	 * @param context
	 * @return
	 */
	public ArrayList<Integer> validBlockNumbers(KeysFetchingLocally fetching, boolean onlyLowestTries,
			ObjectContainer container, ClientContext context) {
		long now = System.currentTimeMillis();
		if(keys == null) 
			migrateToKeys(container);
		else {
			if(persistent) container.activate(keys, 1);
		}
		int maxTries = getMaxRetries(container);
		synchronized(this) {
			int minRetries = Integer.MAX_VALUE;
			ArrayList<Integer> list = null;
			if(startedDecode || isFinishing(container)) return null;
			for(int i=0;i<dataBuckets.length+checkBuckets.length;i++) {
				if(foundKeys[i]) continue;
				if(getCooldownWakeup(i, maxTries, container, context) > now) continue;
				// Double check
				if(getBlockBucket(i, container) != null) continue;
				// Possible ...
				Key key = keys.getNodeKey(i, null, true);
				if(fetching.hasKey(key, getter, persistent, container)) continue;
				// Do not check RecentlyFailed here.
				// 1. We're synchronized, so it would be a bad idea here.
				// 2. It's way too heavyweight given we're not going to send most of the items added.
				// Check it in the caller.
				if(onlyLowestTries) {
					int retryCount = this.getRetries(i, container, context);
					if(retryCount > minRetries) {
						// Ignore
						continue;
					}
					if(retryCount < minRetries && list != null)
						list.clear();
				}
				if(list == null) list = new ArrayList<Integer>();
				list.add(i);
			}
			return list;
		}
	}
	
	public boolean checkRecentlyFailed(int blockNum, ObjectContainer container, ClientContext context, KeysFetchingLocally keys, long now) {
		if(keys == null) 
			migrateToKeys(container);
		else {
			if(persistent) container.activate(keys, 1);
		}
		Key key = this.keys.getNodeKey(blockNum, null, true);
		long timeout = keys.checkRecentlyFailed(key, realTimeFlag);
		if(timeout <= now) return false;
		int maxRetries = getMaxRetries(container);
		if(maxRetries == -1 || (maxRetries >= RequestScheduler.COOLDOWN_RETRIES)) {
			// Concurrency is fine here, it won't go away before the given time.
			setMaxCooldownWakeup(timeout, blockNum, this.getMaxRetries(container), container, context);
		} else {
			FetchException e = new FetchException(FetchException.RECENTLY_FAILED);
			incErrors(e, container);
			onNonFatalFailure(e, blockNum, container, context);
		}
		return true;
	}

	private void incErrors(FetchException e, ObjectContainer container) {
		if(persistent)
			container.activate(errors, 1);
    	errors.inc(e.getMode());
		if(persistent)
			errors.storeTo(container);
	}

	/** Separate method because we will need to create the Key anyway for checking against
	 * the KeysFetchingLocally, and we can reuse that in the created Block. Yes we don't 
	 * pass that in at the moment but we will in future. 
	 * future.
	 * @param request
	 * @param sched
	 * @param getter 
	 * @param keysFetching 
	 * @param container
	 * @param context
	 * @return
	 */
	public List<PersistentChosenBlock> makeBlocks(
			PersistentChosenRequest request, RequestScheduler sched,
			KeysFetchingLocally fetching, SplitFileFetcherSegmentGet getter, ObjectContainer container, ClientContext context) {
		long now = System.currentTimeMillis();
		ArrayList<PersistentChosenBlock> list = null;
		if(keys == null) 
			migrateToKeys(container);
		else {
			if(persistent) container.activate(keys, 1);
		}
		int maxTries = getMaxRetries(container);
		synchronized(this) {
			if(startedDecode || isFinishing(container)) return null;
			for(int i=0;i<dataBuckets.length+checkBuckets.length;i++) {
				if(foundKeys[i]) continue;
				if(getCooldownWakeup(i, maxTries, container, context) > now) continue;
				// Double check
				if(getBlockBucket(i, container) != null) continue;
				// Possible ...
				Key key = keys.getNodeKey(i, null, true);
				if(fetching.hasKey(key, getter, persistent, container)) continue;
				if(list == null) list = new ArrayList<PersistentChosenBlock>();
				ClientCHK ckey = keys.getKey(i, null, true); // FIXME Duplicates the routingKey field
				list.add(new PersistentChosenBlock(false, request, new SplitFileFetcherSegmentSendableRequestItem(i), key, ckey, sched));
			}
		}
		if(list == null) return null;
		for(Iterator<PersistentChosenBlock> i = list.iterator();i.hasNext();) {
			PersistentChosenBlock block = i.next();
			// We must do the actual check outside the lock.
			long l = fetching.checkRecentlyFailed(block.key, realTimeFlag);
			if(l < now) continue; // Okay
			i.remove();
			if(maxTries == -1 || (maxTries >= RequestScheduler.COOLDOWN_RETRIES)) {
				// Concurrency is fine here, it won't go away before the given time.
				setMaxCooldownWakeup(l, ((SplitFileFetcherSegmentSendableRequestItem)block.token).blockNum, maxTries, container, context);
			} else {
				FetchException e = new FetchException(FetchException.RECENTLY_FAILED);
				incErrors(e, container);
				onNonFatalFailure(e, ((SplitFileFetcherSegmentSendableRequestItem)(block.token)).blockNum, container, context);
			}
		}
		return list;
	}
	
	public long getCooldownTime(ObjectContainer container, ClientContext context, HasCooldownCacheItem parentRGA, long now) {
		if(keys == null) 
			migrateToKeys(container);
		else {
			if(persistent) container.activate(keys, 1);
		}
		int maxTries = getMaxRetries(container);
		KeysFetchingLocally fetching = context.getChkFetchScheduler(realTimeFlag).fetchingKeys();
		long cooldownWakeup = Long.MAX_VALUE;
		synchronized(this) {
			if(startedDecode || isFinishing(container)) return -1; // Remove
			for(int i=0;i<dataBuckets.length+checkBuckets.length;i++) {
				if(foundKeys[i]) continue;
				// Double check
				if(getBlockBucket(i, container) != null) {
					continue;
				}
				// Possible ...
				long wakeup = getCooldownWakeup(i, maxTries, container, context);
				if(wakeup > now) {
					if(wakeup < cooldownWakeup) cooldownWakeup = wakeup;
					continue;
				}
				Key key = keys.getNodeKey(i, null, true);
				if(fetching.hasKey(key, getter, persistent, container)) continue;
				
				return 0; // Stuff to send right now.
			}
		}
		return cooldownWakeup;
	}

	public long countAllKeys(ObjectContainer container, ClientContext context) {
		int count = 0;
		synchronized(this) {
			if(startedDecode || isFinishing(container)) return 0;
			for(int i=0;i<dataBuckets.length+checkBuckets.length;i++) {
				if(foundKeys[i]) continue;
				// Double check
				if(getBlockBucket(i, container) != null) continue;
				count++;
			}
			return count;
		}
	}

	public long countSendableKeys(ObjectContainer container, ClientContext context) {
		int count = 0;
		long now = System.currentTimeMillis();
		int maxTries = getMaxRetries(container);
		synchronized(this) {
			if(startedDecode || isFinishing(container)) return 0;
			for(int i=0;i<dataBuckets.length+checkBuckets.length;i++) {
				if(foundKeys[i]) continue;
				if(getCooldownWakeup(i, maxTries, container, context) > now) continue;
				// Double check
				if(getBlockBucket(i, container) != null) continue;
				count++;
			}
			return count;
		}
	}

	public synchronized SplitFileFetcherSegmentGet makeGetter(ObjectContainer container, ClientContext context) {
		if(finishing || startedDecode || finished) return null;
		if(getter == null) {
			boolean parentActive = true;
			if(persistent) {
				parentActive = container.ext().isActive(parent);
				if(!parentActive) container.activate(parent, 1);
			}
			getter = new SplitFileFetcherSegmentGet(parent, this, realTimeFlag);
			if(!parentActive) container.deactivate(parent, 1);
			System.out.println("Auto-migrated from subsegments to SegmentGet on "+this+" : "+getter);
			getter.storeTo(container);
			container.store(this);
			this.removeSubSegments(container, context, false);
			return getter;
		} else {
			return getter;
		}
	}

	public CooldownTrackerItem makeCooldownTrackerItem() {
		return new MyCooldownTrackerItem(dataBuckets.length, checkBuckets.length);
	}

	public synchronized int getMaxRetries(ObjectContainer container) {
		if(maxRetries != 0) return maxRetries;
		return innerGetMaxRetries(container);
	}
	
	private synchronized int innerGetMaxRetries(ObjectContainer container) {
		boolean contextActive = true;
		if(persistent) {
			contextActive = container.ext().isActive(blockFetchContext);
			if(!contextActive) container.activate(blockFetchContext, 1);
		}
		maxRetries = blockFetchContext.maxSplitfileBlockRetries;
		if(persistent) {
			container.store(this);
			if(!contextActive) container.deactivate(blockFetchContext, 1);
		}
		return maxRetries;
	}

	/** Reread the cached cooldown values (and anything else) from the FetchContext
	 * after it changes. FIXME: Ideally this should be a generic mechanism, but
	 * that looks too complex without significant changes to data structures.
	 * For now it's just a hack to make changing the polling interval in USKs work.
	 * See https://bugs.freenetproject.org/view.php?id=4984
	 * @param container The database if this is a persistent request.
	 * @param context The context object.
	 */
	public void onChangedFetchContext(ObjectContainer container, ClientContext context) {
		synchronized(this) {
			if(finished) return;
		}
		innerCheckCachedCooldownData(container);
		innerGetMaxRetries(container);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.node.Node;
import freenet.support.SimpleFieldSet;

/**
 * Sent by the node back to the client after it receives a SubscribeUSK message.
 * 
 * SubscribedUSK
 * URI=USK@60I8H8HinpgZSOuTSD66AVlIFAy-xsppFr0YCzCar7c,NzdivUGCGOdlgngOGRbbKDNfSCnjI0FXjHLzJM4xkJ4,AQABAAE/index/4
 * DontPoll=true // meaning passively subscribe, don't cause the node to actively probe for it
 * Identifier=identifier
 * End
 * 
 * @author Florent Daigni&egrave;re &lt;nextgens@freenetproject.org&gt;
 */
public class SubscribedUSKMessage extends FCPMessage {
	public static final String name = "SubscribedUSK";
	
	public final SubscribeUSKMessage message;
	
	SubscribedUSKMessage(SubscribeUSKMessage m) {
		this.message = m;
	}
	
	@Override
	public SimpleFieldSet getFieldSet() {
		SimpleFieldSet sfs = new SimpleFieldSet(true);
		sfs.putSingle("Identifier", message.identifier);
		sfs.putSingle("URI", message.key.getURI().toString());
		sfs.put("DontPoll", message.dontPoll);
		
		return sfs;
	}

	@Override
	public String getName() {
		return name;
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node) throws MessageInvalidException {
		throw new MessageInvalidException(ProtocolErrorMessage.INVALID_MESSAGE, name + " goes from server to client not the other way around", name, false);
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		container.activate(message, 1);
		message.removeFrom(container);	// FIXME this is throwing UnsupportedOperationException
		container.delete(this);
	}
}/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.plugins.helpers1;

import java.io.PrintWriter;
import java.io.StringWriter;

import freenet.pluginmanager.PluginNotFoundException;
import freenet.pluginmanager.PluginReplySender;
import freenet.support.Logger;
import freenet.support.SimpleFieldSet;
import freenet.support.api.Bucket;

public abstract class AbstractFCPHandler {

	private static volatile boolean logMINOR;
	private static volatile boolean logDEBUG;

	static {
		Logger.registerClass(AbstractFCPHandler.class);
	}

	public static class FCPException extends Exception {
		private static final long serialVersionUID = 1L;
		public static final int UNKNOWN_ERROR = -1;
		public static final int OK = 0;
		public static final int MISSING_IDENTIFIER = 1;
		public static final int MISSING_COMMAND = 2;
		public static final int NO_SUCH_COMMAND = 3;
		public static final int UNSUPPORTED_OPERATION = 4;
		public static final int INTERNAL_ERROR = 5;

		final int code;

		protected FCPException(int eCode, String message) {
			super(message);
			code = eCode;
		}
	}

	protected final PluginContext pluginContext;

	protected AbstractFCPHandler(PluginContext pluginContext2) {
		this.pluginContext = pluginContext2;
	}

	public final void handle(PluginReplySender replysender, SimpleFieldSet params, Bucket data, int accesstype) throws PluginNotFoundException {

		if (logDEBUG) {
			Logger.debug(this, "Got Message: " + params.toOrderedString());
		}

		final String command = params.get("Command");
		final String identifier = params.get("Identifier");

		if ("Ping".equals(command)) {
			SimpleFieldSet sfs = new SimpleFieldSet(true);
			sfs.put("Pong", System.currentTimeMillis());
			if (identifier != null)
				sfs.putSingle("Identifier", identifier);
			replysender.send(sfs);
			return;
		}

		if (identifier == null || identifier.trim().length() == 0) {
			sendError(replysender, FCPException.MISSING_IDENTIFIER, "<invalid>", "Empty identifier!");
			return;
		}

		if (command == null || command.trim().length() == 0) {
			sendError(replysender, FCPException.MISSING_COMMAND, identifier, "Empty Command name");
			return;
		}
		try {
			handle(replysender, command, identifier, params, data, accesstype);
		} catch (FCPException e) {
			sendError(replysender, identifier, e);
		} catch (UnsupportedOperationException uoe) {
			sendError(replysender, FCPException.UNSUPPORTED_OPERATION, identifier, uoe.toString());
		}
	}

	protected abstract void handle(PluginReplySender replysender, String command,
			String identifier, SimpleFieldSet params, Bucket data,
			int accesstype) throws FCPException, PluginNotFoundException;

	public static void sendErrorWithTrace(PluginReplySender replysender, String identifier, Exception error) throws PluginNotFoundException {
		StringWriter sw = new StringWriter();
		PrintWriter pw = new PrintWriter(sw);
		error.printStackTrace(pw);
		pw.flush();

		sendError(replysender, FCPException.INTERNAL_ERROR, identifier, error.getLocalizedMessage());
	}

	public static void sendError(PluginReplySender replysender, String identifier, FCPException error) throws PluginNotFoundException {
		sendError(replysender, error.code, identifier, error.getLocalizedMessage());
	}

	public static void sendError(PluginReplySender replysender, int code, String identifier, String description) throws PluginNotFoundException {
		sendError(replysender, code, identifier, description, null);
	}

	public static void sendError(PluginReplySender replysender, int code, String identifier, String description, byte[] data) throws PluginNotFoundException {
		SimpleFieldSet sfs = new SimpleFieldSet(true);
		sfs.putOverwrite("Status", "Error");
		sfs.put("Code", code);
		sfs.putSingle("Identifier", identifier);
		sfs.putOverwrite("Description", description);
		replysender.send(sfs, data);
	}

	public static void sendNOP(PluginReplySender replysender, String identifier) throws PluginNotFoundException {
		sendError(replysender, -1, identifier, "Not implemented", null);
	}

	public static void sendSuccess(PluginReplySender replysender, String identifier, String description) throws PluginNotFoundException {
		SimpleFieldSet sfs = new SimpleFieldSet(true);
		sfs.putOverwrite("Status", "Success");
		sfs.put("Code", 0);
		sfs.putSingle("Identifier", identifier);
		sfs.putSingle("Description", description);
		replysender.send(sfs);
	}

	public static void sendProgress(PluginReplySender replysender,  String identifier, String description) throws PluginNotFoundException {
		SimpleFieldSet sfs = new SimpleFieldSet(true);
		sfs.putSingle("Status", "Progress");
		sfs.putSingle("Identifier", identifier);
		sfs.putSingle("Description", description);
		replysender.send(sfs);
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.pluginmanager;

import java.net.InetAddress;

/**
 * Class returned by a FredPluginIPDetector.
 * 
 * Indicates:
 * - Whether there is no UDP connectivity at all.
 * - Whether there is full inbound IP connectivity.
 * - A list of detected public IPs.
 */
public class DetectedIP {

	public final InetAddress publicAddress;
	public final short natType;
	/** The MTU as advertized by the JVM */
	public int mtu;
	// Constants
	/** The plugin does not support detecting the NAT type. */
	public static final short NOT_SUPPORTED = 1;
	/** Full internet access! */
	public static final short FULL_INTERNET = 2;
	/** Full cone NAT. Once we have sent a packet out on a port, any node anywhere can send us
	 * a packet on that port. The nicest option, but very rare unfortunately. */
	public static final short FULL_CONE_NAT = 3;
	/** Restricted cone NAT. Once we have sent a packet out to a specific IP, it can send us 
	 * packets on the port we just used. */
	public static final short RESTRICTED_CONE_NAT = 4;
	/** Port restricted cone NAT. Once we have sent a packet to a specific IP+Port, that IP+Port
	 * can send us packets on the port we just used. */
	public static final short PORT_RESTRICTED_NAT = 5;
	/** Symmetric NAT. Uses a separate port number for each IP+port ! Not much hope for symmetric
	 * to symmetric... */
	public static final short SYMMETRIC_NAT = 6;
	/** Symmetric UDP firewall. We are not NATed, but the firewall behaves as if we were. */
	public static final short SYMMETRIC_UDP_FIREWALL = 7;
	/** No UDP connectivity at all */
	public static final short NO_UDP = 8;
	
	public DetectedIP(InetAddress addr, short type) {
		this.publicAddress = addr;
		this.natType = type;
		this.mtu = 1500;
	}
	
	@Override
	public boolean equals(Object o) {
		if(!(o instanceof DetectedIP)) {
			return false;
		}
		DetectedIP d = (DetectedIP)o;
		return ((d.natType == natType) && d.publicAddress.equals(publicAddress));
	}
	
	@Override
	public int hashCode() {
		return publicAddress.hashCode() ^ natType;
	}
	
	@Override
	public String toString() {
		return publicAddress.toString()+":"+natType+":"+mtu;
	}
}
package freenet.pluginmanager;

public class ForwardPortStatus {
	
	public final int status;
	/** The port forward definitely succeeded. */
	public static final int DEFINITE_SUCCESS = 3;
	/** The port forward probably succeeded. I.e. it succeeded unless there was
	 * for example hostile action on the part of the router. */
	public static final int PROBABLE_SUCCESS = 2;
	/** The port forward may have succeeded. Or it may not have. We should 
	 * definitely try to check out of band. See UP&P: Many routers say they've
	 * forwarded the port when they haven't. */
	public static final int MAYBE_SUCCESS = 1;
	/** The port forward is in progress */
	public static final int IN_PROGRESS = 0;
	/** The port forward probably failed */
	public static final int PROBABLE_FAILURE = -1;
	/** The port forward definitely failed. */
	public static final int DEFINITE_FAILURE = -2;
	
	public final String reasonString;
	
	/** Some plugins may need to change the external port. They can return it
	 * to the node here. */
	public final int externalPort;
	
	public ForwardPortStatus(int status, String reason, int externalPort) {
		this.status = status;
		this.reasonString = reason;
		this.externalPort = externalPort;
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.io;

import java.io.EOFException;
import java.io.FilterInputStream;
import java.io.IOException;
import java.io.InputStream;

import freenet.support.HexUtil;

/**
 * A FilterInputStream which provides readLine().
 */
public class LineReadingInputStream extends FilterInputStream implements LineReader {

	public LineReadingInputStream(InputStream in) {
		super(in);
	}

	/**
	 * Read a \n or \r\n terminated line of UTF-8 or ISO-8859-1.
	 * @param maxLength The maximum length of a line. If a line is longer than this, we throw IOException rather
	 * than keeping on reading it forever.
	 * @param bufferSize The initial size of the read buffer.
	 * @param utf If true, read as UTF-8, if false, read as ISO-8859-1.
	 */
	public String readLine(int maxLength, int bufferSize, boolean utf) throws IOException {
		if(maxLength < 1)
			return null;
		if(maxLength <= bufferSize)
			bufferSize = maxLength + 1; // Buffer too big, shrink it (add 1 for the optional \r)

		if(!markSupported())
			return readLineWithoutMarking(maxLength, bufferSize, utf);

		byte[] buf = new byte[Math.max(Math.min(128, maxLength), Math.min(1024, bufferSize))];
		int ctr = 0;
		mark(maxLength + 2); // in case we have both a \r and a \n
		while(true) {
			assert(buf.length - ctr > 0);
			int x = read(buf, ctr, buf.length - ctr);
			if(x < 0) {
				if(ctr == 0)
					return null;
				return new String(buf, 0, ctr, utf ? "UTF-8" : "ISO-8859-1");
			}
			if(x == 0) {
				// Don't busy-loop. Probably a socket closed or something.
				// If not, it's not a salavageable situation; either way throw.
				throw new EOFException();
			}
			// REDFLAG this is definitely safe with the above charsets, it may not be safe with some wierd ones.
			int end = ctr + x;
			for(; ctr < end; ctr++) {
				if(buf[ctr] == '\n') {
					String toReturn = "";
					if(ctr != 0) {
						boolean removeCR = (buf[ctr - 1] == '\r');
						toReturn = new String(buf, 0, (removeCR ? ctr - 1 : ctr), utf ? "UTF-8" : "ISO-8859-1");
					}
					reset();
					skip(ctr + 1);
					return toReturn;
				}
				if(ctr >= maxLength)
					throw new TooLongException("We reached maxLength="+maxLength+ " parsing\n "+HexUtil.bytesToHex(buf, 0, ctr) + "\n" + new String(buf, 0, ctr, utf ? "UTF-8" : "ISO-8859-1"));
			}
			if((buf.length < maxLength) && (buf.length - ctr < bufferSize)) {
				byte[] newBuf = new byte[Math.min(buf.length * 2, maxLength)];
				System.arraycopy(buf, 0, newBuf, 0, ctr);
				buf = newBuf;
			}
		}
	}

	protected String readLineWithoutMarking(int maxLength, int bufferSize, boolean utf) throws IOException {
		if(maxLength < bufferSize)
			bufferSize = maxLength + 1; // Buffer too big, shrink it (add 1 for the optional \r)
		byte[] buf = new byte[Math.max(Math.min(128, maxLength), Math.min(1024, bufferSize))];
		int ctr = 0;
		while(true) {
			int x = read();
			if(x == -1) {
				if(ctr == 0)
					return null;
				return new String(buf, 0, ctr, utf ? "UTF-8" : "ISO-8859-1");
			}
			// REDFLAG this is definitely safe with the above charsets, it may not be safe with some wierd ones.
			if(x == '\n') {
				if(ctr == 0)
					return "";
				if(buf[ctr - 1] == '\r')
					ctr--;
				return new String(buf, 0, ctr, utf ? "UTF-8" : "ISO-8859-1");
			}
			if(ctr >= maxLength)
					throw new TooLongException("We reached maxLength="+maxLength+ " parsing\n "+HexUtil.bytesToHex(buf, 0, ctr) + "\n" + new String(buf, 0, ctr, utf ? "UTF-8" : "ISO-8859-1"));
			if(ctr >= buf.length) {
				byte[] newBuf = new byte[Math.min(buf.length * 2, maxLength)];
				System.arraycopy(buf, 0, newBuf, 0, buf.length);
				buf = newBuf;
			}
			buf[ctr++] = (byte) x;
		}
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.client.async;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.io.PipedInputStream;
import java.io.PipedOutputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.Random;

import freenet.support.math.MersenneTwister;

import com.db4o.ObjectContainer;

import freenet.client.ArchiveContext;
import freenet.client.ClientMetadata;
import freenet.client.FetchContext;
import freenet.client.FetchException;
import freenet.client.HighLevelSimpleClientImpl;
import freenet.client.InsertContext;
import freenet.client.FetchResult;
import freenet.client.InsertContext.CompatibilityMode;
import freenet.client.Metadata;
import freenet.client.MetadataParseException;
import freenet.keys.CHKBlock;
import freenet.keys.ClientCHK;
import freenet.keys.NodeCHK;
import freenet.node.SendableGet;
import freenet.support.BinaryBloomFilter;
import freenet.support.BloomFilter;
import freenet.support.CountingBloomFilter;
import freenet.support.Fields;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.OOMHandler;
import freenet.support.api.Bucket;
import freenet.support.compress.Compressor;
import freenet.support.io.Closer;
import freenet.support.io.FileUtil;

/**
 * Fetch a splitfile, decompress it if need be, and return it to the GetCompletionCallback.
 * Most of the work is done by the segments, and we do not need a thread.
 */
public class SplitFileFetcher implements ClientGetState, HasKeyListener {

	private static volatile boolean logMINOR;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback() {

			@Override
			public void shouldUpdate() {
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	final FetchContext fetchContext;
	final FetchContext blockFetchContext;
	final boolean deleteFetchContext;
	final ArchiveContext archiveContext;
	final List<? extends Compressor> decompressors;
	final ClientMetadata clientMetadata;
	final ClientRequester parent;
	final GetCompletionCallback cb;
	final int recursionLevel;
	/** The splitfile type. See the SPLITFILE_ constants on Metadata. */
	final short splitfileType;
	/** The segment length. -1 means not segmented and must get everything to decode. */
	final int blocksPerSegment;
	/** The segment length in check blocks. */
	final int checkBlocksPerSegment;
	final int deductBlocksFromSegments;
	/** Total number of segments */
	final int segmentCount;
	/** The detailed information on each segment */
	final SplitFileFetcherSegment[] segments;
	/** Maximum temporary length */
	final long maxTempLength;
	/** Have all segments finished? Access synchronized. */
	private boolean allSegmentsFinished;
	/** Override length. If this is positive, truncate the splitfile to this length. */
	private final long overrideLength;
	private boolean finished;
	private long token;
	final boolean persistent;
	private FetchException otherFailure;
	final boolean realTimeFlag;

	// A persistent hashCode is helpful in debugging, and also means we can put
	// these objects into sets etc when we need to.

	private final int hashCode;

	@Override
	public int hashCode() {
		return hashCode;
	}

	// Bloom filter stuff
	/** The main bloom filter, which includes every key in the segment, is stored
	 * in this file. It is a counting filter and is updated when a key is found. */
	File mainBloomFile;
	/** The per-segment bloom filters are kept in this (slightly larger) file,
	 * appended one after the next. */
	File altBloomFile;
	
	// The above are obsolete. We now store the bloom filter in the database.
	CountingBloomFilter cachedMainBloomFilter;
	BinaryBloomFilter[] cachedSegmentBloomFilters;
	
	/** Size of the main Bloom filter in bytes. */
	final int mainBloomFilterSizeBytes;
	/** Default mainBloomElementsPerKey. False positives is approx
	 * 0.6185^[this number], so 19 gives us 0.01% false positives, which should
	 * be acceptable even if there are thousands of splitfiles on the queue. */
	static final int DEFAULT_MAIN_BLOOM_ELEMENTS_PER_KEY = 19;
	/** Number of hashes for the main filter. */
	final int mainBloomK;
	/** What proportion of false positives is acceptable for the per-segment
	 * Bloom filters? This is divided by the number of segments, so it is (roughly)
	 * an overall probability of any false positive given that we reach the
	 * per-segment filters. IMHO 1 in 100 is adequate. */
	static final double ACCEPTABLE_BLOOM_FALSE_POSITIVES_ALL_SEGMENTS = 0.01;
	/** Size of per-segment bloom filter in bytes. This is calculated from the
	 * above constant and the number of segments, and rounded up. */
	final int perSegmentBloomFilterSizeBytes;
	/** Number of hashes for the per-segment bloom filters. */
	final int perSegmentK;
	private int keyCount;
	/** Salt used in the secondary Bloom filters if the primary matches.
	 * The primary Bloom filters use the already-salted saltedKey. */
	private final byte[] localSalt;
	/** Reference set on the first call to makeKeyListener().
	 * NOTE: db4o DOES NOT clear transient variables on deactivation.
	 * So as long as this is paged in (i.e. there is a reference to it, i.e. the
	 * KeyListener), it will remain valid, once it is set by the first call
	 * during resuming. */
	private transient SplitFileFetcherKeyListener tempListener;

	private final int crossCheckBlocks;
	private final SplitFileFetcherCrossSegment[] crossSegments;
	
	public SplitFileFetcher(Metadata metadata, GetCompletionCallback rcb, ClientRequester parent2,
			FetchContext newCtx, boolean deleteFetchContext, boolean realTimeFlag, List<? extends Compressor> decompressors2, ClientMetadata clientMetadata,
			ArchiveContext actx, int recursionLevel, long token2, boolean topDontCompress, short topCompatibilityMode, ObjectContainer container, ClientContext context) throws FetchException, MetadataParseException {
		this.persistent = parent2.persistent();
		this.realTimeFlag = realTimeFlag;
		this.deleteFetchContext = deleteFetchContext;
		if(logMINOR)
			Logger.minor(this, "Persistence = "+persistent+" from "+parent2, new Exception("debug"));
		int hash = super.hashCode();
		if(hash == 0) hash = 1;
		this.hashCode = hash;
		this.finished = false;
		this.fetchContext = newCtx;
		if(newCtx == null)
			throw new NullPointerException();
		this.archiveContext = actx;
		blockFetchContext = new FetchContext(fetchContext, FetchContext.SPLITFILE_DEFAULT_BLOCK_MASK, true, null);
		this.decompressors = persistent ? new ArrayList<Compressor>(decompressors2) : decompressors2;
		if(decompressors.size() > 1) {
			Logger.error(this, "Multiple decompressors: "+decompressors.size()+" - this is almost certainly a bug", new Exception("debug"));
		}
		this.clientMetadata = clientMetadata == null ? new ClientMetadata() : clientMetadata.clone(); // copy it as in SingleFileFetcher
		this.cb = rcb;
		this.recursionLevel = recursionLevel + 1;
		this.parent = parent2;
		localSalt = new byte[32];
		context.random.nextBytes(localSalt);
		if(parent2.isCancelled())
			throw new FetchException(FetchException.CANCELLED);
		overrideLength = metadata.dataLength();
		this.splitfileType = metadata.getSplitfileType();
		SplitFileSegmentKeys[] segmentKeys = metadata.grabSegmentKeys(container);
		if(persistent) {
			// Clear them here so they don't get deleted and we don't need to clone them.
			metadata.clearSplitfileKeys();
			container.store(metadata);
		}
		long eventualLength = Math.max(overrideLength, metadata.uncompressedDataLength());
		boolean wasActive = true;
		if(persistent) {
			wasActive = container.ext().isActive(cb);
			if(!wasActive)
				container.activate(cb, 1);
		}
		cb.onExpectedSize(eventualLength, container, context);
		String mimeType = metadata.getMIMEType();
		if(mimeType != null)
			cb.onExpectedMIME(mimeType, container, context);
		if(metadata.uncompressedDataLength() > 0)
			cb.onFinalizedMetadata(container);
		if(!wasActive)
			container.deactivate(cb, 1);
		if(eventualLength > 0 && newCtx.maxOutputLength > 0 && eventualLength > newCtx.maxOutputLength)
			throw new FetchException(FetchException.TOO_BIG, eventualLength, true, clientMetadata.getMIMEType());

		this.token = token2;
		
		CompatibilityMode minCompatMode = metadata.getMinCompatMode();
		CompatibilityMode maxCompatMode = metadata.getMaxCompatMode();

		int crossCheckBlocks = metadata.getCrossCheckBlocks();
		
		blocksPerSegment = metadata.getDataBlocksPerSegment();
		checkBlocksPerSegment = metadata.getCheckBlocksPerSegment();
		
		int splitfileDataBlocks = 0;
		int splitfileCheckBlocks = 0;
		
		for(SplitFileSegmentKeys keys : segmentKeys) {
			splitfileDataBlocks += keys.getDataBlocks();
			splitfileCheckBlocks += keys.getCheckBlocks();
		}
		
		if(splitfileType == Metadata.SPLITFILE_NONREDUNDANT) {
			if(splitfileCheckBlocks > 0) {
				Logger.error(this, "Splitfile type is SPLITFILE_NONREDUNDANT yet "+splitfileCheckBlocks+" check blocks found!! : "+this);
				throw new FetchException(FetchException.INVALID_METADATA, "Splitfile type is non-redundant yet have "+splitfileCheckBlocks+" check blocks");
			}
		} else if(splitfileType == Metadata.SPLITFILE_ONION_STANDARD) {
			
			boolean dontCompress = decompressors.isEmpty();
			if(topCompatibilityMode != 0) {
				// If we have top compatibility mode, then we can give a definitive answer immediately, with the splitfile key, with dontcompress, etc etc.
				if(minCompatMode == CompatibilityMode.COMPAT_UNKNOWN ||
						!(minCompatMode.ordinal() > topCompatibilityMode || maxCompatMode.ordinal() < topCompatibilityMode)) {
					minCompatMode = maxCompatMode = CompatibilityMode.values()[topCompatibilityMode];
					dontCompress = topDontCompress;
				} else
					throw new FetchException(FetchException.INVALID_METADATA, "Top compatibility mode is incompatible with detected compatibility mode");
			}
			cb.onSplitfileCompatibilityMode(minCompatMode, maxCompatMode, metadata.getCustomSplitfileKey(), dontCompress, true, topCompatibilityMode != 0, container, context);

			if((blocksPerSegment > fetchContext.maxDataBlocksPerSegment)
					|| (checkBlocksPerSegment > fetchContext.maxCheckBlocksPerSegment))
				throw new FetchException(FetchException.TOO_MANY_BLOCKS_PER_SEGMENT, "Too many blocks per segment: "+blocksPerSegment+" data, "+checkBlocksPerSegment+" check");
			
				
		} else throw new MetadataParseException("Unknown splitfile format: "+splitfileType);
		segmentCount = metadata.getSegmentCount();
		this.maxTempLength = fetchContext.maxTempLength;
		if(logMINOR)
			Logger.minor(this, "Algorithm: "+splitfileType+", blocks per segment: "+blocksPerSegment+
					", check blocks per segment: "+checkBlocksPerSegment+", segments: "+segmentCount+
					", data blocks: "+splitfileDataBlocks+", check blocks: "+splitfileCheckBlocks);
		segments = new SplitFileFetcherSegment[segmentCount]; // initially null on all entries

		this.crossCheckBlocks = crossCheckBlocks;
		
		long finalLength = 1L * (splitfileDataBlocks - segmentCount * crossCheckBlocks) * CHKBlock.DATA_LENGTH;
		if(finalLength > overrideLength) {
			if(finalLength - overrideLength > CHKBlock.DATA_LENGTH)
				throw new FetchException(FetchException.INVALID_METADATA, "Splitfile is "+finalLength+" but length is "+finalLength);
			finalLength = overrideLength;
		}
		
		mainBloomFile = null;
		altBloomFile = null;
		int mainElementsPerKey = DEFAULT_MAIN_BLOOM_ELEMENTS_PER_KEY;
		int origSize = splitfileDataBlocks + splitfileCheckBlocks;
		mainBloomK = (int) (mainElementsPerKey * 0.7);
		long elementsLong = origSize * mainElementsPerKey;
		// REDFLAG: SIZE LIMIT: 3.36TB limit!
		if(elementsLong > Integer.MAX_VALUE)
			throw new FetchException(FetchException.TOO_BIG, "Cannot fetch splitfiles with more than "+(Integer.MAX_VALUE/mainElementsPerKey)+" keys! (approx 3.3TB)");
		int mainSizeBits = (int)elementsLong; // counting filter
		if((mainSizeBits & 7) != 0)
			mainSizeBits += (8 - (mainSizeBits & 7));
		mainBloomFilterSizeBytes = mainSizeBits / 8 * 2; // counting filter
		double acceptableFalsePositives = ACCEPTABLE_BLOOM_FALSE_POSITIVES_ALL_SEGMENTS / segments.length;
		int perSegmentBitsPerKey = (int) Math.ceil(Math.log(acceptableFalsePositives) / Math.log(0.6185));
		int segBlocks = blocksPerSegment + checkBlocksPerSegment;
		if(segBlocks > origSize)
			segBlocks = origSize;
		int perSegmentSize = perSegmentBitsPerKey * segBlocks;
		if((perSegmentSize & 7) != 0)
			perSegmentSize += (8 - (perSegmentSize & 7));
		perSegmentBloomFilterSizeBytes = perSegmentSize / 8;
		perSegmentK = BloomFilter.optimialK(perSegmentSize, segBlocks);
		keyCount = origSize;
		// Now create it.
		if(logMINOR)
			Logger.minor(this, "Creating block filter for "+this+": keys="+(splitfileDataBlocks+splitfileCheckBlocks)+" main bloom size "+mainBloomFilterSizeBytes+" bytes, K="+mainBloomK+", filename="+mainBloomFile+" alt bloom filter: filename="+altBloomFile+" segments: "+segments.length+" each is "+perSegmentBloomFilterSizeBytes+" bytes k="+perSegmentK);
		try {
			tempListener = new SplitFileFetcherKeyListener(this, keyCount, mainBloomFile, altBloomFile, mainBloomFilterSizeBytes, mainBloomK, localSalt, segments.length, perSegmentBloomFilterSizeBytes, perSegmentK, persistent, true, null, null, container, false, realTimeFlag);
		} catch (IOException e) {
			throw new FetchException(FetchException.BUCKET_ERROR, "Unable to write Bloom filters for splitfile");
		}

		if(persistent)
			container.store(this);

		boolean pre1254 = !(minCompatMode == CompatibilityMode.COMPAT_CURRENT || minCompatMode.ordinal() >= CompatibilityMode.COMPAT_1255.ordinal());
		boolean pre1250 = (minCompatMode == CompatibilityMode.COMPAT_UNKNOWN || minCompatMode == CompatibilityMode.COMPAT_1250_EXACT);
		
		int maxRetries = blockFetchContext.maxSplitfileBlockRetries;
		for(int i=0;i<segments.length;i++) {
			// splitfile* will be overwritten, this is bad
			// so copy them
			SplitFileSegmentKeys keys = segmentKeys[i];
			int dataBlocks = keys.getDataBlocks();
			int checkBlocks = keys.getCheckBlocks();
			if((dataBlocks > fetchContext.maxDataBlocksPerSegment)
					|| (checkBlocks > fetchContext.maxCheckBlocksPerSegment))
				throw new FetchException(FetchException.TOO_MANY_BLOCKS_PER_SEGMENT, "Too many blocks per segment: "+blocksPerSegment+" data, "+checkBlocksPerSegment+" check");
			segments[i] = new SplitFileFetcherSegment(splitfileType, keys,
					this, archiveContext, blockFetchContext, maxTempLength, recursionLevel, parent, i, pre1250, pre1254, crossCheckBlocks, metadata.getSplitfileCryptoAlgorithm(), metadata.getSplitfileCryptoKey(), maxRetries, realTimeFlag);
			int data = keys.getDataBlocks();
			int check = keys.getCheckBlocks();
			for(int j=0;j<(data+check);j++) {
				tempListener.addKey(keys.getKey(j, null, false).getNodeKey(false), i, context);
			}
			if(persistent) {
				container.store(segments[i]);
				segments[i].deactivateKeys(container);
			}
		}
		int totalCrossCheckBlocks = segments.length * crossCheckBlocks;
		parent.addMustSucceedBlocks(splitfileDataBlocks - totalCrossCheckBlocks, container);
		parent.addBlocks(splitfileCheckBlocks + totalCrossCheckBlocks, container);
		parent.notifyClients(container, context);
		
		deductBlocksFromSegments = metadata.getDeductBlocksFromSegments();
		
		if(crossCheckBlocks != 0) {
			Random random = new MersenneTwister(Metadata.getCrossSegmentSeed(metadata.getHashes(), metadata.getHashThisLayerOnly()));
			// Cross segment redundancy: Allocate the blocks.
			crossSegments = new SplitFileFetcherCrossSegment[segments.length];
			int segLen = blocksPerSegment;
			for(int i=0;i<crossSegments.length;i++) {
				Logger.normal(this, "Allocating blocks (on fetch) for cross segment "+i);
				if(segments.length - i == deductBlocksFromSegments) {
					segLen--;
				}
				SplitFileFetcherCrossSegment seg = new SplitFileFetcherCrossSegment(persistent, segLen, crossCheckBlocks, parent, this, metadata.getSplitfileType());
				crossSegments[i] = seg;
				for(int j=0;j<segLen;j++) {
					// Allocate random data blocks
					allocateCrossDataBlock(seg, random);
				}
				for(int j=0;j<crossCheckBlocks;j++) {
					// Allocate check blocks
					allocateCrossCheckBlock(seg, random);
				}
				if(persistent) seg.storeTo(container);
			}
		} else {
			crossSegments = null;
		}
		
		if(persistent) {
			for(SplitFileFetcherSegment seg : segments) {
				if(crossCheckBlocks != 0)
					container.store(seg);
				container.deactivate(seg, 1);
			}
		}

		try {
			tempListener.writeFilters(container, "construction");
		} catch (IOException e) {
			throw new FetchException(FetchException.BUCKET_ERROR, "Unable to write Bloom filters for splitfile");
		}
	}
	
	private void allocateCrossDataBlock(SplitFileFetcherCrossSegment segment, Random random) {
		int x = 0;
		for(int i=0;i<10;i++) {
			x = random.nextInt(segments.length);
			SplitFileFetcherSegment seg = segments[x];
			int blockNum = seg.allocateCrossDataBlock(segment, random);
			if(blockNum >= 0) {
				segment.addDataBlock(seg, blockNum);
				return;
			}
		}
		for(int i=0;i<segments.length;i++) {
			x++;
			if(x == segments.length) x = 0;
			SplitFileFetcherSegment seg = segments[x];
			int blockNum = seg.allocateCrossDataBlock(segment, random);
			if(blockNum >= 0) {
				segment.addDataBlock(seg, blockNum);
				return;
			}
		}
		throw new IllegalStateException("Unable to allocate cross data block!");
	}

	private void allocateCrossCheckBlock(SplitFileFetcherCrossSegment segment, Random random) {
		int x = 0;
		for(int i=0;i<10;i++) {
			x = random.nextInt(segments.length);
			SplitFileFetcherSegment seg = segments[x];
			int blockNum = seg.allocateCrossCheckBlock(segment, random);
			if(blockNum >= 0) {
				segment.addDataBlock(seg, blockNum);
				return;
			}
		}
		for(int i=0;i<segments.length;i++) {
			x++;
			if(x == segments.length) x = 0;
			SplitFileFetcherSegment seg = segments[x];
			int blockNum = seg.allocateCrossCheckBlock(segment, random);
			if(blockNum >= 0) {
				segment.addDataBlock(seg, blockNum);
				return;
			}
		}
		throw new IllegalStateException("Unable to allocate cross data block!");
	}

	/** Return the final status of the fetch. Throws an exception, or returns an
	 * InputStream from which the fetched data may be read.
	 * @throws FetchException If the fetch failed for some reason.
	 */
	private SplitFileStreamGenerator finalStatus(final ObjectContainer container, final ClientContext context) throws FetchException {
		long length = 0;
		for(int i=0;i<segments.length;i++) {
			SplitFileFetcherSegment s = segments[i];
			if(persistent)
				container.activate(s, 1);
			if(!s.succeeded()) {
				throw new IllegalStateException("Not all finished");
			}
			s.throwError(container);
			// If still here, it succeeded
			long sz = s.decodedLength(container);
			length += sz;
			if(logMINOR)
				Logger.minor(this, "Segment "+i+" decoded length "+sz+" total length now "+length+" for "+s.dataBuckets.length+" blocks which should be "+(s.dataBuckets.length * CHKBlock.DATA_LENGTH));
			// Healing is done by Segment
		}
		if(length > overrideLength) {
			if(length - overrideLength > CHKBlock.DATA_LENGTH)
				throw new FetchException(FetchException.INVALID_METADATA, "Splitfile is "+length+" but length is "+length);
			length = overrideLength;
		}
		SplitFileStreamGenerator streamGenerator = new SplitFileStreamGenerator(segments, length, crossCheckBlocks);
		return streamGenerator;
}

	public void segmentFinished(SplitFileFetcherSegment segment, ObjectContainer container, ClientContext context) {
		if(persistent)
			container.activate(this, 1);
		if(logMINOR) Logger.minor(this, "Finished segment: "+segment);
		boolean finish = false;
		synchronized(this) {
			boolean allDone = true;
			for(int i=0;i<segments.length;i++) {
				if(persistent)
					container.activate(segments[i], 1);
				if(!segments[i].succeeded()) {
					if(logMINOR) Logger.minor(this, "Segment "+segments[i]+" is not finished");
					allDone = false;
				}
			}
			if(allDone) {
				if(allSegmentsFinished) {
					if(logMINOR)
						// Race condition. No problem.
						Logger.minor(this, "Was already finished! (segmentFinished("+segment+ ')', new Exception("debug"));
				} else {
					allSegmentsFinished = true;
					finish = true;
				}
			} else {
				for(int i=0;i<segments.length;i++) {
					if(segments[i] == segment) continue;
					if(persistent)
						container.deactivate(segments[i], 1);
				}
			}
			notifyAll();
		}
		if(persistent) container.store(this);
		if(finish) finish(container, context);
	}

	private void finish(ObjectContainer container, ClientContext context) {
		if(persistent) {
			container.activate(cb, 1);
		}
		context.getChkFetchScheduler(realTimeFlag).removePendingKeys(this, true);
		boolean cbWasActive = true;
		SplitFileStreamGenerator data = null;
		try {
			synchronized(this) {
				if(otherFailure != null) {
					throw otherFailure;
				}
				if(finished) {
					Logger.error(this, "Was already finished");
					return;
				}
				finished = true;
			}
			context.jobRunner.setCommitThisTransaction();
			if(persistent)
				container.store(this);
			data = finalStatus(container, context);
			cb.onSuccess(data, clientMetadata, decompressors, this, container, context);
		}
		catch (FetchException e) {
			cb.onFailure(e, this, container, context);
		} finally {
			if(!cbWasActive) container.deactivate(cb, 1);
		}
		if(crossCheckBlocks != 0 && !persistent) finishSegments(container, context);
	}

	public void schedule(ObjectContainer container, ClientContext context) throws KeyListenerConstructionException {
		if(persistent)
			container.activate(this, 1);
		if(logMINOR) Logger.minor(this, "Scheduling "+this);
		SendableGet[] getters = new SendableGet[segments.length];
		for(int i=0;i<segments.length;i++) {
			if(logMINOR)
				Logger.minor(this, "Scheduling segment "+i+" : "+segments[i]);
			if(persistent)
				container.activate(segments[i], 1);
			getters[i] = segments[i].schedule(container, context);
			if(persistent)
				container.deactivate(segments[i], 1);
		}
		BlockSet blocks = fetchContext.blocks;
		context.getChkFetchScheduler(realTimeFlag).register(this, getters, persistent, container, blocks, false);
	}

	public void cancel(ObjectContainer container, ClientContext context) {
		boolean persist = persistent;
		if(persist)
			container.activate(this, 1);
		for(int i=0;i<segments.length;i++) {
			if(logMINOR)
				Logger.minor(this, "Cancelling segment "+i);
			if(segments == null && persist && !container.ext().isActive(this)) {
				// FIXME is this normal? If so just reactivate.
				Logger.error(this, "Deactivated mid-cancel on "+this, new Exception("error"));
				container.activate(this, 1);
			}
			if(segments[i] == null) {
				synchronized(this) {
					if(finished) {
						// Not unusual, if some of the later segments are already finished when cancel() is called.
						if(logMINOR) Logger.minor(this, "Finished mid-cancel on "+this);
						return;
					}
				}
			}
			if(persist)
				container.activate(segments[i], 1);
			segments[i].cancel(container, context);
		}
	}
	
	public boolean realTimeFlag() {
		return realTimeFlag;
	}

	public long getToken() {
		return token;
	}

	/**
	 * Make our SplitFileFetcherKeyListener. Returns the one we created in the
	 * constructor if possible, otherwise makes a new one. We must have already
	 * constructed one at some point, maybe before a restart.
	 * @throws FetchException
	 */
	public KeyListener makeKeyListener(ObjectContainer container, ClientContext context, boolean onStartup) throws KeyListenerConstructionException {
		synchronized(this) {
			if(finished) return null;
			if(tempListener != null) {
				// Recently constructed
				return tempListener;
			}
			File main;
			File alt;
			if(fetchContext == null) {
				Logger.error(this, "fetchContext deleted without splitfile being deleted!");
				return null;
			}
			if(persistent) {
				container.activate(mainBloomFile, 5);
				container.activate(altBloomFile, 5);
				if(mainBloomFile != null) {
					main = new File(mainBloomFile.getPath());
					container.delete(mainBloomFile);
					mainBloomFile = null;
					if(persistent) container.store(this);
				} else
					main = null;
				if(altBloomFile != null) {
					alt = new File(altBloomFile.getPath());
					container.delete(altBloomFile);
					altBloomFile = null;
					if(persistent) container.store(this);
				} else
					alt = null;
				container.deactivate(mainBloomFile, 1);
				container.deactivate(altBloomFile, 1);
			} else {
				main = null;
				alt = null;
			}
			try {
				if(logMINOR)
					Logger.minor(this, "Attempting to read Bloom filter for "+this+" main file="+main+" alt file="+alt);
				tempListener =
					new SplitFileFetcherKeyListener(this, keyCount, main, alt, mainBloomFilterSizeBytes, mainBloomK, localSalt, segments.length, perSegmentBloomFilterSizeBytes, perSegmentK, persistent, false, cachedMainBloomFilter, cachedSegmentBloomFilters, container, onStartup, realTimeFlag);
				if(main != null) {
					try {
						FileUtil.secureDelete(main, context.fastWeakRandom);
					} catch (IOException e) {
						System.err.println("Failed to delete old bloom filter file: "+main+" - this may leak information about a download : "+e);
						e.printStackTrace();
					}
				}
				if(alt != null) {
					try {
						FileUtil.secureDelete(alt, context.fastWeakRandom);
					} catch (IOException e) {
						System.err.println("Failed to delete old segment filters file: "+alt+" - this may leak information about a download : "+e);
					}
				}
			} catch (IOException e) {
				Logger.error(this, "Unable to read Bloom filter for "+this+" attempting to reconstruct...", e);
				try {
					FileUtil.secureDelete(main, context.fastWeakRandom);
				} catch (IOException e2) {
					// Ignore
				}
				try {
					FileUtil.secureDelete(alt, context.fastWeakRandom);
				} catch (IOException e2) {
					// Ignore
				}
				mainBloomFile = null;
				altBloomFile = null;
				if(persistent)
					container.store(this);

				try {
					tempListener =
						new SplitFileFetcherKeyListener(this, keyCount, mainBloomFile, altBloomFile, mainBloomFilterSizeBytes, mainBloomK, localSalt, segments.length, perSegmentBloomFilterSizeBytes, perSegmentK, persistent, true, cachedMainBloomFilter, cachedSegmentBloomFilters, container, onStartup, realTimeFlag);
				} catch (IOException e1) {
					throw new KeyListenerConstructionException(new FetchException(FetchException.BUCKET_ERROR, "Unable to reconstruct Bloom filters: "+e1, e1));
				}
			}
			return tempListener;
		}
	}

	public synchronized boolean isCancelled(ObjectContainer container) {
		return finished;
	}

	public SplitFileFetcherSegment getSegment(int i) {
		return segments[i];
	}

	public void removeMyPendingKeys(SplitFileFetcherSegment segment, ObjectContainer container, ClientContext context) {
		keyCount = tempListener.killSegment(segment, container, context);
	}

	void setKeyCount(int keyCount2, ObjectContainer container) {
		this.keyCount = keyCount2;
		if(persistent)
			container.store(this);
	}

	public void onFailed(FetchException e, ObjectContainer container, ClientContext context) {
		synchronized(this) {
			if(finished) return;
			otherFailure = e;
		}
		cancel(container, context);
	}
	
	public void onFailed(KeyListenerConstructionException e, ObjectContainer container, ClientContext context) {
		onFailed(e.getFetchException(), container, context);
	}

	private boolean toRemove = false;
	// Leaks can happen if it is still in memory after removal.
	// It shouldn't be referred to by anything but it's good to detect such problems.
	private boolean removed = false;
	
	/** Remove from the database, but only if all the cross-segments have finished.
	 * If not, wait for them to report in. */
	public void removeFrom(ObjectContainer container, ClientContext context) {
		synchronized(this) {
			toRemove = true;
			if(removed) return;
		}
		if(crossCheckBlocks > 0) {
			boolean allGone = true;
			for(int i=0;i<crossSegments.length;i++) {
				if(crossSegments[i] != null) {
					boolean active = true;
					if(persistent) {
						active = container.ext().isActive(crossSegments[i]);
						if(!active) container.activate(crossSegments[i], 1);
					}
					crossSegments[i].preRemove(container, context);
					if(!crossSegments[i].isFinished()) {
						allGone = false;
						if(logMINOR) Logger.minor(this, "Waiting for "+crossSegments[i]+" in removeFrom()");
					}
					if(!active) container.deactivate(crossSegments[i], 1);
				}
			}
			if(!allGone) {
				container.store(this);
				return;
			}
		}
		innerRemoveFrom(container, context);
	}
	
	/** Actually do the remove from the database. */
	public void innerRemoveFrom(ObjectContainer container, ClientContext context) {
		synchronized(this) {
			if(removed) {
				Logger.error(this, "innerRemoveFrom() called twice", new Exception("error"));
				return;
			}
			removed = true;
		}
		if(logMINOR) Logger.minor(this, "removeFrom() on "+this, new Exception("debug"));
		if(!container.ext().isStored(this)) {
			return;
		}
		container.activate(blockFetchContext, 1);
		blockFetchContext.removeFrom(container);
		if(deleteFetchContext) {
			container.activate(fetchContext, 1);
			fetchContext.removeFrom(container);
		}
		container.activate(clientMetadata, 1);
		clientMetadata.removeFrom(container);
		container.activate(decompressors, 1);
		container.delete(decompressors);
		finishSegments(container, context);
		if(crossCheckBlocks != 0) {
			for(int i=0;i<crossSegments.length;i++) {
				SplitFileFetcherCrossSegment segment = crossSegments[i];
				crossSegments[i] = null;
				container.activate(segment, 1);
				segment.removeFrom(container, context);
			}
		}
		container.activate(mainBloomFile, 5);
		container.activate(altBloomFile, 5);
		if(mainBloomFile != null && !mainBloomFile.delete() && mainBloomFile.exists())
			Logger.error(this, "Unable to delete main bloom file: "+mainBloomFile+" for "+this);
		if(altBloomFile != null && !altBloomFile.delete() && altBloomFile.exists())
			Logger.error(this, "Unable to delete alt bloom file: "+altBloomFile+" for "+this);
		container.delete(mainBloomFile);
		container.delete(altBloomFile);
		container.activate(cachedMainBloomFilter, Integer.MAX_VALUE);
		cachedMainBloomFilter.removeFrom(container);
		for(int i=0;i<cachedSegmentBloomFilters.length;i++) {
			container.activate(cachedSegmentBloomFilters[i], Integer.MAX_VALUE);
			cachedSegmentBloomFilters[i].removeFrom(container);
		}
		container.delete(this);
	}

	/** Call fetcherFinished() on all the segments. Necessary when we have cross-segment
	 * redundancy, because we cannot free the data blocks until the cross-segment encodes
	 * have finished.
	 * @param container
	 * @param context
	 */
	private void finishSegments(ObjectContainer container, ClientContext context) {
		for(int i=0;i<segments.length;i++) {
			SplitFileFetcherSegment segment = segments[i];
			segments[i] = null;
			if(persistent) container.activate(segment, 1);
			if(segment != null)
				segment.fetcherFinished(container, context);
		}
	}

	public boolean objectCanUpdate(ObjectContainer container) {
		if(hashCode == 0) {
			Logger.error(this, "Trying to update with hash 0 => already deleted! active="+container.ext().isActive(this)+" stored="+container.ext().isStored(this), new Exception("error"));
			return false;
		}
		synchronized(this) {
			if(removed) {
				Logger.error(this, "Trying to write but already removed", new Exception("error"));
				return false;
			}
		}
		return true;
	}

	public boolean objectCanNew(ObjectContainer container) {
		if(hashCode == 0) {
			Logger.error(this, "Trying to write with hash 0 => already deleted! active="+container.ext().isActive(this)+" stored="+container.ext().isStored(this), new Exception("error"));
			return false;
		}
		synchronized(this) {
			if(removed) {
				Logger.error(this, "Trying to write but already removed", new Exception("error"));
				return false;
			}
		}
		return true;
	}

	/** A cross-segment has completed. When all the cross-segments have completed, and 
	 * removeFrom() has been called, we call innerRemoveFrom() to finish removing the 
	 * fetcher from the database. If the splitfile is not persistent, we still need to 
	 * call finishSegments() on each. 
	 * @return True if we finished the fetcher (and removed the segment from the database
	 * or called finishSegments()). */
	public boolean onFinishedCrossSegment(ObjectContainer container, ClientContext context, SplitFileFetcherCrossSegment seg) {
		boolean allGone = true;
		for(int i=0;i<crossSegments.length;i++) {
			if(crossSegments[i] != null) {
				boolean active = true;
				if(persistent) {
					active = container.ext().isActive(crossSegments[i]);
					if(!active) container.activate(crossSegments[i], 1);
				}
				if(!crossSegments[i].isFinished()) {
					allGone = false;
					if(logMINOR) Logger.minor(this, "Waiting for "+crossSegments[i]);
				}
				if(!active) container.deactivate(crossSegments[i], 1);
				if(!allGone) break;
			}
		}
		synchronized(this) {
			if(persistent && !toRemove) {
				// Waiting for removeFrom().
				return false;
			}
		}
		if(allGone) {
			if(persistent)
				innerRemoveFrom(container, context);
			else
				finishSegments(container, context);
			return true;
		} else if(persistent)
			container.store(this);
		return false;
	}

	public void setCachedMainFilter(CountingBloomFilter filter) {
		this.cachedMainBloomFilter = filter;
	}

	public void setCachedSegFilters(BinaryBloomFilter[] segmentFilters) {
		this.cachedSegmentBloomFilters = segmentFilters;
	}

	public SplitFileFetcherKeyListener getListener() {
		return tempListener;
	}


}
package freenet.client.filter;

import java.net.MalformedURLException;
import java.net.URISyntaxException;

import freenet.client.filter.HTMLFilter.ParsedTag;
import freenet.clients.http.FProxyFetchTracker;
import freenet.clients.http.ToadletContext;
import freenet.clients.http.updateableelements.ImageElement;
import freenet.keys.FreenetURI;
import freenet.l10n.NodeL10n;
import freenet.support.HTMLEncoder;

/** This TagReplcaerCallback adds pushing support for freesites, and replaces their img's to pushed ones */
public class PushingTagReplacerCallback implements TagReplacerCallback {

	/** The FProxyFetchTracker */
	private FProxyFetchTracker	tracker;
	/** The maxSize used for fetching */
	private long				maxSize;
	/** The current ToadletContext */
	private ToadletContext		ctx;

	/**
	 * Constructor
	 * 
	 * @param tracker
	 *            - The FProxyFetchTracker
	 * @param maxSize
	 *            - The maxSize used for fetching
	 * @param ctx
	 *            - The current ToadletContext
	 */
	public PushingTagReplacerCallback(FProxyFetchTracker tracker, long maxSize, ToadletContext ctx) {
		this.tracker = tracker;
		this.maxSize = maxSize;
		this.ctx = ctx;
	}

	/**
	 * Returns the javascript code that initializes the l10n on the client side. It must be inserted to the page.
	 * 
	 * @return The javascript code that needs to be inserted in order to l10n work
	 */
	public static String getClientSideLocalizationScript() {
		StringBuilder l10nBuilder = new StringBuilder("var l10n={\n");
		boolean isNamePresentAtLeastOnce=false;
		for (String key : NodeL10n.getBase().getAllNamesWithPrefix("fproxy.push")) {
			l10nBuilder.append(key.substring("fproxy.push".length() + 1) + ": \"" + HTMLEncoder.encode(NodeL10n.getBase().getString(key)) + "\",\n");
			isNamePresentAtLeastOnce=true;
		}
		String l10n = isNamePresentAtLeastOnce?l10nBuilder.substring(0, l10nBuilder.length() - 2):l10nBuilder.toString();
		l10n = l10n.concat("\n};");
		return l10n;
	}

	public String processTag(ParsedTag pt, URIProcessor uriProcessor) {
		// If javascript or pushing is disabled, then it won't need pushing
		if (ctx.getContainer().isFProxyJavascriptEnabled() && ctx.getContainer().isFProxyWebPushingEnabled()) {
			if (pt.element.toLowerCase().compareTo("img") == 0) {
				// Img's needs to be replaced with pushed ImageElement's
				for (int i = 0; i < pt.unparsedAttrs.length; i++) {
					String attr = pt.unparsedAttrs[i];
					String name = attr.substring(0, attr.indexOf("="));
					String value = attr.substring(attr.indexOf("=") + 2, attr.length() - 1);
					if (name.compareTo("src") == 0) {
						String src;
						try {
							// We need absolute URI
							src = uriProcessor.makeURIAbsolute(uriProcessor.processURI(value, null, false, false));
						} catch (CommentException ce) {
							return null;
						} catch (URISyntaxException use) {
							return null;
						}
						if (src.startsWith("/")) {
							src = src.substring(1);
						}
						try {
							// Create the ImageElement
							return new ImageElement(tracker, new FreenetURI(src), maxSize, ctx, pt, true).generate();
						} catch (MalformedURLException mue) {
							return null;
						}
					}
				}
			} else if (pt.element.toLowerCase().compareTo("body") == 0 && pt.startSlash==true) {
				// After the <body>, we need to insert the requestId and the l10n script
				return "".concat(/*new XmlAlertElement(ctx).generate()*/"".concat("<input id=\"requestId\" type=\"hidden\" value=\"" + ctx.getUniqueId() + "\" name=\"requestId\"/>")).concat("<script type=\"text/javascript\" language=\"javascript\">".concat(getClientSideLocalizationScript()).concat("</script>")).concat("</body>");
			} else if (pt.element.toLowerCase().compareTo("head") == 0) {
				// After the <head>, we need to add GWT support
				return "<head><script type=\"text/javascript\" language=\"javascript\" src=\"/static/freenetjs/freenetjs.nocache.js\"></script><noscript><style> .jsonly {display:none;}</style></noscript><link href=\"/static/reset.css\" rel=\"stylesheet\" type=\"text/css\" />";
			}
		}
		return null;
	}

}
package freenet.node;

import java.lang.ref.WeakReference;
import java.util.HashSet;

import freenet.keys.Key;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;

/** Tracks recent requests for a specific key. If we have recently routed to a specific 
 * node, and failed, we should not route to it again, unless it is at a higher HTL. 
 * Different failures cause different timeouts. Similarly we track the nodes that have
 * requested the key, because for both sets of nodes, when we find the data we offer them
 * it; this greatly improves latency and efficiency for polling-based tools. For nodes
 * we have routed to, we keep up to HTL separate entries; for nodes we have received 
 * requests from, we keep only one entry.
 * 
 * SECURITY: All this could be a security risk if not regularly cleared - which it is,
 * of course: We forget about either kind of node after a fixed period, in 
 * cleanupRequested(), which the FailureTable calls regularly. Against a near-omnipotent 
 * attacker able to compromise nodes at will of course it is still a security risk to 
 * track anything but we have bigger problems at that level.
 * @author toad
 */
class FailureTableEntry implements TimedOutNodesList {
	
	/** The key */
	final Key key; // FIXME should this be stored compressed somehow e.g. just the routing key?
	/** Time of creation of this entry */
	long creationTime;
	/** Time we last received a request for the key */
	long receivedTime;
	/** Time we last received a DNF after sending a request for a key */
	long sentTime;
	/** WeakReference's to PeerNode's who have requested the key */
	WeakReference<PeerNode>[] requestorNodes;
	/** Times at which they requested it */
	long[] requestorTimes;
	/** Boot ID when they requested it. We don't send it to restarted nodes, as a 
	 * (weak, but useful if combined with other measures) protection against seizure. */
	long[] requestorBootIDs;
	short[] requestorHTLs;
	
	// FIXME Note that just because a node is in this list doesn't mean it DNFed or RFed.
	// We include *ALL* nodes we routed to here!
	/** WeakReference's to PeerNode's we have requested it from */
	WeakReference<PeerNode>[] requestedNodes;
	/** Their locations when we requested it. This may be needed in the future to
	 * determine whether to let a request through that we would otherwise have
	 * failed with RecentlyFailed, because the node we would route it to is closer
	 * to the target than any we've routed to in the past. */
	double[] requestedLocs;
	long[] requestedBootIDs;
	long[] requestedTimes;
	/** Timeouts for each node for purposes of RecentlyFailed. We accept what
	 * they say, subject to an upper limit, because we MUST NOT suppress too
	 * many requests, as that could lead to a self-sustaining key blocking. */
	long[] requestedTimeoutsRF;
	/** Timeouts for each node for purposes of per-node failure tables. We use
	 * our own estimates, based on time elapsed, for most failure modes; a fixed
	 * period for DNF and RecentlyFailed. */
	long[] requestedTimeoutsFT;
	
	short[] requestedTimeoutHTLs;
	
	private static volatile boolean logMINOR;
	
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback() {
			@Override
			public void shouldUpdate() {
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}
	
	/** We remember that a node has asked us for a key for up to an hour; after that, we won't offer the key, and
	 * if we receive an offer from that node, we will reject it */
	static final int MAX_TIME_BETWEEN_REQUEST_AND_OFFER = 60 * 60 * 1000;
	
        public static final long[] EMPTY_LONG_ARRAY = new long[0];
        public static final short[] EMPTY_SHORT_ARRAY = new short[0];
        public static final double[] EMPTY_DOUBLE_ARRAY = new double[0];
        @SuppressWarnings("unchecked")
        public static final WeakReference<PeerNode>[] EMPTY_WEAK_REFERENCE = new WeakReference[0];
        
	FailureTableEntry(Key key) {
		this.key = key.archivalCopy();
		if(key == null) throw new NullPointerException();
		long now = System.currentTimeMillis();
		creationTime = now;
		receivedTime = -1;
		sentTime = -1;
		requestorNodes = EMPTY_WEAK_REFERENCE;
		requestorTimes = EMPTY_LONG_ARRAY;
		requestorBootIDs = EMPTY_LONG_ARRAY;
		requestorHTLs = EMPTY_SHORT_ARRAY;
		requestedNodes = EMPTY_WEAK_REFERENCE;
		requestedLocs = EMPTY_DOUBLE_ARRAY;
		requestedBootIDs = EMPTY_LONG_ARRAY;
		requestedTimes = EMPTY_LONG_ARRAY;
		requestedTimeoutsRF = EMPTY_LONG_ARRAY;
		requestedTimeoutsFT = EMPTY_LONG_ARRAY;
		requestedTimeoutHTLs = EMPTY_SHORT_ARRAY;
	}
	
	public synchronized void failedTo(PeerNode routedTo, int rfTimeout, int ftTimeout, long now, short htl) {
		if(logMINOR) {
			Logger.minor(this, "Failed sending request to "+routedTo.shortToString()+" : timeout "+rfTimeout+" / "+ftTimeout);
		}
		int idx = addRequestedFrom(routedTo, htl, now);
		if(rfTimeout > 0) {
			long curTimeoutTime = requestedTimeoutsRF[idx];
			long newTimeoutTime = now + rfTimeout;
			if(newTimeoutTime > curTimeoutTime) {
				requestedTimeoutsRF[idx] = newTimeoutTime;
				requestedTimeoutHTLs[idx] = htl;
			}
		}
		if(ftTimeout > 0) {
			long curTimeoutTime = requestedTimeoutsFT[idx];
			long newTimeoutTime = now +  ftTimeout;
			if(newTimeoutTime > curTimeoutTime) {
				requestedTimeoutsFT[idx] = newTimeoutTime;
				requestedTimeoutHTLs[idx] = htl;
			}
		}
	}

	// These are rather low level, in an attempt to absolutely minimize memory usage...
	// The two methods have almost identical code/logic.
	// Dunno if there's a more elegant way of dealing with this which doesn't significantly increase
	// per entry byte cost.
	// Note also this will generate some churn...
	
	synchronized int addRequestor(PeerNode requestor, long now, short origHTL) {
		if(logMINOR) Logger.minor(this, "Adding requestors: "+requestor+" at "+now);
		receivedTime = now;
		boolean includedAlready = false;
		int nulls = 0;
		int ret = -1;
		for(int i=0;i<requestorNodes.length;i++) {
			PeerNode got = requestorNodes[i] == null ? null : requestorNodes[i].get();
			// No longer subscribed if they have rebooted, or expired
			if(got == requestor) {
				// Update existing entry
				includedAlready = true;
				requestorTimes[i] = now;
				requestorBootIDs[i] = requestor.getBootID();
				requestorHTLs[i] = origHTL;
				ret = i;
				break;
			} else if(got != null && 
					(got.getBootID() != requestorBootIDs[i] || now - requestorTimes[i] > MAX_TIME_BETWEEN_REQUEST_AND_OFFER)) {
				requestorNodes[i] = null;
				got = null;
			}
			if(got == null)
				nulls++;
		}
		if(nulls == 0 && includedAlready) return ret;
		int notIncluded = includedAlready ? 0 : 1;
		// Because weak, these can become null; doesn't matter, but we want to minimise memory usage
		if(nulls == 1 && !includedAlready) {
			// Nice special case
			for(int i=0;i<requestorNodes.length;i++) {
				if(requestorNodes[i] == null || requestorNodes[i].get() == null) {
					requestorNodes[i] = requestor.myRef;
					requestorTimes[i] = now;
					requestorBootIDs[i] = requestor.getBootID();
					requestorHTLs[i] = origHTL;
					return i;
				}
			}
		}
        @SuppressWarnings("unchecked")
		WeakReference<PeerNode>[] newRequestorNodes = new WeakReference[requestorNodes.length+notIncluded-nulls];
		long[] newRequestorTimes = new long[requestorNodes.length+notIncluded-nulls];
		long[] newRequestorBootIDs = new long[requestorNodes.length+notIncluded-nulls];
		short[] newRequestorHTLs = new short[requestorNodes.length+notIncluded-nulls];
		int toIndex = 0;
		
		for(int i=0;i<requestorNodes.length;i++) {
			WeakReference<PeerNode> ref = requestorNodes[i];
			PeerNode pn = ref == null ? null : ref.get();
			if(pn == null) continue;
			if(pn == requestor) ret = toIndex;
			newRequestorNodes[toIndex] = requestorNodes[i];
			newRequestorTimes[toIndex] = requestorTimes[i];
			newRequestorBootIDs[toIndex] = requestorBootIDs[i];
			newRequestorHTLs[toIndex] = requestorHTLs[i];
			toIndex++;
		}
		
		if(!includedAlready) {
			newRequestorNodes[toIndex] = requestor.myRef;
			newRequestorTimes[toIndex] = now;
			newRequestorBootIDs[toIndex] = requestor.getBootID();
			newRequestorHTLs[toIndex] = origHTL;
			ret = toIndex;
			toIndex++;
		}
		
		for(int i=toIndex;i<newRequestorNodes.length;i++) newRequestorNodes[i] = null;
		if(toIndex > newRequestorNodes.length + 2) {
			@SuppressWarnings("unchecked")
			WeakReference<PeerNode>[] newNewRequestorNodes = new WeakReference[toIndex];
			long[] newNewRequestorTimes = new long[toIndex];
			long[] newNewRequestorBootIDs = new long[toIndex];
			short[] newNewRequestorHTLs = new short[toIndex];
			System.arraycopy(newRequestorNodes, 0, newNewRequestorNodes, 0, toIndex);
			System.arraycopy(newRequestorTimes, 0, newNewRequestorTimes, 0, toIndex);
			System.arraycopy(newRequestorBootIDs, 0, newNewRequestorBootIDs, 0, toIndex);
			System.arraycopy(newRequestorHTLs, 0, newNewRequestorHTLs, 0, toIndex);
			newRequestorNodes = newNewRequestorNodes;
			newRequestorTimes = newNewRequestorTimes;
			newRequestorBootIDs = newNewRequestorBootIDs;
			newRequestorHTLs = newNewRequestorHTLs;
		}
		requestorNodes = newRequestorNodes;
		requestorTimes = newRequestorTimes;
		requestorBootIDs = newRequestorBootIDs;
		requestorHTLs = newRequestorHTLs;
		
		return ret;
	}

	/** Add a requested from entry to the node. If there already is one reuse it but only
	 * if the HTL matches. Return the index so we can update timeouts etc.
	 * @param requestedFrom The node we have routed the request to.
	 * @param htl The HTL at which the request was sent.
	 * @param now The current time.
	 * @return The index of the new or old entry.
	 */
	private synchronized int addRequestedFrom(PeerNode requestedFrom, short htl, long now) {
		if(logMINOR) Logger.minor(this, "Adding requested from: "+requestedFrom+" at "+now);
		sentTime = now;
		boolean includedAlready = false;
		int nulls = 0;
		int ret = -1;
		for(int i=0;i<requestedNodes.length;i++) {
			PeerNode got = requestedNodes[i] == null ? null : requestedNodes[i].get();
			if(got == requestedFrom && (requestedTimeoutsRF[i] == -1 || requestedTimeoutsFT[i] == -1 || requestedTimeoutHTLs[i] == htl)) {
				includedAlready = true;
				requestedLocs[i] = requestedFrom.getLocation();
				requestedBootIDs[i] = requestedFrom.getBootID();
				requestedTimes[i] = now;
				ret = i;
			} else if(got != null && 
					(got.getBootID() != requestedBootIDs[i] || now - requestedTimes[i] > MAX_TIME_BETWEEN_REQUEST_AND_OFFER)) {
				requestedNodes[i] = null;
				got = null;
			}
			if(got == null)
				nulls++;
		}
		if(includedAlready && nulls == 0) return ret;
		int notIncluded = includedAlready ? 0 : 1;
		// Because weak, these can become null; doesn't matter, but we want to minimise memory usage
		if(nulls == 1 && !includedAlready) {
			// Nice special case
			for(int i=0;i<requestedNodes.length;i++) {
				if(requestedNodes[i] == null || requestedNodes[i].get() == null) {
					requestedNodes[i] = requestedFrom.myRef;
					requestedLocs[i] = requestedFrom.getLocation();
					requestedBootIDs[i] = requestedFrom.getBootID();
					requestedTimes[i] = now;
					requestedTimeoutsRF[i] = -1;
					requestedTimeoutsFT[i] = -1;
					requestedTimeoutHTLs[i] = (short) -1;
					return i;
				}
			}
		}
		@SuppressWarnings("unchecked")
		WeakReference<PeerNode>[] newRequestedNodes = new WeakReference[requestedNodes.length+notIncluded-nulls];
		double[] newRequestedLocs = new double[requestedNodes.length+notIncluded-nulls];
		long[] newRequestedBootIDs = new long[requestedNodes.length+notIncluded-nulls];
		long[] newRequestedTimes = new long[requestedNodes.length+notIncluded-nulls];
		long[] newRequestedTimeoutsFT = new long[requestedNodes.length+notIncluded-nulls];
		long[] newRequestedTimeoutsRF = new long[requestedNodes.length+notIncluded-nulls];
		short[] newRequestedTimeoutHTLs = new short[requestedNodes.length+notIncluded-nulls];

		int toIndex = 0;
		for(int i=0;i<requestedNodes.length;i++) {
			WeakReference<PeerNode> ref = requestedNodes[i];
			PeerNode pn = ref == null ? null : ref.get();
			if(pn == null) continue;
			if(pn == requestedFrom) ret = toIndex;
			newRequestedNodes[toIndex] = requestedNodes[i];
			newRequestedTimes[toIndex] = requestedTimes[i];
			newRequestedBootIDs[toIndex] = requestedBootIDs[i];
			newRequestedLocs[toIndex] = requestedLocs[i];
			newRequestedTimeoutsFT[toIndex] = requestedTimeoutsFT[i];
			newRequestedTimeoutsRF[toIndex] = requestedTimeoutsRF[i];
			newRequestedTimeoutHTLs[toIndex] = requestedTimeoutHTLs[i];
			toIndex++;
		}
		
		if(!includedAlready) {
			ret = toIndex;
			newRequestedNodes[toIndex] = requestedFrom.myRef;
			newRequestedTimes[toIndex] = now;
			newRequestedBootIDs[toIndex] = requestedFrom.getBootID();
			newRequestedLocs[toIndex] = requestedFrom.getLocation();
			newRequestedTimeoutsFT[toIndex] = -1;
			newRequestedTimeoutsRF[toIndex] = -1;
			newRequestedTimeoutHTLs[toIndex] = (short) -1;
			ret = toIndex;
			toIndex++;
		}
		
		for(int i=toIndex;i<newRequestedNodes.length;i++) newRequestedNodes[i] = null;
		if(toIndex > newRequestedNodes.length + 2) {
	        @SuppressWarnings("unchecked")
			WeakReference<PeerNode>[] newNewRequestedNodes = new WeakReference[toIndex];
			double[] newNewRequestedLocs = new double[toIndex];
			long[] newNewRequestedBootIDs = new long[toIndex];
			long[] newNewRequestedTimes = new long[toIndex];
			long[] newNewRequestedTimeoutsFT = new long[toIndex];
			long[] newNewRequestedTimeoutsRF = new long[toIndex];
			short[] newNewRequestedTimeoutHTLs = new short[toIndex];
			System.arraycopy(newRequestedNodes, 0, newNewRequestedNodes, 0, toIndex);
			System.arraycopy(newRequestedLocs, 0, newNewRequestedLocs, 0, toIndex);
			System.arraycopy(newRequestedBootIDs, 0, newNewRequestedBootIDs, 0, toIndex);
			System.arraycopy(newRequestedTimes, 0, newNewRequestedTimes, 0, toIndex);
			System.arraycopy(newRequestedTimeoutsRF, 0, newNewRequestedTimeoutsRF, 0, toIndex);
			System.arraycopy(newRequestedTimeoutsFT, 0, newNewRequestedTimeoutsFT, 0, toIndex);
			System.arraycopy(newRequestedTimeoutHTLs, 0, newNewRequestedTimeoutHTLs, 0, toIndex);
			newRequestedNodes = newNewRequestedNodes;
			newRequestedLocs = newNewRequestedLocs;
			newRequestedBootIDs = newNewRequestedBootIDs;
			newRequestedTimes = newNewRequestedTimes;
			newRequestedTimeoutsRF = newNewRequestedTimeoutsRF;
			newRequestedTimeoutsFT = newNewRequestedTimeoutsFT;
			newRequestedTimeoutHTLs = newNewRequestedTimeoutHTLs;
		}
		requestedNodes = newRequestedNodes;
		requestedLocs = newRequestedLocs;
		requestedBootIDs = newRequestedBootIDs;
		requestedTimes = newRequestedTimes;
		requestedTimeoutsRF = newRequestedTimeoutsRF;
		requestedTimeoutsFT = newRequestedTimeoutsFT;
		requestedTimeoutHTLs = newRequestedTimeoutHTLs;
		
		return ret;
	}

	/** Offer this key to all the nodes that have requested it, and all the nodes it has been requested from.
	 * Called after a) the data has been stored, and b) this entry has been removed from the FT */
	public synchronized void offer() {
		HashSet<PeerNode> set = new HashSet<PeerNode>();
		if(logMINOR) Logger.minor(this, "Sending offers to nodes which requested the key from us: ("+requestorNodes.length+") for "+key);
		for(int i=0;i<requestorNodes.length;i++) {
			WeakReference<PeerNode> ref = requestorNodes[i];
			if(ref == null) continue;
			PeerNode pn = ref.get();
			if(pn == null) continue;
			if(pn.getBootID() != requestorBootIDs[i]) continue;
			if(!set.add(pn)) {
				Logger.error(this, "Node is in requestorNodes twice: "+pn);
			}
			if(logMINOR) Logger.minor(this, "Offering to "+pn);
			pn.offer(key);
		}
		if(logMINOR) Logger.minor(this, "Sending offers to nodes which we sent the key to: ("+requestedNodes.length+") for "+key);
		for(int i=0;i<requestedNodes.length;i++) {
			WeakReference<PeerNode> ref = requestedNodes[i];
			if(ref == null) continue;
			PeerNode pn = ref.get();
			if(pn == null) continue;
			if(pn.getBootID() != requestedBootIDs[i]) continue;
			if(!set.add(pn)) continue;
			if(logMINOR) Logger.minor(this, "Offering to "+pn);
			pn.offer(key);
		}
	}

	/**
	 * Has any node asked for this key?
	 */
	public synchronized boolean othersWant(PeerNode peer) {
		boolean anyValid = false;
		for(int i=0;i<requestorNodes.length;i++) {
			WeakReference<PeerNode> ref = requestorNodes[i];
			if(ref == null) continue;
			PeerNode pn = ref.get();
			if(pn == null) {
				requestorNodes[i] = null;
				continue;
			}
			long bootID = pn.getBootID();
			if(bootID != requestorBootIDs[i]) {
				requestorNodes[i] = null;
				continue;
			}
			anyValid = true;
		}
		if(!anyValid) {
			requestorNodes = EMPTY_WEAK_REFERENCE;
			requestorTimes = requestorBootIDs = EMPTY_LONG_ARRAY;
			requestorHTLs = EMPTY_SHORT_ARRAY;
		}
		return anyValid;
	}

	/**
	 * Has this peer asked us for the key?
	 */
	public synchronized boolean askedByPeer(PeerNode peer, long now) {
		boolean anyValid = false;
		boolean ret = false;
		for(int i=0;i<requestorNodes.length;i++) {
			WeakReference<PeerNode> ref = requestorNodes[i];
			if(ref == null) continue;
			PeerNode pn = ref.get();
			if(pn == null) {
				requestorNodes[i] = null;
				continue;
			}
			long bootID = pn.getBootID();
			if(bootID != requestorBootIDs[i]) {
				requestorNodes[i] = null;
				continue;
			}
			if(now - requestorTimes[i] < MAX_TIME_BETWEEN_REQUEST_AND_OFFER) {
				if(pn == peer) ret = true;
				anyValid = true;
			} 
		}
		if(!anyValid) {
			requestorNodes = EMPTY_WEAK_REFERENCE;
			requestorTimes = requestorBootIDs = EMPTY_LONG_ARRAY;;
			requestorHTLs = EMPTY_SHORT_ARRAY;
		}
		return ret;
	}

	/**
	 * Have we asked this peer for the key?
	 */
	public synchronized boolean askedFromPeer(PeerNode peer, long now) {
		boolean anyValid = false;
		boolean ret = false;
		for(int i=0;i<requestedNodes.length;i++) {
			WeakReference<PeerNode> ref = requestedNodes[i];
			if(ref == null) continue;
			PeerNode pn = ref.get();
			if(pn == null) {
				requestedNodes[i] = null;
				continue;
			}
			long bootID = pn.getBootID();
			if(bootID != requestedBootIDs[i]) {
				requestedNodes[i] = null;
				continue;
			}
			anyValid = true;
			if(now - requestedTimes[i] < MAX_TIME_BETWEEN_REQUEST_AND_OFFER) {
				if(pn == peer) ret = true;
				anyValid = true;
			}
		}
		if(!anyValid) {
			requestedNodes = EMPTY_WEAK_REFERENCE;
			requestedTimes = requestedBootIDs = requestedTimeoutsRF = requestedTimeoutsFT = EMPTY_LONG_ARRAY;
			requestedTimeoutHTLs =EMPTY_SHORT_ARRAY;
		}
		return ret;
	}

	public synchronized boolean isEmpty(long now) {
		if(requestedNodes.length > 0) return false;
		if(requestorNodes.length > 0) return false;
		return true;
	}

	/** Get the timeout time for the given peer, taking HTL into account.
	 * If there was a timeout at HTL 1, and we are now sending a request at
	 * HTL 2, we ignore the timeout. */
	public synchronized long getTimeoutTime(PeerNode peer, short htl, long now, boolean forPerNodeFailureTables) {
		long timeout = -1;
		for(int i=0;i<requestedNodes.length;i++) {
			WeakReference<PeerNode> ref = requestedNodes[i];
			if(ref != null && ref.get() == peer) {
				if(requestedTimeoutHTLs[i] >= htl) {
					long thisTimeout = forPerNodeFailureTables ? requestedTimeoutsFT[i] : requestedTimeoutsRF[i];
					if(thisTimeout > timeout && thisTimeout > now)
						timeout = thisTimeout;
				}
			}
		}
		return timeout;
	}
	
	public synchronized boolean cleanup() {
		long now = System.currentTimeMillis(); // don't pass in as a pass over the whole FT may take a while. get it in the method.
		
		boolean empty = cleanupRequestor(now);
		empty &= cleanupRequested(now);
		return empty;
	}

	private boolean cleanupRequestor(long now) {
		boolean empty = true;
		int x = 0;
		for(int i=0;i<requestorNodes.length;i++) {
			WeakReference<PeerNode> ref = requestorNodes[i];
			if(ref == null) continue;
			PeerNode pn = ref.get();
			if(pn == null) continue;
			long bootID = pn.getBootID();
			if(bootID != requestorBootIDs[i]) continue;
			if(!pn.isConnected()) continue;
			if(now - requestorTimes[i] > MAX_TIME_BETWEEN_REQUEST_AND_OFFER) continue;
			empty = false;
			requestorNodes[x] = requestorNodes[i];
			requestorTimes[x] = requestorTimes[i];
			requestorBootIDs[x] = requestorBootIDs[i];
			requestorHTLs[x] = requestorHTLs[i];
			x++;
		}
		if(x < requestorNodes.length) {
			@SuppressWarnings("unchecked")
			WeakReference<PeerNode>[] newRequestorNodes = new WeakReference[x];
			long[] newRequestorTimes = new long[x];
			long[] newRequestorBootIDs = new long[x];
			short[] newRequestorHTLs = new short[x];
			System.arraycopy(requestorNodes, 0, newRequestorNodes, 0, x);
			System.arraycopy(requestorTimes, 0, newRequestorTimes, 0, x);
			System.arraycopy(requestorBootIDs, 0, newRequestorBootIDs, 0, x);
			System.arraycopy(requestorHTLs, 0, newRequestorHTLs, 0, x);
			requestorNodes = newRequestorNodes;
			requestorTimes = newRequestorTimes;
			requestorBootIDs = newRequestorBootIDs;
			requestorHTLs = newRequestorHTLs;
		}
		
		return empty;
	}
	
	private boolean cleanupRequested(long now) {
		boolean empty = true;
		int x = 0;
		for(int i=0;i<requestedNodes.length;i++) {
			WeakReference<PeerNode> ref = requestedNodes[i];
			if(ref == null) continue;
			PeerNode pn = ref.get();
			if(pn == null) continue;
			long bootID = pn.getBootID();
			if(bootID != requestedBootIDs[i]) continue;
			if(!pn.isConnected()) continue;
			if(now - requestedTimes[i] > MAX_TIME_BETWEEN_REQUEST_AND_OFFER) continue;
			empty = false;
			requestedNodes[x] = requestedNodes[i];
			requestedTimes[x] = requestedTimes[i];
			requestedBootIDs[x] = requestedBootIDs[i];
			requestedLocs[x] = requestedLocs[i];
			if(now < requestedTimeoutsRF[x] || now < requestedTimeoutsFT[x]) { 
				requestedTimeoutsRF[x] = requestedTimeoutsRF[i];
				requestedTimeoutsFT[x] = requestedTimeoutsFT[i];
				requestedTimeoutHTLs[x] = requestedTimeoutHTLs[i];
			} else {
				requestedTimeoutsRF[x] = -1;
				requestedTimeoutsFT[x] = -1;
				requestedTimeoutHTLs[x] = (short)-1;
			}
			x++;
		}
		if(x < requestedNodes.length) {
			@SuppressWarnings("unchecked")
			WeakReference<PeerNode>[] newRequestedNodes = new WeakReference[x];
			long[] newRequestedTimes = new long[x];
			long[] newRequestedBootIDs = new long[x];
			double[] newRequestedLocs = new double[x];
			long[] newRequestedTimeoutsRF = new long[x];
			long[] newRequestedTimeoutsFT = new long[x];
			short[] newRequestedTimeoutHTLs = new short[x];
			System.arraycopy(requestedNodes, 0, newRequestedNodes, 0, x);
			System.arraycopy(requestedTimes, 0, newRequestedTimes, 0, x);
			System.arraycopy(requestedBootIDs, 0, newRequestedBootIDs, 0, x);
			System.arraycopy(requestedLocs, 0, newRequestedLocs, 0, x);
			System.arraycopy(requestedTimeoutsRF, 0, newRequestedTimeoutsRF, 0, x);
			System.arraycopy(requestedTimeoutsFT, 0, newRequestedTimeoutsFT, 0, x);
			System.arraycopy(requestedTimeoutHTLs, 0, newRequestedTimeoutHTLs, 0, x);
			requestedNodes = newRequestedNodes;
			requestedTimes = newRequestedTimes;
			requestedBootIDs = newRequestedBootIDs;
			requestedLocs = newRequestedLocs;
			requestedTimeoutsRF = newRequestedTimeoutsRF;
			requestedTimeoutsFT = newRequestedTimeoutsFT;
			requestedTimeoutHTLs = newRequestedTimeoutHTLs;
		}
		return empty;
	}

	public boolean isEmpty() {
		return isEmpty(System.currentTimeMillis());
	}

	public synchronized short minRequestorHTL(short htl) {
		long now = System.currentTimeMillis();
		boolean anyValid = false;
		for(int i=0;i<requestorNodes.length;i++) {
			WeakReference<PeerNode> ref = requestorNodes[i];
			if(ref == null) continue;
			PeerNode pn = ref.get();
			if(pn == null) {
				requestorNodes[i] = null;
				continue;
			}
			long bootID = pn.getBootID();
			if(bootID != requestorBootIDs[i]) {
				requestorNodes[i] = null;
				continue;
			}
			if(now - requestorTimes[i] < MAX_TIME_BETWEEN_REQUEST_AND_OFFER) {
				if(requestorHTLs[i] < htl) htl = requestorHTLs[i];
			}
			anyValid = true;
		}
		if(!anyValid) {
			requestorNodes = EMPTY_WEAK_REFERENCE;
			requestorTimes = requestorBootIDs = EMPTY_LONG_ARRAY;;
			requestorHTLs = EMPTY_SHORT_ARRAY;
		}
		return htl;
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.keys;

/**
 * @author amphibian
 * 
 * Exception thrown when a CHK encoding fails.
 * Specifically, it is thrown when the data is too big to encode.
 */
public class CHKEncodeException extends KeyEncodeException {
	private static final long serialVersionUID = -1;
    public CHKEncodeException() {
        super();
    }

    public CHKEncodeException(String message) {
        super(message);
    }

    public CHKEncodeException(String message, Throwable cause) {
        super(message, cause);
    }

    public CHKEncodeException(Throwable cause) {
        super(cause);
    }
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.keys;

import java.io.IOException;
import java.net.MalformedURLException;
import java.security.MessageDigest;

import net.i2p.util.NativeBigInteger;

import freenet.support.math.MersenneTwister;

import com.db4o.ObjectContainer;

import freenet.crypt.DSA;
import freenet.crypt.DSAGroup;
import freenet.crypt.DSAPrivateKey;
import freenet.crypt.DSAPublicKey;
import freenet.crypt.DSASignature;
import freenet.crypt.Global;
import freenet.crypt.PCFBMode;
import freenet.crypt.RandomSource;
import freenet.crypt.SHA256;
import freenet.crypt.UnsupportedCipherException;
import freenet.crypt.ciphers.Rijndael;
import freenet.keys.Key.Compressed;
import freenet.support.api.Bucket;
import freenet.support.compress.InvalidCompressionCodecException;

/** A ClientSSK that has a private key and therefore can be inserted. */
public class InsertableClientSSK extends ClientSSK {

	public final DSAPrivateKey privKey;
	
	public InsertableClientSSK(String docName, byte[] pubKeyHash, DSAPublicKey pubKey, DSAPrivateKey privKey, byte[] cryptoKey, byte cryptoAlgorithm) throws MalformedURLException {
		super(docName, pubKeyHash, getExtraBytes(cryptoAlgorithm), pubKey, cryptoKey);
		if(pubKey == null) throw new NullPointerException();
		this.privKey = privKey;
	}
	
	public static InsertableClientSSK create(FreenetURI uri) throws MalformedURLException {
		if(uri.getKeyType().equalsIgnoreCase("KSK"))
			return ClientKSK.create(uri);

		if(uri.getRoutingKey() == null)
			throw new MalformedURLException("Insertable SSK URIs must have a private key!: "+uri);
		if(uri.getCryptoKey() == null)
			throw new MalformedURLException("Insertable SSK URIs must have a private key!: "+uri);
		
		byte keyType;

		byte[] extra = uri.getExtra();
		if(uri.getKeyType().equals("SSK")) {
			if(extra == null)
				throw new MalformedURLException("Inserting pre-1010 keys not supported");
			// Formatted exactly as ,extra on fetching
			if(extra.length < 5)
				throw new MalformedURLException("SSK private key ,extra too short");
			if(extra[1] != 1) {
				throw new MalformedURLException("SSK not a private key");
			}
			keyType = extra[2];
			if(keyType != Key.ALGO_AES_PCFB_256_SHA256)
				throw new MalformedURLException("Unrecognized crypto type in SSK private key");
		}
		else {
			throw new MalformedURLException("Not a valid SSK insert URI type: "+uri.getKeyType());
		}
		
		if((uri.getDocName() == null) || (uri.getDocName().length() == 0))
			throw new MalformedURLException("SSK URIs must have a document name (to avoid ambiguity)");
		DSAGroup g = Global.DSAgroupBigA;
		DSAPrivateKey privKey = new DSAPrivateKey(new NativeBigInteger(1, uri.getRoutingKey()), g);
		DSAPublicKey pubKey = new DSAPublicKey(g, privKey);
		byte[] pkHash = pubKey.asBytesHash();
		return new InsertableClientSSK(uri.getDocName(), pkHash, pubKey, privKey, uri.getCryptoKey(), keyType);
	}
	
	public ClientSSKBlock encode(Bucket sourceData, boolean asMetadata, boolean dontCompress, short alreadyCompressedCodec, long sourceLength, RandomSource r, String compressordescriptor, boolean pre1254) throws SSKEncodeException, IOException, InvalidCompressionCodecException {
		byte[] compressedData;
		short compressionAlgo;
		try {
			Compressed comp = Key.compress(sourceData, dontCompress, alreadyCompressedCodec, sourceLength, ClientSSKBlock.MAX_DECOMPRESSED_DATA_LENGTH, SSKBlock.DATA_LENGTH, true, compressordescriptor, pre1254);
			compressedData = comp.compressedData;
			compressionAlgo = comp.compressionAlgorithm;
		} catch (KeyEncodeException e) {
			throw new SSKEncodeException(e.getMessage(), e);
		}
		// Pad it
		MessageDigest md256 = SHA256.getMessageDigest();
		try {
			byte[] data;
			// First pad it
			if (compressedData.length != SSKBlock.DATA_LENGTH) {
				// Hash the data
				if (compressedData.length != 0)
					md256.update(compressedData);
				byte[] digest = md256.digest();
				MersenneTwister mt = new MersenneTwister(digest);
				data = new byte[SSKBlock.DATA_LENGTH];
				if (compressedData.length > data.length) {
					throw new RuntimeException("compressedData.length = " + compressedData.length + " but data.length="
							+ data.length);
				}
				System.arraycopy(compressedData, 0, data, 0, compressedData.length);
				byte[] randomBytes = new byte[SSKBlock.DATA_LENGTH - compressedData.length];
				mt.nextBytes(randomBytes);
				System.arraycopy(randomBytes, 0, data, compressedData.length, SSKBlock.DATA_LENGTH
						- compressedData.length);
			} else {
				data = compressedData;
			}

			// Implicit hash of data
			byte[] origDataHash = md256.digest(data);

			Rijndael aes;
			try {
				aes = new Rijndael(256, 256);
			} catch (UnsupportedCipherException e) {
				throw new Error("256/256 Rijndael not supported!");
			}

			// Encrypt data. Data encryption key = H(plaintext data).

			aes.initialize(origDataHash);
			PCFBMode pcfb = PCFBMode.create(aes, origDataHash);

			pcfb.blockEncipher(data, 0, data.length);

			byte[] encryptedDataHash = md256.digest(data);

			// Create headers

			byte[] headers = new byte[SSKBlock.TOTAL_HEADERS_LENGTH];
			// First two bytes = hash ID
			int x = 0;
			headers[x++] = (byte) (KeyBlock.HASH_SHA256 >> 8);
			headers[x++] = (byte) (KeyBlock.HASH_SHA256);
			// Then crypto ID
			headers[x++] = (byte) (Key.ALGO_AES_PCFB_256_SHA256 >> 8);
			headers[x++] = Key.ALGO_AES_PCFB_256_SHA256;
			// Then E(H(docname))
			// Copy to headers
			System.arraycopy(ehDocname, 0, headers, x, ehDocname.length);
			x += ehDocname.length;
			// Now the encrypted headers
			byte[] encryptedHeaders = new byte[SSKBlock.ENCRYPTED_HEADERS_LENGTH];
			System.arraycopy(origDataHash, 0, encryptedHeaders, 0, origDataHash.length);
			int y = origDataHash.length;
			short len = (short) compressedData.length;
			if (asMetadata)
				len |= 32768;
			encryptedHeaders[y++] = (byte) (len >> 8);
			encryptedHeaders[y++] = (byte) len;
			encryptedHeaders[y++] = (byte) (compressionAlgo >> 8);
			encryptedHeaders[y++] = (byte) compressionAlgo;
			if (encryptedHeaders.length != y)
				throw new IllegalStateException("Have more bytes to generate encoding SSK");
			aes.initialize(cryptoKey);
			pcfb.reset(ehDocname);
			pcfb.blockEncipher(encryptedHeaders, 0, encryptedHeaders.length);
			System.arraycopy(encryptedHeaders, 0, headers, x, encryptedHeaders.length);
			x += encryptedHeaders.length;
			// Generate implicit overall hash.
			md256.update(headers, 0, x);
			md256.update(encryptedDataHash);
			byte[] overallHash = md256.digest();
			// Now sign it
			DSASignature sig = DSA.sign(pubKey.getGroup(), privKey, new NativeBigInteger(1, overallHash), r);
			// Pack R and S into 32 bytes each, and copy to headers.

			// Then create and return the ClientSSKBlock.
			byte[] rBuf = truncate(sig.getR().toByteArray(), SSKBlock.SIG_R_LENGTH);
			byte[] sBuf = truncate(sig.getS().toByteArray(), SSKBlock.SIG_S_LENGTH);
			System.arraycopy(rBuf, 0, headers, x, rBuf.length);
			x += rBuf.length;
			System.arraycopy(sBuf, 0, headers, x, sBuf.length);
			x += sBuf.length;
			if (x != SSKBlock.TOTAL_HEADERS_LENGTH)
				throw new IllegalStateException("Too long");
			try {
				return new ClientSSKBlock(data, headers, this, true);
			} catch (SSKVerifyException e) {
				throw (AssertionError)new AssertionError("Impossible encoding error").initCause(e);
			}
		} finally {
			SHA256.returnMessageDigest(md256);
		}
	}

	private byte[] truncate(byte[] bs, int len) {
		if(bs.length == len)
			return bs;
		else if (bs.length < len) {
			byte[] buf = new byte[len];
			System.arraycopy(bs, 0, buf, len - bs.length, bs.length);
			return buf;
		} else { // if (bs.length > len) {
			for(int i=0;i<(bs.length-len);i++) {
				if(bs[i] != 0)
					throw new IllegalStateException("Cannot truncate");
			}
			byte[] buf = new byte[len];
			System.arraycopy(bs, (bs.length-len), buf, 0, len);
			return buf;
		}
	}

	public static InsertableClientSSK createRandom(RandomSource r, String docName) {
		byte[] ckey = new byte[CRYPTO_KEY_LENGTH];
		r.nextBytes(ckey);
		DSAGroup g = Global.DSAgroupBigA;
		DSAPrivateKey privKey = new DSAPrivateKey(g, r);
		DSAPublicKey pubKey = new DSAPublicKey(g, privKey);
		try {
			byte[] pkHash = SHA256.digest(pubKey.asBytes());
			return new InsertableClientSSK(docName, pkHash, pubKey, privKey, ckey, 
					Key.ALGO_AES_PCFB_256_SHA256);
		} catch (MalformedURLException e) {
			throw new Error(e);
		}
	}

	public FreenetURI getInsertURI() {
		return new FreenetURI("SSK", docName, privKey.getX().toByteArray(), cryptoKey, getInsertExtraBytes());
	}

	private byte[] getInsertExtraBytes() {
		byte[] extra = getExtraBytes();
		extra[1] = 1; // insert
		return extra;
	}

	public DSAGroup getCryptoGroup() {
		return Global.DSAgroupBigA;
	}
	
	@Override
	public void removeFrom(ObjectContainer container) {
		container.activate(privKey, 5);
		privKey.removeFrom(container);
		super.removeFrom(container);
	}
	
}
package freenet.clients.http.ajaxpush;

import java.io.IOException;
import java.net.URI;

import freenet.client.HighLevelSimpleClient;
import freenet.clients.http.PageNode;
import freenet.clients.http.RedirectException;
import freenet.clients.http.Toadlet;
import freenet.clients.http.ToadletContext;
import freenet.clients.http.ToadletContextClosedException;
import freenet.clients.http.updateableelements.TesterElement;
import freenet.support.api.HTTPRequest;

/** This toadlet provides a simple page with pushed elements, making it suitable for automated tests. */
public class PushTesterToadlet extends Toadlet {

	public PushTesterToadlet(HighLevelSimpleClient client) {
		super(client);
	}

	public void handleMethodGET(URI uri, HTTPRequest req, ToadletContext ctx) throws ToadletContextClosedException, IOException, RedirectException {
		PageNode pageNode = ctx.getPageMaker().getPageNode("Push tester", false, ctx);
		for (int i = 0; i < 600; i++) {
			pageNode.content.addChild(new TesterElement(ctx, String.valueOf(i), 100));
		}
		writeHTMLReply(ctx, 200, "OK", pageNode.outer.generate());
	}

	@Override
	public String path() {
		return "/pushtester/";
	}

}
package freenet.clients.http.bookmark;

import freenet.l10n.NodeL10n;
import freenet.support.SimpleFieldSet;

public abstract class Bookmark {

	protected String name;

	public String getName() {
		return name;
	}

	protected void setName(String s) {
		name = (s.length() > 0 ? s : NodeL10n.getBase().getString("Bookmark.noName"));
	}

	@Override
	public boolean equals(Object o) {
		if(o == this)
			return true;
		if(o instanceof Bookmark) {
			Bookmark b = (Bookmark) o;
			if(!b.name.equals(name))
				return false;
			return true;
		} else
			return false;
	}

	@Override
	public int hashCode() {
		return name.hashCode();
	}

	public abstract SimpleFieldSet getSimpleFieldSet();
}
package freenet.support.io;

import java.io.IOException;
import java.io.OutputStream;
import java.util.ArrayList;

import com.db4o.ObjectContainer;
import freenet.support.LogThresholdCallback;

import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;

public class SegmentedChainBucketSegment {
	
	private final ArrayList<Bucket> buckets;

        private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	public SegmentedChainBucketSegment(SegmentedBucketChainBucket bucket) {
		this.buckets = new ArrayList<Bucket>();
	}

	public void free() {
		for(Bucket bucket : buckets) {
			if(bucket == null) {
				Logger.error(this, "Bucket is null on "+this);
				continue;
			}
			bucket.free();
		}
	}

	public void storeTo(ObjectContainer container) {
		if(logMINOR)
			Logger.minor(this, "Storing segment "+this);
		for(Bucket bucket : buckets)
			bucket.storeTo(container);
		container.ext().store(buckets, 1);
		container.ext().store(this, 1);
	}

	public synchronized Bucket[] shallowCopyBuckets() {
		int sz = buckets.size();
		Bucket[] out = new Bucket[sz];
		for(int i=0;i<sz;i++) out[i] = buckets.get(i);
		return out;
	}

	public synchronized void shallowCopyBuckets(Bucket[] out, int index) {
		int sz = buckets.size();
		for(int i=0;i<sz;i++) out[index++] = buckets.get(i);
	}

	public OutputStream makeBucketStream(int bucketNo, SegmentedBucketChainBucket bcb) throws IOException {
		if(bucketNo >= bcb.segmentSize)
			throw new IllegalArgumentException("Too many buckets in segment");
		Bucket b = bcb.bf.makeBucket(bcb.bucketSize);
		synchronized(this) {
			if(buckets.size() != bucketNo)
				throw new IllegalArgumentException("Next bucket should be "+buckets.size()+" but is "+bucketNo);
			buckets.add(b);
		}
		return b.getOutputStream();
	}

	public int size() {
		return buckets.size();
	}
	
	void activateBuckets(ObjectContainer container) {
		container.activate(buckets, 1);
		for(Bucket bucket : buckets)
			container.activate(bucket, 1); // will cascade
	}

	public void clear(ObjectContainer container) {
		buckets.clear();
		container.delete(buckets);
		container.delete(this);
	}
	
	public void removeFrom(ObjectContainer container) {
		for(Bucket bucket : buckets) {
			if(bucket == null) {
				// Probably not a problem...
				continue;
			}
			container.activate(bucket, 1);
			bucket.removeFrom(container);
		}
		container.delete(buckets);
		container.delete(this);
	}

}
/*
 * Created on Jul 17, 2004
 *
 */
package freenet.support.CPUInformation;

/**
 * @author Iakin
 * An interface for classes that provide lowlevel information about Intel CPU's
 *
 * free (adj.): unencumbered; not under the control of others
 * Written by Iakin in 2004 and released into the public domain 
 * with no warranty of any kind, either expressed or implied.  
 * It probably won't make your computer catch on fire, or eat 
 * your children, but it might.  Use at your own risk.
 */
public interface IntelCPUInfo extends CPUInfo {
	/**
	 * @return true iff the CPU is at least a Pentium CPU.
	 */
	public boolean IsPentiumCompatible();
	/**
	 * @return true iff the CPU is at least a Pentium which implements the MMX instruction/feature set.
	 */
	public boolean IsPentiumMMXCompatible();
	/**
	 * @return true iff the CPU implements at least the p6 instruction set (Pentium II or better).
	 * Please note that an PentimPro CPU causes/should cause this method to return false (due to that CPU using a
	 * very early implementation of the p6 instruction set. No MMX etc.)
	 */
	public boolean IsPentium2Compatible();
	/**
	 * @return true iff the CPU implements at least a Pentium III level of the p6 instruction/feature set.
	 */
	public boolean IsPentium3Compatible();
	/**
	 * @return true iff the CPU implements at least a Pentium IV level instruction/feature set.
	 */
	public boolean IsPentium4Compatible();
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.client.async;

import java.util.ArrayList;
import java.util.Collection;
import java.util.Comparator;
import java.util.HashMap;
import java.util.Map;
import java.util.WeakHashMap;

import org.tanukisoftware.wrapper.WrapperManager;

import com.db4o.ObjectContainer;
import com.db4o.ObjectSet;

import freenet.client.FetchContext;
import freenet.keys.FreenetURI;
import freenet.node.RequestClient;
import freenet.node.SendableRequest;
import freenet.node.useralerts.SimpleUserAlert;
import freenet.node.useralerts.UserAlert;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;

/** A high level client request. A request (either fetch or put) started
 * by a Client. Has a suitable context and a URI; is fulfilled only when
 * we have followed all the redirects etc, or have an error. Can be
 * retried.
 */
// WARNING: THIS CLASS IS STORED IN DB4O -- THINK TWICE BEFORE ADD/REMOVE/RENAME FIELDS
public abstract class ClientRequester {
	private static volatile boolean logMINOR;
	
	static {
		Logger.registerClass(ClientRequester.class);
	}

	public abstract void onTransition(ClientGetState oldState, ClientGetState newState, ObjectContainer container);
	
	// FIXME move the priority classes from RequestStarter here
	/** Priority class of the request or insert. */
	protected short priorityClass;
	/** Whether this is a real-time request */
	protected final boolean realTimeFlag;
	/** Has the request or insert been cancelled? */
	protected boolean cancelled;
	/** The RequestClient, used to determine whether this request is 
	 * persistent, and also we round-robin between different RequestClient's
	 * in scheduling within a given priority class and retry count. */
	protected RequestClient client;
	/** The set of queued low-level requests or inserts for this request or
	 * insert. */
	protected final SendableRequestSet requests;

	/** What is our priority class? */
	public short getPriorityClass() {
		return priorityClass;
	}

	/**
	 * zero arg c'tor for db4o on jamvm
	 */
	protected ClientRequester() {
		realTimeFlag = false;
		creationTime = 0;
		hashCode = 0;
		requests = null;
	}

	protected ClientRequester(short priorityClass, RequestClient client) {
		this.priorityClass = priorityClass;
		this.client = client;
		this.realTimeFlag = client.realTimeFlag();
		if(client == null)
			throw new NullPointerException();
		hashCode = super.hashCode(); // the old object id will do fine, as long as we ensure it doesn't change!
		requests = persistent() ? new PersistentSendableRequestSet() : new TransientSendableRequestSet();
		synchronized(allRequesters) {
			if(!persistent())
				allRequesters.put(this, dumbValue);
		}
		creationTime = System.currentTimeMillis();
	}

	/** Cancel the request. Inner method, subclasses should actually tell 
	 * the ClientGetState or whatever to cancel itself: this does not do 
	 * anything apart from set a flag!
	 * @return Whether we were already cancelled.
	 */
	protected synchronized boolean cancel() {
		boolean ret = cancelled;
		cancelled = true;
		return ret;
	}

	/** Cancel the request. Subclasses must implement to actually tell the
	 * ClientGetState's or ClientPutState's to cancel.
	 * @param container The database. Must be non-null if the request or 
	 * insert is persistent, in which case we must be on the database thread.
	 * @param context The ClientContext object including essential but 
	 * non-persistent objects such as the schedulers.
	 */
	public abstract void cancel(ObjectContainer container, ClientContext context);

	/** Is the request or insert cancelled? */
	public boolean isCancelled() {
		return cancelled;
	}

	/** Get the URI for the request or insert. For a request this is set at
	 * creation, but for an insert, it is set when we know what the final 
	 * URI will be. */
	public abstract FreenetURI getURI();

	/** Is the request or insert completed (succeeded, failed, or 
	 * cancelled, which is a kind of failure)? */
	public abstract boolean isFinished();
	
	private final int hashCode;
	
	/**
	 * We need a hash code that persists across restarts.
	 */
	@Override
	public int hashCode() {
		return hashCode;
	}

	/** Total number of blocks this request has tried to fetch/put. */
	protected int totalBlocks;
	/** Number of blocks we have successfully completed a fetch/put for. */
	protected int successfulBlocks;
	/** Number of blocks which have failed. */
	protected int failedBlocks;
	/** Number of blocks which have failed fatally. */
	protected int fatallyFailedBlocks;
	/** Minimum number of blocks required to succeed for success. */
	protected int minSuccessBlocks;
	/** Has totalBlocks stopped growing? */
	protected boolean blockSetFinalized;
	/** Has at least one block been scheduled to be sent to the network? 
	 * Requests can be satisfied entirely from the datastore sometimes. */
	protected boolean sentToNetwork;

	protected synchronized void resetBlocks() {
		totalBlocks = 0;
		successfulBlocks = 0;
		failedBlocks = 0;
		fatallyFailedBlocks = 0;
		minSuccessBlocks = 0;
		blockSetFinalized = false;
		sentToNetwork = false;
	}
	
	/** The set of blocks has been finalised, total will not change any
	 * more. Notify clients.
	 * @param container The database. Must be non-null if the request or 
	 * insert is persistent, in which case we must be on the database thread.
	 * @param context The ClientContext object including essential but 
	 * non-persistent objects such as the schedulers.
	 */
	public void blockSetFinalized(ObjectContainer container, ClientContext context) {
		synchronized(this) {
			if(blockSetFinalized) return;
			blockSetFinalized = true;
		}
		if(logMINOR)
			Logger.minor(this, "Finalized set of blocks for "+this, new Exception("debug"));
		if(persistent())
			container.store(this);
		notifyClients(container, context);
	}

	/** Add a block to our estimate of the total. Don't notify clients. */
	public void addBlock(ObjectContainer container) {
		boolean wasFinalized;
		synchronized (this) {
			totalBlocks++;
			wasFinalized = blockSetFinalized;
		}

		if (wasFinalized) {
			if (LogLevel.MINOR.matchesThreshold(Logger.globalGetThresholdNew()))
				Logger.error(this, "addBlock() but set finalized! on " + this, new Exception("error"));
			else
				Logger.error(this, "addBlock() but set finalized! on " + this);
		}
		
		if(logMINOR) Logger.minor(this, "addBlock(): total="+totalBlocks+" successful="+successfulBlocks+" failed="+failedBlocks+" required="+minSuccessBlocks);
		if(persistent()) container.store(this);
	}

	/** Add several blocks to our estimate of the total. Don't notify clients. */
	public void addBlocks(int num, ObjectContainer container) {
		boolean wasFinalized;
		synchronized (this) {
			totalBlocks += num;
			wasFinalized = blockSetFinalized;
		}

		if (wasFinalized) {
			if (LogLevel.MINOR.matchesThreshold(Logger.globalGetThresholdNew()))
				Logger.error(this, "addBlocks() but set finalized! on "+this, new Exception("error"));
			else
				Logger.error(this, "addBlocks() but set finalized! on "+this);
		}
		
		if(logMINOR) Logger.minor(this, "addBlocks("+num+"): total="+totalBlocks+" successful="+successfulBlocks+" failed="+failedBlocks+" required="+minSuccessBlocks); 
		if(persistent()) container.store(this);
	}

	/** We completed a block. Count it and notify clients unless dontNotify. */
	public void completedBlock(boolean dontNotify, ObjectContainer container, ClientContext context) {
		if(logMINOR)
			Logger.minor(this, "Completed block ("+dontNotify+ "): total="+totalBlocks+" success="+successfulBlocks+" failed="+failedBlocks+" fatally="+fatallyFailedBlocks+" finalised="+blockSetFinalized+" required="+minSuccessBlocks+" on "+this);
		synchronized(this) {
			if(cancelled) return;
			successfulBlocks++;
		}
		if(checkForBrokenClient(container, context)) return;
		if(persistent()) container.store(this);
		if(dontNotify) return;
		notifyClients(container, context);
	}
	
	transient static final UserAlert brokenClientAlert = new SimpleUserAlert(true, "Some broken downloads/uploads were cancelled. Please restart them.", "Some downloads/uploads were broken due to a bug (some time before 1287) causing unrecoverable database corruption. They have been cancelled. Please restart them from the Downloads or Uploads page.", "Some downloads/uploads were broken due to a pre-1287 bug, please restart them.", UserAlert.ERROR);

	public boolean checkForBrokenClient(ObjectContainer container,
			ClientContext context) {
		if(container != null && client == null) {
			if(container.ext().isStored(this) && container.ext().isActive(this)) {
				// Data corruption?!?!?
				// Obviously broken, possibly associated with a busted FCPClient.
				// Lets fail it.
				Logger.error(this, "Stored and active "+this+" but client is null!");
				if(!isFinished()) {
					context.postUserAlert(brokenClientAlert);
					System.err.println("Cancelling download/upload because of bug causing database corruption. The bug has been fixed but the download/upload will be cancelled. You can restart it.");
				}
				// REDFLAG this leaks a RequestClient. IMHO this is better than the alternative.
				this.client = new RequestClient() {

					public boolean persistent() {
						return true;
					}

					public void removeFrom(ObjectContainer container) {
						container.delete(this);
					}

					public boolean realTimeFlag() {
						return realTimeFlag;
					}
					
				};
				container.store(client);
				container.store(this);
				if(!isFinished()) {
					cancel(container, context);
				}
				return true;
			} else if(container.ext().isStored(this) && !container.ext().isActive(this)) {
				// Definitely a bug, hopefully a simple one.
				Logger.error(this, "Not active in completedBlock on "+this, new Exception("error"));
				return true;
			} else
				throw new IllegalStateException("Client is null on persistent request "+this);
		}
		return false;
	}

	/** A block failed. Count it and notify our clients. */
	public void failedBlock(ObjectContainer container, ClientContext context) {
		synchronized(this) {
			failedBlocks++;
		}
		if(persistent()) container.store(this);
		notifyClients(container, context);
	}

	/** A block failed fatally. Count it and notify our clients. */
	public void fatallyFailedBlock(ObjectContainer container, ClientContext context) {
		synchronized(this) {
			fatallyFailedBlocks++;
		}
		if(persistent()) container.store(this);
		notifyClients(container, context);
	}

	/** Add one or more blocks to the number of requires blocks, and don't notify the clients. */
	public synchronized void addMustSucceedBlocks(int blocks, ObjectContainer container) {
		totalBlocks += blocks;
		minSuccessBlocks += blocks;
		if(persistent()) container.store(this);
		if(logMINOR) Logger.minor(this, "addMustSucceedBlocks("+blocks+"): total="+totalBlocks+" successful="+successfulBlocks+" failed="+failedBlocks+" required="+minSuccessBlocks); 
	}

	/** Insertors should override this. The method is duplicated rather than calling addMustSucceedBlocks to avoid confusing consequences when addMustSucceedBlocks does other things. */
	public synchronized void addRedundantBlocks(int blocks, ObjectContainer container) {
		totalBlocks += blocks;
		minSuccessBlocks += blocks;
		if(persistent()) container.store(this);
		if(logMINOR) Logger.minor(this, "addMustSucceedBlocks("+blocks+"): total="+totalBlocks+" successful="+successfulBlocks+" failed="+failedBlocks+" required="+minSuccessBlocks); 
	}
	
	/** Notify clients, usually via a SplitfileProgressEvent, of the current progress. */
	public abstract void notifyClients(ObjectContainer container, ClientContext context);
	
	/** Called when we first send a request to the network. Ensures that it really is the first time and
	 * passes on to innerToNetwork().
	 */
	public void toNetwork(ObjectContainer container, ClientContext context) {
		synchronized(this) {
			if(sentToNetwork) return;
			sentToNetwork = true;
			if(persistent()) container.store(this);
		}
		innerToNetwork(container, context);
	}

	/** Notify clients that a request has gone to the network, for the first time, i.e. we have finished
	 * checking the datastore for at least one part of the request. */
	protected abstract void innerToNetwork(ObjectContainer container, ClientContext context);

	protected void clearCountersOnRestart() {
		this.blockSetFinalized = false;
		this.cancelled = false;
		this.failedBlocks = 0;
		this.fatallyFailedBlocks = 0;
		this.minSuccessBlocks = 0;
		this.sentToNetwork = false;
		this.successfulBlocks = 0;
		this.totalBlocks = 0;
	}

	/** Get client context object */
	public RequestClient getClient() {
		return client;
	}

	/** Change the priority class of the request (request includes inserts here).
	 * @param newPriorityClass The new priority class for the request or insert.
	 * @param ctx The ClientContext, contains essential transient objects such as the schedulers.
	 * @param container The database. If the request is persistent, this must be non-null, and we must
	 * be running on the database thread; you should schedule a job using the DBJobRunner.
	 */
	public void setPriorityClass(short newPriorityClass, ClientContext ctx, ObjectContainer container) {
		short oldPrio;
		synchronized(this) {
			oldPrio = priorityClass;
			this.priorityClass = newPriorityClass;
		}
		if(logMINOR) Logger.minor(this, "Changing priority class of "+this+" from "+oldPrio+" to "+newPriorityClass);
		ctx.getChkFetchScheduler(realTimeFlag).reregisterAll(this, container, oldPrio);
		ctx.getChkInsertScheduler(realTimeFlag).reregisterAll(this, container, oldPrio);
		ctx.getSskFetchScheduler(realTimeFlag).reregisterAll(this, container, oldPrio);
		ctx.getSskInsertScheduler(realTimeFlag).reregisterAll(this, container, oldPrio);
		if(persistent()) container.store(this);
	}

	public boolean realTimeFlag() {
		return realTimeFlag;
	}

	/** Is this request persistent? */
	public boolean persistent() {
		return client.persistent();
	}

	/** Remove this request from the database */
	public void removeFrom(ObjectContainer container, ClientContext context) {
		container.activate(requests, 1);
		requests.removeFrom(container);
		container.delete(this);
	}
	
	/** When the request is activated, so is the request client, because we have to know whether we are
	 * persistent! */
	public void objectOnActivate(ObjectContainer container) {
		container.activate(client, 1);
	}
	
	public boolean objectCanNew(ObjectContainer container) {
		if(client == null)
			throw new NullPointerException();
		return true;
	}

	public boolean objectCanUpdate(ObjectContainer container) {
		if(client == null)
			throw new NullPointerException();
		return true;
	}

	/** Add a low-level request to the list of requests belonging to this high-level request (request here
	 * includes inserts). */
	public void addToRequests(SendableRequest req, ObjectContainer container) {
		if(persistent())
			container.activate(requests, 1);
		requests.addRequest(req, container);
		if(persistent())
			container.deactivate(requests, 1);
	}

	/** Get all known low-level requests belonging to this high-level request.
	 * @param container The database, must be non-null if this is a persistent request or persistent insert.
	 */
	public SendableRequest[] getSendableRequests(ObjectContainer container) {
		if(persistent())
			container.activate(requests, 1);
		SendableRequest[] reqs = requests.listRequests(container);
		if(persistent())
			container.deactivate(requests, 1);
		return reqs;
	}

	/** Remove a low-level request or insert from the list of known requests belonging to this 
	 * high-level request or insert. */
	public void removeFromRequests(SendableRequest req, ObjectContainer container, boolean dontComplain) {
		if(persistent())
			container.activate(requests, 1);
		if(!requests.removeRequest(req, container) && !dontComplain) {
			Logger.error(this, "Not in request list for "+this+": "+req);
		}
		if(persistent())
			container.deactivate(requests, 1);
	}

	/** FIXME get rid. */
	public static void checkAll(ObjectContainer container,
			ClientContext clientContext) {
		ObjectSet<ClientRequester> requesters = container.query(ClientRequester.class);
		for(ClientRequester req : requesters) {
			try {
				if(logMINOR) Logger.minor(req, "Checking "+req);
				if(req.isCancelled() || req.isFinished()) {
					if(logMINOR) Logger.minor(req, "Cancelled or finished");
				} else {
					if(logMINOR) Logger.minor(req, "Checking for broken client: "+req);
					if(!req.checkForBrokenClient(container, clientContext))
						if(logMINOR) Logger.minor(req, "Request is clean.");
					else {
						WrapperManager.signalStarting(5*60*1000);
						container.commit();
					}
				}
			} catch (Throwable t) {
				Logger.error(ClientRequester.class, "Caught error while checking on startup", t);
			}
			container.deactivate(req, 1);
		}
	}

	private static WeakHashMap<ClientRequester,Object> allRequesters = new WeakHashMap<ClientRequester,Object>();
	private static Object dumbValue = new Object();
	public final long creationTime;

	public static ClientRequester[] getAll() {
		synchronized(allRequesters) {
			return allRequesters.keySet().toArray(new ClientRequester[0]);
		}
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.io;

public class CannotCreateFromFieldSetException extends Exception {
	private static final long serialVersionUID = 1L;
	public CannotCreateFromFieldSetException(String msg) {
		super(msg);
	}

	public CannotCreateFromFieldSetException(String msg, Exception e) {
		super(msg+" : "+e, e);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.keys;

public class KeyVerifyException extends Exception {
	private static final long serialVersionUID = -1;

	public KeyVerifyException(String message) {
		super(message);
	}

	public KeyVerifyException(String message, Throwable cause) {
		super(message, cause);
	}

	public KeyVerifyException() {
		super();
	}

	public KeyVerifyException(Throwable cause) {
		super(cause);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

/** Callback for a locally initiated probe request */
public interface ProbeCallback {

	void onCompleted(String reason, double target, double best, double nearest, long id, short counter, short uniqueCounter, short linearCounter);

	void onTrace(long uid, double target, double nearest, double best, short htl, short counter, double location, long nodeUID, double[] peerLocs, long[] peerUIDs, double[] locsNotVisited, short forkCount, short linearCount, String reason, long prevUID);

	/**
	 * Got a RejectedOverload passed down from some upstream node. Note that not all probe request
	 * implementations may generate these.
	 */
	void onRejectOverload();

}
package freenet.client.async;

import java.util.ArrayList;
import java.util.Random;

import freenet.support.math.MersenneTwister;

import com.db4o.ObjectContainer;
import com.db4o.ObjectSet;
import com.db4o.query.Query;

import freenet.keys.Key;
import freenet.keys.KeyBlock;
import freenet.keys.NodeSSK;
import freenet.node.LowLevelGetException;
import freenet.node.Node;
import freenet.node.PrioRunnable;
import freenet.node.RequestStarter;
import freenet.node.SendableGet;
import freenet.support.Executor;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.io.NativeThread;

/**
 * @author Matthew Toseland <toad@amphibian.dyndns.org> (0xE43DA450)
 */
public class DatastoreChecker implements PrioRunnable {

	// Setting these to 1, 3 kills 1/3rd of datastore checks.
	// 2, 5 gives 40% etc.
	// In normal operation KILL_BLOCKS should be 0 !!!!
	static final int KILL_BLOCKS = 0;
	static final int RESET_COUNTER = 100;
	
	private static volatile boolean logMINOR;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback() {

			@Override
			public void shouldUpdate() {
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	static final int MAX_PERSISTENT_KEYS = 1024;

	/** List of arrays of keys to check for persistent requests. PARTIAL:
	 * When we run out we will look up some more DatastoreCheckerItem's. */
	private final ArrayList<Key[]>[] persistentKeys;
	/** List of persistent requests which we will call finishRegister() for
	 * when we have checked the keys lists. PARTIAL: When we run out we
	 * will look up some more DatastoreCheckerItem's. Deactivated. */
	private final ArrayList<SendableGet>[] persistentGetters;
	private final ArrayList<ClientRequestScheduler>[] persistentSchedulers;
	private final ArrayList<DatastoreCheckerItem>[] persistentCheckerItems;
	private final ArrayList<BlockSet>[] persistentBlockSets;

	/** List of arrays of keys to check for transient requests. */
	private final ArrayList<Key[]>[] transientKeys;
	/** List of transient requests which we will call finishRegister() for
	 * when we have checked the keys lists. */
	private final ArrayList<SendableGet>[] transientGetters;
	private final ArrayList<BlockSet>[] transientBlockSets;

	private ClientContext context;
	private final Node node;

	public synchronized void setContext(ClientContext context) {
		this.context = context;
	}

	@SuppressWarnings("unchecked")
    public DatastoreChecker(Node node) {
		this.node = node;
		int priorities = RequestStarter.NUMBER_OF_PRIORITY_CLASSES;
		persistentKeys = new ArrayList[priorities];
		for(int i=0;i<priorities;i++)
			persistentKeys[i] = new ArrayList<Key[]>();
		persistentGetters = new ArrayList[priorities];
		for(int i=0;i<priorities;i++)
			persistentGetters[i] = new ArrayList<SendableGet>();
		persistentSchedulers = new ArrayList[priorities];
		for(int i=0;i<priorities;i++)
			persistentSchedulers[i] = new ArrayList<ClientRequestScheduler>();
		persistentCheckerItems = new ArrayList[priorities];
		for(int i=0;i<priorities;i++)
			persistentCheckerItems[i] = new ArrayList<DatastoreCheckerItem>();
		persistentBlockSets = new ArrayList[priorities];
		for(int i=0;i<priorities;i++)
			persistentBlockSets[i] = new ArrayList<BlockSet>();
		transientKeys = new ArrayList[priorities];
		for(int i=0;i<priorities;i++)
			transientKeys[i] = new ArrayList<Key[]>();
		transientGetters = new ArrayList[priorities];
		for(int i=0;i<priorities;i++)
			transientGetters[i] = new ArrayList<SendableGet>();
		transientBlockSets = new ArrayList[priorities];
		for(int i=0;i<priorities;i++)
			transientBlockSets[i] = new ArrayList<BlockSet>();
	}

	private final DBJob loader =  new DBJob() {

		public boolean run(ObjectContainer container, ClientContext context) {
			loadPersistentRequests(container, context);
			return false;
		}
		
		public String toString() {
			return "DatastoreCheckerPersistentRequestLoader";
		}

	};

    public void loadPersistentRequests(ObjectContainer container, final ClientContext context) {
		int totalSize = 0;
		synchronized(this) {
			for(int i=0;i<persistentKeys.length;i++) {
				for(int j=0;j<persistentKeys[i].size();j++)
					totalSize += persistentKeys[i].get(j).length;
			}
			if(totalSize > MAX_PERSISTENT_KEYS) {
				if(logMINOR) Logger.minor(this, "Persistent datastore checker queue already full");
				return;
			}
		}
		for(short p = RequestStarter.MAXIMUM_PRIORITY_CLASS; p <= RequestStarter.MINIMUM_PRIORITY_CLASS; p++) {
			final short prio = p;
			Query query = container.query();
			query.constrain(DatastoreCheckerItem.class);
			query.descend("nodeDBHandle").constrain(context.nodeDBHandle).
				and(query.descend("prio").constrain(prio));
			@SuppressWarnings("unchecked")
			ObjectSet<DatastoreCheckerItem> results = query.execute();
			for(DatastoreCheckerItem item : results) {
				if(item.chosenBy == context.bootID) continue;
				SendableGet getter = item.getter;
				if(getter == null || !container.ext().isStored(getter)) {
					if(logMINOR) Logger.minor(this, "Ignoring DatastoreCheckerItem because the SendableGet has already been deleted from the database");
					container.delete(item);
					continue;
				}
				try {
					BlockSet blocks = item.blocks;
					container.activate(getter, 1);
					if(getter.isStorageBroken(container)) {
						Logger.error(this, "Getter is broken as stored: "+getter);
						container.delete(getter);
						container.delete(item);
						continue;
					}
					ClientRequestScheduler sched = getter.getScheduler(container, context);
					synchronized(this) {
						if(persistentGetters[prio].contains(getter)) continue;
					}
					Key[] keys = getter.listKeys(container);
					// FIXME check the store bloom filter using store.probablyInStore().
					item.chosenBy = context.bootID;
					container.store(item);
					synchronized(this) {
						if(persistentGetters[prio].contains(getter)) continue;
						ArrayList<Key> finalKeysToCheck = new ArrayList<Key>();
						for(Key key : keys) {
							key = key.cloneKey();
							finalKeysToCheck.add(key);
						}
						Key[] finalKeys =
							finalKeysToCheck.toArray(new Key[finalKeysToCheck.size()]);
						persistentKeys[prio].add(finalKeys);
						persistentGetters[prio].add(getter);
						persistentSchedulers[prio].add(sched);
						persistentCheckerItems[prio].add(item);
						persistentBlockSets[prio].add(blocks);
						if(totalSize == 0)
							notifyAll();
						totalSize += finalKeys.length;
						if(totalSize > MAX_PERSISTENT_KEYS) {
							boolean full = trimPersistentQueue(prio, container);
							notifyAll();
							if(full) return;
						} else {
							notifyAll();
						}
					}
					container.deactivate(getter, 1);
				} catch (NullPointerException e) {
					Logger.error(this, "NPE for getter in DatastoreChecker: "+e+" - probably leftover data from an incomplete deletion", e);
					try {
						Logger.error(this, "Getter: "+getter);
					} catch (Throwable t) {
						// Ignore
					}
				}
			}
		}
	}

	/**
	 * Trim the queue of persistent requests until it is just over the limit.
	 * @param minPrio Only drop from priorities lower than this one.
	 * @return True unless the queue is under the limit.
	 */
	private boolean trimPersistentQueue(short prio, ObjectContainer container) {
		synchronized(this) {
			int preQueueSize = 0;
			for(int i=0;i<prio;i++) {
				for(int x=0;x<persistentKeys[i].size();x++)
					preQueueSize += persistentKeys[i].get(x).length;
			}
			if(preQueueSize > MAX_PERSISTENT_KEYS) {
				// Dump everything
				for(int i=prio+1;i<persistentKeys.length;i++) {
					for(DatastoreCheckerItem item : persistentCheckerItems[i]) {
						item.chosenBy = 0;
						container.store(item);
					}
					persistentSchedulers[i].clear();
					persistentGetters[i].clear();
					persistentKeys[i].clear();
					persistentBlockSets[i].clear();
				}
				return true;
			} else {
				int postQueueSize = 0;
				for(int i=prio+1;i<persistentKeys.length;i++) {
					for(int x=0;x<persistentKeys[i].size();x++)
						postQueueSize += persistentKeys[i].get(x).length;
				}
				if(postQueueSize + preQueueSize < MAX_PERSISTENT_KEYS)
					return false;
				// Need to dump some stuff.
				for(int i=persistentKeys.length-1;i>prio;i--) {
					while(!persistentKeys[i].isEmpty()) {
						int idx = persistentKeys[i].size() - 1;
						DatastoreCheckerItem item = persistentCheckerItems[i].remove(idx);
						persistentSchedulers[i].remove(idx);
						persistentGetters[i].remove(idx);
						Key[] keys = persistentKeys[i].remove(idx);
						persistentBlockSets[i].remove(idx);
						item.chosenBy = 0;
						container.store(item);
						if(postQueueSize + preQueueSize - keys.length < MAX_PERSISTENT_KEYS) {
							return false;
						} else postQueueSize -= keys.length;
					}
				}
				// Still over the limit.
				return true;
			}
		}
	}

	public void queueTransientRequest(SendableGet getter, BlockSet blocks) {
		Key[] checkKeys = getter.listKeys(null);
		short prio = getter.getPriorityClass(null);
		if(logMINOR) Logger.minor(this, "Queueing transient request "+getter+" priority "+prio+" keys "+checkKeys.length);
		// FIXME check using store.probablyInStore
		ArrayList<Key> finalKeysToCheck = new ArrayList<Key>();
		// Add it to the list of requests running here, so that priority changes while the data is on the store checker queue will work.
		ClientRequester requestor = getter.getClientRequest();
		requestor.addToRequests(getter, null);
		synchronized(this) {
			for(Key key : checkKeys) {
				finalKeysToCheck.add(key);
			}
			if(logMINOR && transientGetters[prio].indexOf(getter) != -1) {
				Logger.error(this, "Transient request "+getter+" is already queued!");
				return;
			}
			transientGetters[prio].add(getter);
			transientKeys[prio].add(finalKeysToCheck.toArray(new Key[finalKeysToCheck.size()]));
			transientBlockSets[prio].add(blocks);
			notifyAll();
		}
	}

	/**
	 * Queue a persistent request. We will store a DatastoreCheckerItem, then
	 * check the datastore (on the datastore checker thread), and then call
	 * finishRegister() (on the database thread). Caller must have already
	 * stored and registered the HasKeyListener if any.
	 * @param getter
	 */
	public void queuePersistentRequest(SendableGet getter, BlockSet blocks, ObjectContainer container, ClientContext context) {
		if(getter.isCancelled(container)) { // We do not care about cooldowns here; we will check all keys regardless, so only ask isCancelled().
			if(logMINOR) Logger.minor(this, "Request is empty, not checking store: "+getter);
			return;
		}
		Key[] checkKeys = getter.listKeys(container);
		short prio = getter.getPriorityClass(container);
		ClientRequestScheduler sched = getter.getScheduler(container, context);
		DatastoreCheckerItem item = new DatastoreCheckerItem(getter, context.nodeDBHandle, prio, blocks);
		container.store(item);
		container.activate(blocks, 5);
		// Add it to the list of requests running here, so that priority changes while the data is on the store checker queue will work.
		ClientRequester requestor = getter.getClientRequest();
		container.activate(requestor, 1);
		requestor.addToRequests(getter, container);
		synchronized(this) {
			// FIXME only add if queue not full.
			int queueSize = 0;
			// Only count queued keys at no higher priority than this request.
			for(short p = 0;p<=prio;p++) {
				for(int x = 0;x<persistentKeys[p].size();x++) {
					queueSize += persistentKeys[p].get(x).length;
				}
			}
			if(queueSize > MAX_PERSISTENT_KEYS) return;
			item.chosenBy = context.bootID;
			container.store(item);
			// FIXME check using store.probablyInStore
			ArrayList<Key> finalKeysToCheck = new ArrayList<Key>();
			for(Key key : checkKeys) {
				finalKeysToCheck.add(key);
			}
			persistentGetters[prio].add(getter);
			persistentKeys[prio].add(finalKeysToCheck.toArray(new Key[finalKeysToCheck.size()]));
			persistentSchedulers[prio].add(sched);
			persistentCheckerItems[prio].add(item);
			persistentBlockSets[prio].add(blocks);
			trimPersistentQueue(prio, container);
			notifyAll();
		}
	}

	public void run() {
		while(true) {
			try {
				realRun();
			} catch (Throwable t) {
				Logger.error(this, "Caught "+t+" in datastore checker thread", t);
			}
		}
	}

	private void realRun() {
		Random random;
		if(KILL_BLOCKS != 0)
			random = new MersenneTwister();
		else
			random = null;
		Key[] keys = null;
		SendableGet getter = null;
		boolean persistent = false;
		ClientRequestScheduler sched = null;
		DatastoreCheckerItem item = null;
		BlockSet blocks = null;
		// If the queue is too large, don't check any more blocks. It is possible
		// that we can check the datastore faster than we can handle the resulting
		// blocks, this will cause OOM.
		int queueSize = context.jobRunner.getQueueSize(ClientRequestScheduler.TRIP_PENDING_PRIORITY);
		// If it's over 100, don't check blocks from persistent requests.
		boolean notPersistent = queueSize > 100;
		// FIXME: Ideally, for really big queues, we wouldn't datastore check transient keys that are also wanted by persistent requests.
		// Looking it up in the bloom filters is trivial. But I am not sure it is safe to take the CRSBase lock inside the DatastoreChecker lock,
		// especially given that sometimes SendableGet methods get called within it, and sometimes those call back here.
		// Maybe we can separate the lock for the Bloom filters from that for everything else?
		// Checking whether keys are wanted by persistent requests outside the lock would likely result in busy-looping.
		synchronized(this) {
			while(true) {
				for(short prio = 0;prio<transientKeys.length;prio++) {
					if(!transientKeys[prio].isEmpty()) {
						keys = transientKeys[prio].remove(0);
						getter = transientGetters[prio].remove(0);
						persistent = false;
						item = null;
						blocks = transientBlockSets[prio].remove(0);
						if(logMINOR)
							Logger.minor(this, "Checking transient request "+getter+" prio "+prio+" of "+transientKeys[prio].size());
						break;
					} else if((!notPersistent) && (!persistentGetters[prio].isEmpty())) {
						keys = persistentKeys[prio].remove(0);
						getter = persistentGetters[prio].remove(0);
						persistent = true;
						sched = persistentSchedulers[prio].remove(0);
						item = persistentCheckerItems[prio].remove(0);
						blocks = persistentBlockSets[prio].remove(0);
						if(logMINOR)
							Logger.minor(this, "Checking persistent request at prio "+prio);
						break;
					}
				}
				if(keys == null) {
					try {
						context.jobRunner.queue(loader, NativeThread.HIGH_PRIORITY, true);
					} catch (DatabaseDisabledException e1) {
						// Ignore
					}
					if(logMINOR) Logger.minor(this, "Waiting for more persistent requests");
					try {
						wait(100*1000);
					} catch (InterruptedException e) {
						// Ok
					}
					continue;
				}
				break;
			}
		}
		if(!persistent) {
			sched = getter.getScheduler(null, context);
		}
		boolean anyValid = false;
		for(Key key : keys) {
			if(random != null) {
				if(random.nextInt(RESET_COUNTER) < KILL_BLOCKS) {
					anyValid = true;
					continue;
				}
			}
			KeyBlock block = null;
			if(blocks != null)
				block = blocks.get(key);
			if(blocks == null)
				block = node.fetch(key, true, true, false, false, null);
			if(block != null) {
				if(logMINOR) Logger.minor(this, "Found key");
				if(key instanceof NodeSSK)
					sched.tripPendingKey(block);
				else // CHK
					sched.tripPendingKey(block);
			} else {
				anyValid = true;
			}
//			synchronized(this) {
//				keysToCheck[priority].remove(key);
//			}
		}
		if(logMINOR) Logger.minor(this, "Checked "+keys.length+" keys");
		if(persistent)
			try {
				context.jobRunner.queue(loader, NativeThread.HIGH_PRIORITY, true);
			} catch (DatabaseDisabledException e) {
				// Ignore
			}
		if(persistent) {
			final SendableGet get = getter;
			final ClientRequestScheduler scheduler = sched;
			final boolean valid = anyValid;
			final DatastoreCheckerItem it = item;
			try {
				context.jobRunner.queue(new DBJob() {

					public boolean run(ObjectContainer container, ClientContext context) {
						if(container.ext().isActive(get)) {
							Logger.warning(this, "ALREADY ACTIVATED: "+get);
						}
						if(!container.ext().isStored(get)) {
							// Completed and deleted already.
							if(logMINOR)
								Logger.minor(this, "Already deleted from database");
							container.delete(it);
							return false;
						}
						container.activate(get, 1);
						try {
							scheduler.finishRegister(new SendableGet[] { get }, true, container, valid, it);
						} catch (Throwable t) {
							Logger.error(this, "Failed to register "+get+": "+t, t);
							try {
								get.onFailure(new LowLevelGetException(LowLevelGetException.INTERNAL_ERROR, "Internal error: "+t, t), null, container, context);
							} catch (Throwable t1) {
								Logger.error(this, "Failed to fail: "+t, t);
							}
						}
						container.deactivate(get, 1);
						loader.run(container, context);
						return false;
					}
					
					public String toString() {
						return "DatastoreCheckerFinishRegister";
					}

				}, NativeThread.NORM_PRIORITY, false);
			} catch (DatabaseDisabledException e) {
				// Impossible
			}
		} else {
			sched.finishRegister(new SendableGet[] { getter }, false, null, anyValid, item);
		}
	}

	synchronized void wakeUp() {
		notifyAll();
	}

	public void start(Executor executor, String name) {
		try {
			context.jobRunner.queue(loader, NativeThread.HIGH_PRIORITY-1, true);
		} catch (DatabaseDisabledException e) {
			// Ignore
		}
		executor.execute(this, name);
	}

	public int getPriority() {
		return NativeThread.NORM_PRIORITY;
	}

	public boolean objectCanNew(ObjectContainer container) {
		Logger.error(this, "Not storing DatastoreChecker in database", new Exception("error"));
		return false;
	}

	public void removeRequest(SendableGet request, boolean persistent, ObjectContainer container, ClientContext context, short prio) {
		if(logMINOR) Logger.minor(this, "Removing request prio="+prio+" persistent="+persistent);
		if(!persistent) {
			synchronized(this) {
				int index = transientGetters[prio].indexOf(request);
				if(index == -1) return;
				transientGetters[prio].remove(index);
				transientKeys[prio].remove(index);
				transientBlockSets[prio].remove(index);
				if(logMINOR) Logger.minor(this, "Removed transient request");
			}
		} else {
			synchronized(this) {
				int index = persistentGetters[prio].indexOf(request);
				if(index != -1) {
					persistentKeys[prio].remove(index);
					persistentGetters[prio].remove(index);
					persistentSchedulers[prio].remove(index);
					persistentCheckerItems[prio].remove(index);
					persistentBlockSets[prio].remove(index);
				}
			}
			Query query =
				container.query();
			query.constrain(DatastoreCheckerItem.class);
			query.descend("getter").constrain(request).identity();
			ObjectSet<DatastoreCheckerItem> results = query.execute();
			int deleted = 0;
			for(DatastoreCheckerItem item : results) {
				if(item.nodeDBHandle != context.nodeDBHandle) continue;
				if(deleted == 1) {
					try {
						Logger.error(this, "Multiple DatastoreCheckerItem's for "+request);
					} catch (Throwable e) {
						// Ignore, toString() error
						Logger.error(this, "Multiple DatastoreCheckerItem's for request");
					}
				}
				deleted++;
				container.delete(item);
			}
		}
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.crypt;
import java.security.DigestException;
import java.security.MessageDigest;

class JavaSHA1 implements Digest, Cloneable {
    
    MessageDigest digest;
    
    @Override
	public Object clone() throws CloneNotSupportedException {
	return new JavaSHA1((MessageDigest)(digest.clone()));
    }
    
    protected JavaSHA1(MessageDigest d) {
	digest = d;
    }
    
    public JavaSHA1() throws Exception {
	digest = MessageDigest.getInstance("SHA1");
    }
    
    public void extract(int[] digest, int offset) {
	throw new UnsupportedOperationException();
    }
    
    public void update(byte b) {
	digest.update(b);
    }
    
    public void update(byte[] data, int offset, int length) {
	digest.update(data, offset, length);
    }
    
    public void update(byte[] data) {
	digest.update(data);
    }
    
    public byte[] digest() {
	return digest.digest();
    }
    
    public void digest(boolean reset, byte[] buffer, int offset) {
	if(reset != true) throw new UnsupportedOperationException();
	try {
	    digest.digest(buffer, offset, digest.getDigestLength());
	} catch (DigestException e) {
	    throw new IllegalStateException(e.toString());
	}
    }
    
//     protected final int blockIndex() {
// 	return (int)((count / 8) & 63);
//     }
    
//     public void finish() {
// 	byte bits[] = new byte[8];
//         int i, j;
//         for (i = 0; i < 8; i++) {
//             bits[i] = (byte)((count >>> (((7 - i) << 3))) & 0xff);
//         }
	
//         update((byte) 128);
//         while (blockIndex() != 56)
//             update((byte) 0);
//         // This should cause a transform to happen.
//         for(i=0; i<8; ++i) update(bits[i]);
//     }
    
    public int digestSize() {
	return 160;
    }
}
package freenet.support;

import java.util.Hashtable;
import java.util.Timer;
import java.util.TimerTask;

import freenet.node.FastRunnable;

/**
 * Ticker implemented using Timer's.
 * 
 * If deploying this to replace PacketSender, be careful to handle priority changes properly.
 * Hopefully that can be achieved simply by creating at max priority during startup.
 * 
 * @author Matthew Toseland <toad@amphibian.dyndns.org> (0xE43DA450)
 *
 */
public class TrivialTicker implements Ticker {

	private final Timer timer = new Timer(true);
	
	private final Executor executor;
	
	private final Hashtable<Runnable, TimerTask> jobs = new Hashtable<Runnable, TimerTask>();
	
	private boolean running = true;
	
	public TrivialTicker(Executor executor) {
		this.executor = executor;
	}
	
	public void queueTimedJob(final Runnable job, long offset) {
		TimerTask t = new TimerTask() {
			@Override
			public void run() {
				synchronized(TrivialTicker.this) {
					jobs.remove(job); // We must do this before job.run() in case the job re-schedules itself.
				}
				
				if(job instanceof FastRunnable) {
					job.run();
				} else {
					executor.execute(job, "Delayed task: "+job);
				}
			}
		};
		
		synchronized(this) {
			if(!running)
				return;
		
			timer.schedule(t, offset);
			jobs.put(job, t);
		}
	}

	public void queueTimedJob(final Runnable job, final String name, long offset,
			boolean runOnTickerAnyway, boolean noDupes) {
		TimerTask t = new TimerTask() {

			@Override
			public void run() {
				synchronized(TrivialTicker.this) {
					jobs.remove(job); // We must do this before job.run() in case the job re-schedules itself.
				}
				
				if(job instanceof FastRunnable) {
					job.run();
				} else {
					executor.execute(job, name);
				}
			}
			
		};
		
		synchronized(this) {
			if(!running)
				return;
			
			if(noDupes && jobs.containsKey(job))
				return;
			
			timer.schedule(t, offset);
			jobs.put(job, t);
		}
	}
	
	public void cancelTimedJob(final Runnable job) {
		removeQueuedJob(job);
	}
	
	public void removeQueuedJob(final Runnable job) {
		synchronized(this) {
			if(!running)
				return;
			
			TimerTask t = jobs.get(job);
			if(t != null) {
				t.cancel();
				jobs.remove(t);
			}
		}
	}
	
	/**
	 * Changes the offset of a already-queued job.
	 * If the given job was not queued yet it will be queued nevertheless.
	 */
	public void rescheduleTimedJob(final Runnable job, final String name, long newOffset) {
		synchronized(this) {
			removeQueuedJob(job);
			queueTimedJob(job, name, newOffset, false, false); // Don't dupe-check, we are synchronized
		}
	}
	
	private Thread shutdownThread = null;
	
	public void shutdown() {
		synchronized(this) {
			running = false;
			
			timer.schedule(new TimerTask() {

				public void run() {
					// According to the JavaDoc of cancel(), calling it inside a TimerTask guarantees that the task is the last one which is run.
					timer.cancel();
					synchronized(TrivialTicker.this) {
						shutdownThread = Thread.currentThread();
						TrivialTicker.this.notifyAll();
					}
				}
				
			}, 0);
			
			while(shutdownThread == null) {
				try {
					wait();
				} catch (InterruptedException e) { } // Valid to happen due to spurious wakeups
			}
			
			while(shutdownThread.isAlive()) { // Ignore InterruptedExceptions
				try {
					shutdownThread.join();
				} catch (InterruptedException e) { 
					Logger.error(this, "Got an unexpected InterruptedException", e);
				}
			}
		}
	}

	public Executor getExecutor() {
		return executor;
	}

}
package freenet.support;

/**
 * Indicates an attempt to link two DoublyLinkedList.Item's,
 * neither of which are a member of a DoublyLinkedList.
 * @author tavin
 */
public class VirginItemException extends RuntimeException {
	private static final long serialVersionUID = -1;
    VirginItemException(DoublyLinkedList.Item item) {
        super(item.toString());
    }
}
package freenet.support;

import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.TimeUnit;

import freenet.node.PrioRunnable;
import freenet.support.Logger.LogLevel;
import freenet.support.io.NativeThread;

public class SerialExecutor implements Executor {

	private static volatile boolean logMINOR;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	private final LinkedBlockingQueue<Runnable> jobs;
	private final Object syncLock;
	private final int priority;

	private volatile boolean threadWaiting;
	private volatile boolean threadStarted;

	private String name;
	private Executor realExecutor;

	private static final int NEWJOB_TIMEOUT = 5*60*1000;
	
	private Thread runningThread;

	private final Runnable runner = new PrioRunnable() {

		public int getPriority() {
			return priority;
		}

		public void run() {
			synchronized(syncLock) {
				runningThread = Thread.currentThread();
			}
			try {
			while(true) {
				synchronized (syncLock) {
						threadWaiting = true;
				}
				Runnable job = null;
						try {
					job = jobs.poll(NEWJOB_TIMEOUT, TimeUnit.MILLISECONDS);
						} catch (InterruptedException e) {
					// ignore
						}
				synchronized (syncLock) {
						threadWaiting=false;
						}
				if (job == null) {
					synchronized (syncLock) {
						threadStarted = false;
					}
					return;
				}

				try {
					job.run();
				} catch (Throwable t) {
					Logger.error(this, "Caught "+t, t);
					Logger.error(this, "While running "+job+" on "+this);
				}
			}
			} finally {
				synchronized(syncLock) {
					runningThread = null;
				}
			}
		}

	};

	public SerialExecutor(int priority) {
		jobs = new LinkedBlockingQueue<Runnable>();
		this.priority = priority;
		this.syncLock = new Object();
	}

	public void start(Executor realExecutor, String name) {
		assert(realExecutor != this);
		this.realExecutor=realExecutor;
		this.name=name;
		synchronized (syncLock) {
			if (!jobs.isEmpty())
				reallyStart();
		}
	}

	private void reallyStart() {
		synchronized (syncLock) {
		threadStarted=true;
		}
		if (logMINOR)
			Logger.minor(this, "Starting thread... " + name + " : " + runner);
		realExecutor.execute(runner, name);
	}

	public void execute(Runnable job) {
		execute(job, "<noname>");
	}

	public void execute(Runnable job, String jobName) {
		if (logMINOR)
			Logger.minor(this, "Running " + jobName + " : " + job + " started=" + threadStarted + " waiting="
			        + threadWaiting);
		jobs.add(job);

		synchronized (syncLock) {
			if (!threadStarted && realExecutor != null)
				reallyStart();
		}
	}

	public void execute(Runnable job, String jobName, boolean fromTicker) {
		execute(job, jobName);
	}

	public int[] runningThreads() {
		int[] retval = new int[NativeThread.JAVA_PRIORITY_RANGE+1];
		if (threadStarted && !threadWaiting)
			retval[priority] = 1;
		return retval;
	}

	public int[] waitingThreads() {
		int[] retval = new int[NativeThread.JAVA_PRIORITY_RANGE+1];
		synchronized (syncLock) {
			if(threadStarted && threadWaiting)
				retval[priority] = 1;
		}
		return retval;
	}

	public int getWaitingThreadsCount() {
		synchronized (syncLock) {
			return (threadStarted && threadWaiting) ? 1 : 0;
		}
	}

	public boolean onThread() {
		synchronized(syncLock) {
			return Thread.currentThread() == runningThread;
		}
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

import freenet.io.comm.AsyncMessageCallback;
import freenet.io.comm.ByteCounter;
import freenet.io.comm.DMT;
import freenet.io.comm.Message;
import freenet.support.Logger;

/** A queued byte[], maybe including a Message, and a callback, which may be null.
 * Note that we always create the byte[] on construction, as almost everywhere
 * which uses a MessageItem needs to know its length immediately. */
public class MessageItem {

	final Message msg;
	final byte[] buf;
	final AsyncMessageCallback[] cb;
	final long submitted;
	/** If true, the buffer may contain several messages, and is formatted
	 * for sending as a single packet.
	 */
	final boolean formatted;
	final ByteCounter ctrCallback;
	private final short priority;
	private long cachedID;
	private boolean hasCachedID;
	final boolean sendLoadRT;
	final boolean sendLoadBulk;
	private long deadline;

	public MessageItem(Message msg2, AsyncMessageCallback[] cb2, ByteCounter ctr, short overridePriority) {
		this.msg = msg2;
		this.cb = cb2;
		formatted = false;
		this.ctrCallback = ctr;
		this.submitted = System.currentTimeMillis();
		if(overridePriority > 0)
			priority = overridePriority;
		else
			priority = msg2.getPriority();
		this.sendLoadRT = msg2 == null ? false : msg2.needsLoadRT();
		this.sendLoadBulk = msg2 == null ? false : msg2.needsLoadBulk();
		buf = msg.encodeToPacket();
		if(buf.length > NewPacketFormat.MAX_MESSAGE_SIZE) {
			// This is bad because fairness between UID's happens at the level of message queueing,
			// and the window size is frequently very small, so if we have really big messages they
			// could cause big problems e.g. starvation of other messages, resulting in timeouts 
			// (especially if there are retransmits).
			Logger.error(this, "WARNING: Message too big: "+buf.length+" for "+msg2, new Exception("error"));
		}
	}

	public MessageItem(Message msg2, AsyncMessageCallback[] cb2, ByteCounter ctr) {
		this.msg = msg2;
		this.cb = cb2;
		formatted = false;
		this.ctrCallback = ctr;
		this.submitted = System.currentTimeMillis();
		priority = msg2.getPriority();
		this.sendLoadRT = msg2.needsLoadRT();
		this.sendLoadBulk = msg2.needsLoadBulk();
		buf = msg.encodeToPacket();
		if(buf.length > NewPacketFormat.MAX_MESSAGE_SIZE) {
			// This is bad because fairness between UID's happens at the level of message queueing,
			// and the window size is frequently very small, so if we have really big messages they
			// could cause big problems e.g. starvation of other messages, resulting in timeouts 
			// (especially if there are retransmits).
			Logger.error(this, "WARNING: Message too big: "+buf.length+" for "+msg2, new Exception("error"));
		}
	}

	public MessageItem(byte[] data, AsyncMessageCallback[] cb2, boolean formatted, ByteCounter ctr, short priority, boolean sendLoadRT, boolean sendLoadBulk) {
		this.cb = cb2;
		this.msg = null;
		this.buf = data;
		this.formatted = formatted;
		if(formatted && buf == null)
			throw new NullPointerException();
		this.ctrCallback = ctr;
		this.submitted = System.currentTimeMillis();
		this.priority = priority;
		this.sendLoadRT = sendLoadRT;
		this.sendLoadBulk = sendLoadBulk;
	}

	/**
	 * Return the data contents of this MessageItem.
	 */
	public byte[] getData() {
		return buf;
	}

	public int getLength() {
		return buf.length;
	}

	/**
	 * @param length The actual number of bytes sent to send this message, including our share of the packet overheads,
	 * *and including alreadyReportedBytes*, which is only used when deciding how many bytes to report to the throttle.
	 */
	public void onSent(int length) {
		//NB: The fact that the bytes are counted before callback notifications is important for load management.
		if(ctrCallback != null) {
			try {
				ctrCallback.sentBytes(length);
			} catch (Throwable t) {
				Logger.error(this, "Caught "+t+" reporting "+length+" sent bytes on "+this, t);
			}
		}
	}

	public short getPriority() {
		return priority;
	}

	@Override
	public String toString() {
		return super.toString()+":formatted="+formatted+",msg="+msg;
	}

	public void onDisconnect() {
		if(cb != null) {
			for(int i=0;i<cb.length;i++) {
				try {
					cb[i].disconnected();
				} catch (Throwable t) {
					Logger.error(this, "Caught "+t+" calling sent() on "+cb[i]+" for "+this, t);
				}
			}
		}
	}

	public void onFailed() {
		if(cb != null) {
			for(int i=0;i<cb.length;i++) {
				try {
					cb[i].fatalError();
				} catch (Throwable t) {
					Logger.error(this, "Caught "+t+" calling sent() on "+cb[i]+" for "+this, t);
				}
			}
		}
	}

	public synchronized long getID() {
		if(hasCachedID) return cachedID;
		cachedID = generateID();
		hasCachedID = true;
		return cachedID;
	}
	
	private long generateID() {
		if(msg == null) return -1;
		Object o = msg.getObject(DMT.UID);
		if(o == null || !(o instanceof Long)) {
			return -1;
		} else {
			return (Long)o;
		}
	}

	/** Called the first time we have sent all of the message. */
	public void onSentAll() {
		if(cb != null) {
			for(int i=0;i<cb.length;i++) {
				try {
					cb[i].sent();
				} catch (Throwable t) {
					Logger.error(this, "Caught "+t+" calling sent() on "+cb[i]+" for "+this, t);
				}
			}
		}
	}

	/** Set the deadline for this message. Called when a message is unqueued, when
	 * we start to send it. Used if the message does not entirely fit in the 
	 * packet, and also if it is retransmitted.
	 * @param time The time (in the future) to set the deadline to.
	 */
	public synchronized void setDeadline(long time) {
		deadline = time;
	}
	
	/** Clear the deadline for this message. */
	public synchronized void clearDeadline() {
		deadline = 0;
	}
	
	/** Get the deadline for this message. 0 means no deadline has been set. */
	public synchronized long getDeadline() {
		return deadline;
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.node.Node;
import freenet.support.SimpleFieldSet;

public class EndListPeerNotesMessage extends FCPMessage {

	final String nodeIdentifier;
	static final String name = "EndListPeerNotes";
	private String identifier;
	
	public EndListPeerNotesMessage(String id, String identifier) {
		this.nodeIdentifier = id;
		this.identifier = identifier;
	}
	
	@Override
	public SimpleFieldSet getFieldSet() {
		SimpleFieldSet sfs = new SimpleFieldSet(true);
		sfs.putSingle("NodeIdentifier", nodeIdentifier);
		if(identifier != null)
			sfs.putSingle("Identifier", identifier);
		return sfs;
	}

	@Override
	public String getName() {
		return name;
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node)
			throws MessageInvalidException {
		throw new MessageInvalidException(ProtocolErrorMessage.INVALID_MESSAGE, "EndListPeerNotes goes from server to client not the other way around", null, false);
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		container.delete(this);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

import freenet.io.comm.PeerParseException;
import freenet.io.comm.ReferenceSignatureVerificationException;
import freenet.support.SimpleFieldSet;

/**
 * Seed node's representation of a client node connecting in order to announce.
 * @author toad
 */
public class SeedClientPeerNode extends PeerNode {

	public SeedClientPeerNode(SimpleFieldSet fs, Node node2, NodeCrypto crypto, PeerManager peers, boolean fromLocal, boolean noSig, OutgoingPacketMangler mangler) throws FSParseException, PeerParseException, ReferenceSignatureVerificationException {
		super(fs, node2, crypto, peers, fromLocal, noSig, mangler, true);
	}

	@Override
	public PeerNodeStatus getStatus(boolean noHeavy) {
		return new PeerNodeStatus(this, noHeavy);
	}

	@Override
	public boolean isDarknet() {
		return false;
	}

	@Override
	public boolean isOpennet() {
		return false; // Not exactly
	}

	@Override
	public boolean isSeed() {
		return true;
	}

	@Override
	public boolean isRealConnection() {
		return false; // We may be connected to the same node as a seed and as a regular connection.
	}

	@Override
	public boolean equals(Object o) {
		if(o == this) return true;
		// Only equal to seednode of its own type.
		// Different to an OpennetPeerNode with the same identity!
		if(o instanceof SeedClientPeerNode) {
			return super.equals(o);
		} else return false;
	}
	
	@Override
	public void onSuccess(boolean insert, boolean ssk) {
		// Ignore
	}
	
	@Override
	public boolean isRoutingCompatible() {
		return false;
	}

	@Override
	public boolean canAcceptAnnouncements() {
		return true;
	}

	@Override
	public boolean recordStatus() {
		return false;
	}
	
	@Override
	public boolean handshakeUnknownInitiator() {
		return true;
	}

	@Override
	public int handshakeSetupType() {
		return FNPPacketMangler.SETUP_OPENNET_SEEDNODE;
	}
	
	@Override
	public boolean shouldSendHandshake() {
		return false;
	}

	@Override
	public boolean disconnected(boolean dumpMessageQueue, boolean dumpTrackers) {
		boolean ret = super.disconnected(true, true);
		node.peers.disconnect(this, false, false, false);
		return ret;
	}

	@Override
	protected boolean generateIdentityFromPubkey() {
		return true;
	}

	@Override
	protected boolean ignoreLastGoodVersion() {
		return true;
	}
	
	@Override
	void startARKFetcher() {
		// Do not start an ARK fetcher.
	}
	
	@Override
	public boolean shouldDisconnectAndRemoveNow() {
		if(!isConnected()) {
			// SeedClientPeerNode's always start off unverified.
			// If it doesn't manage to connect in 60 seconds, dump it.
			// However, we don't want to be dumped *before* we connect,
			// so we need to check that first.
			// Synchronize to avoid messy races.
			synchronized(this) {
				if(timeLastConnectionCompleted() > 0 &&
						System.currentTimeMillis() - lastReceivedPacketTime() > 60*1000)
				return true;
			}
		} else {
			// Disconnect after an hour in any event.
			if(System.currentTimeMillis() - timeLastConnectionCompleted() > 60*60*1000)
				return true;
		}
		return false;
	}

	@Override
	protected void maybeClearPeerAddedTimeOnConnect() {
		// Do nothing
	}

	@Override
	protected boolean shouldExportPeerAddedTime() {
		return true; // For diagnostic purposes only.
	}
	
	@Override
	protected void maybeClearPeerAddedTimeOnRestart(long now) {
		// Do nothing.
	}

	@Override
	public void fatalTimeout() {
		// Disconnect.
		forceDisconnect(true);
	}
	
	public boolean shallWeRouteAccordingToOurPeersLocation() {
		return false; // Irrelevant
	}
	
	protected void onConnect() {
		OpennetManager om = node.getOpennet();
		if(om != null)
			om.seedTracker.onConnectSeed(this);
		super.onConnect();
	}

	@Override
	boolean dontKeepFullFieldSet() {
		return true;
	}


}
package freenet.io.comm;

/** AsyncMessageFilterCallback where the callbacks may do things that take significant time. */
public interface SlowAsyncMessageFilterCallback extends
		AsyncMessageFilterCallback {

	public int getPriority();
	
}
package freenet.support;

import java.util.HashMap;

/**
 * UpdatableSortedLinkedList plus a hashtable. Each item has
 * an indexItem(), which we use to track them. This is completely
 * independant of their sort order, hence "foreign".
 * Note that this class, unlike its parent, does not permit 
 * duplicates.
 */
public class UpdatableSortedLinkedListWithForeignIndex<T extends IndexableUpdatableSortedLinkedListItem<T>> extends UpdatableSortedLinkedList<T> {

    final HashMap<Object, T> map;

    public UpdatableSortedLinkedListWithForeignIndex() {
        super();
        map = new HashMap<Object, T>();
    }
    
    @Override
	public synchronized void add(T item) throws UpdatableSortedLinkedListKilledException {
    	if(killed) throw new UpdatableSortedLinkedListKilledException();
        T i = item;
        if(map.get(i.indexValue()) != null) {
            // Ignore duplicate
            Logger.error(this, "Ignoring duplicate: "+i+" was already present: "+map.get(i.indexValue()));
            return;
        }
        super.add(i);
        map.put(i.indexValue(), item);
        checkList();
    }
    
    @Override
	public synchronized T remove(T item) throws UpdatableSortedLinkedListKilledException {
    	if(killed) throw new UpdatableSortedLinkedListKilledException();
        map.remove(item.indexValue());
        return super.remove(item);
    }
    
	public synchronized T get(Object key) {
		return map.get(key);
	}

    public synchronized boolean containsKey(Object key) {
        return map.containsKey(key);
    }
    
    public synchronized boolean contains(IndexableUpdatableSortedLinkedListItem<?> item) {
    	return containsKey(item.indexValue());
    }

    /**
     * Remove an element from the list by its key.
     * @throws UpdatableSortedLinkedListKilledException 
     */
    public synchronized T removeByKey(Object key) throws UpdatableSortedLinkedListKilledException {
    	if(killed) throw new UpdatableSortedLinkedListKilledException();
        T item = map.get(key);
        if(item != null) remove(item);
        checkList();
        return item;
    }
    
    @Override
	public synchronized void clear() {
    	map.clear();
    	super.clear();
    }
}
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.node.Node;
import freenet.support.SimpleFieldSet;

public class ExpectedMIME extends FCPMessage {

	final String identifier;
	final boolean global;
	final String expectedMIME;
	
	ExpectedMIME(String identifier, boolean global, String expectedMIME) {
		this.identifier = identifier;
		this.global = global;
		this.expectedMIME = expectedMIME;
	}
	
	@Override
	public SimpleFieldSet getFieldSet() {
		SimpleFieldSet fs = new SimpleFieldSet(false);
		fs.putOverwrite("Identifier", identifier);
		fs.put("Global", global);
		fs.putOverwrite("Metadata.ContentType", expectedMIME);
		return fs;
	}

	@Override
	public String getName() {
		return "ExpectedMIME";
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node)
			throws MessageInvalidException {
		// Not supported
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		container.delete(this);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.client.FetchResult;
import freenet.node.Node;
import freenet.support.SimpleFieldSet;

public class DataFoundMessage extends FCPMessage {

	final String identifier;
	final boolean global;
	final String mimeType;
	final long dataLength;
	
	public DataFoundMessage(FetchResult fr, String identifier, boolean global) {
		this.identifier = identifier;
		this.global = global;
		this.mimeType = fr.getMimeType();
		this.dataLength = fr.size();
	}

	public DataFoundMessage(long foundDataLength, String foundDataMimeType, String identifier, boolean global) {
		this.mimeType = foundDataMimeType;
		this.identifier = identifier;
		this.global = global;
		this.dataLength = foundDataLength;
	}

	@Override
	public SimpleFieldSet getFieldSet() {
		SimpleFieldSet fs = new SimpleFieldSet(true);
		fs.putSingle("Identifier", identifier);
		if(global) fs.putSingle("Global", "true");
		fs.putSingle("Metadata.ContentType", mimeType);
		fs.putSingle("DataLength", Long.toString(dataLength));
		return fs;
	}

	@Override
	public String getName() {
		return "DataFound";
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node) throws MessageInvalidException {
		throw new MessageInvalidException(ProtocolErrorMessage.INVALID_MESSAGE, "DataFound goes from server to client not the other way around", identifier, global);
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		container.delete(this);
	}

}
package freenet.pluginmanager;

/**
 * A plugin which needs to know its ClassLoader. This is usually necessary for db4o.
 * 
 * @author xor
 * @deprecated Use PluginClass.class.getClassLoader() instead!
 */
@Deprecated
public interface FredPluginWithClassLoader {
	
	public void setClassLoader(ClassLoader myClassLoader);
	
}
package freenet.node.fcp;

import java.io.IOException;
import java.io.OutputStream;

import com.db4o.ObjectContainer;

import freenet.node.Node;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.SimpleFieldSet;
import freenet.support.Logger.LogLevel;
import freenet.support.api.BucketFactory;
import freenet.support.io.PersistentTempBucketFactory;

public abstract class FCPMessage {
        private static volatile boolean logDEBUG;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logDEBUG = Logger.shouldLog(LogLevel.DEBUG, this);
			}
		});
	}

	public void send(OutputStream os) throws IOException {
		SimpleFieldSet sfs = getFieldSet();
		sfs.setEndMarker(getEndString());
		String msg = sfs.toString();
		os.write((getName()+ '\n').getBytes("UTF-8"));
		os.write(msg.getBytes("UTF-8"));
		if(logDEBUG) {
			Logger.debug(this, "Outgoing FCP message:\n"+getName()+'\n'+sfs.toString());
			Logger.debug(this, "Being handled by "+this);
		}
	}

	String getEndString() {
		return "EndMessage";
	}
	
	public abstract SimpleFieldSet getFieldSet();

	public abstract String getName();
	
	/**
	 * Create a message from a SimpleFieldSet, and the message's name, if possible. 
	 */
	public static FCPMessage create(String name, SimpleFieldSet fs, BucketFactory bfTemp, PersistentTempBucketFactory bfPersistent) throws MessageInvalidException {
		if(name.equals(AddPeer.NAME))
			return new AddPeer(fs);
		if(name.equals(ClientGetMessage.NAME))
			return new ClientGetMessage(fs);
		if(name.equals(ClientHelloMessage.NAME))
			return new ClientHelloMessage(fs);
		if(name.equals(ClientPutComplexDirMessage.NAME))
			return new ClientPutComplexDirMessage(fs, bfTemp, bfPersistent);
		if(name.equals(ClientPutDiskDirMessage.NAME))
			return new ClientPutDiskDirMessage(fs);
		if(name.equals(ClientPutMessage.NAME))
			return new ClientPutMessage(fs);
		if(name.equals(SendBookmarkMessage.NAME))
			return new SendBookmarkMessage(fs);
		if(name.equals(SendURIMessage.NAME))
			return new SendURIMessage(fs);
		if(name.equals(SendTextMessage.NAME))
			return new SendTextMessage(fs);
		if(name.equals(DisconnectMessage.NAME))
			return new DisconnectMessage(fs);
		if(name.equals(FCPPluginMessage.NAME))
			return new FCPPluginMessage(fs);
		if(name.equals(GenerateSSKMessage.NAME))
			return new GenerateSSKMessage(fs);
		if(name.equals(GetConfig.NAME))
			return new GetConfig(fs);
		if(name.equals(GetNode.NAME))
			return new GetNode(fs);
		if(name.equals(GetPluginInfo.NAME))
			return new GetPluginInfo(fs);
		if(name.equals(GetRequestStatusMessage.NAME))
			return new GetRequestStatusMessage(fs);
		if(name.equals(ListPeerMessage.NAME))
			return new ListPeerMessage(fs);
		if(name.equals(ListPeersMessage.NAME))
			return new ListPeersMessage(fs);
		if(name.equals(ListPeerNotesMessage.NAME))
			return new ListPeerNotesMessage(fs);
		if(name.equals(ListPersistentRequestsMessage.NAME))
			return new ListPersistentRequestsMessage(fs);
		if(name.equals(LoadPlugin.NAME))
			return new LoadPlugin(fs);
		if(name.equals(ModifyConfig.NAME))
			return new ModifyConfig(fs);
		if(name.equals(ModifyPeer.NAME))
			return new ModifyPeer(fs);
		if(name.equals(ModifyPeerNote.NAME))
			return new ModifyPeerNote(fs);
		if(name.equals(ModifyPersistentRequest.NAME))
			return new ModifyPersistentRequest(fs);
		if(name.equals(ReloadPlugin.NAME))
			return new ReloadPlugin(fs);
		if(name.equals(RemovePeer.NAME))
			return new RemovePeer(fs);
		if(name.equals(RemovePersistentRequest.NAME)
				|| name.equals(RemovePersistentRequest.ALT_NAME))
			return new RemovePersistentRequest(fs);
		if(name.equals(RemovePlugin.NAME))
			return new RemovePlugin(fs);
		if(name.equals(ShutdownMessage.NAME))
			return new ShutdownMessage();
		if(name.equals(WatchFeedsMessage.NAME))
			return new WatchFeedsMessage(fs);
		if(name.equals(SubscribeUSKMessage.NAME))
			return new SubscribeUSKMessage(fs);
		if(name.equals(WatchFeedsMessage.NAME))
			return new WatchFeedsMessage(fs);
		if(name.equals(UnsubscribeUSKMessage.NAME))
			return new UnsubscribeUSKMessage(fs);
		if(name.equals(TestDDARequestMessage.NAME))
			return new TestDDARequestMessage(fs);
		if(name.equals(TestDDAResponseMessage.NAME))
			return new TestDDAResponseMessage(fs);
		if(name.equals(WatchGlobal.NAME))
			return new WatchGlobal(fs);
		if(name.equals("Void"))
			return null;

		throw new MessageInvalidException(ProtocolErrorMessage.INVALID_MESSAGE, "Unknown message name "+name, null, false);
	}
	
	/**
	 * Create a message from a SimpleFieldSet, and the message's name, if possible. 
	 * Usefull for FCPClients
	 */
	public static FCPMessage create(String name, SimpleFieldSet fs) throws MessageInvalidException {
		return FCPMessage.create(name, fs, null, null);
	}

	/** Do whatever it is that we do with this type of message. 
	 * @throws MessageInvalidException */
	public abstract void run(FCPConnectionHandler handler, Node node) throws MessageInvalidException;

	/** Remove this message and its dependancies (internal objects) from the database. */
	public abstract void removeFrom(ObjectContainer container);

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.client;

import com.db4o.ObjectContainer;

import freenet.keys.FreenetURI;
import freenet.support.api.Bucket;

/**
 * Class to contain everything needed for an insert.
 */
// WARNING: THIS CLASS IS STORED IN DB4O -- THINK TWICE BEFORE ADD/REMOVE/RENAME FIELDS
public class InsertBlock {

	private Bucket data;
	private boolean isFreed;
	public FreenetURI desiredURI;
	public ClientMetadata clientMetadata;
	
	public InsertBlock(Bucket data, ClientMetadata metadata, FreenetURI desiredURI) {
		if(data == null) throw new NullPointerException();
		this.data = data;
		this.isFreed = false;
		if(metadata == null)
			clientMetadata = new ClientMetadata();
		else
			clientMetadata = metadata;
		this.desiredURI = desiredURI;
	}
	
	public Bucket getData() {
		return (isFreed ? null : data);
	}
	
	public void free(ObjectContainer container){
		synchronized (this) {
			if(isFreed) return;
			isFreed = true;
			if(data == null) return;
		}
		data.free();
		if(container != null) {
			data.removeFrom(container);
			data = null; // don't remove twice
		}
		if(container != null)
			container.store(this);
	}
	
	public void removeFrom(ObjectContainer container) {
		if(data != null) {
			container.activate(data, 1);
			data.removeFrom(container);
		}
		if(desiredURI != null) {
			container.activate(desiredURI, 5);
			desiredURI.removeFrom(container);
		}
		if(clientMetadata != null) {
			container.activate(clientMetadata, 5);
			clientMetadata.removeFrom(container);
		}
		container.delete(this);
	}
	
	public void objectOnActivate(ObjectContainer container) {
		// Cascading activation of dependancies
		container.activate(data, 1); // will cascade
		container.activate(desiredURI, 5);
	}

	/** Null out the data so it doesn't get removed in removeFrom().
	 * Call this when the data becomes somebody else's problem. */
	public void nullData() {
		data = null;
	}

	/** Null out the URI so it doesn't get removed in removeFrom().
	 * Call this when the URI becomes somebody else's problem. */
	public void nullURI() {
		this.desiredURI = null;
	}

	public void nullMetadata() {
		this.clientMetadata = null;
	}
	
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.pluginmanager;

public interface FredPluginIPDetector {

	public DetectedIP[] getAddress();
	
}
package freenet.node;

import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.TreeMap;

import freenet.io.xfer.PacketThrottle;
import freenet.node.NewPacketFormat.SentPacket;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.SentTimes;
import freenet.support.Logger.LogLevel;

/** NewPacketFormat's context for each SessionKey. Specifically, packet numbers are unique
 * to a SessionKey, because the packet number is used in encrypting the packet. Hence this
 * class has everything to do with packet numbers - which to use next, which we've sent
 * packets on and are waiting for acks, which we've received and should ack etc.
 * @author toad
 */
public class NewPacketFormatKeyContext {

	public int firstSeqNumUsed = -1;
	public int nextSeqNum;
	public int highestReceivedSeqNum;

	public byte[][] seqNumWatchList = null;
	/** Index of the packet with the lowest sequence number */
	public int watchListPointer = 0;
	public int watchListOffset = 0;
	
	private final TreeMap<Integer, Long> acks = new TreeMap<Integer, Long>();
	private final HashMap<Integer, SentPacket> sentPackets = new HashMap<Integer, SentPacket>();
	/** Keep this many sent times even if the packets are not acked, so we can compute an
	 * accurate round trip time if they are acked after we had decided they were lost. */
	private static final int MAX_SENT_TIMES = 16384;
	/** This must be memory efficient. Given we have one per peer, a TreeMap would be way
	 * too big. */
	private final SentTimes sentTimes = new SentTimes(MAX_SENT_TIMES);
	
	private final Object sequenceNumberLock = new Object();
	
	private static final int REKEY_THRESHOLD = 100;
	/** All acks must be sent within 200ms */
	static final int MAX_ACK_DELAY = 200;
	/** Minimum RTT for purposes of calculating whether to retransmit. 
	 * Must be greater than MAX_ACK_DELAY */
	private static final int MIN_RTT_FOR_RETRANSMIT = 250;
	
	private int maxSeenInFlight;
	
	private static volatile boolean logMINOR;
	private static volatile boolean logDEBUG;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
				logDEBUG = Logger.shouldLog(LogLevel.DEBUG, this);
			}
		});
	}

	NewPacketFormatKeyContext(int ourFirstSeqNum, int theirFirstSeqNum) {
		ourFirstSeqNum &= 0x7FFFFFFF;
		theirFirstSeqNum &= 0x7FFFFFFF;
		
		this.nextSeqNum = ourFirstSeqNum;
		this.watchListOffset = theirFirstSeqNum;
		
		this.highestReceivedSeqNum = theirFirstSeqNum - 1;
		if(this.highestReceivedSeqNum == -1) this.highestReceivedSeqNum = Integer.MAX_VALUE;
	}
	
	boolean canAllocateSeqNum() {
		synchronized(sequenceNumberLock) {
			return nextSeqNum != firstSeqNumUsed;
		}
	}

	int allocateSequenceNumber(BasePeerNode pn) {
		synchronized(sequenceNumberLock) {
			if(firstSeqNumUsed == -1) {
				firstSeqNumUsed = nextSeqNum;
				if(logMINOR) Logger.minor(this, "First seqnum used for " + this + " is " + firstSeqNumUsed);
			} else {
				if(nextSeqNum == firstSeqNumUsed) {
					Logger.error(this, "Blocked because we haven't rekeyed yet");
					pn.startRekeying();
					return -1;
				}
				
				if(firstSeqNumUsed > nextSeqNum) {
					if(firstSeqNumUsed - nextSeqNum < REKEY_THRESHOLD) pn.startRekeying();
				} else {
					if((NewPacketFormat.NUM_SEQNUMS - nextSeqNum) + firstSeqNumUsed < REKEY_THRESHOLD) pn.startRekeying();
				}
			}
			int seqNum = nextSeqNum++;
			if((nextSeqNum == NewPacketFormat.NUM_SEQNUMS) || (nextSeqNum < 0)) {
				nextSeqNum = 0;
			}
			return seqNum;
		}
	}

	/** One of our outgoing packets has been acknowledged.
	 * @return False if we have already acked the packet */
	public void ack(int ack, BasePeerNode pn, SessionKey key) {
		long rtt;
		int maxSize;
		boolean lostBeforeAcked = false;
		boolean validAck = false;
		synchronized(sentPackets) {
			if(logDEBUG) Logger.debug(this, "Acknowledging packet "+ack+" from "+pn);
			SentPacket sent = sentPackets.remove(ack);
			if(sent != null) {
				rtt = sent.acked(key);
				maxSize = (maxSeenInFlight * 2) + 10;
				sentTimes.removeTime(ack);
				validAck = true;
			} else {
				if(logDEBUG) Logger.debug(this, "Already acked or lost "+ack);
				lostBeforeAcked = true;
				long l = sentTimes.removeTime(ack);
				if(l < 0) {
					if(logDEBUG) Logger.debug(this, "No time for "+ack+" - maybe acked twice?");
					return;
				} else {
					rtt = System.currentTimeMillis() - l;
					maxSize = (maxSeenInFlight * 2) + 10;
				}
			}
		}
		
		int rt = (int) Math.min(rtt, Integer.MAX_VALUE);
		PacketThrottle throttle = null;
		if(pn != null) {
			pn.reportPing(rt);
			throttle = pn.getThrottle();
			if(validAck)
				pn.receivedAck(System.currentTimeMillis());
		}
		if(throttle != null) {
			throttle.setRoundTripTime(rt);
			if(!lostBeforeAcked)
				throttle.notifyOfPacketAcknowledged(maxSize);
		}
	}

	/** Queue an ack.
	 * @return -1 If the ack was already queued, or the total number queued.
	 */
	public int queueAck(int seqno) {
		synchronized(acks) {
			if(!acks.containsKey(seqno)) {
				acks.put(seqno, System.currentTimeMillis());
				return acks.size();
			} else return -1;
		}
	}

	public void sent(int sequenceNumber, int length) {
		synchronized(sentPackets) {
			SentPacket sentPacket = sentPackets.get(sequenceNumber);
			if(sentPacket != null) sentPacket.sent(length);
		}
	}

	class AddedAcks {
		/** Are there any urgent acks? */
		final boolean anyUrgentAcks;
		private final HashMap<Integer, Long> moved;
		
		public AddedAcks(boolean mustSend, HashMap<Integer, Long> moved) {
			this.anyUrgentAcks = mustSend;
			this.moved = moved;
		}

		public void abort() {
			synchronized(acks) {
				acks.putAll(moved);
			}
		}
	}
	
	/** Add as many acks as possible to the packet.
	 * @return True if there are any old acks i.e. acks that will force us to send a packet
	 * even if there isn't anything else in it. */
	public AddedAcks addAcks(NPFPacket packet, int maxPacketSize, long now) {
		boolean mustSend = false;
		HashMap<Integer, Long> moved = null;
		int numAcks = 0;
		synchronized(acks) {
			Iterator<Map.Entry<Integer, Long>> it = acks.entrySet().iterator();
			while (it.hasNext() && packet.getLength() < maxPacketSize) {
				Map.Entry<Integer, Long> entry = it.next();
				int ack = entry.getKey();
				// All acks must be sent within 200ms.
				if(entry.getValue() + MAX_ACK_DELAY < now)
					mustSend = true;
				if(logDEBUG) Logger.debug(this, "Trying to ack "+ack);
				if(!packet.addAck(ack)) {
					if(logDEBUG) Logger.debug(this, "Can't add ack "+ack);
					break;
				}
				if(moved == null) {
					// FIXME some more memory efficient representation, since this will normally be very small?
					moved = new HashMap<Integer, Long>();
				}
				moved.put(ack, entry.getValue());
				++numAcks;
				it.remove();
			}
		}
		if(numAcks == 0)
			return null;
		return new AddedAcks(mustSend, moved);
	}

	public int countSentPackets() {
		synchronized(sentPackets) {
			return sentPackets.size();
		}
	}

	public void sent(SentPacket sentPacket, int seqNum, int length) {
		synchronized(sentPackets) {
			if(!sentPacket.messages.isEmpty()) {
				sentTimes.add(seqNum, System.currentTimeMillis());
			}
			sentPacket.sent(length);
			sentPackets.put(seqNum, sentPacket);
			int inFlight = sentPackets.size();
			if(inFlight > maxSeenInFlight) {
				maxSeenInFlight = inFlight;
				if(logDEBUG) Logger.debug(this, "Max seen in flight new record: "+maxSeenInFlight+" for "+this);
			}
		}
	}

	public long timeCheckForLostPackets(double averageRTT) {
		long timeCheck = Long.MAX_VALUE;
		synchronized(sentPackets) {
			// Because MIN_RTT_FOR_RETRANSMIT > MAX_ACK_DELAY, and because averageRTT() includes the actual ack delay, we don't need to add it on here.
			double avgRtt = Math.max(MIN_RTT_FOR_RETRANSMIT, averageRTT);
			Iterator<Map.Entry<Integer, SentPacket>> it = sentPackets.entrySet().iterator();
			while(it.hasNext()) {
				Map.Entry<Integer, SentPacket> e = it.next();
				SentPacket s = e.getValue();
				long t = (long) (s.getSentTime() + (avgRtt + MAX_ACK_DELAY * 1.1));
				if(t < timeCheck) timeCheck = t;
			}
		}
		return timeCheck;
	}

	public void checkForLostPackets(double averageRTT, long curTime, BasePeerNode pn) {
		//Mark packets as lost
		int bigLostCount = 0;
		int count = 0;
		synchronized(sentPackets) {
			// Because MIN_RTT_FOR_RETRANSMIT > MAX_ACK_DELAY, and because averageRTT() includes the actual ack delay, we don't need to add it on here.
			double avgRtt = Math.max(MIN_RTT_FOR_RETRANSMIT, averageRTT);

			Iterator<Map.Entry<Integer, SentPacket>> it = sentPackets.entrySet().iterator();
			while(it.hasNext()) {
				Map.Entry<Integer, SentPacket> e = it.next();
				SentPacket s = e.getValue();
				if(s.getSentTime() < (curTime - (avgRtt + MAX_ACK_DELAY * 1.1))) {
					if(logMINOR) {
						Logger.minor(this, "Assuming packet " + e.getKey() + " has been lost. "
						                + "Delay " + (curTime - s.getSentTime()) + "ms, "
						                + "threshold " + (avgRtt + MAX_ACK_DELAY * 1.1) + "ms");
					}
					s.lost();
					it.remove();
					bigLostCount++;
				} else
					count++;
			}
			if(count > 0 && logMINOR)
				Logger.minor(this, ""+count+" packets in flight with threshold "+(avgRtt + MAX_ACK_DELAY * 1.1) + "ms");
		}
		if(bigLostCount != 0 && pn != null) {
			PacketThrottle throttle = pn.getThrottle();
			if(throttle != null) {
				for(int i=0;i<bigLostCount;i++) {
					throttle.notifyOfPacketLost();
				}
			}
			pn.backoffOnResend();
		}
	}

	public long timeCheckForAcks() {
		long ret = Long.MAX_VALUE;
		synchronized(acks) {
			for(Long l : acks.values()) {
				long timeout = l + MAX_ACK_DELAY;
				if(ret > timeout) ret = timeout;
			}
		}
		return ret;
	}

	public void disconnected() {
		synchronized(sentPackets) {
			Iterator<Map.Entry<Integer, SentPacket>> it = sentPackets.entrySet().iterator();
			while(it.hasNext()) {
				Map.Entry<Integer, SentPacket> e = it.next();
				SentPacket s = e.getValue();
				s.lost();
			}
			sentPackets.clear();
		}
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.node.DarknetPeerNode;
import freenet.node.FSParseException;
import freenet.node.Node;
import freenet.node.PeerNode;
import freenet.support.Base64;
import freenet.support.IllegalBase64Exception;
import freenet.support.Logger;
import freenet.support.SimpleFieldSet;

public class ModifyPeerNote extends FCPMessage {

	static final String NAME = "ModifyPeerNote";
	
	final SimpleFieldSet fs;
	final String identifier;
	
	public ModifyPeerNote(SimpleFieldSet fs) {
		this.fs = fs;
		identifier = fs.get("Identifier");
		fs.removeValue("Identifier");
	}

	@Override
	public SimpleFieldSet getFieldSet() {
		return new SimpleFieldSet(true);
	}

	@Override
	public String getName() {
		return NAME;
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node) throws MessageInvalidException {
		if(!handler.hasFullAccess()) {
			throw new MessageInvalidException(ProtocolErrorMessage.ACCESS_DENIED, "ModifyPeerNote requires full access", identifier, false);
		}
		String nodeIdentifier = fs.get("NodeIdentifier");
		if( nodeIdentifier == null ) {
			throw new MessageInvalidException(ProtocolErrorMessage.MISSING_FIELD, "Error: NodeIdentifier field missing", identifier, false);
		}
		PeerNode pn = node.getPeerNode(nodeIdentifier);
		if(pn == null) {
			FCPMessage msg = new UnknownNodeIdentifierMessage(nodeIdentifier, identifier);
			handler.outputHandler.queue(msg);
			return;
		}
		if(!(pn instanceof DarknetPeerNode)) {
			throw new MessageInvalidException(ProtocolErrorMessage.DARKNET_ONLY, "ModifyPeerNote only available for darknet peers", identifier, false);
		}
		DarknetPeerNode dpn = (DarknetPeerNode) pn;
		int peerNoteType;
		try {
			peerNoteType = fs.getInt("PeerNoteType");
		} catch (FSParseException e) {
			throw new MessageInvalidException(ProtocolErrorMessage.INVALID_FIELD, "Error parsing PeerNoteType field: "+e.getMessage(), identifier, false);
		}
		String encodedNoteText = fs.get("NoteText");
		if( encodedNoteText == null ) {
			throw new MessageInvalidException(ProtocolErrorMessage.MISSING_FIELD, "Error: NoteText field missing", identifier, false);
		}
		String noteText;
		// **FIXME** this should be generalized for multiple peer notes per peer, after PeerNode is similarly generalized
		try {
			noteText = new String(Base64.decode(encodedNoteText));
		} catch (IllegalBase64Exception e) {
			Logger.error(this, "Bad Base64 encoding when decoding a FCP-received private darknet comment SimpleFieldSet", e);
			return;
		}
		if(peerNoteType == Node.PEER_NOTE_TYPE_PRIVATE_DARKNET_COMMENT) {
			dpn.setPrivateDarknetCommentNote(noteText);
		} else {
			FCPMessage msg = new UnknownPeerNoteTypeMessage(peerNoteType, identifier);
			handler.outputHandler.queue(msg);
			return;
		}
		handler.outputHandler.queue(new PeerNote(nodeIdentifier, noteText, peerNoteType, identifier));
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		throw new UnsupportedOperationException();
	}

}
package freenet.client.filter;

import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.EOFException;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.util.HashMap;

import freenet.l10n.NodeL10n;
import freenet.support.Logger;

public class MP3Filter implements ContentDataFilter {

	// Various sources on the Internet.
	// The most comprehensive one appears to be:
	// http://mpgedit.org/mpgedit/mpeg_format/mpeghdr.htm
	// Others:
	// http://www.mp3-tech.org/programmer/frame_header.html
	// http://www.codeproject.com/KB/audio-video/mpegaudioinfo.aspx
	// http://www.id3.org/mp3Frame
	// http://www.mp3-converter.com/mp3codec/
	
	static final short[] [] [] bitRateIndices = {
		//Version 2.5
		{
			{},
			{0, 8, 16, 24, 32, 40, 48, 56, 64, 80, 96, 112, 128, 144, 160},
			{0, 8, 16, 24, 32, 40, 48, 56, 64, 80, 96, 112, 128, 144, 160},
			{0, 32, 48, 56, 64, 80, 96, 112, 128, 144, 160, 176, 192, 224, 256}
		},
		//Reserved
		{
		},
		//Version 2.0
		{
			{},
			{0, 8, 16, 24, 32, 40, 48, 56, 64, 80, 96, 112, 128, 144, 160},
			{0, 8, 16, 24, 32, 40, 48, 56, 64, 80, 96, 112, 128, 144, 160},
			{0, 32, 48, 56, 64, 80, 96, 112, 128, 144, 160, 176, 192, 224, 256}
		},
		//Version 1
		{
			{},
			{0, 32, 40, 48, 56, 64, 80, 96, 112, 128, 160, 192, 224, 256, 320},
			{0, 32, 48, 56, 64, 80, 96, 112, 128, 160, 192, 224, 256, 320, 384},
			{0, 32, 64, 96, 128, 160, 192, 224, 256, 288, 320, 352, 384, 416, 448}
		}
	};

	static final int[] [] sampleRateIndices = {
		//Version 2.5
		{11025, 12000, 8000},
		//Reserved
		{},
		//Version 2.0
		{22050, 24000, 16000},
		//Version 1
		{44100, 48000, 32000}
	};

	public void readFilter(InputStream input, OutputStream output,
			String charset, HashMap<String, String> otherParams,
			FilterCallback cb) throws DataFilterException, IOException {
		filter(input, output);
	}
	
	public void filter(InputStream input, OutputStream output) throws DataFilterException, IOException {
		//FIXME: Add support for free formatted files(highly uncommon)
		DataInputStream in = new DataInputStream(input);
		DataOutputStream out = new DataOutputStream(output);
		boolean foundStream = true;
		int totalFrames = 0;
		int totalCRCs = 0;
		int foundFrames = 0;
		int maxFoundFrames = 0;
		long countLostSyncBytes = 0;
		int countFreeBitrate = 0;
		try {
		int frameHeader = in.readInt();
		foundStream = (frameHeader & 0xffe00000) == 0xffe00000;
		//Seek ahead until we find the Frame sync
		// FIXME surely the sync should be 0xffe00000 ? First 11 bits set, right?
		while(true) {
			if(foundStream && (frameHeader & 0xffe00000) == 0xffe00000){
				//Populate header details
				byte version = (byte) ((frameHeader & 0x00180000) >>> 19); //2 bits
				if(version == 1) {
					foundStream = false;
					continue; // Not valid
				}
				byte layer = (byte) ((frameHeader & 0x00060000) >>> 17); //2 bits
				if(layer == 0) {
					foundStream = false;
					continue; // Not valid
				}
				// WARNING: layer is encoded! 1 = layer 3, 2 = layer 2, 3 = layer 1!
				boolean hasCRC = ((frameHeader & 0x00010000) >>> 16) == 1 ? false : true; //1 bit, but inverted
				byte bitrateIndex = (byte) ((frameHeader & 0x0000f000) >>> 12); //4 bits
				if(bitrateIndex == 0) {
					// FIXME It looks like it would be very hard to support free bitrate.
					// Unfortunately, this is used occasionally e.g. on the chaosradio mp3's.
//					if(!foundStream) {
						// Probably just noise.
						foundStream = false;
						countFreeBitrate++;
						continue; // Not valid
//					}
//					// FIXME l10n
//					throw new DataFilterException("free bitrate MP3 files not supported", "free bitrate MP3 files not supported", "free bitrate MP3 files not supported");
				}
				if(bitrateIndex == 15) {
					foundStream = false;
					continue; // Not valid
				}
				byte samplerateIndex = (byte) ((frameHeader & 0x0000c0000) >>> 10); //2 bits
				if(samplerateIndex == 3) continue; // Not valid
				boolean paddingBit = ((frameHeader & 0x00000300) >>> 9) == 1 ? true : false;
				boolean privateBit = ((frameHeader & 0x00000100) >>> 8) == 1 ? true : false;
				byte channelMode = (byte) ((frameHeader & 0x000000c0) >>> 6); //2 bits
				byte modeExtension = (byte) ((frameHeader & 0x00000030) >>> 4); //2 bits
				/*A small boost in security might be gained by flipping the next
				 * two bits to false. */
				boolean copyright = ((frameHeader & 0x00000008) >>> 3) == 1 ? true : false;
				boolean original = ((frameHeader & 0x00000004) >>> 2) == 1 ? true : false;
				byte emphasis = (byte) ((frameHeader & 0x00000002));
				if(emphasis == 2) {
					foundStream = false;
					continue; // Not valid
				}

				//Generate other values from tables
				int bitrate = bitRateIndices[version][layer][bitrateIndex]*1000;
				int samplerate = sampleRateIndices[version][samplerateIndex];
				int frameLength = 0;
				if(layer == 1 || layer == 2) {
					frameLength = 144*bitrate/samplerate+(paddingBit ? 1 : 0);
				}
				else if(layer == 3) frameLength = (12*bitrate/samplerate+(paddingBit ? 1 : 0))*4;

				short crc = 0;
				
				if(hasCRC) {
					totalCRCs++;
					crc = in.readShort();
					Logger.normal(this, "Found a CRC");
					// FIXME calculate the CRC. It applies to a large number of frames, dependant on the format.
				}
				//Write out the frame
				byte[] frame = null;
				frame = new byte[frameLength-4];
				in.readFully(frame);
				out.writeInt(frameHeader);
				// FIXME CRCs may or may not work. I have not been able to find an mp3 file with CRCs but without free bitrate.
				if(hasCRC)
					out.writeShort(crc);
				out.write(frame);
				totalFrames++;
				foundFrames++;
				if(countLostSyncBytes != 0)
					Logger.normal(this, "Lost sync for "+countLostSyncBytes+" bytes");
				countLostSyncBytes = 0;
				frameHeader = in.readInt();
			} else {
				if(foundFrames != 0)
					Logger.normal(this, "Series of frames: "+foundFrames);
				if(foundFrames > maxFoundFrames) maxFoundFrames = foundFrames;
				foundFrames = 0;
				frameHeader = frameHeader << 8;
				frameHeader |= (in.readUnsignedByte());
				if((frameHeader & 0xffe00000) == 0xffe00000) {
					foundStream = true;
				} else {
					countLostSyncBytes++;
				}
			}

		}
		} catch (EOFException e) {
			if(foundFrames != 0)
				Logger.normal(this, "Series of frames: "+foundFrames);
			if(countLostSyncBytes != 0)
				Logger.normal(this, "Lost sync for "+countLostSyncBytes+" bytes");
			if(totalFrames == 0 || maxFoundFrames < 10) {
				if(countFreeBitrate > 100)
					throw new DataFilterException(l10n("freeBitrateNotSupported"), l10n("freeBitrateNotSupported"), l10n("freeBitrateNotSupportedExplanation"));
				if(totalFrames == 0)
					throw new DataFilterException(l10n("bogusMP3NoFrames"), l10n("bogusMP3NoFrames"), l10n("bogusMP3NoFramesExplanation"));
			}
			
			out.flush();
			Logger.normal(this, totalFrames+" frames, of which "+totalCRCs+" had a CRC");
			return;
		}
	}

	private String l10n(String key) {
		return NodeL10n.getBase().getString("MP3Filter."+key);
	}

	public void writeFilter(InputStream input, OutputStream output,
			String charset, HashMap<String, String> otherParams,
			FilterCallback cb) throws DataFilterException, IOException {
		filter(input, output);
	}
	
	public static void main(String[] args) throws DataFilterException, IOException {
		File f = new File(args[0]);
		FileInputStream fis = new FileInputStream(f);
		File out = new File(args[0]+".filtered.mp3");
		FileOutputStream fos = new FileOutputStream(out);
		MP3Filter filter = new MP3Filter();
//		// Skip some bytes for testing resyncing.
//		byte[] buf = new byte[4096];
//		fis.read(buf);
//		fis.read(buf);
//		fis.read(buf);
//		fis.read(buf);
		filter.readFilter(fis, fos, null, null, null);
		fis.close();
		fos.close();
	}

}
package freenet.clients.http;

import java.io.IOException;
import java.net.URI;

import freenet.client.HighLevelSimpleClient;
import freenet.node.NodeClientCore;
import freenet.support.HTMLNode;
import freenet.support.api.HTTPRequest;

/**
 * Browser Test Toadlet. Accessible from <code>http://.../test/</code>.
 * 
 * Useful to test browser's capabilities:
 * <ul>
 * <li>warn the user about useless enabled features/plugins which might be dangerous</li>
 * <li>Assist the user in configuring his browser properly to surf on freenet</li>
 * <ul>
 */
public class BrowserTestToadlet extends Toadlet {
	BrowserTestToadlet(HighLevelSimpleClient client, NodeClientCore c) {
		super(client);
		this.core=c;
	}
	
	final NodeClientCore core;
	final static String imgWarningMime = 
		"R0lGODdh1AE8AOf9AAABAAcAAAkBAAoDARAAAQcECRYAAxoCAB4BACIBARMK" +
		"ACUBBCcBACoAABQMAw0PDBMPAC4BAiUHADQBABkQASMLAjoABRQSFj0CAUEA" +
		"AyASAC0MASQSAB8VAUYCAk4AAUkEAEgEBS8SAFYAAFEDAFEDBVkCBGAAAVwE" +
		"ACkeAB4fHWUDAGsAAjMdAC0iATIhAW8DAHYAAyQkIjAmAH8AAnoEAEEkACoq" +
		"KosAA4UEADUtAI0BBY4DAJUAAksmAZgCBUMvAKEAAjIxKj00AJsFADIzMUM0" +
		"AqsAAFMuAK0AAqYGBVMzAbAEA7kAALcABzY5OEo9AMIABcYCAL0HAklDAM4A" +
		"Az5APdEABmU6AGA9ANgAAFVDANkAAdsAAtIEB9wAA8oLAVVHANQJAGFFAN8F" +
		"BUhHRHJCAGxHAFpUAExOS21QAGZVAHVOAGtUAGdbAFJVY1dVWXBeAH5aAFxc" +
		"XHFjAG1nAHtmAHtsAH9qAGVmZJBlAnpyAINzAJ5qAZNxAG5yddVKSpt3AI5/" +
		"AHZ2dJeGAJ6DAHx+e6aAAKp/AJSLAIWGg6WPAImFhrqHAImLiKeXALWQAoiN" +
		"kK2cAKmjAK2hALqcAJSWk5mUk6itALulAMqfANGeAJuenLmuAduhAM2pANek" +
		"AMCzANaoAMewAKOloqOlqOGrAOCrBtiwBs+4AN+wANyzAMi+ANq3ANW5A62v" +
		"rM++AOa1AOW6ALOytrK0sd/AANjFANzDAOW/ALm0s7e5ttfLAOrEAOLJAO/D" +
		"AN/MAbu9ur+8wPHJANrVAOnPAO7NAOLUAObSAMHDwO/UAOvXAujZBPXTB+Hf" +
		"ANPDvsfJxfTYAPHbAPrXAO7fAMzKztDLyfbfAM7QzfzfAe7nBPniBfvjAP/h" +
		"APLpAPjnAP3lAP7mAPbrAPvpAP/nANTW0/nuAP3sAPXxAP/tAP/uA9na3vzx" +
		"BP7yANrc2fn1APn0CN/c4dzf2+Di3+Pl4ebm3Ojs6PDy7vb0+PT38/v68fn7" +
		"+Pr7//v9+v/8/v/+9f7//CwAAAAA1AE8AAAI/gABCBxIsKDBgwgTKlzIsKHD" +
		"hxAjSpxIsaLFixgzatzIsaPHjyBDihxJsqTJkyhTqlzJsqXLlzBjypxJs6bN" +
		"mzhz6tzJ8+W/n0CDAuVHtJ8+QJysJLwwqFo+f/z06eMntOpPeT2zat3KtatI" +
		"q1X79btnqYDAAAoFACgAZ57YfmCFwvNKt67du3bjBp2XxyCSPps2gRIlChEW" +
		"AgILoH3wjKren/PwSp5MuXLLqo6J6nNkVuCBQ6JOxeIlDBozacmYFSsmC5KP" +
		"gQWK1CNK2+pcy7hz6949UShcfv3yOesMIAsoU76S/eokKMyM51vqSLqFDZs0" +
		"XoEACFBbKp9YfVax/nIkQ4ZghCBeyJOJYfADmS4JBCboQubDwAVE0nsJ0oAg" +
		"ef/lHZSBE/Spx8NAHkxRIHkHApDggmQ0SFAI6qnX338DYYghAANCKGFBFWrx" +
		"A2IENdCEFjigRdACTKCo4oYsuijQhg9W2GCFZHiBg1oKuQeffPTZJxB++vEH" +
		"4JEJmSgjgOrpyGONBiZ5YooDUYhjfyBW6OSMAXJIoI0ZdgnAAThMkJJV/ewz" +
		"y0AibIIcMYJQAABaKpqlIgBt/IKNNb6oMRAc+ABnW0cbRqBFFwvCcNAPZBAh" +
		"EBFk/DAQAmIs2IUXCISJZEEIHArhDkDiSAao84kKakHuXQnAhqsGiGGn/ohW" +
		"eKpBFSL6IQBJvFdfQbm+hwKXA/Xaxa+thorjqTgiOmtCjDoKAKSSCkSppZhq" +
		"ai1CuQaZpXrKAlDqsQllu6tAqVaIJZPkdVssALB+ai0BNRxqJkpD0ZZPJAMh" +
		"4gkuvaARwJ0K2QmLONi0slYBF8xGlFCRjdclo0EYsJAB6VlgARlgkAgADRGa" +
		"VQCjOVy7bkEokMEEcXeWnATKAqnMckI5brouhiWfPBDAIHohX8yTkhcFGUEU" +
		"1MUVNZCRBLACDV300eu6fHOGOgvUAM8KUUyGxRhrzDEPHoMsMquc+gx0zgNN" +
		"rbPTZyWEgNhBk40Q1WYjXfPLXAoAwxXkTREf/r0/vZXPJQNtIkoy34yCc0NL" +
		"fAMON9K4MJB3RgV120bpDQQGGZkyZILJuYZA0M8L3EdGFANVXrqYA23OxOEA" +
		"mHDF6ga5DntDrJoukOmmq846rQEmQIYYBLEwOoViaNxqAVd0gZjt5CGvPAC5" +
		"v856ob83pHrnn5MR+pCjn06Q7QYJHwXxxlMPvOy7FyQ++VnC3GUE1UMfoO4I" +
		"pYc3GVOQMIBKQfGzDycD8cQqsPENa2ijAxJBBTjAIQ5uYMNPZrFHPxZ2FY4E" +
		"YEO/wxF7EKIE8kQrMekB2NDOsqELok5a6VGPFpTAAO0QYAqi4oEAXhhD991M" +
		"VP8xIQpDxMK3VegK/ooaiBPWI78TAGgAjDKBDrmERDIo8T8zhCGObpQsHDik" +
		"g5EiSAFCSJARzqmEYCPIENmTHiOGqVY7ouEUETLGIrbPh2j8YoAQkELyrLCF" +
		"XxQDGZxQAuKcqW/7eMZAQLEKbmjjG+MQxy8icgZvgOMb3xCHNawBBIHcAB9i" +
		"AcrkNMI8UQXxIHQkg/EAQB/iFOA93kulQTDAhPuRRwpoOYAUZTWmWapnWW+8" +
		"HY5w1yVWupIMsDxIjoZJBhaUjTyhw4HRjrQ5SXWydVk0nSxNNaM6emEHPGJI" +
		"KEdZStigUpffO6HUkAkAZTKtmk3CZi2paZCpaa+cy5RZztLJI9v5skLB/mwV" +
		"DXzyk3zQI3Cx4AY3xLG4R/rrIcWApOIWWEAOFKAAlpgKUBqmkQFgUJwIsegJ" +
		"Ybg9ACygewAYQHp4RIBvGqQA+xMIAdITAWgq4WWbe+nTbCgQi0YtpP/RKGxS" +
		"CoCVkqGlvHuAt6hWtCYIxAL0yVyxCNAFMegUWEx1KoZiSjfy6IcFPGXIUwnC" +
		"UdGRLqQjValJC1LUoyY1TFdNKVVnapCyAgCpmJMngNJa0w2hdCA+Bap6mOCB" +
		"9I0kKGkQSCNUYY2FjsMcibTGQw6hDYKKQxyQJGA0EqMOuPxkkxmxHfgYokeD" +
		"7EAMREDLAHhABisKhAlkyAEBBlA0JjQkr9Cc/h1B6NcQ8NUuQJvFa3rmlSWz" +
		"WBR4a4mCHskzXGOCEwCojUH85Hfa9cSPtiDiwghS6ATecvaEnw1tSElrWuSm" +
		"drWtPUgBhKue4mZIutQ1E3QRMt7hZrCY4YQZeslT3eMaZKVimJfYyNAEEPgV" +
		"JD/ZxzUEwgZRQAOy4BgHOMwhjnCE4xEOicZjF0jQRwo0FQK5AD6AIp6KYkgA" +
		"YUzIKU+4AC9AyAt7AwAGYqUezwlTVGBQy9qoOeM1HgTEYtrQb81yURxFIZvo" +
		"IsMVonWxXfK3rvPbK5IFsjnyuJZdOLyRFx5gAEZ9M8TeJLGJtZTiFUPIxQUp" +
		"spaOXE0qW7k8NQZT/phFlR6jCgTHMJtyldOF0y7hEAwvIkMBWJDCJijVJAvr" +
		"jCd48Q1uLFjBCyZoORryCXGUwxwMZuACLRwHgehCopjFiGYxWj9xNgA9OfrB" +
		"uQSCASV4wQtJADOtrKmEc7HSvREiNRNgjcv4fq08D2Aeq0ftnxTuh0Qc+2AB" +
		"Qokl2/XAC0zoqLGR3dFX03LEAymB6bCsyhKB2guiLkipT51qhAQbNqFsIbQF" +
		"AgLTOfuWBvl2YohdbS1uSNq4tXMdr5CEc5FnfwSIgR6tS5ICGIUaAulDKwxd" +
		"aAWPYxzoEAekQ7EQH4gDHeZIsMEZqDhuKPah94ALRTOS1aw2xOMDATLI/lEK" +
		"8oPcCciwETFB0ldyk6P85NpJCI/82HKCjNzmJ805W+8rqRNQ7SAtF/lJB1Dz" +
		"mkZELQSQkB8PV/SBNJ3nIS0IzGVeAAPkgN8k8Z8M5nQKYSg0weaA+GEfPY4h" +
		"KKQW4Xg4O9AB8Yg/stDWwMNagLEPfmSaN3jPCYO6gvK6+BHQ9xCIHgqpuMMu" +
		"cBzpULg50lGOaSREDuJI/KEPjmiGcoMYaynDPf7R4bx7HideyMF/P096fZBC" +
		"IKDweiQVHPZ0uB7h7GDwHhAyDXSw4/WuRyyFH0kwxwHgGvu4O+mH/xLFEP/4" +
		"K9+HCuZkCgMu9PCLz3061tH4MSCiFavJBBIq/oGOcEA8HWJPR6QX9w1tcKMQ" +
		"2qEFPzaO/Pa7//0puYdZzCALgho6wa9HB/jFT312lMMdAHgO5tB47OAOt+d6" +
		"r3dwbgdJ4uB4AMAI9iB88DeBFFiBFRE1jUBok5ZIiAd+bBd76+AO5RAOdRAA" +
		"alEH4VAOCscOB4iA6TBpbydJa1EE6sB+FniDOJiDCBEMArEJ0LBQD4d4kMZ2" +
		"4jd95tAMa1EQ2cCCj9aClDdpCvUNDqAd1yCBCqEkVLIiLZKFPZYjRoIQHaJm" +
		"IzMyROKFo8YqFwVHZkhTSOMlHoIgCiKG7TQlOBMjXMhpAICFOBNiVmIuDhKH" +
		"UdIqTbIjQ7KFeVYQ/npYJaJyIQGiTN3VU1pABlgCJXvHO3Y0ImdkVYQ4JPmx" +
		"htYSADggBkyQYrlSAwMRhoH4h28oiJrYdxhhCQLhCdgAgwmneB1YDuXAgjpg" +
		"cu6QDv7ni+vAguJ3cJBVaNxgBGuBDPEwEUlwP0ISLLpCLF1IH2KQYgQxLeDS" +
		"husSSumSI0qFhnaGUbXyO9YoM69SKdn4LbSELc7IK9GojQbRjOTxjPA4J31o" +
		"IeqIbqwYK6ciLtKIOvI4LgFwj+TBiGblBXeyOW6Wj+RRa4KIN10gIbVCH6fC" +
		"jbpSLUjTZPFkAO+BJdi4jgw5KlySLhTpEYMgEKogaYiGcA8HfgbIDu+w/mgI" +
		"sQsiyIKQJg6HBWkRx4Df0AYCkQtWiBAzdjnOEibKxTQbYjoFsANdEDIGMTcr" +
		"d2sCkQMR6TGk5ZRjmIYHoZRMiZW51DQms3Qt0wUrE5WgRB5EmSVHWY89gzGN" +
		"8pUGYTsoQJZiyVzjBFzksZZjOJRvWRC21SVR0AUeMBCQYlxzWZY75z0B4Ds3" +
		"ZTtTA1xUyTVrcZXW0kHKBVwrsEcDAZUzdZh16ZjLtRF/oB2qsCfa4A2PFUkN" +
		"lnYNNoDDgBABIAjUt5opWIzf4A3WMA2FBQ5r8JPtEBF7NjzvUT7lgTeIgSFL" +
		"JAC+81UFsTlJsDvgKBA/g0cF8FHMGZ1smZzL/ulDSOKchxMA3skQwTk+w3kk" +
		"xjmGWrQ+5SkyUoec4dmeASIAEXApwHKedqWez3NDOdYlRdMDAsGRYpAp4Gk0" +
		"/3VBUbM2UZOc8xk105kY1qkpTOUF0FYAqGVc0PScBjGgGAqfb7agJikQsXAM" +
		"+3MndKIiAYAG7JANCVEI78BwAfB3SSgAAQAL39CbADALQXkQMERGZGBGAtFZ" +
		"QUAGI2CXRHpKwMUpdUQGd3Q6Y2ZfONWYYrJpneZNR+qXUTpHSbqkPWVLlWgQ" +
		"O+pGAwGkQkqkXkpEZWRrU6pSXBpr8qMeeGNaYjqk4POlZ4qmRAo/YqAWPndO" +
		"aiSHmUgen5Skb+qk/jblPSCQRf9xAORxAJOSpT20pTXUpuQxqB1xkgCwCsrA" +
		"ENtRgAlRC+fAB+JZCdYQBgLxCuQQEe4UOlR5TtBGApGyRFt1SlpgEGrBSkOj" +
		"HsFkQqKiHXiTTbJKQl2yREtUELGqpCYHRuVRq0xwq68kWmt6K+P0TquaZQDg" +
		"qj8wrIhITtMKrAmxVQPwrDilR/qhTq36qhuSqgCwrUvGrQPxMyQgAKhFLCEF" +
		"rrwzXF1gXL81TNekFgLQq940q3IEADswpngTAHsaciq2rK4UTN9aQ/k6rq54" +
		"EQAEAKrADQ4xDO4wCQdhA+JwDg7hCt/wAj+ZowWhXE9WZEpVOQAqUl1i/jsf" +
		"NQUqh3Qs5aSmIwXvJBAMgD+noyIgdlM/IzFx2SUv22k8+3NmIbM/daF+BKMA" +
		"YLJHRR4pWx4rm1sC4bRvBbXtSgZAu5Vd4pxLO5LkYUwqorK/w7IDYbUoa6fg" +
		"IzxB8FFamzpG87W8YxZrc6Q4IrYCYbMdlbMwq0sBcDnHWR6oZQI5h7Qt5bVa" +
		"BLbwNXoSgQzaIQrcICcMcQDnwA6gShAM0AvigAkO0QvfoCK0sIwPIQA/80Pw" +
		"VWfNVTQB8lQEwCj7xBCw5a0YkgNk0AN0QlqzglqqJQDhpR3NsrXr2rpk8LoG" +
		"obsEwLsmgxB5VQDvKXOl66an+1Sopbo39ryT/nq6AvC7QIchzEugLwZv+GMm" +
		"0ksG1Ksd1itkp4u667ozYkC7Rdm9G6oQGBKhYQK+UzAvtGu7c4K7NaVHGEAG" +
		"RzAjYgA/oqQQy9u8AGK/WHcRAdAwgYANduAQe5CLvaAGEoAEmhAN6/CaDgEN" +
		"igUAZQAMJDsQYkZMZGaXPkce5ySoKKaG6hFjNBsgCwBrOZJiGZA8CwJm84kj" +
		"gympQtbCAoLDLfanaCljSdqlJDxm5OFmKKwe50QQJVxHTLzDFdLD9mWRUbJF" +
		"5SG8GGI6KRxPUaweTEymm9UrZFAC13jEbDpP6hEtWtxTZzYkNAzEuKIexGLG" +
		"tXZnRlxDb8zFeDgR/v4mEEvADAznEHVAfeZwDo62DqzwEBzADbAgEJegjBDx" +
		"bQPwUFbTQlu1A8yGZHokBm0bZ8TlBUoAVLLbJZ+mH61mcxhwBKfGBKoGAOdR" +
		"R4OpUZ8cyhnVyq+sasQkBqQMVAnrLgVhyZicHpq8IZycbAdBzFVnzAMxy+ph" +
		"xVsVzOvYPAIRAnijvgLbyRuTRZfczJK4vtMsy07gBTKkbcu6jttyakSgMdYM" +
		"ANiMyte2yvfBBNdUz17Qzqs2yqU8ELZazXp2zdncEfpQBHPCC9jwEAYQDow3" +
		"gCyosQyBFofwDZUGAKVADSOcFhJBHBGbNmtxyYCchAVxtDFHEB0t0iUd/rMp" +
		"7dEoxbRmqakcgXInbRFJVxEzLV5v1hDp81B0khCti9Nya3N3BZuJAdIT8dMd" +
		"0Q+5IBCogA2k2hCr4H/ssA4DuA7igFAJbWnzYIM62NUacdNerRP6sA8CgQXQ" +
		"YAuIo3DiZw4HWA67ENFjgA2jIBB5MAv3kNFhndd6jRsBIBVKAQC1YA2+pxCt" +
		"UA76V4SLp3BmpxABgAvYsIsF8ArPwHl7XdmW7XnAoQ4Exg3FsBB+8A1sjYCH" +
		"PYAOmBBsYA23YBZPgKP/gNeX/dqwzRP6YBRbBwC48A13oBDJoJMdGHtrFw7m" +
		"cLkIkVAaIBCkAAzgwdWxvdzMzRP7gEm3sQHT/vANjHoQkKCAwOgO65B46AAO" +
		"j5YQqxANi2AWVjAL7QAXrt3c6r3eKWFZ//AH/xII3LAMB1EB1sCCU70O+s2C" +
		"bIcOI5gJB4EI2LBIpRoMEkTZlU2Q4exRnbgfvEYQqNil2MmKObKJsgxqeYkQ" +
		"ZejgWrgk1AbNGU7hW6KKakaJbGqHhygQJn4gCv7g1LbcAcYP+DAQo/ANBE4Q" +
		"txB75qDfwajfKkhQ4zCCFT0QlZAMysAjlkALbrEPwWfZ5WIh7IKO6YKRnOIp" +
		"6jzhydIFp2IoLEYGn3SNUv4eVC4s/4gQXJ4oitstIUkqENKQAkHmSLPmAPDk" +
		"Bck77E0QfTNBkYEW/rXwDbdAEIQgDizY4+kwDvvNDgqXSOZgDAQBCdmgDRwg" +
		"EHMwC9Tg3sod1j+3NV2TWgfBmXNijvJ2TFEDMcC7zB2zFl6TNEQTT1hG6qsm" +
		"6gCANp8e63HLVkrD6gEi63t4U3Z+5wIBFLPdD44rEJ2ADcpQaSkQDSOIWJSn" +
		"k27HDf7dsW7A1Lg5AwIxCNfwDPZAQek9gawCOl51EOultuH4zPFzOX+GEODO" +
		"PdepZ8mzPBiF7i9m7ucjPbFj73P77naJPmwYtL7+6/8wQQH/D7oQc5KgcLAw" +
		"Ci3J7G7HYBXGQBHXYJLAC7m5iyA8C8Gwee7deZVtV1w0EF6EpDyER8M6/qyi" +
		"0gWm9V7qsUFa9PGqHiZN9ETiSMMbdPJp9KxRFKn3lkQ6lPM2xpYB++9AsQ8/" +
		"ARyntxZ08FiOVWiHl2D3N2mOdpva0FiSoAACYQi6oAvz4BiaBNus0k2JMVYE" +
		"cU+4OrZXCjX0pLiAehBgvxYm9R/NRKZBtvarYk3qNE3ggvcADU3OFCB6r49A" +
		"L/fr7Rv7cA/6wINmgQbLYIySVlCJFkkLNIAEBA63sAedAQfUEAzyIPB74fVi" +
		"0lXszl48Fbth1VNvr69YxZ4IAfoeBVJL1VTjrPpcQlcuBVNkIFMe/fpSlWS4" +
		"P+uyH1I54JDNDRYaRxAvkAqFBVmHJGmQRHFK/n8MWwAAQgUAtDAL5HDg7n1Z" +
		"sA0+2SVa3PVau9VcqsVayTsj87VH80K1ngVa3l9aqZRcoWmn/nH+9TXuscU6" +
		"pgP/wGX/7Za4/w4AAPFP4MCB/fjhKyUDwEIAdlJhwyaOHTtx4rhh48aNGKEU" +
		"FBhaCUaN2j1+/AgOlMdQ5UqWLV2+hBlT5kyaMQWQIaNygZcuOHF6SeDS59Ao" +
		"AxZi6NLTZwiGZLw8MPADZxcABXDO3Kn0Z1AAA64CMOGTScubOVs6hSr1KoKh" +
		"PncAYNuWDI+uX8PiHAtX7lyGZV9+rRlY8GDChQefJLiv3+J6l1hSSUSpEytK" +
		"d7Z4XCmEFC1g8fD9/luM+B88w6VNnz7txSzDBkFUe/nR4K/qn0pkM8SgxIuX" +
		"JEwXWl1dQvXC4TNbv46tsjiAHl6YLHC5XCVwhiCKY2AiZihdANi1uyW+uvlz" +
		"3NmHvmUofSVg1O3dvw8seuA+fYv5wWMkeM6sWbrULd5HPoHmga9AA00zaiUB" +
		"VErwpQD6AmBBhgoYoMGYCKDLQpsYXEnDql7y0KUFMZzwpQJUerArlk5cicWF" +
		"UlwoxANnpPFAAQXqJ8AA+4kHGEfSmOMJFR54QKUbDNksmGDUuWcfk24crUYp" +
		"p6SyMBerNExCLLfkssv4oBTIycX6uWceedQhRx1dXqGlzZCoaccegeoDb/Of" +
		"lLzEM0899+SzTz9pdCRQQQclNNBIInGEEUYSVURRRxVRJFFEC6VU0EH+xDRT" +
		"TTfltFP4CgA1VFFFraqAAAIANUIAYJzwRFRHhRVWT2eltVZbb8U1V1135bVX" +
		"X38FNlhhhyW2WGOPRTZZZbEMCAA7====";
	
	public void handleMethodGET(URI uri, HTTPRequest request, ToadletContext ctx) throws ToadletContextClosedException, IOException {
		// Yes, we need that in order to test the browser (number of connections per server)
		if (request.isParameterSet("wontload")) return;
		else if (request.isParameterSet("mimeTest")){
			this.writeHTMLReply(ctx, 200, "OK", imgWarningMime);
			return;		
		}
		
		PageNode page = ctx.getPageMaker().getPageNode("Freenet browser testing tool", ctx);
		HTMLNode pageNode = page.outer;
		HTMLNode contentNode = page.content;
		
		if(ctx.isAllowedFullAccess())
			contentNode.addChild(core.alerts.createSummary());
		
		// #### Test MIME inline
		ctx.getPageMaker().getInfobox("infobox-warning", "MIME Inline", contentNode, "mime-inline-test", true).
			//addChild("img", new String[]{"src", "alt"}, new String[]{"data:image/gif;base64,"+imgWarningMime, "Your browser is probably safe."});
			addChild("img", new String[]{"src", "alt"}, new String[]{"?mimeTest", "Your browser is probably safe."});
		
		// #### Test whether we can have more than 10 simultaneous connections to fproxy
		
		HTMLNode maxConnectionsPerServerContent = ctx.getPageMaker().getInfobox("infobox-warning", "Number of connections", contentNode, "browser-connections", true);
		maxConnectionsPerServerContent.addChild("#", "If you do not see a green picture below, your browser is probably missconfigured! Ensure it allows more than 10 connections per server.");
		for(int i = 0; i < 10 ; i++)
			maxConnectionsPerServerContent.addChild("img", "src", ".?wontload");
		maxConnectionsPerServerContent.addChild("img", new String[]{"src", "alt"}, new String[]{"/static/themes/clean/success.gif", "fail!"});

		// #### Test whether JS is available. : should do the test with pictures instead!
		HTMLNode jsTestContent= ctx.getPageMaker().getInfobox("infobox-warning", "Javascript", contentNode, "javascript-test", true);
		HTMLNode jsTest = jsTestContent.addChild("div");
		jsTest.addChild("img", new String[]{"id", "src", "alt"}, new String[]{"JSTEST", "/static/themes/clean/success.gif", "fail!"});
		jsTest.addChild("script", "type", "text/javascript").addChild("%", "document.getElementById('JSTEST').src = '/static/themes/clean/warning.gif';");
		
		this.writeHTMLReply(ctx, 200, "OK", pageNode.generate());
	}

	@Override
	public String path() {
		return "/test/";
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

import java.util.List;

import com.db4o.ObjectContainer;

import freenet.client.async.ChosenBlock;
import freenet.client.async.ClientContext;
import freenet.client.async.ClientRequestScheduler;
import freenet.client.async.ClientRequester;
import freenet.client.async.PersistentChosenBlock;
import freenet.client.async.PersistentChosenRequest;
import freenet.keys.CHKBlock;
import freenet.keys.ClientKey;
import freenet.keys.KeyBlock;
import freenet.keys.SSKBlock;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;

/**
 * Simple SendableInsert implementation. No feedback, no retries, just insert the
 * block. Not designed for use by the client layer. Used by the node layer for the
 * 1 in every 200 successful requests which starts an insert.
 */
public class SimpleSendableInsert extends SendableInsert {

	public final KeyBlock block;
	public final short prioClass;
	private boolean finished;
	public final RequestClient client;
	public final ClientRequestScheduler scheduler;
	      
        private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	public SimpleSendableInsert(NodeClientCore core, KeyBlock block, short prioClass) {
		super(false, false);
		this.block = block;
		this.prioClass = prioClass;
		this.client = core.node.nonPersistentClientBulk;
		if(block instanceof CHKBlock)
			scheduler = core.requestStarters.chkPutSchedulerBulk;
		else if(block instanceof SSKBlock)
			scheduler = core.requestStarters.sskPutSchedulerBulk;
		else
			throw new IllegalArgumentException("Don't know what to do with "+block);
		if(!scheduler.isInsertScheduler())
			throw new IllegalStateException("Scheduler "+scheduler+" is not an insert scheduler!");
	}
	
	public SimpleSendableInsert(KeyBlock block, short prioClass, RequestClient client, ClientRequestScheduler scheduler) {
		super(false, false);
		this.block = block;
		this.prioClass = prioClass;
		this.client = client;
		this.scheduler = scheduler;
	}
	
	@Override
	public void onSuccess(Object keyNum, ObjectContainer container, ClientContext context) {
		// Yay!
		if(logMINOR)
			Logger.minor(this, "Finished insert of "+block);
	}

	@Override
	public void onFailure(LowLevelPutException e, Object keyNum, ObjectContainer container, ClientContext context) {
		if(logMINOR)
			Logger.minor(this, "Failed insert of "+block+": "+e);
	}

	@Override
	public short getPriorityClass(ObjectContainer container) {
		return prioClass;
	}

	@Override
	public SendableRequestSender getSender(ObjectContainer container, ClientContext context) {
		return new SendableRequestSender() {

			public boolean send(NodeClientCore core, RequestScheduler sched, ClientContext context, ChosenBlock req) {
				// Ignore keyNum, key, since this is a single block
				try {
					if(logMINOR) Logger.minor(this, "Starting request: "+this);
					// FIXME bulk flag
					core.realPut(block, req.canWriteClientCache, Node.FORK_ON_CACHEABLE_DEFAULT, Node.PREFER_INSERT_DEFAULT, Node.IGNORE_LOW_BACKOFF_DEFAULT, false);
				} catch (LowLevelPutException e) {
					onFailure(e, req.token, null, context);
					if(logMINOR) Logger.minor(this, "Request failed: "+this+" for "+e);
					return true;
				} finally {
					finished = true;
				}
				if(logMINOR) Logger.minor(this, "Request succeeded: "+this);
				onSuccess(req.token, null, context);
				return true;
			}
		};
	}

	@Override
	public RequestClient getClient(ObjectContainer container) {
		return client;
	}

	@Override
	public ClientRequester getClientRequest() {
		return null;
	}

	@Override
	public boolean isCancelled(ObjectContainer container) {
		return finished;
	}
	
	public boolean isEmpty(ObjectContainer container) {
		return finished;
	}

	public void schedule() {
		finished = false; // can reschedule
		scheduler.registerInsert(this, false, false, null);
	}

	public void cancel(ObjectContainer container, ClientContext context) {
		synchronized(this) {
			if(finished) return;
			finished = true;
		}
		super.unregister(container, context, prioClass);
	}

	@Override
	public synchronized long countAllKeys(ObjectContainer container, ClientContext context) {
		if(finished) return 0;
		return 1;
	}

	@Override
	public synchronized long countSendableKeys(ObjectContainer container, ClientContext context) {
		if(finished) return 0;
		return 1;
	}

	@Override
	public synchronized SendableRequestItem chooseKey(KeysFetchingLocally keys, ObjectContainer container, ClientContext context) {
		if(keys.hasTransientInsert(this, NullSendableRequestItem.nullItem))
			return null;
		if(finished) return null;
		else
			return NullSendableRequestItem.nullItem;
	}

	@Override
	public boolean isSSK() {
		return block instanceof SSKBlock;
	}

	@Override
	public List<PersistentChosenBlock> makeBlocks(PersistentChosenRequest request, RequestScheduler sched, KeysFetchingLocally keys, ObjectContainer container, ClientContext context) {
		// Transient-only so no makeBlocks().
		throw new UnsupportedOperationException();
	}

	@Override
	public boolean canWriteClientCache(ObjectContainer container) {
		return false;
	}

	public void removeFrom(ObjectContainer container, ClientContext context) {
		// Transient-only
		throw new UnsupportedOperationException();
	}

	@Override
	public boolean forkOnCacheable(ObjectContainer container) {
		return Node.FORK_ON_CACHEABLE_DEFAULT;
	}

	@Override
	public void onEncode(SendableRequestItem token, ClientKey key, ObjectContainer container, ClientContext context) {
		// Ignore.
	}

	@Override
	public boolean localRequestOnly(ObjectContainer container) {
		return false;
	}

}
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.node.Node;
import freenet.support.SimpleFieldSet;

/** Status message sent when the whole of a request is waiting for a cooldown.
 * Not when it's all running - that would be a different event.
 * @author toad
 */
public class EnterFiniteCooldown extends FCPMessage {
	
	final String identifier;
	final boolean global;
	final long wakeupTime;

	EnterFiniteCooldown(String identifier, boolean global, long wakeupTime) {
		this.identifier = identifier;
		this.global = global;
		this.wakeupTime = wakeupTime;
	}
	
	@Override
	public SimpleFieldSet getFieldSet() {
		SimpleFieldSet fs = new SimpleFieldSet(false);
		fs.putOverwrite("Identifier", identifier);
		fs.put("Global", global);
		fs.put("WakeupTime", wakeupTime);
		return fs;
	}

	@Override
	public String getName() {
		return "EnterFiniteCooldown";
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node)
			throws MessageInvalidException {
		// Not supported
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		container.delete(this);
	}
	
}
package freenet.node.useralerts;

import java.lang.ref.WeakReference;

import freenet.keys.FreenetURI;
import freenet.l10n.NodeL10n;
import freenet.node.DarknetPeerNode;
import freenet.node.PeerNode;
import freenet.node.fcp.FCPMessage;
import freenet.node.fcp.URIFeedMessage;
import freenet.support.HTMLNode;

public class DownloadFeedUserAlert extends AbstractUserAlert {
	private final WeakReference<PeerNode> peerRef;
	private final FreenetURI uri;
	private final int fileNumber;
	private final String description;
	private final long composed;
	private final long sent;
	private final long received;
	private String sourceNodeName;

	public DownloadFeedUserAlert(DarknetPeerNode sourcePeerNode, 
			String description, int fileNumber, FreenetURI uri, long composed, long sent, long received) {
		super(true, null, null, null, null, UserAlert.MINOR, true, null, true, null);
		this.description = description;
		this.uri = uri;
		this.fileNumber = fileNumber;
		this.composed = composed;
		this.sent = sent;
		this.received = received;
		peerRef = sourcePeerNode.getWeakRef();
		sourceNodeName = sourcePeerNode.getName();
	}

	@Override
	public String getTitle() {
		return l10n("title", "from", sourceNodeName);
	}

	@Override
	public String getText() {
		StringBuilder sb = new StringBuilder();
		sb.append(l10n("fileURI")).append(" ").append(uri).append("\n");
		if(description != null && description.length() != 0)
			sb.append(l10n("fileDescription")).append(" ").append(description);
		return sb.toString();
	}

	@Override
	public String getShortText() {
		return getTitle();
	}

	@Override
	public HTMLNode getHTMLText() {
		HTMLNode alertNode = new HTMLNode("div");
		alertNode.addChild("a", "href", "/" + uri).addChild("#", uri.toShortString());
		if (description != null && description.length() != 0) {
			String[] lines = description.split("\n");
			alertNode.addChild("br");
			alertNode.addChild("br");
			alertNode.addChild("#", l10n("fileDescription"));
			alertNode.addChild("br");
			for (int i = 0; i < lines.length; i++) {
				alertNode.addChild("#", lines[i]);
				if (i != lines.length - 1)
					alertNode.addChild("br");
			}
		}
		return alertNode;
	}

	@Override
	public String dismissButtonText() {
		return l10n("delete");
	}

	private String l10n(String key) {
		return NodeL10n.getBase().getString("DownloadFeedUserAlert." + key);
	}

	private String l10n(String key, String pattern, String value) {
		return NodeL10n.getBase().getString("DownloadFeedUserAlert." + key, pattern, value);
	}

	@Override
	public void onDismiss() {
		DarknetPeerNode pn = (DarknetPeerNode) peerRef.get();
		if(pn != null)
			pn.deleteExtraPeerDataFile(fileNumber);
	}

	@Override
	public FCPMessage getFCPMessage() {
		return new URIFeedMessage(getTitle(), getShortText(), getText(), getPriorityClass(), getUpdatedTime(),
				sourceNodeName, composed, sent, received, uri, description);
	}

	@Override
	public boolean isValid() {
		DarknetPeerNode pn = (DarknetPeerNode) peerRef.get();
		if(pn != null)
			sourceNodeName = pn.getName();
		return true;
	}
}
package freenet.support;

import java.util.LinkedList;

import freenet.node.NodeStats;
import freenet.node.PrioRunnable;
import freenet.support.Logger.LogLevel;
import freenet.support.io.NativeThread;

public class PrioritizedSerialExecutor implements Executor {
	private static volatile boolean logMINOR;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback() {
			@Override
			public void shouldUpdate() {
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	private final LinkedList<Runnable>[] jobs;
	private final int priority;
	private final int defaultPriority;
	private boolean waiting;
	private final boolean invertOrder;

	private String name;
	private Executor realExecutor;
	private boolean running;
	private final ExecutorIdleCallback callback;

	private static final int DEFAULT_JOB_TIMEOUT = 5*60*1000;
	private final int jobTimeout;

	private final Runner runner = new Runner();
	
	private final NodeStats statistics;

	class Runner implements PrioRunnable {

		Thread current;

		public int getPriority() {
			return priority;
		}

		public void run() {
			long lastDumped = System.currentTimeMillis();
			synchronized(jobs) {
				if(current != null) {
					if(current.isAlive()) {
						Logger.error(this, "Already running a thread for "+this+" !!", new Exception("error"));
						return;
					}
				}
				current = Thread.currentThread();
			}
			try {
			while(true) {
				Runnable job = null;
				synchronized(jobs) {
					job = checkQueue();
					if(job == null) {
						waiting = true;
						try {
							//NB: notify only on adding work or this quits early.
							jobs.wait(jobTimeout);
						} catch (InterruptedException e) {
							// Ignore
						}
						waiting=false;
						job = checkQueue();
						if(job == null) {
							running=false;
							return;
						}
					}
				}
				try {
					if(logMINOR)
						Logger.minor(this, "Running job "+job);
					long start = System.currentTimeMillis();
					job.run();
					long end = System.currentTimeMillis();
					if(logMINOR) {
						Logger.minor(this, "Job "+job+" took "+(end-start)+"ms");
					}
				
					if(statistics != null) {
						statistics.reportDatabaseJob(job.toString(), end-start);
					}
				} catch (Throwable t) {
					Logger.error(this, "Caught "+t, t);
					Logger.error(this, "While running "+job+" on "+this);
				}
			}
			} finally {
				synchronized(jobs) {
					current = null;
					running = false;
				}
			}
		}

		private Runnable checkQueue() {
			if(!invertOrder) {
				for(int i=0;i<jobs.length;i++) {
					if(!jobs[i].isEmpty()) {
						if(logMINOR)
							Logger.minor(this, "Chosen job at priority "+i);
						return jobs[i].removeFirst();
					}
				}
			} else {
				for(int i=jobs.length-1;i>=0;i--) {
					if(!jobs[i].isEmpty()) {
						if(logMINOR)
							Logger.minor(this, "Chosen job at priority "+i);
						return jobs[i].removeFirst();
					}
				}
			}
			return null;
		}

	};

	/**
	 *
	 * @param priority
	 * @param internalPriorityCount
	 * @param defaultPriority
	 * @param invertOrder Set if the priorities are thread priorities. Unset if they are request priorities. D'oh!
	 */
	public PrioritizedSerialExecutor(int priority, int internalPriorityCount, int defaultPriority, boolean invertOrder, int jobTimeout, ExecutorIdleCallback callback, NodeStats statistics) {
		@SuppressWarnings("unchecked") LinkedList<Runnable>[] jobs = (LinkedList<Runnable>[])new LinkedList[internalPriorityCount];
		for (int i=0;i<jobs.length;i++) {
			jobs[i] = new LinkedList<Runnable>();
		}
		this.jobs = jobs;
		this.priority = priority;
		this.defaultPriority = defaultPriority;
		this.invertOrder = invertOrder;
		this.jobTimeout = jobTimeout;
		this.callback = callback;
		this.statistics = statistics;
	}

	public PrioritizedSerialExecutor(int priority, int internalPriorityCount, int defaultPriority, boolean invertOrder) {
		this(priority, internalPriorityCount, defaultPriority, invertOrder, DEFAULT_JOB_TIMEOUT, null, null);
	}

	public void start(Executor realExecutor, String name) {
		this.realExecutor=realExecutor;
		this.name=name;
		synchronized (jobs) {
			boolean empty = true;
			for(int i=0;i<jobs.length;i++) {
				if(!jobs[i].isEmpty()) {
					empty = false;
					break;
				}
			}
			if(!empty)
				reallyStart();
		}
	}

	private void reallyStart() {
		synchronized(jobs) {
			if(running) {
				Logger.error(this, "Not reallyStart()ing: ALREADY RUNNING", new Exception("error"));
				return;
			}
			running=true;
			if(logMINOR) Logger.minor(this, "Starting thread... "+name+" : "+runner, new Exception("debug"));
			realExecutor.execute(runner, name);
		}
	}

	public void execute(Runnable job) {
		execute(job, "<noname>");
	}

	public void execute(Runnable job, String jobName) {
		int prio = defaultPriority;
		if(job instanceof PrioRunnable)
			prio = ((PrioRunnable) job).getPriority();
		execute(job, prio, jobName);
	}

	public void execute(Runnable job, int prio, String jobName) {
		synchronized(jobs) {
			if(logMINOR)
				Logger.minor(this, "Queueing "+jobName+" : "+job+" priority "+prio+", executor state: running="+running+" waiting="+waiting);
			jobs[prio].addLast(job);
			jobs.notifyAll();
			if(!running && realExecutor != null) {
				reallyStart();
			}
		}
	}

	public void executeNoDupes(Runnable job, int prio, String jobName) {
		synchronized(jobs) {
			if(jobs[prio].contains(job)) {
				if(logMINOR)
					Logger.minor(this, "Not queueing job: Job already queued: "+job);
				return;
			}

			if(logMINOR)
				Logger.minor(this, "Queueing "+jobName+" : "+job+" priority "+prio+", executor state: running="+running+" waiting="+waiting);

			jobs[prio].addLast(job);
			jobs.notifyAll();
			if(!running && realExecutor != null) {
				reallyStart();
			}
		}
	}

	public void execute(Runnable job, String jobName, boolean fromTicker) {
		execute(job, jobName);
	}

	public int[] runningThreads() {
		int[] retval = new int[NativeThread.JAVA_PRIORITY_RANGE+1];
		if (running)
			retval[priority] = 1;
		return retval;
	}

	public int[] waitingThreads() {
		int[] retval = new int[NativeThread.JAVA_PRIORITY_RANGE+1];
		synchronized(jobs) {
			if(waiting)
				retval[priority] = 1;
		}
		return retval;
	}

	public boolean onThread() {
		Thread running = Thread.currentThread();
		synchronized(jobs) {
			if(runner == null) return false;
			return runner.current == running;
		}
	}

	public int[] getQueuedJobsCountByPriority() {
		int[] retval = new int[jobs.length];
		synchronized(jobs) {
			for(int i=0;i<retval.length;i++)
				retval[i] = jobs[i].size();
		}
		return retval;
	}
	
	@SuppressWarnings("unchecked")
	public LinkedList<Runnable>[] getQueuedJobsByPriority() {
		final LinkedList<Runnable>[] jobsClone = (LinkedList<Runnable>[])new LinkedList[jobs.length];
		
		synchronized(jobs) {
			for(int i=0; i < jobs.length; ++i) {
				jobsClone[i] = (LinkedList<Runnable>) jobs[i].clone();
			}
		}
		
		return jobsClone;
	}

	public int getQueueSize(int priority) {
		synchronized(jobs) {
			return jobs[priority].size();
		}
	}

	public int getWaitingThreadsCount() {
		synchronized(jobs) {
			return (waiting ? 1 : 0);
		}
	}

	public boolean anyQueued() {
		synchronized(jobs) {
			for(int i=0;i<jobs.length;i++)
				if(jobs[i].size() > 0) return true;
		}
		return false;
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.io;

import java.io.BufferedInputStream;
import java.io.DataInputStream;
import java.io.EOFException;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.io.RandomAccessFile;
import java.lang.reflect.Method;
import java.nio.CharBuffer;
import java.nio.charset.Charset;
import java.util.Random;

import freenet.client.DefaultMIMETypes;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.SizeUtil;
import freenet.support.StringValidityChecker;
import freenet.support.Logger.LogLevel;

final public class FileUtil {

	private static final int BUFFER_SIZE = 32*1024;

	public static enum OperatingSystem {
		All,
		MacOS,
		Unix,
		Windows
	};

	private static final OperatingSystem detectedOS;

	private static final Charset fileNameCharset;

	static {
		detectedOS = detectOperatingSystem();

		// I did not find any way to detect the Charset of the file system so I'm using the file encoding charset.
		// On Windows and Linux this is set based on the users configured system language which is probably equal to the filename charset.
		// The worst thing which can happen if we misdetect the filename charset is that downloads fail because the filenames are invalid:
		// We disallow path and file separator characters anyway so its not possible to cause files to be stored in arbitrary places.
		fileNameCharset = getFileEncodingCharset();
	}

        private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	/**
	 * Detects the operating system in which the JVM is running. Returns OperatingSystem.All if the OS is unknown or an error occured.
	 * Therefore this function should never throw.
	 */
	private static final OperatingSystem detectOperatingSystem() { // TODO: Move to the proper class
		try {
			final String name =  System.getProperty("os.name").toLowerCase();

			// Order the if() by probability instead alphabetically to decrease the false-positive rate in case they decide to call it "Windows Mac" or whatever

			// Please adapt sanitizeFileName when adding new OS.

			if(name.indexOf("win") >= 0)
				return OperatingSystem.Windows;

			if(name.indexOf("mac") >= 0)
				return OperatingSystem.MacOS;

			if(name.indexOf("unix") >= 0 || name.indexOf("linux") >= 0 || name.indexOf("freebsd") >= 0)
				return OperatingSystem.Unix;

			Logger.error(FileUtil.class, "Unknown operating system:" + name);
		} catch(Throwable t) {
			Logger.error(FileUtil.class, "Operating system detection failed", t);
		}

		return OperatingSystem.All;
	}

	/**
	 * Returns the Charset which is equal to the "file.encoding" property.
	 * This property is set to the users configured system language on windows for example.
	 *
	 * If any error occurs, the default Charset is returned. Therefore this function should never throw.
	 */
	public static final Charset getFileEncodingCharset() {
		try {
			return Charset.forName(System.getProperty("file.encoding"));
		} catch(Throwable t) {
			return Charset.defaultCharset();
		}
	}


	/** Round up a value to the next multiple of a power of 2 */
	private static final long roundup_2n (long val, int blocksize) {
		int mask=blocksize-1;
		return (val+mask)&~mask;
	}

	/**
	 * Guesstimate real disk usage for a file with a given filename, of a given length.
	 */
	public static long estimateUsage(File file, long flen) {
		/**
		 * It's possible that none of these assumptions are accurate for any filesystem;
		 * this is intended to be a plausible worst case.
		 */
		// Assume 4kB clusters for calculating block usage (NTFS)
		long blockUsage = roundup_2n(flen, 4096);
		// Assume 512 byte filename entries, with 100 bytes overhead, for filename overhead (NTFS)
		String filename = file.getName();
		int nameLength = filename.getBytes().length + 100;
		long filenameUsage = roundup_2n(nameLength, 512);
		// Assume 50 bytes per block tree overhead with 1kB blocks (reiser3 worst case)
		long extra = (roundup_2n(flen, 1024) / 1024) * 50;
		return blockUsage + filenameUsage + extra;
	}

	/**
	 *  Is possParent a parent of filename?
	 * Why doesn't java provide this? :(
	 * */
	public static boolean isParent(File poss, File filename) {
		File canon = FileUtil.getCanonicalFile(poss);
		File canonFile = FileUtil.getCanonicalFile(filename);

		if(isParentInner(poss, filename)) return true;
		if(isParentInner(poss, canonFile)) return true;
		if(isParentInner(canon, filename)) return true;
		if(isParentInner(canon, canonFile)) return true;
		return false;
	}

	private static boolean isParentInner(File possParent, File filename) {
		while(true) {
			if(filename.equals(possParent)) return true;
			filename = filename.getParentFile();
			if(filename == null) return false;
		}
	}

	public static File getCanonicalFile(File file) {
		// Having some problems storing File's in db4o ...
		// It would start up, and canonicalise a file with path "/var/lib/freenet-experimental/persistent-temp-24374"
		// to /var/lib/freenet-experimental/var/lib/freenet-experimental/persistent-temp-24374
		// (where /var/lib/freenet-experimental is the current working dir)
		// Regenerating from path worked. So do that here.
		// And yes, it's voodoo.
		file = new File(file.getPath());
		File result;
		try {
			result = file.getAbsoluteFile().getCanonicalFile();
		} catch (IOException e) {
			result = file.getAbsoluteFile();
		}
		return result;
	}

        public static String readUTF(File file) throws FileNotFoundException, IOException {
            return readUTF(file, 0);
        }

	public static String readUTF(File file, long offset) throws FileNotFoundException, IOException {
		StringBuilder result = new StringBuilder();
		FileInputStream fis = null;
		BufferedInputStream bis = null;
		InputStreamReader isr = null;

		try {
			fis = new FileInputStream(file);
			skipFully(fis, offset);
			bis = new BufferedInputStream(fis);
			isr = new InputStreamReader(bis, "UTF-8");

			char[] buf = new char[4096];
			int length = 0;

			while((length = isr.read(buf)) > 0) {
				result.append(buf, 0, length);
			}

		} finally {
			Closer.close(isr);
			Closer.close(bis);
			Closer.close(fis);
		}
		return result.toString();
	}

	/**
	 * Reliably skip a number of bytes or throw.
	 */
	public static void skipFully(InputStream is, long skip) throws IOException {
		long skipped = 0;
		while(skipped < skip) {
			long x = is.skip(skip - skipped);
			if(x <= 0) throw new IOException("Unable to skip "+(skip - skipped)+" bytes");
			skipped += x;
		}
	}

	public static boolean writeTo(InputStream input, File target) throws FileNotFoundException, IOException {
		DataInputStream dis = null;
		FileOutputStream fos = null;
		File file = File.createTempFile("temp", ".tmp", target.getParentFile());
		if(logMINOR)
			Logger.minor(FileUtil.class, "Writing to "+file+" to be renamed to "+target);

		try {
			dis = new DataInputStream(input);
			fos = new FileOutputStream(file);

			int len = 0;
			byte[] buffer = new byte[4096];
			while ((len = dis.read(buffer)) > 0) {
				fos.write(buffer, 0, len);
			}
		} catch (IOException e) {
			throw e;
		} finally {
			if(dis != null) dis.close();
			if(fos != null) fos.close();
		}

		if(FileUtil.renameTo(file, target))
			return true;
		else {
			file.delete();
			return false;
		}
	}

        public static boolean renameTo(File orig, File dest) {
            // Try an atomic rename
            // Shall we prevent symlink-race-conditions here ?
            if(orig.equals(dest))
                throw new IllegalArgumentException("Huh? the two file descriptors are the same!");
            if(!orig.exists()) {
            	throw new IllegalArgumentException("Original doesn't exist!");
            }
            if (!orig.renameTo(dest)) {
                // Not supported on some systems (Windows)
                if (!dest.delete()) {
                    if (dest.exists()) {
                        Logger.error("FileUtil", "Could not delete " + dest + " - check permissions");
                        System.err.println("Could not delete " + dest + " - check permissions");
                    }
                }
                if (!orig.renameTo(dest)) {
                	String err = "Could not rename " + orig + " to " + dest +
                    	(dest.exists() ? " (target exists)" : "") +
                    	(orig.exists() ? " (source exists)" : "") +
                    	" - check permissions";
                    Logger.error(FileUtil.class, err);
                    System.err.println(err);
                    return false;
                }
            }
            return true;
        }

        /**
         * Like renameTo(), but can move across filesystems, by copying the data.
         * @param f
         * @param file
         */
    	public static boolean moveTo(File orig, File dest, boolean overwrite) {
            if(orig.equals(dest))
                throw new IllegalArgumentException("Huh? the two file descriptors are the same!");
            if(!orig.exists()) {
            	throw new IllegalArgumentException("Original doesn't exist!");
            }
            if(dest.exists()) {
            	if(overwrite)
            		dest.delete();
            	else {
            		System.err.println("Not overwriting "+dest+" - already exists moving "+orig);
            		return false;
            	}
            }
    		if(!orig.renameTo(dest)) {
    			// Copy the data
    			InputStream is = null;
    			OutputStream os = null;
    			try {
    				is = new FileInputStream(orig);
    				os = new FileOutputStream(dest);
    				copy(is, os, orig.length());
    				is.close();
    				is = null;
    				os.close();
    				os = null;
    				orig.delete();
    				return true;
    			} catch (IOException e) {
    				dest.delete();
    				Logger.error(FileUtil.class, "Move failed from "+orig+" to "+dest+" : "+e, e);
    				System.err.println("Move failed from "+orig+" to "+dest+" : "+e);
    				e.printStackTrace();
    				return false;
    			} finally {
    				Closer.close(is);
    				Closer.close(os);
    			}
    		} else return true;
    	}

    /**
     * Sanitizes the given filename to be valid on the given operating system.
     * If OperatingSystem.All is specified this function will generate a filename which fullfils the restrictions of all known OS, currently
     * this is MacOS, Unix and Windows.
     */
	public static String sanitizeFileName(final String fileName, OperatingSystem targetOS, String extraChars) {
		// Filter out any characters which do not exist in the charset.
		final CharBuffer buffer = fileNameCharset.decode(fileNameCharset.encode(fileName)); // Charset are thread-safe

		final StringBuilder sb = new StringBuilder(fileName.length() + 1);

		switch(targetOS) {
			case All: break;
			case MacOS: break;
			case Unix: break;
			case Windows: break;
			default:
				Logger.error(FileUtil.class, "Unsupported operating system: " + targetOS);
				targetOS = OperatingSystem.All;
				break;
		}
		
		char def = ' ';
		if(extraChars.indexOf(' ') != -1) {
			def = '_';
			if(extraChars.indexOf(def) != -1) {
				def = '-';
				if(extraChars.indexOf(def) != -1)
					throw new IllegalArgumentException("What do you want me to use instead of spaces???");
			}
		}

		for(char c : buffer.array()) {
			
			if(extraChars.indexOf(c) != -1) {
				sb.append(def);
				continue;
			}
			
			// Control characters and whitespace are converted to space for all OS.
			// We do not check for the file separator character because it is included in each OS list of reserved characters.
			if(Character.getType(c) == Character.CONTROL || Character.isWhitespace(c)) {
				sb.append(def);
				continue;
			}


			if(targetOS == OperatingSystem.All || targetOS == OperatingSystem.Windows) {
				if(StringValidityChecker.isWindowsReservedPrintableFilenameCharacter(c)) {
					sb.append(def);
					continue;
				}
			}

			if(targetOS == OperatingSystem.All || targetOS == OperatingSystem.MacOS) {
				if(StringValidityChecker.isMacOSReservedPrintableFilenameCharacter(c)) {
					sb.append(def);
					continue;
				}
			}
			
			if(targetOS == OperatingSystem.All || targetOS == OperatingSystem.Unix) {
				if(StringValidityChecker.isUnixReservedPrintableFilenameCharacter(c)) {
					sb.append(def);
					continue;
				}
			}
			
			// Nothing did continue; so the character is okay
			sb.append(c);
		}

		// In windows, the last character of a filename may not be space or dot. We cut them off
		if(targetOS == OperatingSystem.All || targetOS == OperatingSystem.Windows) {
			int lastCharIndex = sb.length() - 1;
			while(lastCharIndex >= 0) {
				char lastChar = sb.charAt(lastCharIndex);
				if(lastChar == ' ' ||  lastChar == '.')
					sb.deleteCharAt(lastCharIndex--);
				else
					break;
			}
		}

		// Now the filename might be one of the reserved filenames in Windows (CON etc.) and we must replace it if it is...
		if(targetOS == OperatingSystem.All || targetOS == OperatingSystem.Windows) {
			if(StringValidityChecker.isWindowsReservedFilename(sb.toString()))
				sb.insert(0, '_');
		}

		if(sb.length() == 0) {
			sb.append("Invalid filename"); // TODO: L10n
		}

		return sb.toString();
	}

	public static String sanitize(String fileName) {
		return sanitizeFileName(fileName, detectedOS, "");
	}

	public static String sanitizeFileNameWithExtras(String fileName, String extraChars) {
		return sanitizeFileName(fileName, detectedOS, extraChars);
	}


	public static String sanitize(String filename, String mimeType) {
		filename = sanitize(filename);
		if(mimeType == null) return filename;
		if(filename.indexOf('.') >= 0) {
			String oldExt = filename.substring(filename.lastIndexOf('.'));
			if(DefaultMIMETypes.isValidExt(mimeType, oldExt)) return filename;
		}
		String defaultExt = DefaultMIMETypes.getExtension(filename);
		if(defaultExt == null) return filename;
		else return filename + '.' + defaultExt;
	}

	/**
	 * Find the length of an input stream. This method will consume the complete
	 * input stream until its {@link InputStream#read(byte[])} method returns
	 * <code>-1</code>, thus signalling the end of the stream.
	 *
	 * @param source
	 *            The input stream to find the length of
	 * @return The numbe of bytes that can be read from the stream
	 * @throws IOException
	 *             if an I/O error occurs
	 */
	public static long findLength(InputStream source) throws IOException {
		long length = 0;
		byte[] buffer = new byte[BUFFER_SIZE];
		int read = 0;
		while (read > -1) {
			read = source.read(buffer);
			if (read != -1) {
				length += read;
			}
		}
		return length;
	}

	/**
	 * Copies <code>length</code> bytes from the source input stream to the
	 * destination output stream. If <code>length</code> is <code>-1</code>
	 * as much bytes as possible will be copied (i.e. until
	 * {@link InputStream#read()} returns <code>-1</code> to signal the end of
	 * the stream).
	 *
	 * @param source
	 *            The input stream to read from
	 * @param destination
	 *            The output stream to write to
	 * @param length
	 *            The number of bytes to copy
	 * @throws IOException
	 *             if an I/O error occurs
	 */
	public static void copy(InputStream source, OutputStream destination, long length) throws IOException {
		long remaining = length;
		byte[] buffer = new byte[BUFFER_SIZE];
		int read = 0;
		while ((remaining == -1) || (remaining > 0)) {
			read = source.read(buffer, 0, ((remaining > BUFFER_SIZE) || (remaining == -1)) ? BUFFER_SIZE : (int) remaining);
			if (read == -1) {
				if (length == -1) {
					return;
				}
				throw new EOFException("stream reached eof");
			}
			destination.write(buffer, 0, read);
			if (remaining > 0)
				remaining -= read;
		}
	}

	/** Delete everything in a directory. Only use this when we are *very sure* there is no
	 * important data below it! */
	public static boolean removeAll(File wd) {
		if(!wd.isDirectory()) {
			System.err.println("DELETING FILE "+wd);
			if(!wd.delete() && wd.exists()) {
				Logger.error(FileUtil.class, "Could not delete file: "+wd);
				return false;
			}
		} else {
			File[] subfiles = wd.listFiles();
			for(int i=0;i<subfiles.length;i++) {
				if(!removeAll(subfiles[i])) return false;
			}
			if(!wd.delete()) {
				Logger.error(FileUtil.class, "Could not delete directory: "+wd);
			}
		}
		return true;
	}

	public static void secureDelete(File file, Random random) throws IOException {
		// FIXME somebody who understands these things should have a look at this...
		if(!file.exists()) return;
		long size = file.length();
		if(size > 0) {
			RandomAccessFile raf = null;
			try {
				System.out.println("Securely deleting "+file+" which is of length "+size+" bytes...");
				raf = new RandomAccessFile(file, "rw");
				raf.seek(0);
				long count;
				byte[] buf = new byte[4096];
				// First zero it out
				count = 0;
				while(count < size) {
					int written = (int) Math.min(buf.length, size - count);
					raf.write(buf, 0, written);
					count += written;
				}
				raf.getFD().sync();
				// Then ffffff it out
				for(int i=0;i<buf.length;i++)
					buf[i] = (byte)0xFF;
				raf.seek(0);
				count = 0;
				while(count < size) {
					int written = (int) Math.min(buf.length, size - count);
					raf.write(buf, 0, written);
					count += written;
				}
				raf.getFD().sync();
				// Then random data
				random.nextBytes(buf);
				raf.seek(0);
				count = 0;
				while(count < size) {
					int written = (int) Math.min(buf.length, size - count);
					raf.write(buf, 0, written);
					count += written;
				}
				raf.getFD().sync();
				raf.seek(0);
				// Then 0's again
				for(int i=0;i<buf.length;i++)
					buf[i] = 0;
				count = 0;
				while(count < size) {
					int written = (int) Math.min(buf.length, size - count);
					raf.write(buf, 0, written);
					count += written;
				}
				raf.getFD().sync();
				raf.close();
				raf = null;
			} finally {
				Closer.close(raf);
			}
		}
		if((!file.delete()) && file.exists())
			throw new IOException("Unable to delete file "+file);
	}

	public static final long getFreeSpace(File dir) {
		// Use JNI to find out the free space on this partition.
		long freeSpace = -1;
		try {
			Class<? extends File> c = dir.getClass();
			Method m = c.getDeclaredMethod("getFreeSpace", new Class<?>[0]);
			if(m != null) {
				Long lFreeSpace = (Long) m.invoke(dir, new Object[0]);
				if(lFreeSpace != null) {
					freeSpace = lFreeSpace.longValue();
					System.err.println("Found free space on node's partition: on " + dir + " = " + SizeUtil.formatSize(freeSpace));
				}
			}
		} catch(NoSuchMethodException e) {
			// Ignore
			freeSpace = -1;
		} catch(Throwable t) {
			System.err.println("Trying to access 1.6 getFreeSpace(), caught " + t);
			freeSpace = -1;
		}
		return freeSpace;
	}

	/**
	** Set owner-only RW on the given file.
	*/
	public static boolean setOwnerRW(File f) {
		return setOwnerPerm(f, true, true, false);
	}

	/**
	** Set owner-only RWX on the given file.
	*/
	public static boolean setOwnerRWX(File f) {
		return setOwnerPerm(f, true, true, true);
	}

	/**
	** Set owner-only permissions on the given file.
	*/
	public static boolean setOwnerPerm(File f, boolean r, boolean w, boolean x) {
		/* JDK6 replace when we upgrade
		boolean b = f.setReadable(false, false);
		b &= f.setWritable(false, false);
		b &= f.setExecutable(false, false);
		b &= f.setReadable(r, true);
		b &= f.setWritable(w, true);
		b &= f.setExecutable(x, true);
		return b;
		*/

		boolean success = true;
		try {

			String[] methods = {"setReadable", "setWritable", "setExecutable"};
			boolean[] perms = {r, w, x};

			for (int i=0; i<methods.length; ++i) {
				Method m = File.class.getDeclaredMethod(methods[i], boolean.class, boolean.class);
				if (m != null) {
					success &= (Boolean)m.invoke(f, false, false);
					success &= (Boolean)m.invoke(f, perms[i], true);
				}
			}

		} catch (NoSuchMethodException e) {
			success = false;
		} catch (java.lang.reflect.InvocationTargetException e) {
			success = false;
		} catch (IllegalAccessException e) {
			success = false;
		} catch (ExceptionInInitializerError e) {
			success = false;
		} catch (RuntimeException e) {
			success = false;
		}
		return success;
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.api;

import freenet.config.ConfigCallback;

/**
 * A callback to be called when a config value of integer type changes.
 * Also reports the current value.
 */
public abstract class IntCallback extends ConfigCallback<Integer> {
}
/*
 * Dijjer - A Peer to Peer HTTP Cache
 * Copyright (C) 2004,2005 Change.Tv, Inc
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */

package freenet.io.comm;

import java.util.HashMap;
import java.util.LinkedList;
import java.util.Map;

import freenet.support.Logger;
import freenet.support.Serializer;
import freenet.support.ShortBuffer;

public class MessageType {

    public static final String VERSION = "$Id: MessageType.java,v 1.6 2005/08/25 17:28:19 amphibian Exp $";

	private static HashMap<Integer, MessageType> _specs = new HashMap<Integer, MessageType>();

	private final String _name;
	private final LinkedList<String> _orderedFields = new LinkedList<String>();
	private final HashMap<String, Class<?>> _fields = new HashMap<String, Class<?>>();
	private final HashMap<String, Class<?>> _linkedListTypes = new HashMap<String, Class<?>>();
	private final boolean internalOnly;
	private final short priority;
	private final boolean isLossyPacketMessage;

	public MessageType(String name, short priority) {
	    this(name, priority, false, false);
	}
	
	public MessageType(String name, short priority, boolean internal, boolean isLossyPacketMessage) {
		_name = name;
		this.priority = priority;
		this.isLossyPacketMessage = isLossyPacketMessage;
		internalOnly = internal;
		Integer id = Integer.valueOf(name.hashCode());
		if (_specs.containsKey(id)) {
			throw new RuntimeException("A message type by the name of " + name + " already exists!");
		}
		_specs.put(id, this);
	}

	public void unregister() {
		_specs.remove(Integer.valueOf(_name.hashCode()));
	}
	
	public void addLinkedListField(String name, Class<?> parameter) {
		_linkedListTypes.put(name, parameter);
		addField(name, LinkedList.class);
	}

	public void addField(String name, Class<?> type) {
		_fields.put(name, type);
		_orderedFields.addLast(name);
	}
	
	public void addRoutedToNodeMessageFields() {
        addField(DMT.UID, Long.class);
        addField(DMT.TARGET_LOCATION, Double.class);
        addField(DMT.HTL, Short.class);
        addField(DMT.NODE_IDENTITY, ShortBuffer.class);
	}

	public boolean checkType(String fieldName, Object fieldValue) {
		if (fieldValue == null) {
			return false;
		}
		Class<?> defClass = _fields.get(fieldName);
		Class<?> valueClass = fieldValue.getClass();
		if(defClass == valueClass) return true;
		if(defClass.isAssignableFrom(valueClass)) return true;
		return false;
	}

	public Class<?> typeOf(String field) {
		return _fields.get(field);
	}

	@Override
	public boolean equals(Object o) {
		if (!(o instanceof MessageType)) {
			return false;
		}
		// We can only register one MessageType for each name.
		// So we can do == here.
		return ((MessageType) o)._name == _name;
	}

	@Override
	public int hashCode() {
	    return _name.hashCode();
	}
	
	public static MessageType getSpec(Integer specID, boolean dontLog) {
		if (!_specs.containsKey(specID)) {
			if(!dontLog)
				Logger.error(MessageType.class, "Unrecognised message type received (" + specID + ')');
			return null;
		}
		return _specs.get(specID);
	}

	public String getName() {
		return _name;
	}

	public Map<String, Class<?>> getFields() {
		return _fields;
	}

	public LinkedList<String> getOrderedFields() {
		return _orderedFields;
	}
	
	public Map<String, Class<?>> getLinkedListTypes() {
		return _linkedListTypes;
	}

    /**
     * @return True if this message is internal-only.
     * If this is the case, any incoming messages in UDP form of this
     * spec will be silently discarded.
     */
    public boolean isInternalOnly() {
        return internalOnly;
    }
	
    /** @return The default priority for the message type. Messages's don't necessarily
     * use this: Message.boostPriority() can increase it for a realtime message, for 
     * instance. */
	public short getDefaultPriority() {
		return priority;
	}

	/** Only works for simple messages!! */
	public int getMaxSize(int maxStringLength) {
		// This method mirrors Message.encodeToPacket.
		int length = 0;
		length += 4; // _spec.getName().hashCode()
		for (Map.Entry<String, Class<?>> entry : _fields.entrySet()) {
			length += Serializer.length(entry.getValue(), maxStringLength);
		}
		return length;
	}

	public boolean isLossyPacketMessage() {
		return isLossyPacketMessage;
	}
}/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support;

import java.util.HashMap;

/**
 * Encodes any character mentioned with a substitute in the HTML spec. This
 * includes nulls, <>, quotes, but not control characters. It should be 
 * safe to put the output of this function into a web page; if it is not 
 * then we have big problems. Because we encode quotes it should also be 
 * safe to include it inside attributes. I am not certain where the list in
 * HTMLEntities came from, but the list of potentially markup-significant
 * characters in [X]HTML is *really* small.
 * 
 * Originally from com.websiteasp.ox pasckage.
 * 
 * @author avian (Yves Lempereur)
 * @author Unique Person@w3nO30p4p9L81xKTXbCaQBOvUww (via Frost)
 */
public class HTMLEncoder {
	public final static CharTable charTable = 
		new CharTable(HTMLEntities.encodeMap);
	
	public static String encode(String s) {
		int n = s.length();
		StringBuilder sb = new StringBuilder(n);
		encodeToBuffer(n, s, sb);
		return sb.toString();
	}

	public static void encodeToBuffer(String s, StringBuilder sb) {
		encodeToBuffer(s.length(), s, sb);
	}
	
	private static void encodeToBuffer(int n, String s, StringBuilder sb) {
		for (int i = 0; i < n; i++) {
			char c = s.charAt(i);
			if(Character.isLetterOrDigit(c)){ //only special characters need checking
				sb.append(c);
			} else if(charTable.containsKey(c)){
                sb.append('&');
                sb.append(charTable.get(c));
                sb.append(';');
			} else{
				sb.append(c);
		}
		}
		
	}

	/**
	 * Encode String so it is safe to be used in XML attribute value and text.
	 * 
	 * HTMLEncode.encode() use some HTML-specific entities (e.g. &amp;) hence not suitable for
	 * generic XML.
	 */
	public static String encodeXML(String s) {
		// Extensible Markup Language (XML) 1.0 (Fifth Edition)
		// [10]   	AttValue	   ::=   	'"' ([^<&"] | Reference)* '"'
		// 								|   "'" ([^<&'] | Reference)* "'"
		// [14]   	CharData	   ::=   	[^<&]* - ([^<&]* ']]>' [^<&]*)
		s = s.replace("&", "&#38;");

		s = s.replace("\"", "&#34;");
		s = s.replace("'", "&#39;");

		s = s.replace("<", "&#60;");
		s = s.replace(">", "&#62;"); // CharData can't contain ']]>'

		return s;
	}
		
	private final static class CharTable{
		private char[] chars;
		private String[] strings;
		private int modulo = 0;
		
		public CharTable(HashMap<Character, String> map){
			int[] keys = new int[map.size()]; 
			int keyIndex = 0;
			
			int max = 0;
			for (Character key : map.keySet()) {
				int val = key.charValue();
				keys[keyIndex++] = val;
				if(val > max) max = val;
			}
			
			modulo = map.size();
			int[] collisionTable = new int[max+1]; //using integers instead of booleans (no cleanup)
			boolean ok=false;
			while (!ok) {
			    ++modulo; //try a higher modulo
			    ok = true;
			    for (int i = 0; ok && i < keys.length; ++i){
			    	keyIndex = keys[i]%modulo; //try this modulo
			    	if (collisionTable[keyIndex] == modulo){ //is this value already used
			    		ok = false;
			    	}
			    	else{
			    		collisionTable[keyIndex] = modulo;
					}
			    }
			}
			//System.out.println("The modulo is:" + modulo); //was The modulo is:1474
			
			chars = new char[modulo];
			strings = new String[modulo];
			for (Character character : map.keySet()) {
				keyIndex = character.charValue()%modulo;
				chars[keyIndex] = character.charValue();
				strings[keyIndex] = map.get(character);
			}
			if (chars[0] == 0 && strings[0] != null) chars[0] = 1;
		}
		
		public boolean containsKey(char key){
			return chars[key%modulo] == key;
		}
		
		public String get(char key){
			return chars[key%modulo] == key? strings[key%modulo]:null;
		}
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

import java.util.ArrayList;
import java.util.HashSet;
import java.util.Vector;

import freenet.io.comm.Peer;
import freenet.l10n.NodeL10n;
import freenet.node.useralerts.AbstractUserAlert;
import freenet.node.useralerts.UserAlert;
import freenet.support.HTMLNode;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.OOMHandler;
import freenet.support.TimeUtil;
import freenet.support.io.NativeThread;
import freenet.support.math.MersenneTwister;

/**
 * @author amphibian
 *
 *         Thread that sends a packet whenever: - A packet needs to be resent immediately -
 *         Acknowledgments or resend requests need to be sent urgently.
 */
// j16sdiz (22-Dec-2008):
// FIXME this is the only class implements Ticker, everbody is using this as
// a generic task scheduler. Either rename this class, or create another tricker for non-Packet tasks
public class PacketSender implements Runnable {

	private static volatile boolean logMINOR;
	private static volatile boolean logDEBUG;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
				logDEBUG = Logger.shouldLog(LogLevel.DEBUG, this);
			}
		});
	}

	/** Maximum time we will queue a message for in milliseconds */
	static final int MAX_COALESCING_DELAY = 100;
	/** Maximum time we will queue a message for in milliseconds if it is bulk data.
	 * Note that we will send the data immediately anyway because it will normally be big
	 * enough to send a full packet. However this impacts the choice of whether to send
	 * realtime or bulk data, see PeerMessageQueue.addMessages(). */
	static final int MAX_COALESCING_DELAY_BULK = 5000;
	/** If opennet is enabled, and there are fewer than this many connections,
	 * we MAY attempt to contact old opennet peers (opennet peers we have
	 * dropped from the routing table but kept around in case we can't connect). */
	static final int MIN_CONNECTIONS_TRY_OLD_OPENNET_PEERS = 5;
	/** We send connect attempts to old-opennet-peers no more than once every
	 * this many milliseconds. */
	static final int MIN_OLD_OPENNET_CONNECT_DELAY_NO_CONNS = 10 * 1000;
	/** We send connect attempts to old-opennet-peers no more than once every
	 * this many milliseconds. */
	static final int MIN_OLD_OPENNET_CONNECT_DELAY = 60 * 1000;
	final NativeThread myThread;
	final Node node;
	NodeStats stats;
	long lastReportedNoPackets;
	long lastReceivedPacketFromAnyNode;
	private Vector<ResendPacketItem> rpiTemp;
	private int[] rpiIntTemp;
	private MersenneTwister localRandom;

	PacketSender(Node node) {
		this.node = node;
		myThread = new NativeThread(this, "PacketSender thread for " + node.getDarknetPortNumber(), NativeThread.MAX_PRIORITY, false);
		myThread.setDaemon(true);
		rpiTemp = new Vector<ResendPacketItem>();
		rpiIntTemp = new int[64];
		localRandom = node.createRandom();
	}

	void start(NodeStats stats) {
		this.stats = stats;
		Logger.normal(this, "Starting PacketSender");
		System.out.println("Starting PacketSender");
		myThread.start();
	}

	private void schedulePeriodicJob() {
		
		node.ticker.queueTimedJob(new Runnable() {

			public void run() {
				try {
					long now = System.currentTimeMillis();
					if (logMINOR)
						Logger.minor(PacketSender.class,
								"Starting shedulePeriodicJob() at " + now);
					PeerManager pm = node.peers;
					pm.maybeLogPeerNodeStatusSummary(now);
					pm.maybeUpdateOldestNeverConnectedDarknetPeerAge(now);
					stats.maybeUpdatePeerManagerUserAlertStats(now);
					stats.maybeUpdateNodeIOStats(now);
					pm.maybeUpdatePeerNodeRoutableConnectionStats(now);

					if (logMINOR)
						Logger.minor(PacketSender.class,
								"Finished running shedulePeriodicJob() at "
										+ System.currentTimeMillis());
				} finally {
					node.ticker.queueTimedJob(this, 1000);
				}
			}
		}, 1000);
	}

	public void run() {
		if(logMINOR) Logger.minor(this, "In PacketSender.run()");
		freenet.support.Logger.OSThread.logPID(this);

                schedulePeriodicJob();
		/*
		 * Index of the point in the nodes list at which we sent a packet and then
		 * ran out of bandwidth. We start the loop from here next time.
		 */
		int brokeAt = 0;
		while(true) {
			lastReceivedPacketFromAnyNode = lastReportedNoPackets;
			try {
				realRun();
			} catch(OutOfMemoryError e) {
				OOMHandler.handleOOM(e);
				System.err.println("Will retry above failed operation...");
			} catch(Throwable t) {
				Logger.error(this, "Caught in PacketSender: " + t, t);
				System.err.println("Caught in PacketSender: " + t);
				t.printStackTrace();
			}
		}
	}

	private void realRun() {
		long now = System.currentTimeMillis();
                PeerManager pm;
		PeerNode[] nodes;

        pm = node.peers;
        synchronized(pm) {
        	nodes = pm.myPeers;
        }

		long nextActionTime = Long.MAX_VALUE;
		long oldTempNow = now;

		boolean canSendThrottled = false;

		int MAX_PACKET_SIZE = node.darknetCrypto.socket.getMaxPacketSize();
		long count = node.outputThrottle.getCount();
		if(count > MAX_PACKET_SIZE)
			canSendThrottled = true;
		else {
			long canSendAt = node.outputThrottle.getNanosPerTick() * (MAX_PACKET_SIZE - count);
			canSendAt = (canSendAt / (1000*1000)) + (canSendAt % (1000*1000) == 0 ? 0 : 1);
			if(logMINOR)
				Logger.minor(this, "Can send throttled packets in "+canSendAt+"ms");
			nextActionTime = Math.min(nextActionTime, now + canSendAt);
		}
		
		/** The earliest time at which a peer needs to send a packet, which is before
		 * now. Throttled if canSendThrottled, otherwise not throttled. */
		long lowestUrgentSendTime = Long.MAX_VALUE;
		/** The peer(s) which lowestUrgentSendTime is referring to */
		ArrayList<PeerNode> urgentSendPeers = null;
		/** The earliest time at which a peer needs to send a packet, which is after
		 * now, where there is a full packet's worth of data to send. 
		 * Throttled if canSendThrottled, otherwise not throttled. */
		long lowestFullPacketSendTime = Long.MAX_VALUE;
		/** The peer(s) which lowestFullPacketSendTime is referring to */
		ArrayList<PeerNode> urgentFullPacketPeers = null;
		/** The earliest time at which a peer needs to send an ack, before now. */
		long lowestAckTime = Long.MAX_VALUE;
		/** The peer(s) which lowestAckTime is referring to */
		ArrayList<PeerNode> ackPeers = null;
		/** The earliest time at which a peer needs to handshake. */
		long lowestHandshakeTime = Long.MAX_VALUE;
		/** The peer(s) which lowestHandshakeTime is referring to */
		ArrayList<PeerNode> handshakePeers = null;

		for(int i = 0; i < nodes.length; i++) {
			now = System.currentTimeMillis();
			int idx = i;
			
			// Basic peer maintenance.
			
			PeerNode pn = nodes[idx];
			// For purposes of detecting not having received anything, which indicates a 
			// serious connectivity problem, we want to look for *any* packets received, 
			// including auth packets.
			lastReceivedPacketFromAnyNode =
				Math.max(pn.lastReceivedPacketTime(), lastReceivedPacketFromAnyNode);
			pn.maybeOnConnect();
			if(pn.shouldDisconnectAndRemoveNow() && !pn.isDisconnecting()) {
				// Might as well do it properly.
				node.peers.disconnect(pn, true, true, false);
			}

			if(pn.isConnected()) {
				
				boolean shouldThrottle = pn.shouldThrottle();
				
				pn.checkForLostPackets();

				// Is the node dead?
				// It might be disconnected in terms of FNP but trying to reconnect via JFK's, so we need to use the time when we last got a *data* packet.
				if(now - pn.lastReceivedDataPacketTime() > pn.maxTimeBetweenReceivedPackets()) {
					Logger.normal(this, "Disconnecting from " + pn + " - haven't received packets recently");
					// Hopefully this is a transient network glitch, but stuff will have already started to timeout, so lets dump the pending messages.
					pn.disconnected(true, false);
					continue;
				} else if(now - pn.lastReceivedAckTime() > pn.maxTimeBetweenReceivedAcks()) {
					// FIXME better to disconnect immediately??? Or check canSend()???
					Logger.normal(this, "Disconnecting from " + pn + " - haven't received acks recently");
					// Do it properly.
					// There appears to be connectivity from them to us but not from us to them.
					// So it is helpful for them to know that we are disconnecting.
					node.peers.disconnect(pn, true, true, false, true, false, 5*1000);
					continue;
				} else if(pn.isRoutable() && pn.noLongerRoutable()) {
					/*
					 NOTE: Whereas isRoutable() && noLongerRoutable() are generally mutually exclusive, this
					 code will only execute because of the scheduled-runnable in start() which executes
					 updateVersionRoutablity() on all our peers. We don't disconnect the peer, but mark it
					 as being incompatible.
					 */
					pn.invalidate(now);
					Logger.normal(this, "shouldDisconnectNow has returned true : marking the peer as incompatible: "+pn);
					continue;
				}

				// The peer is connected.
				
				if(canSendThrottled || !shouldThrottle) {
					// We can send to this peer.
					long sendTime = pn.getNextUrgentTime(now);
					if(sendTime != Long.MAX_VALUE) {
						if(sendTime <= now) {
							// Message is urgent.
							if(sendTime < lowestUrgentSendTime) {
								lowestUrgentSendTime = sendTime;
								if(urgentSendPeers != null)
									urgentSendPeers.clear();
								else
									urgentSendPeers = new ArrayList<PeerNode>();
							}
							if(sendTime <= lowestUrgentSendTime)
								urgentSendPeers.add(pn);
						} else if(pn.fullPacketQueued()) {
							if(sendTime < lowestFullPacketSendTime) {
								lowestFullPacketSendTime = sendTime;
								if(urgentFullPacketPeers != null)
									urgentFullPacketPeers.clear();
								else
									urgentFullPacketPeers = new ArrayList<PeerNode>();
							}
							if(sendTime <= lowestFullPacketSendTime)
								urgentFullPacketPeers.add(pn);
						}
					}
				} else if(shouldThrottle && !canSendThrottled) {
					long ackTime = pn.timeSendAcks();
					if(ackTime != Long.MAX_VALUE) {
						if(ackTime <= now) {
							if(ackTime < lowestAckTime) {
								lowestAckTime = ackTime;
								if(ackPeers != null)
									ackPeers.clear();
								else
									ackPeers = new ArrayList<PeerNode>();
							}
							if(ackTime <= lowestAckTime)
								ackPeers.add(pn);
						}
					}
				}
				
				if(canSendThrottled || !shouldThrottle) {
					long urgentTime = pn.getNextUrgentTime(now);
					// Should spam the logs, unless there is a deadlock
					if(urgentTime < Long.MAX_VALUE && logMINOR)
						Logger.minor(this, "Next urgent time: " + urgentTime + "(in "+(urgentTime - now)+") for " + pn);
					nextActionTime = Math.min(nextActionTime, urgentTime);
				} else {
					nextActionTime = Math.min(nextActionTime, pn.timeCheckForLostPackets());
				}
			} else
				// Not connected

				if(pn.noContactDetails())
					pn.startARKFetcher();

			long handshakeTime = pn.timeSendHandshake(now);
			if(handshakeTime != Long.MAX_VALUE) {
				if(handshakeTime < lowestHandshakeTime) {
					lowestHandshakeTime = handshakeTime;
					if(handshakePeers != null)
						handshakePeers.clear();
					else
						handshakePeers = new ArrayList<PeerNode>();
				}
				if(handshakeTime <= lowestHandshakeTime)
					handshakePeers.add(pn);
			}
			
			long tempNow = System.currentTimeMillis();
			if((tempNow - oldTempNow) > (5 * 1000))
				Logger.error(this, "tempNow is more than 5 seconds past oldTempNow (" + (tempNow - oldTempNow) + ") in PacketSender working with " + pn.userToString());
			oldTempNow = tempNow;
		}
		
		// We may send a packet, send an ack-only packet, or send a handshake.
		
		PeerNode toSendPacket = null;
		PeerNode toSendAckOnly = null;
		PeerNode toSendHandshake = null;
		
		long t = Long.MAX_VALUE;
		
		if(lowestUrgentSendTime <= now) {
			// We need to send a full packet.
			toSendPacket = urgentSendPeers.get(localRandom.nextInt(urgentSendPeers.size()));
			t = lowestUrgentSendTime;
		} else if(lowestFullPacketSendTime < Long.MAX_VALUE) {
			toSendPacket = urgentFullPacketPeers.get(localRandom.nextInt(urgentFullPacketPeers.size()));
			t = lowestFullPacketSendTime;
		} else if(lowestAckTime <= now) {
			// We need to send an ack
			toSendAckOnly = ackPeers.get(localRandom.nextInt(ackPeers.size()));
			t = lowestAckTime;
		}
		
		if(lowestHandshakeTime <= now && t > lowestHandshakeTime) {
			toSendHandshake = handshakePeers.get(localRandom.nextInt(handshakePeers.size()));
			toSendPacket = null;
			toSendAckOnly = null;
		}
		
		if(toSendPacket != null) {
			try {
				if(toSendPacket.maybeSendPacket(now, rpiTemp, rpiIntTemp, false)) {
					count = node.outputThrottle.getCount();
					if(count > MAX_PACKET_SIZE)
						canSendThrottled = true;
					else {
						canSendThrottled = false;
						long canSendAt = node.outputThrottle.getNanosPerTick() * (MAX_PACKET_SIZE - count);
						canSendAt = (canSendAt / (1000*1000)) + (canSendAt % (1000*1000) == 0 ? 0 : 1);
						if(logMINOR)
							Logger.minor(this, "Can send throttled packets in "+canSendAt+"ms");
						nextActionTime = Math.min(nextActionTime, now + canSendAt);
					}
				}
			} catch (BlockedTooLongException e) {
				Logger.error(this, "Waited too long: "+TimeUtil.formatTime(e.delta)+" to allocate a packet number to send to "+toSendPacket+" on "+e.tracker+" : "+(toSendPacket.isOldFNP() ? "(old packet format)" : "(new packet format)")+" (version "+toSendPacket.getVersionNumber()+") - DISCONNECTING!");
				toSendPacket.forceDisconnect(true);
				onForceDisconnectBlockTooLong(toSendPacket, e);
			}

			if(canSendThrottled || !toSendPacket.shouldThrottle()) {
				long urgentTime = toSendPacket.getNextUrgentTime(now);
				// Should spam the logs, unless there is a deadlock
				if(urgentTime < Long.MAX_VALUE && logMINOR)
					Logger.minor(this, "Next urgent time: " + urgentTime + "(in "+(urgentTime - now)+") for " + toSendPacket);
				nextActionTime = Math.min(nextActionTime, urgentTime);
			} else {
				nextActionTime = Math.min(nextActionTime, toSendPacket.timeCheckForLostPackets());
			}

		} else if(toSendAckOnly != null) {
			try {
				if(toSendAckOnly.maybeSendPacket(now, rpiTemp, rpiIntTemp, true)) {
					count = node.outputThrottle.getCount();
					if(count > MAX_PACKET_SIZE)
						canSendThrottled = true;
					else {
						canSendThrottled = false;
						long canSendAt = node.outputThrottle.getNanosPerTick() * (MAX_PACKET_SIZE - count);
						canSendAt = (canSendAt / (1000*1000)) + (canSendAt % (1000*1000) == 0 ? 0 : 1);
						if(logMINOR)
							Logger.minor(this, "Can send throttled packets in "+canSendAt+"ms");
						nextActionTime = Math.min(nextActionTime, now + canSendAt);
					}
				}
			} catch (BlockedTooLongException e) {
				Logger.error(this, "Waited too long: "+TimeUtil.formatTime(e.delta)+" to allocate a packet number to send to "+toSendAckOnly+" on "+e.tracker+" : "+(toSendAckOnly.isOldFNP() ? "(old packet format)" : "(new packet format)")+" (version "+toSendAckOnly.getVersionNumber()+") - DISCONNECTING!");
				toSendAckOnly.forceDisconnect(true);
				onForceDisconnectBlockTooLong(toSendAckOnly, e);
			}

			if(canSendThrottled || !toSendAckOnly.shouldThrottle()) {
				long urgentTime = toSendAckOnly.getNextUrgentTime(now);
				// Should spam the logs, unless there is a deadlock
				if(urgentTime < Long.MAX_VALUE && logMINOR)
					Logger.minor(this, "Next urgent time: " + urgentTime + "(in "+(urgentTime - now)+") for " + toSendAckOnly);
				nextActionTime = Math.min(nextActionTime, urgentTime);
			} else {
				nextActionTime = Math.min(nextActionTime, toSendAckOnly.timeCheckForLostPackets());
			}
		}
		
		if(toSendHandshake != null) {
			// Send handshake if necessary
			long beforeHandshakeTime = System.currentTimeMillis();
			toSendHandshake.getOutgoingMangler().sendHandshake(toSendHandshake, false);
			long afterHandshakeTime = System.currentTimeMillis();
			if((afterHandshakeTime - beforeHandshakeTime) > (2 * 1000))
				Logger.error(this, "afterHandshakeTime is more than 2 seconds past beforeHandshakeTime (" + (afterHandshakeTime - beforeHandshakeTime) + ") in PacketSender working with " + toSendHandshake.userToString());
		}
		
		// All of these take into account whether the data can be sent already.
		// So we can include them in nextActionTime.
		nextActionTime = Math.min(nextActionTime, lowestUrgentSendTime);
		nextActionTime = Math.min(nextActionTime, lowestFullPacketSendTime);
		nextActionTime = Math.min(nextActionTime, lowestAckTime);
		nextActionTime = Math.min(nextActionTime, lowestHandshakeTime);

		// FIXME: If we send something we will have to go around the loop again.
		// OPTIMISATION: We could track the second best, and check how many are in the array.
		
		/* Attempt to connect to old-opennet-peers.
		 * Constantly send handshake packets, in order to get through a NAT.
		 * Most JFK(1)'s are less than 300 bytes. 25*300/15 = avg 500B/sec bandwidth cost.
		 * Well worth it to allow us to reconnect more quickly. */

		OpennetManager om = node.getOpennet();
		if(om != null && node.getUptime() > 30*1000) {
			PeerNode[] peers = om.getOldPeers();

			for(PeerNode pn : peers) {
				if(pn.timeLastConnected() <= 0)
					Logger.error(this, "Last connected is zero or negative for old-opennet-peer "+pn);
				// Will be removed by next line.
				if(now - pn.timeLastConnected() > OpennetManager.MAX_TIME_ON_OLD_OPENNET_PEERS) {
					om.purgeOldOpennetPeer(pn);
					if(logMINOR) Logger.minor(this, "Removing old opennet peer (too old): "+pn+" age is "+TimeUtil.formatTime(now - pn.timeLastConnected()));
					continue;
				}
				if(pn.isConnected()) continue; // Race condition??
				if(pn.noContactDetails()) {
					pn.startARKFetcher();
					continue;
				}
				if(pn.shouldSendHandshake()) {
					// Send handshake if necessary
					long beforeHandshakeTime = System.currentTimeMillis();
					pn.getOutgoingMangler().sendHandshake(pn, true);
					long afterHandshakeTime = System.currentTimeMillis();
					if((afterHandshakeTime - beforeHandshakeTime) > (2 * 1000))
						Logger.error(this, "afterHandshakeTime is more than 2 seconds past beforeHandshakeTime (" + (afterHandshakeTime - beforeHandshakeTime) + ") in PacketSender working with " + pn.userToString());
				}
			}

		}

		long oldNow = now;

		// Send may have taken some time
		now = System.currentTimeMillis();

		if((now - oldNow) > (10 * 1000))
			Logger.error(this, "now is more than 10 seconds past oldNow (" + (now - oldNow) + ") in PacketSender");

		long sleepTime = nextActionTime - now;
		
		// MAX_COALESCING_DELAYms maximum sleep time - same as the maximum coalescing delay
		sleepTime = Math.min(sleepTime, MAX_COALESCING_DELAY);

		if(now - node.startupTime > 60 * 1000 * 5)
			if(now - lastReceivedPacketFromAnyNode > Node.ALARM_TIME) {
				Logger.error(this, "Have not received any packets from any node in last " + Node.ALARM_TIME / 1000 + " seconds");
				lastReportedNoPackets = now;
			}

		if(sleepTime > 0) {
			// Update logging only when have time to do so
			try {
				if(logMINOR)
					Logger.minor(this, "Sleeping for " + sleepTime);
				synchronized(this) {
					wait(sleepTime);
				}
			} catch(InterruptedException e) {
			// Ignore, just wake up. Probably we got interrupt()ed
			// because a new packet came in.
			}
		} else {
			if(logDEBUG)
				Logger.debug(this, "Next urgent time is "+(now - nextActionTime)+"ms in the past");
		}
	}

	private final HashSet<Peer> peersDumpedBlockedTooLong = new HashSet<Peer>();

	private void onForceDisconnectBlockTooLong(PeerNode pn, BlockedTooLongException e) {
		Peer p = pn.getPeer();
		synchronized(peersDumpedBlockedTooLong) {
			peersDumpedBlockedTooLong.add(p);
			if(peersDumpedBlockedTooLong.size() > 1) return;
		}
		if(node.clientCore == null || node.clientCore.alerts == null)
			return;
		// FIXME XXX: We have had this alert enabled for MONTHS which got us hundreds of bug reports about it. Unfortunately, nobody spend any work on fixing
		// the issue after the alert was added so I have disabled it to quit annoying our users. We should not waste their time if we don't do anything. xor
		// Notice that the same alert is commented out in FNPPacketMangler.
		// node.clientCore.alerts.register(peersDumpedBlockedTooLongAlert);
	}

	@SuppressWarnings("unused")
	private final UserAlert peersDumpedBlockedTooLongAlert = new AbstractUserAlert() {

        @Override
		public String anchor() {
			return "disconnectedStillNotAcked";
		}

        @Override
		public String dismissButtonText() {
			return null;
		}

        @Override
		public short getPriorityClass() {
			return UserAlert.ERROR;
		}

        @Override
		public String getShortText() {
			int sz;
			synchronized(peersDumpedBlockedTooLong) {
				sz = peersDumpedBlockedTooLong.size();
			}
			return l10n("somePeersDisconnectedBlockedTooLong", "count", Integer.toString(sz));
		}

        @Override
		public HTMLNode getHTMLText() {
			HTMLNode div = new HTMLNode("div");
			Peer[] peers;
			synchronized(peersDumpedBlockedTooLong) {
				peers = peersDumpedBlockedTooLong.toArray(new Peer[peersDumpedBlockedTooLong.size()]);
			}
			NodeL10n.getBase().addL10nSubstitution(div, "PacketSender.somePeersDisconnectedBlockedTooLongDetail",
					new String[] { "count", "link" }
					, new HTMLNode[] { HTMLNode.text(peers.length), HTMLNode.link("/?_CHECKED_HTTP_=https://bugs.freenetproject.org/")});
			HTMLNode list = div.addChild("ul");
			for(Peer peer : peers) {
				list.addChild("li", peer.toString());
			}
			return div;
		}

        @Override
		public String getText() {
			StringBuilder sb = new StringBuilder();
			Peer[] peers;
			synchronized(peersDumpedBlockedTooLong) {
				peers = peersDumpedBlockedTooLong.toArray(new Peer[peersDumpedBlockedTooLong.size()]);
			}
			sb.append(l10n("somePeersDisconnectedStillNotAckedDetail",
					new String[] { "count", "link", "/link" },
					new String[] { Integer.toString(peers.length), "", "" } ));
			sb.append('\n');
			for(Peer peer : peers) {
				sb.append('\t');
				sb.append(peer.toString());
				sb.append('\n');
			}
			return sb.toString();
		}

        @Override
		public String getTitle() {
			return getShortText();
		}

        @Override
		public Object getUserIdentifier() {
			return PacketSender.this;
		}

        @Override
		public boolean isEventNotification() {
			return false;
		}

        @Override
		public boolean isValid() {
			return true;
		}

        @Override
		public void isValid(boolean validity) {
			// Ignore
		}

        @Override
		public void onDismiss() {
			// Ignore
		}

        @Override
		public boolean shouldUnregisterOnDismiss() {
			return false;
		}

        @Override
		public boolean userCanDismiss() {
			return false;
		}

	};

	/** Wake up, and send any queued packets. */
	void wakeUp() {
		// Wake up if needed
		synchronized(this) {
			notifyAll();
		}
	}

	protected String l10n(String key, String[] patterns, String[] values) {
		return NodeL10n.getBase().getString("PacketSender."+key, patterns, values);
	}

	protected String l10n(String key, String pattern, String value) {
		return NodeL10n.getBase().getString("PacketSender."+key, pattern, value);
	}
}
package freenet.store;

import java.io.IOException;

import freenet.crypt.DSAPublicKey;
import freenet.keys.NodeSSK;
import freenet.keys.SSKBlock;
import freenet.keys.SSKVerifyException;
import freenet.node.GetPubkey;

public class SSKStore extends StoreCallback<SSKBlock> {

	private final GetPubkey pubkeyCache;
	
	public SSKStore(GetPubkey pubkeyCache) {
		this.pubkeyCache = pubkeyCache;
	}
	
	@Override
	public SSKBlock construct(byte[] data, byte[] headers,
			byte[] routingKey, byte[] fullKey, 
			boolean canReadClientCache, boolean canReadSlashdotCache, BlockMetadata meta, DSAPublicKey knownPublicKey) 
	throws SSKVerifyException {
		if(data == null || headers == null) throw new SSKVerifyException("Need data and headers");
		if(fullKey == null) throw new SSKVerifyException("Need full key to reconstruct an SSK");
		NodeSSK key;
		key = NodeSSK.construct(fullKey);
		if(knownPublicKey != null)
			key.setPubKey(knownPublicKey);
		else if(!key.grabPubkey(pubkeyCache, canReadClientCache, canReadSlashdotCache, meta))
			throw new SSKVerifyException("No pubkey found");
		SSKBlock block = new SSKBlock(data, headers, key, false);
		return block;
	}
	
	public SSKBlock fetch(NodeSSK chk, boolean dontPromote, boolean canReadClientCache, boolean canReadSlashdotCache, boolean ignoreOldBlocks, BlockMetadata meta) throws IOException {
		return store.fetch(chk.getRoutingKey(), chk.getFullKey(), dontPromote, canReadClientCache, canReadSlashdotCache, ignoreOldBlocks, meta);
	}

	public void put(SSKBlock b, boolean overwrite, boolean isOldBlock) throws IOException, KeyCollisionException {
		store.put(b, b.getRawData(), b.getRawHeaders(), overwrite, isOldBlock);
	}
	
	@Override
	public int dataLength() {
		return SSKBlock.DATA_LENGTH;
	}

	@Override
	public int fullKeyLength() {
		return NodeSSK.FULL_KEY_LENGTH;
	}

	@Override
	public int headerLength() {
		return SSKBlock.TOTAL_HEADERS_LENGTH;
	}

	@Override
	public int routingKeyLength() {
		return NodeSSK.ROUTING_KEY_LENGTH;
	}

	@Override
	public boolean storeFullKeys() {
		return true;
	}

	@Override
	public boolean collisionPossible() {
		return true;
	}

	@Override
	public boolean constructNeedsKey() {
		return true;
	}

	@Override
	public byte[] routingKeyFromFullKey(byte[] keyBuf) {
		return NodeSSK.routingKeyFromFullKey(keyBuf);
	}

}
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.node.Node;
import freenet.support.SimpleFieldSet;

public class UnsubscribeUSKMessage extends FCPMessage {

	public static final String NAME = "UnsubscribeUSK";
	private final String identifier;

	public UnsubscribeUSKMessage(SimpleFieldSet fs) throws MessageInvalidException {
		this.identifier = fs.get("Identifier");
		if(identifier == null)
			throw new MessageInvalidException(ProtocolErrorMessage.MISSING_FIELD, "No Identifier!", null, false);
	}

	@Override
	public SimpleFieldSet getFieldSet() {
		throw new UnsupportedOperationException();
	}

	@Override
	public String getName() {
		return NAME;
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		throw new UnsupportedOperationException();
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node)
			throws MessageInvalidException {
		handler.unsubscribeUSK(identifier);
	}

}
package freenet.clients.http.updateableelements;

import freenet.clients.http.SimpleToadletServer;
import freenet.clients.http.ToadletContext;
import freenet.support.HTMLNode;

/** This abstract Node is the ancestor of all pushed elements. */
public abstract class BaseUpdateableElement extends HTMLNode {

	/** The context of the request */
	protected ToadletContext	ctx;

	public BaseUpdateableElement(String name, ToadletContext ctx) {
		this(name, new String[] {}, new String[] {}, ctx);
	}

	public BaseUpdateableElement(String name, String attributeName, String attributeValue, ToadletContext ctx) {
		this(name, new String[] { attributeName }, new String[] { attributeValue }, ctx);
	}

	public BaseUpdateableElement(String name, String[] attributeNames, String[] attributeValues, ToadletContext ctx) {
		super(name, attributeNames, attributeValues);
		this.ctx = ctx;
	}

	/** Initializes the Node. It needs to be invoked from the constructor */
	protected void init(boolean pushed) {
		// We set the id to easily find the element
		addAttribute("id", getUpdaterId(ctx.getUniqueId()));
		// Updates the state, so the resulting page will have the actual state and content
		updateState(true);
		// Notifies the manager that the element has been rendered
		if(pushed)
			((SimpleToadletServer) ctx.getContainer()).pushDataManager.elementRendered(ctx.getUniqueId(), this);
	}

	/**
	 * Updates the state of the Node. The children should be removed and recreated.
	 * 
	 * @param initial
	 *            - If this is the first update
	 */
	public abstract void updateState(boolean initial);

	/** Returns the id, that identifies the element. It can depend on the request, but it might not use it. 
	 * It should not change e.g. when we follow a redirect as it is used in internal structures to identify the element. */
	public abstract String getUpdaterId(String requestId);

	/** Returns the type of the client-side updater. */
	public abstract String getUpdaterType();

	/** Disposes the Node */
	public abstract void dispose();
}
package freenet.node;

/**
 * This exception is thrown when it we try to do a blocking send of a message, and it takes too long
 * so we timeout. Compare to WaitedTooLongException, which is thrown when we are waiting for clearance
 * from bandwidth limiting and don't get it within a timeout period.
 * @author toad
 */
@SuppressWarnings("serial")
public class SyncSendWaitedTooLongException extends Exception {

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.io;

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.util.Random;

import com.db4o.ObjectContainer;

import freenet.crypt.PCFBMode;
import freenet.crypt.RandomSource;
import freenet.crypt.UnsupportedCipherException;
import freenet.crypt.ciphers.Rijndael;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;
import freenet.support.math.MersenneTwister;

/**
 * A proxy Bucket which adds:
 * - Encryption with the supplied cipher, and a random, ephemeral key.
 * - Padding to the next PO2 size.
 * 
 * CRYPTO WARNING: This uses PCFB with no IV. That means it is only safe if the key is unique!
 */
public class PaddedEphemerallyEncryptedBucket implements Bucket {

	private final Bucket bucket;
	private final int minPaddedSize;
	/** The decryption key. */
	private final byte[] key;
	private final byte[] iv;
	private final byte[] randomSeed;
	private long dataLength;
	private boolean readOnly;
	private int lastOutputStream;
	 
        private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	/**
	 * Create a padded encrypted proxy bucket.
	 * @param bucket The bucket which we are proxying to. Must be empty.
	 * @param pcfb The encryption mode with which to encipher/decipher the data.
	 * @param minSize The minimum padded size of the file (after it has been closed).
	 * @param strongPRNG a strong prng we will key from.
	 * @param weakPRNG a week prng we will padd from.
	 * Serialization: Note that it is not our responsibility to free the random number generators,
	 * but we WILL free the underlying bucket.
	 * @throws UnsupportedCipherException 
	 */
	public PaddedEphemerallyEncryptedBucket(Bucket bucket, int minSize, RandomSource strongPRNG, Random weakPRNG) {
		this.bucket = bucket;
		if(bucket.size() != 0) throw new IllegalArgumentException("Bucket must be empty");
		byte[] tempKey = new byte[32];
		randomSeed = new byte[32];
		weakPRNG.nextBytes(randomSeed);
		strongPRNG.nextBytes(tempKey);
		this.key = tempKey;
		this.iv = new byte[32];
		strongPRNG.nextBytes(iv);
		this.minPaddedSize = minSize;
		readOnly = false;
		lastOutputStream = 0;
		dataLength = 0;
	}

	public PaddedEphemerallyEncryptedBucket(PaddedEphemerallyEncryptedBucket orig, Bucket newBucket) {
		this.dataLength = orig.dataLength;
		this.key = new byte[orig.key.length];
		System.arraycopy(orig.key, 0, key, 0, orig.key.length);
		this.randomSeed = null; // Will be read-only
		setReadOnly();
		this.bucket = newBucket;
		this.minPaddedSize = orig.minPaddedSize;
		if(orig.iv != null) {
			iv = new byte[32];
			System.arraycopy(orig.iv, 0, iv, 0, 32);
		} else {
			iv = null;
		}
	}

	public OutputStream getOutputStream() throws IOException {
		if(readOnly) throw new IOException("Read only");
		OutputStream os = bucket.getOutputStream();
		synchronized(this) {
			dataLength = 0;
		}
		return new PaddedEphemerallyEncryptedOutputStream(os, ++lastOutputStream);
	}

	private class PaddedEphemerallyEncryptedOutputStream extends OutputStream {

		final PCFBMode pcfb;
		final OutputStream out;
		final int streamNumber;
		private boolean closed;
		
		public PaddedEphemerallyEncryptedOutputStream(OutputStream out, int streamNumber) {
			this.out = out;
			dataLength = 0;
			this.streamNumber = streamNumber;
			pcfb = getPCFB();
		}
		
		@Override
		public void write(int b) throws IOException {
			if(closed) throw new IOException("Already closed!");
			if(streamNumber != lastOutputStream)
				throw new IllegalStateException("Writing to old stream in "+getName());
			//if((b < 0) || (b > 255))
			//	throw new IllegalArgumentException();
			int toWrite = pcfb.encipher(b);
			synchronized(PaddedEphemerallyEncryptedBucket.this) {
				out.write(toWrite);
				dataLength++;
			}
		}
		
		// Override this or FOS will use write(int)
		@Override
		public void write(byte[] buf) throws IOException {
			if(closed)
				throw new IOException("Already closed!");
			if(streamNumber != lastOutputStream)
				throw new IllegalStateException("Writing to old stream in "+getName());
			write(buf, 0, buf.length);
		}
		
		@Override
		public void write(byte[] buf, int offset, int length) throws IOException {
			if(closed) throw new IOException("Already closed!");
			if(streamNumber != lastOutputStream)
				throw new IllegalStateException("Writing to old stream in "+getName());
			if(length == 0) return;
			byte[] enc = new byte[length];
			System.arraycopy(buf, offset, enc, 0, length);
			pcfb.blockEncipher(enc, 0, enc.length);
			synchronized(PaddedEphemerallyEncryptedBucket.this) {
				out.write(enc, 0, enc.length);
				dataLength += enc.length;
			}
		}
		
        @Override
		@SuppressWarnings("cast")
		public void close() throws IOException {
			if(closed) return;
			try {
				if(streamNumber != lastOutputStream) {
					Logger.normal(this, "Not padding out to length because have been superceded: "+getName());
					return;
				}
				Random random = new MersenneTwister(randomSeed);
				synchronized(PaddedEphemerallyEncryptedBucket.this) {
					long finalLength = paddedLength();
					long padding = finalLength - dataLength;
					int sz = 65536;
					if(padding < (long)sz)
						sz = (int)padding;
					byte[] buf = new byte[sz];
					long writtenPadding = 0;
					while(writtenPadding < padding) {
						int left = (int) Math.min((long) (padding - writtenPadding), (long) buf.length);
						random.nextBytes(buf);
						out.write(buf, 0, left);
						writtenPadding += left;
					}
				}
			} finally {
				closed = true;
				out.flush();
				out.close();
			}
		}
	}

	public InputStream getInputStream() throws IOException {
		return new PaddedEphemerallyEncryptedInputStream(bucket.getInputStream());
	}

	private class PaddedEphemerallyEncryptedInputStream extends InputStream {

		final InputStream in;
		final PCFBMode pcfb;
		long ptr;
		
		public PaddedEphemerallyEncryptedInputStream(InputStream in) {
			this.in = in;
			pcfb = getPCFB();
			ptr = 0;
		}
		
		@Override
		public int read() throws IOException {
			if(ptr >= dataLength) return -1;
			int x = in.read();
			if(x == -1) return x;
			ptr++;
			return pcfb.decipher(x);
		}
		
		@Override
		public final int available() {
			int x = (int)Math.min(dataLength - ptr, Integer.MAX_VALUE);
			return (x < 0) ? 0 : x;
		}
		
		@Override
		public int read(byte[] buf, int offset, int length) throws IOException {
			// FIXME remove debugging
			if((length+offset > buf.length) || (offset < 0) || (length < 0))
				throw new ArrayIndexOutOfBoundsException("a="+offset+", b="+length+", length "+buf.length);
			int x = available();
			if(x <= 0) return -1;
			length = Math.min(length, x);
			int readBytes = in.read(buf, offset, length);
			if(readBytes <= 0) return readBytes;
			ptr += readBytes;
			pcfb.blockDecipher(buf, offset, readBytes);
			return readBytes;
		}

		@Override
		public int read(byte[] buf) throws IOException {
			return read(buf, 0, buf.length);
		}
		
		@Override
		public long skip(long bytes) throws IOException {
			byte[] buf = new byte[(int)Math.min(4096, bytes)];
			long skipped = 0;
			while(skipped < bytes) {
				int x = read(buf, 0, (int)Math.min(bytes-skipped, buf.length));
				if(x <= 0) return skipped;
				skipped += x;
			}
			return skipped;
		}
		
		@Override
		public void close() throws IOException {
			in.close();
		}
	}

	/**
	 * Return the length of the data in the proxied bucket, after padding.
	 */
	public synchronized long paddedLength() {
		long size = dataLength;
		if(size < minPaddedSize) size = minPaddedSize;
		if(size == minPaddedSize) return size;
		long min = minPaddedSize;
		long max = (long)minPaddedSize << 1;
		while(true) {
			if(max < 0)
				throw new Error("Impossible size: "+size+" - min="+min+", max="+max);
			if(size < min)
				throw new IllegalStateException("???");
			if((size >= min) && (size <= max)) {
				if(logMINOR)
					Logger.minor(this, "Padded: "+max+" was: "+dataLength+" for "+getName());
				return max;
			}
			min = max;
			max = max << 1;
		}
	}

	private synchronized Rijndael getRijndael() {
		Rijndael aes;
		try {
			aes = new Rijndael(256, 256);
		} catch (UnsupportedCipherException e) {
			throw new Error(e);
		}
		aes.initialize(key);
		return aes;
	}

	public PCFBMode getPCFB() {
		Rijndael aes = getRijndael();
		if(iv != null)
			return PCFBMode.create(aes, iv);
		else
			// FIXME CRYPTO We should probably migrate all old buckets automatically so we can get rid of this?
			// Since the key is unique it is actually almost safe to use all zeros IV, but it's better to use a real IV.
			return PCFBMode.create(aes);
	}

	public String getName() {
		return "Encrypted:"+bucket.getName();
	}

	@Override
	public String toString() {
		return super.toString()+ ':' +bucket;
	}
	
	public synchronized long size() {
		return dataLength;
	}

	public boolean isReadOnly() {
		return readOnly;
	}
	
	public void setReadOnly() {
		readOnly = true;
	}

	/**
	 * @return The underlying Bucket.
	 */
	public Bucket getUnderlying() {
		return bucket;
	}

	public void free() {
		bucket.free();
	}

	/**
	 * Get the decryption key.
	 */
	public byte[] getKey() {
		return key;
	}

	public void storeTo(ObjectContainer container) {
		bucket.storeTo(container);
		container.store(this);
	}

	public void removeFrom(ObjectContainer container) {
		if(logMINOR)
			Logger.minor(this, "Removing from database: "+this);
		bucket.removeFrom(container);
		container.delete(this);
	}
	
	public void objectOnActivate(ObjectContainer container) {
		Logger.minor(this, "Activating "+super.toString()+" bucket == null = "+(bucket == null));
		// Cascading activation of dependancies
		container.activate(bucket, 1);
	}
	
	public Bucket createShadow() {
		Bucket newUnderlying = bucket.createShadow();
		if(newUnderlying == null) return null;
		return new PaddedEphemerallyEncryptedBucket(this, newUnderlying);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.pluginmanager;

import freenet.support.SimpleFieldSet;
import freenet.support.api.Bucket;

/**
 * Interface that has to be implemented for plugins that want to be talkable
 * via fcp or direct (plugin to plugin)
 *  
 * see plugins.FCPHello for a simple sample.
 * 
 * @author saces
 *
 */
public interface FredPluginFCP {
	
	public static final int ACCESS_DIRECT = 0;
	public static final int ACCESS_FCP_RESTRICTED = 1;
	public static final int ACCESS_FCP_FULL = 2;
	
	/**
	 * @param replysender interface to send a reply
	 * @param params parameters passed in, can be null
	 * @param data a bucket of data passed in, can be null
	 * @param access 0: direct call (plugin to plugin), 1: FCP restricted access,  2: FCP full access  
	 * @throws PluginNotFoundException If the plugin has already been removed.
	 */
	void handle(PluginReplySender replysender, SimpleFieldSet params, Bucket data, int accesstype);
	
}
package freenet.support.CPUInformation;

/**
 * @author Iakin
 * Created on Jul 16, 2004
 */
public class UnknownCPUException extends RuntimeException {
	private static final long serialVersionUID = -1;
	public UnknownCPUException() {
		super();
	}

	public UnknownCPUException(String message) {
		super(message);
	}
}
package freenet.client.filter;

import java.net.URISyntaxException;

/** This interface provides methods for URI transformations */
public interface URIProcessor {

	/** Processes an URI. If it is unsafe, then return null */
	public String processURI(String u, String overrideType, boolean noRelative, boolean inline) throws CommentException;

	/**
	 * Makes an URI absolute
	 * 
	 * @param uri
	 *            - The uri to be absolutize
	 * @return The absolute URI
	 */
	public String makeURIAbsolute(String uri) throws URISyntaxException;
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

/**
 * Thrown when a persistent request cannot be parsed.
 */
public class PersistenceParseException extends Exception {
	private static final long serialVersionUID = -1;

	public PersistenceParseException(String string) {
		super(string);
	}

	public PersistenceParseException(String string, Exception reason) {
		super(string, reason);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.pluginmanager;

import java.io.IOException;
import java.io.InputStream;
import java.net.MalformedURLException;

import com.db4o.ObjectContainer;

import freenet.client.FetchContext;
import freenet.client.FetchException;
import freenet.client.FetchResult;
import freenet.client.FetchWaiter;
import freenet.client.HighLevelSimpleClient;
import freenet.client.async.ClientContext;
import freenet.client.async.ClientGetter;
import freenet.client.async.DatabaseDisabledException;
import freenet.client.events.ClientEvent;
import freenet.client.events.ClientEventListener;
import freenet.client.events.SplitfileProgressEvent;
import freenet.keys.FreenetURI;
import freenet.node.Node;
import freenet.pluginmanager.PluginManager.PluginProgress;
import freenet.support.Logger;

public class PluginDownLoaderFreenet extends PluginDownLoader<FreenetURI> {
	final HighLevelSimpleClient hlsc;
	final boolean desperate;
	final Node node;
	private boolean fatalFailure;
	private ClientGetter get;

	PluginDownLoaderFreenet(HighLevelSimpleClient hlsc, Node node, boolean desperate) {
		this.hlsc = hlsc.clone();
		this.node = node;
		this.desperate = desperate;
	}

	@Override
	public FreenetURI checkSource(String source) throws PluginNotFoundException {
		try {
			return new FreenetURI(source);
		} catch (MalformedURLException e) {
			Logger.error(this, "not a valid freenet key: " + source, e);
			throw new PluginNotFoundException("not a valid freenet key: " + source, e);
		}
	}

	@Override
	InputStream getInputStream(final PluginProgress progress) throws IOException, PluginNotFoundException {
		FreenetURI uri = getSource();
		System.out.println("Downloading plugin from Freenet: "+uri);
		while (true) {
			try {
				progress.setDownloading();
				hlsc.addEventHook(new ClientEventListener() {

					public void onRemoveEventProducer(ObjectContainer container) {
						// Ignore
					}

					public void receive(ClientEvent ce, ObjectContainer maybeContainer, ClientContext context) {
						if(ce instanceof SplitfileProgressEvent) {
							SplitfileProgressEvent split = (SplitfileProgressEvent) ce;
							if(split.finalizedTotal) {
								progress.setDownloadProgress(split.minSuccessfulBlocks, split.succeedBlocks, split.totalBlocks, split.failedBlocks, split.fatallyFailedBlocks, split.finalizedTotal);
							}
						}
					}
					
				});
				FetchContext context = hlsc.getFetchContext();
				if(desperate) {
					context.maxNonSplitfileRetries = -1;
					context.maxSplitfileBlockRetries = -1;
				}
				FetchWaiter fw = new FetchWaiter();

				get = new ClientGetter(fw, uri, context, PluginManager.PRIO, node.nonPersistentClientBulk, null, null);
				try {
					node.clientCore.clientContext.start(get);
				} catch (DatabaseDisabledException e) {
					// Impossible
				}
				FetchResult res = fw.waitForCompletion();
				return res.asBucket().getInputStream();
			} catch (FetchException e) {
				if ((e.getMode() == FetchException.PERMANENT_REDIRECT) || (e.getMode() == FetchException.TOO_MANY_PATH_COMPONENTS)) {
					uri = e.newURI;
					continue;
				}
				if(e.isFatal())
					fatalFailure = true;
				Logger.error(this, "error while fetching plugin: " + getSource(), e);
				throw new PluginNotFoundException("error while fetching plugin: " + e.getMessage() + " for key "  + getSource(), e);
			}
		}
	}

	@Override
	String getPluginName(String source) throws PluginNotFoundException {
		return source.substring(source.lastIndexOf('/') + 1);
	}

	@Override
	String getSHA1sum() throws PluginNotFoundException {
		return null;
	}
	
	@Override
	String getSHA256sum() throws PluginNotFoundException {
		return null;
	}

	public boolean fatalFailure() {
		return fatalFailure;
	}

	@Override
	void tryCancel() {
		if(get != null)
			get.cancel(null, node.clientCore.clientContext);
	}

}
package freenet.client.async;

import java.util.ArrayList;
import java.util.List;

import com.db4o.ObjectContainer;

import freenet.client.FetchContext;
import freenet.client.FetchException;
import freenet.keys.ClientKey;
import freenet.keys.Key;
import freenet.node.BulkCallFailureItem;
import freenet.node.KeysFetchingLocally;
import freenet.node.LowLevelGetException;
import freenet.node.RequestClient;
import freenet.node.RequestScheduler;
import freenet.node.SendableGet;
import freenet.node.SendableRequestItem;
import freenet.node.SupportsBulkCallFailure;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.RandomGrabArrayItemExclusionList;

/** This was going to be part of SplitFileFetcherSegment, but we can't add a new parent
 * class to the class hierarchy in db4o without quite a bit of work.
 * 
 * Anyway we can probably improve things this way ... This class can keep track of retries
 * and cooldowns, we can remove them from the segment itself, meaning the segment could
 * eventually be shared by multiple getters running at different priorities, and would
 * eventually only be concerned with FEC decoding and data buckets ...
 * @author toad
 */
public class SplitFileFetcherSegmentGet extends SendableGet implements SupportsBulkCallFailure {
	
	public SplitFileFetcherSegmentGet(ClientRequester parent, SplitFileFetcherSegment segment, boolean realTimeFlag) {
		super(parent, realTimeFlag);
		this.segment = segment;
	}

	public final SplitFileFetcherSegment segment;

	private static volatile boolean logMINOR;
	
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback() {
			
			@Override
			public void shouldUpdate() {
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}
	
	public boolean isEmpty(ObjectContainer container) {
		if(persistent) container.activate(segment, 1);
		return segment.isFinishing(container);
	}

	@Override
	public ClientKey getKey(Object token, ObjectContainer container) {
		SplitFileFetcherSegmentSendableRequestItem req = (SplitFileFetcherSegmentSendableRequestItem) token;
		if(persistent) container.activate(segment, 1);
		return segment.getBlockKey(req.blockNum, container);
	}

	@Override
	public Key[] listKeys(ObjectContainer container) {
		boolean activated = false;
		if(persistent) {
			activated = container.ext().isActive(segment);
			if(!activated)
				container.activate(segment, 1);
		}
		Key[] keys = segment.listKeys(container);
		if(persistent && !activated)
			container.deactivate(segment, 1);
		return keys;
	}

	@Override
	public FetchContext getContext(ObjectContainer container) {
		boolean segmentActive = true;
		if(persistent) {
			segmentActive = container.ext().isActive(segment);
			if(!segmentActive) container.activate(segment, 1);
		}
		FetchContext ctx = segment.blockFetchContext;
		if(!segmentActive) container.deactivate(segment, 1);
		if(persistent) container.activate(ctx, 1);
		return ctx;
	}
	
	private boolean localRequestOnly(ObjectContainer container,
			ClientContext context) {
		boolean localOnly = false;
		boolean segmentActive = true;
		boolean ctxActive = true;
		if(persistent) {
			segmentActive = container.ext().isActive(segment);
			if(!segmentActive) container.activate(segment, 1);
		}
		FetchContext ctx = segment.blockFetchContext;
		if(!segmentActive) container.deactivate(segment, 1);
		if(persistent) {
			ctxActive = container.ext().isActive(ctx);
			container.activate(ctx, 1);
		}
		localOnly = ctx.localRequestOnly;
		if(!ctxActive) container.deactivate(ctx, 1);
		return localOnly;
	}

	// FIXME refactor this out to a common method; see SimpleSingleFileFetcher
	private FetchException translateException(LowLevelGetException e) {
		switch(e.code) {
		case LowLevelGetException.DATA_NOT_FOUND:
		case LowLevelGetException.DATA_NOT_FOUND_IN_STORE:
			return new FetchException(FetchException.DATA_NOT_FOUND);
		case LowLevelGetException.RECENTLY_FAILED:
			return new FetchException(FetchException.RECENTLY_FAILED);
		case LowLevelGetException.DECODE_FAILED:
			return new FetchException(FetchException.BLOCK_DECODE_ERROR);
		case LowLevelGetException.INTERNAL_ERROR:
			return new FetchException(FetchException.INTERNAL_ERROR);
		case LowLevelGetException.REJECTED_OVERLOAD:
			return new FetchException(FetchException.REJECTED_OVERLOAD);
		case LowLevelGetException.ROUTE_NOT_FOUND:
			return new FetchException(FetchException.ROUTE_NOT_FOUND);
		case LowLevelGetException.TRANSFER_FAILED:
			return new FetchException(FetchException.TRANSFER_FAILED);
		case LowLevelGetException.VERIFY_FAILED:
			return new FetchException(FetchException.BLOCK_DECODE_ERROR);
		case LowLevelGetException.CANCELLED:
			return new FetchException(FetchException.CANCELLED);
		default:
			Logger.error(this, "Unknown LowLevelGetException code: "+e.code);
			return new FetchException(FetchException.INTERNAL_ERROR, "Unknown error code: "+e.code);
		}
	}

	@Override
	public void onFailure(LowLevelGetException e, Object token,
			ObjectContainer container, ClientContext context) {
		if(logMINOR)
			Logger.minor(this, "onFailure("+e+" , "+token+" on "+this);
		onFailure(translateException(e), token, container, context);
	}
	
	public void onFailure(FetchException e, Object token,
			ObjectContainer container, ClientContext context) {
		if(persistent) {
			container.activate(segment, 1);
			container.activate(parent, 1);
			container.activate(segment.errors, 1);
		}
		boolean forceFatal = false;
		if(parent.isCancelled()) {
			if(logMINOR)
				Logger.minor(this, "Failing: cancelled");
			e = new FetchException(FetchException.CANCELLED);
			forceFatal = true;
		}
		segment.errors.inc(e.getMode());
		if(persistent)
			segment.errors.storeTo(container);
		if(e.isFatal() && token == null) {
			segment.fail(e, container, context, false);
		} else if(e.isFatal() || forceFatal) {
			segment.onFatalFailure(e, ((SplitFileFetcherSegmentSendableRequestItem)token).blockNum, container, context);
		} else {
			segment.onNonFatalFailure(e, ((SplitFileFetcherSegmentSendableRequestItem)token).blockNum, container, context);
		}
		if(persistent) {
			container.deactivate(segment, 1);
			container.deactivate(parent, 1);
			container.deactivate(segment.errors, 1);
		}
	}

	@Override
	public long getCooldownWakeup(Object token, ObjectContainer container, ClientContext context) {
		if(persistent) container.activate(segment, 1);
		return segment.getCooldownWakeup(((SplitFileFetcherSegmentSendableRequestItem)token).blockNum, segment.getMaxRetries(container), container, context);
	}

	@Override
	public long getCooldownWakeupByKey(Key key, ObjectContainer container, ClientContext context) {
		/* Only deactivate if was deactivated in the first place. 
		 * See the removePendingKey() stack trace: Segment is the listener (getter) ! */
		boolean activated = false;
		if(persistent) {
			activated = container.ext().isActive(segment);
			if(!activated)
				container.activate(segment, 1);
		}
		long ret = segment.getCooldownWakeupByKey(key, container, context);
		if(persistent) {
			if(!activated)
				container.deactivate(segment, 1);
		}
		return ret;
	}

	@Override
	public void requeueAfterCooldown(Key key, long time,
			ObjectContainer container, ClientContext context) {
		if(persistent) container.activate(segment, 1);
		int blockNum = segment.getBlockNumber(key, container);
		if(blockNum == -1) return;
		reschedule(container, context);
	}

	void reschedule(ObjectContainer container, ClientContext context) {
		if(this.getParentGrabArray() != null) {
			if(logMINOR) Logger.minor(this, "Not rescheduling as already scheduled on "+getParentGrabArray());
			return;
		}
		if(isCancelled(container)) return;
		try {
			getScheduler(container, context).register(null, new SendableGet[] { this }, persistent, container, getContextBlocks(container), true);
		} catch (KeyListenerConstructionException e) {
			Logger.error(this, "Impossible: "+e+" on "+this, e);
		}
	}

	private BlockSet getContextBlocks(ObjectContainer container) {
		FetchContext context = getContext(container);
		BlockSet blocks = context.blocks;
		if(blocks != null) {
			if(persistent) container.activate(blocks, 1);
			return blocks;
		} else return null;
	}

	@Override
	public boolean preRegister(ObjectContainer container, ClientContext context,
			boolean toNetwork) {
		if(!toNetwork) return false;
		if(localRequestOnly(container, context)) {
			if(persistent) container.activate(segment, 1);
			segment.failCheckingDatastore(container, context);
			return true;
		}
		boolean deactivate = false;
		if(persistent) {
			deactivate = !container.ext().isActive(parent);
			container.activate(parent, 1);
		}
		parent.toNetwork(container, context);
		if(deactivate) container.deactivate(parent, 1);
		return false;
	}

	@Override
	public short getPriorityClass(ObjectContainer container) {
		if(persistent) container.activate(parent, 1);
		return parent.priorityClass;
	}

	@Override
	public SendableRequestItem chooseKey(KeysFetchingLocally keys,
			ObjectContainer container, ClientContext context) {
		if(persistent) container.activate(segment, 1);
		ArrayList<Integer> possibles = segment.validBlockNumbers(keys, true, container, context);
		while(true) {
			if(possibles == null || possibles.isEmpty()) return null;
			Integer x = possibles.remove(context.random.nextInt(possibles.size()));
			if(segment.checkRecentlyFailed(x, container, context, keys, System.currentTimeMillis())) continue;
			return new SplitFileFetcherSegmentSendableRequestItem(x);
		}
	}

	@Override
	public long countAllKeys(ObjectContainer container, ClientContext context) {
		if(persistent) container.activate(segment, 1);
		return segment.countAllKeys(container, context);
	}

	@Override
	public long countSendableKeys(ObjectContainer container,
			ClientContext context) {
		if(persistent) container.activate(segment, 1);
		return segment.countSendableKeys(container, context);
	}

	@Override
	public boolean isCancelled(ObjectContainer container) {
		if(persistent) {
			container.activate(parent, 1);
			container.activate(segment, 1);
		}
		synchronized(segment) {
			return parent.cancelled;
		}
	}

	@Override
	public RequestClient getClient(ObjectContainer container) {
		if(persistent) container.activate(parent, 1);
		return parent.getClient();
	}

	@Override
	public ClientRequester getClientRequest() {
		return parent;
	}

	@Override
	public boolean isSSK() {
		return false;
	}

	@Override
	public List<PersistentChosenBlock> makeBlocks(
			PersistentChosenRequest request, RequestScheduler sched, KeysFetchingLocally keys,
			ObjectContainer container, ClientContext context) {
		if(persistent) container.activate(segment, 1);
		// FIXME why is the fetching keys list not passed in? We could at least check for other fetchers for the same key??? Need to modify the parameters ...
		List<PersistentChosenBlock> blocks = segment.makeBlocks(request, sched, keys, this, container, context);
		if(persistent) container.deactivate(segment, 1);
		return blocks;
	}

	public void storeTo(ObjectContainer container) {
		container.store(this);
	}

	public long getCooldownTime(ObjectContainer container, ClientContext context, long now) {
		if(persistent) container.activate(segment, 1);
		HasCooldownCacheItem parentRGA = getParentGrabArray();
		long wakeTime = segment.getCooldownTime(container, context, parentRGA, now);
		if(wakeTime > 0)
			context.cooldownTracker.setCachedWakeup(wakeTime, this, parentRGA, persistent, container, context, true);
		return wakeTime;
	}

	public void onFailure(BulkCallFailureItem[] items,
			ObjectContainer container, ClientContext context) {
        FetchException[] fetchExceptions = new FetchException[items.length];
        int countFatal = 0;
        for(int i=0;i<items.length;i++) {
        	fetchExceptions[i] = translateException(items[i].e);
        	if(fetchExceptions[i].isFatal()) countFatal++;
        }
        if(persistent) {
        	container.activate(segment, 1);
        	container.activate(parent, 1);
        	container.activate(segment.errors, 1);
        }
        if(parent.isCancelled()) {
                if(logMINOR)
                        Logger.minor(this, "Failing: cancelled");
                // Fail the segment.
                segment.fail(new FetchException(FetchException.CANCELLED), container, context, false);
                // FIXME do we need to free the keyNum's??? Or will that happen later anyway?
                return;
        }
        for(int i=0;i<fetchExceptions.length;i++)
        	segment.errors.inc(fetchExceptions[i].getMode());
        if(persistent)
        	segment.errors.storeTo(container);
        int nonFatalExceptions = items.length - countFatal;
        int[] blockNumbers = new int[nonFatalExceptions];
        if(countFatal > 0) {
        	FetchException[] newFetchExceptions = new FetchException[items.length - countFatal];
        	// Call the fatal callbacks directly.
        	int x = 0;
        	for(int i=0;i<items.length;i++) {
        		int blockNum = ((SplitFileFetcherSegmentSendableRequestItem)items[i].token).blockNum;
        		if(fetchExceptions[i].isFatal()) {
        			segment.onFatalFailure(fetchExceptions[i], blockNum, container, context);
        		} else {
        			blockNumbers[x] = blockNum;
        			newFetchExceptions[x] = fetchExceptions[i];
        			x++;
        		}
        	}
        	fetchExceptions = newFetchExceptions;
        } else {
        	for(int i=0;i<blockNumbers.length;i++)
        		blockNumbers[i] = ((SplitFileFetcherSegmentSendableRequestItem)items[i].token).blockNum;
        }
        if(logMINOR) Logger.minor(this, "Calling segment.onNonFatalFailure with "+blockNumbers.length+" failed fetches");
        segment.onNonFatalFailure(fetchExceptions, blockNumbers, container, context);

        if(persistent) {
        	container.deactivate(segment, 1);
        	container.deactivate(parent, 1);
        	container.deactivate(segment.errors, 1);
        }
	}

	public void removeFrom(ObjectContainer container) {
		container.delete(this);
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.io;

import java.io.File;

import com.db4o.ObjectContainer;

import freenet.support.Logger;
import freenet.support.api.Bucket;

/**
 * A file Bucket is an implementation of Bucket that writes to a file.
 * 
 * @author oskar
 */
public class FileBucket extends BaseFileBucket implements Bucket {

	protected final File file;
	protected boolean readOnly;
	protected boolean deleteOnFinalize;
	protected boolean deleteOnFree;
	protected final boolean deleteOnExit;
	protected final boolean createFileOnly;
	// JVM caches File.size() and there is no way to flush the cache, so we
	// need to track it ourselves

	/**
	 * Creates a new FileBucket.
	 * 
	 * @param file The File to read and write to.
	 * @param createFileOnly If true, create the file if it doesn't exist, but if it does exist,
	 * throw a FileExistsException on any write operation. This is safe against symlink attacks
	 * because we write to a temp file and then rename. It is technically possible that the rename
	 * will clobber an existing file if there is a race condition, but since it will not write over
	 * a symlink this is probably not dangerous. User-supplied filenames should in any case be
	 * restricted by higher levels.
	 * @param readOnly If true, any attempt to write to the bucket will result in an IOException.
	 * Can be set later. Irreversible. @see isReadOnly(), setReadOnly()
	 * @param deleteOnFinalize If true, delete the file on finalization. Reversible.
	 * @param deleteOnExit If true, delete the file on a clean exit of the JVM. Irreversible - use with care!
	 */
	public FileBucket(File file, boolean readOnly, boolean createFileOnly, boolean deleteOnFinalize, boolean deleteOnExit, boolean deleteOnFree) {
		super(file, deleteOnExit);
		if(file == null) throw new NullPointerException();
		File origFile = file;
		file = file.getAbsoluteFile();
		// Copy it so we can safely delete it.
		if(origFile == file)
			file = new File(file.getPath());
		this.readOnly = readOnly;
		this.createFileOnly = createFileOnly;
		this.file = file;
		this.deleteOnFinalize = deleteOnFinalize;
		this.deleteOnFree = deleteOnFree;
		this.deleteOnExit = deleteOnExit;
		// Useful for finding temp file leaks.
		// System.err.println("-- FileBucket.ctr(0) -- " +
		// file.getAbsolutePath());
		// (new Exception("get stack")).printStackTrace();
		fileRestartCounter = 0;
	}

	/**
	 * Returns the file object this buckets data is kept in.
	 */
	@Override
	public synchronized File getFile() {
		return file;
	}

	public synchronized boolean isReadOnly() {
		return readOnly;
	}

	public synchronized void setReadOnly() {
		readOnly = true;
	}

	/**
	 * Turn off "delete file on finalize" flag.
	 * Note that if you have already set delete file on exit, there is little that you
	 * can do to recover it! Delete file on finalize, on the other hand, is reversible.
	 */
	public synchronized void dontDeleteOnFinalize() {
		deleteOnFinalize = false;
	}

	@Override
	protected boolean createFileOnly() {
		return createFileOnly;
	}

	@Override
	protected boolean deleteOnExit() {
		return deleteOnExit;
	}

	@Override
	protected boolean deleteOnFinalize() {
		return deleteOnFinalize;
	}

	@Override
	protected boolean deleteOnFree() {
		return deleteOnFree;
	}

	public void storeTo(ObjectContainer container) {
		container.store(this);
	}

	public void removeFrom(ObjectContainer container) {
		Logger.minor(this, "Removing "+this);
		container.activate(file, 5);
		container.delete(file);
		container.delete(this);
	}
	
	public void objectOnActivate(ObjectContainer container) {
		container.activate(file, 5);
	}
	
	// Debugging stuff. If reactivate, add the logging infrastructure and use if(logDEBUG).
//	public void objectOnNew(ObjectContainer container) {
//		Logger.minor(this, "Storing "+this, new Exception("debug"));
//	}
//	
//	public void objectOnUpdate(ObjectContainer container) {
//		Logger.minor(this, "Updating "+this, new Exception("debug"));
//	}
//	
//	public void objectOnDelete(ObjectContainer container) {
//		Logger.minor(this, "Deleting "+this, new Exception("debug"));
//	}
//	
	public Bucket createShadow() {
		String fnam = file.getPath();
		File newFile = new File(fnam);
		return new FileBucket(newFile, true, false, false, false, false);
	}
}
package freenet.node;

/**
 * A SendableRequest may include many SendableRequestItem's.
 * Typically for requests, these are just an integer indicating which key
 * to fetch. But for inserts, these will often include the actual data to
 * insert, or some means of getting it without access to the database.
 * @author Matthew Toseland <toad@amphibian.dyndns.org> (0xE43DA450)
 */
public interface SendableRequestItem {

	/** Called when a request is abandoned. Whether this is called on
	 * a successful request is up to the SendableRequestSender. */
	public void dump();

}
package freenet.support.math;

import freenet.support.Fields;

/*
** Originally, we maintained the below functionality as a fork of the above
** code, in the contrib repo. Eventually this was refactored into this class,
** and now we instead depend on upstream mantissa.
**
** There are three milestone commits:
**
** O: 35a37bfad5b42dead835c9f1fb8b0972e730dab2, the earliest version we have in git, imported from svn
** A: 5d16406e4a20c9e6bd9685bf9a8480a13c8acbaf, the latest version we made edits to
** X: aea5c9d491b8de90b3457c0561938ea2ed14ec1d, representing a pristine 7.2 source
**
** You may view the diffs using something like:
**
**  $ git diff -w [S] [T] -- java{,-test}/org/spaceroots/mantissa/random/
**
** As of fred commit e89a2f63e819e8c088a14eaa3c809770db822956, this class, and
** its associated test, re-implements the diff between X-A, by extending
** o.s.m.r.MersenneTwister. Above that, it is also runtime-compatible with any
** version (O,A,X) of it.
**
** This should provide a smooth upgrade path:
**
** # Move users to this class, still running contrib-26 at version A
** # Move contrib to version X (legacy-27).
*/

/**
** This is a synchronized wrapper around {@link org.spaceroots.mantissa.random.MersenneTwister}
** which also adds additional {@code setSeed()} methods.
**
** @author infinity0
*/
public class MersenneTwister extends org.spaceroots.mantissa.random.MersenneTwister {

	private static final long serialVersionUID = 6555069655883958609L;

	/** Creates a new random number generator using the current time as the seed. */
	public MersenneTwister() { super(); }

	/** Creates a new random number generator using a single int seed. */
	public MersenneTwister(int seed) { super(seed); }

	/** Creates a new random number generator using an int array seed. */
	public MersenneTwister(int[] seed) { super(seed); }

	/** Creates a new random number generator using a single long seed. */
	public MersenneTwister(long seed) { super(seed); }

	/** Creates a new random number generator using a byte array seed. */
	public MersenneTwister(byte[] seed) {
		super(Fields.bytesToInts(seed, 0, seed.length));
	}

	/** {@inheritDoc} */
	@Override public synchronized void setSeed(int seed) { super.setSeed(seed); }

	/** {@inheritDoc} */
	@Override public synchronized void setSeed(int[] seed) { super.setSeed(seed); }

	/** {@inheritDoc} */
	@Override public synchronized void setSeed(long seed) { super.setSeed(seed); }

	/**
	** Reinitialize the generator as if just built with the given byte array seed.
	** <p>The state of the generator is exactly the same as a new
	** generator built with the same seed.</p>
	** @param seed the initial seed (8 bits byte array), if null
	** the seed of the generator will be related to the current time
	*/
	public synchronized void setSeed(byte[] seed) {
		super.setSeed(Fields.bytesToInts(seed, 0, seed.length));
	}

	/** {@inheritDoc} */
	@Override protected synchronized int next(int bits) { return super.next(bits); }

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

import java.util.HashSet;
import java.util.Hashtable;
import java.util.Iterator;
import java.util.concurrent.ArrayBlockingQueue;

import freenet.crypt.HMAC;
import freenet.io.comm.ByteCounter;
import freenet.io.comm.DMT;
import freenet.io.comm.Dispatcher;
import freenet.io.comm.Message;
import freenet.io.comm.MessageType;
import freenet.io.comm.NotConnectedException;
import freenet.io.comm.Peer;
import freenet.keys.Key;
import freenet.keys.KeyBlock;
import freenet.keys.NodeSSK;
import freenet.node.NodeStats.PeerLoadStats;
import freenet.node.NodeStats.RejectReason;
import freenet.store.BlockMetadata;
import freenet.support.Fields;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.ShortBuffer;
import freenet.support.Logger.LogLevel;
import freenet.support.io.NativeThread;

/**
 * @author amphibian
 * 
 * Dispatcher for unmatched FNP messages.
 * 
 * What can we get?
 * 
 * SwapRequests
 * 
 * DataRequests
 * 
 * InsertRequests
 * 
 * Probably a few others; those are the important bits.
 */
public class NodeDispatcher implements Dispatcher, Runnable {

	private static volatile boolean logMINOR;
	private static volatile boolean logDEBUG;

	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
				logDEBUG = Logger.shouldLog(LogLevel.DEBUG, this);
			}
		});
	}

	final Node node;
	private NodeStats nodeStats;
	private NodeDispatcherCallback callback;
	
	private static final long STALE_CONTEXT=20000;
	private static final long STALE_CONTEXT_CHECK=20000;

	NodeDispatcher(Node node) {
		this.node = node;
		this.nodeStats = node.nodeStats;
		node.getTicker().queueTimedJob(this, STALE_CONTEXT_CHECK);
	}

	ByteCounter pingCounter = new ByteCounter() {

		public void receivedBytes(int x) {
			node.nodeStats.pingCounterReceived(x);
		}

		public void sentBytes(int x) {
			node.nodeStats.pingCounterSent(x);
		}

		public void sentPayload(int x) {
			// Ignore
		}
		
	};
	
	public interface NodeDispatcherCallback {
		public void snoop(Message m, Node n);
	}
	
	public boolean handleMessage(Message m) {
		PeerNode source = (PeerNode)m.getSource();
		if(source == null) {
			// Node has been disconnected and garbage collected already! Ouch.
			return true;
		}
		if(logMINOR) Logger.minor(this, "Dispatching "+m+" from "+source);
		if(callback != null) {
			try {
				callback.snoop(m, node);
			} catch (Throwable t) {
				Logger.error(this, "Callback threw "+t, t);
			}
		}
		MessageType spec = m.getSpec();
		if(spec == DMT.FNPPing) {
			// Send an FNPPong
			Message reply = DMT.createFNPPong(m.getInt(DMT.PING_SEQNO));
			try {
				source.sendAsync(reply, null, pingCounter); // nothing we can do if can't contact source
			} catch (NotConnectedException e) {
				if(logMINOR) Logger.minor(this, "Lost connection replying to "+m);
			}
			return true;
		} else if (spec == DMT.FNPStoreSecret) {
			return node.netid.handleStoreSecret(m);
		} else if(spec == DMT.FNPSecretPing) {
			return node.netid.handleSecretPing(m);
		} else if(spec == DMT.FNPDetectedIPAddress) {
			Peer p = (Peer) m.getObject(DMT.EXTERNAL_ADDRESS);
			source.setRemoteDetectedPeer(p);
			node.ipDetector.redetectAddress();
			return true;
		} else if(spec == DMT.FNPTime) {
			return handleTime(m, source);
		} else if(spec == DMT.FNPUptime) {
			return handleUptime(m, source);
		} else if(spec == DMT.FNPSentPackets) {
			source.handleSentPackets(m);
			return true;
		} else if(spec == DMT.FNPVoid) {
			return true;
		} else if(spec == DMT.FNPDisconnect) {
			handleDisconnect(m, source);
			return true;
		} else if(spec == DMT.nodeToNodeMessage) {
			node.receivedNodeToNodeMessage(m, source);
			return true;
		} else if(spec == DMT.UOMAnnounce && node.nodeUpdater.isEnabled() && source.isRealConnection()) {
			return node.nodeUpdater.uom.handleAnnounce(m, source);
		} else if(spec == DMT.UOMRequestRevocation && node.nodeUpdater.isEnabled() && source.isRealConnection()) {
			return node.nodeUpdater.uom.handleRequestRevocation(m, source);
		} else if(spec == DMT.UOMSendingRevocation && node.nodeUpdater.isEnabled() && source.isRealConnection()) {
			return node.nodeUpdater.uom.handleSendingRevocation(m, source);
		} else if(spec == DMT.UOMRequestMain && node.nodeUpdater.isEnabled() && source.isRealConnection()) {
			node.nodeUpdater.uom.handleRequestJar(m, source, false);
			return true;
		} else if(spec == DMT.UOMRequestExtra && node.nodeUpdater.isEnabled() && source.isRealConnection()) {
			node.nodeUpdater.uom.handleRequestJar(m, source, true);
			return true;
		} else if(spec == DMT.UOMSendingMain && node.nodeUpdater.isEnabled() && source.isRealConnection()) {
			return node.nodeUpdater.uom.handleSendingMain(m, source);
		} else if(spec == DMT.UOMSendingExtra && node.nodeUpdater.isEnabled() && source.isRealConnection()) {
			return node.nodeUpdater.uom.handleSendingExt(m, source);
		} else if(spec == DMT.FNPOpennetAnnounceRequest) {
			return handleAnnounceRequest(m, source);
		} else if(spec == DMT.FNPRoutingStatus) {
			if(source instanceof DarknetPeerNode) {
				boolean value = m.getBoolean(DMT.ROUTING_ENABLED);
				if(logMINOR)
					Logger.minor(this, "The peer ("+source+") asked us to set routing="+value);
				((DarknetPeerNode)source).setRoutingStatus(value, false);
			}
			// We claim it in any case
			return true;
		} else if(source.isRealConnection() && spec == DMT.FNPLocChangeNotificationNew) {
			double newLoc = m.getDouble(DMT.LOCATION);
			ShortBuffer buffer = ((ShortBuffer) m.getObject(DMT.PEER_LOCATIONS));
			double[] locs = Fields.bytesToDoubles(buffer.getData());
			
			/**
			 * Do *NOT* remove the sanity check below! 
			 * @see http://archives.freenetproject.org/message/20080718.144240.359e16d3.en.html
			 */
			if((OpennetManager.MAX_PEERS_FOR_SCALING < locs.length) && (source.isOpennet())) {
				if(locs.length > OpennetManager.PANIC_MAX_PEERS) {
					// This can't happen by accident
					Logger.error(this, "We received "+locs.length+ " locations from "+source.toString()+"! That should *NOT* happen! Possible attack!");
					source.forceDisconnect(true);
					return true;
				} else {
					// A few extra can happen by accident. Just use the first 20.
					Logger.normal(this, "Too many locations from "+source.toString()+" : "+locs.length+" could be an accident, using the first "+OpennetManager.MAX_PEERS_FOR_SCALING);
					double[] firstLocs = new double[OpennetManager.MAX_PEERS_FOR_SCALING];
					System.arraycopy(locs, 0, firstLocs, 0, OpennetManager.MAX_PEERS_FOR_SCALING);
					locs = firstLocs;
				}
			}
			// We are on darknet and we trust our peers OR we are on opennet
			// and the amount of locations sent to us seems reasonable
			source.updateLocation(newLoc, locs);
			
			return true;
		}
		
		if(!source.isRoutable()) {
			if(logDEBUG) Logger.debug(this, "Not routable");

			if(spec == DMT.FNPCHKDataRequest) {
				rejectRequest(m, node.nodeStats.chkRequestCtr);
			} else if(spec == DMT.FNPSSKDataRequest) {
				rejectRequest(m, node.nodeStats.sskRequestCtr);
			} else if(spec == DMT.FNPInsertRequest) {
				rejectRequest(m, node.nodeStats.chkInsertCtr);
			} else if(spec == DMT.FNPSSKInsertRequest) {
				rejectRequest(m, node.nodeStats.sskInsertCtr);
			} else if(spec == DMT.FNPSSKInsertRequestNew) {
				rejectRequest(m, node.nodeStats.sskInsertCtr);
			} else if(spec == DMT.FNPGetOfferedKey) {
				rejectRequest(m, node.failureTable.senderCounter);
			}
			return false;
		}

		if(spec == DMT.FNPNetworkID) {
			source.handleFNPNetworkID(m);
			return true;
		} else if(spec == DMT.FNPSwapRequest) {
			return node.lm.handleSwapRequest(m, source);
		} else if(spec == DMT.FNPSwapReply) {
			return node.lm.handleSwapReply(m, source);
		} else if(spec == DMT.FNPSwapRejected) {
			return node.lm.handleSwapRejected(m, source);
		} else if(spec == DMT.FNPSwapCommit) {
			return node.lm.handleSwapCommit(m, source);
		} else if(spec == DMT.FNPSwapComplete) {
			return node.lm.handleSwapComplete(m, source);
		} else if(spec == DMT.FNPCHKDataRequest) {
			handleDataRequest(m, source, false);
			return true;
		} else if(spec == DMT.FNPSSKDataRequest) {
			handleDataRequest(m, source, true);
			return true;
		} else if(spec == DMT.FNPInsertRequest) {
			handleInsertRequest(m, source, false);
			return true;
		} else if(spec == DMT.FNPSSKInsertRequest) {
			handleInsertRequest(m, source, true);
			return true;
		} else if(spec == DMT.FNPSSKInsertRequestNew) {
			handleInsertRequest(m, source, true);
			return true;
		} else if(spec == DMT.FNPRHProbeRequest) {
			return handleProbeRequest(m, source);
		} else if(spec == DMT.FNPRoutedPing) {
			return handleRouted(m, source);
		} else if(spec == DMT.FNPRoutedPong) {
			return handleRoutedReply(m);
		} else if(spec == DMT.FNPRoutedRejected) {
			return handleRoutedRejected(m);
			// FIXME implement threaded probe requests of various kinds.
			// Old probe request code was a major pain, never really worked.
			// We should have threaded probe requests (for simple code),
			// and one for each routing strategy.
//		} else if(spec == DMT.FNPProbeRequest) {
//			return handleProbeRequest(m, source);
//		} else if(spec == DMT.FNPProbeReply) {
//			return handleProbeReply(m, source);
//		} else if(spec == DMT.FNPProbeRejected) {
//			return handleProbeRejected(m, source);
//		} else if(spec == DMT.FNPProbeTrace) {
//			return handleProbeTrace(m, source);
		} else if(spec == DMT.FNPOfferKey) {
			return handleOfferKey(m, source);
		} else if(spec == DMT.FNPGetOfferedKey) {
			return handleGetOfferedKey(m, source);
		} else if(spec == DMT.FNPPeerLoadStatusByte || spec == DMT.FNPPeerLoadStatusShort || spec == DMT.FNPPeerLoadStatusInt) {
			return handlePeerLoadStatus(m, source);
		} else if(spec == DMT.FNPGetYourFullNoderef && source instanceof DarknetPeerNode) {
			((DarknetPeerNode)source).sendFullNoderef();
			return true;
		} else if(spec == DMT.FNPMyFullNoderef && source instanceof DarknetPeerNode) {
			((DarknetPeerNode)source).handleFullNoderef(m);
			return true;
		}
		return false;
	}

	private void rejectRequest(Message m, ByteCounter ctr) {
		long uid = m.getLong(DMT.UID);
		Message msg = DMT.createFNPRejectedOverload(uid, true, false, false);
		// Send the load status anyway, hopefully this is a temporary problem.
		msg.setNeedsLoadBulk();
		msg.setNeedsLoadRT();
		try {
			m.getSource().sendAsync(msg, null, ctr);
		} catch (NotConnectedException e) {
			// Ignore
		}
	}

	private boolean handlePeerLoadStatus(Message m, PeerNode source) {
		PeerLoadStats stat = node.nodeStats.parseLoadStats(source, m);
		source.reportLoadStatus(stat);
		return true;
	}

	private boolean handleUptime(Message m, PeerNode source) {
		byte uptime = m.getByte(DMT.UPTIME_PERCENT_48H);
		source.setUptime(uptime);
		return true;
	}

	private boolean handleOfferKey(Message m, PeerNode source) {
		Key key = (Key) m.getObject(DMT.KEY);
		byte[] authenticator = ((ShortBuffer) m.getObject(DMT.OFFER_AUTHENTICATOR)).getData();
		node.failureTable.onOffer(key, source, authenticator);
		return true;
	}

	private boolean handleGetOfferedKey(Message m, PeerNode source) {
		Key key = (Key) m.getObject(DMT.KEY);
		byte[] authenticator = ((ShortBuffer) m.getObject(DMT.OFFER_AUTHENTICATOR)).getData();
		long uid = m.getLong(DMT.UID);
		if(!HMAC.verifyWithSHA256(node.failureTable.offerAuthenticatorKey, key.getFullKey(), authenticator)) {
			Logger.error(this, "Invalid offer request from "+source+" : authenticator did not verify");
			try {
				source.sendAsync(DMT.createFNPGetOfferedKeyInvalid(uid, DMT.GET_OFFERED_KEY_REJECTED_BAD_AUTHENTICATOR), null, node.failureTable.senderCounter);
			} catch (NotConnectedException e) {
				// Too bad.
			}
			return true;
		}
		if(logMINOR) Logger.minor(this, "Valid GetOfferedKey for "+key+" from "+source);
		
		// Do we want it? We can RejectOverload if we don't have the bandwidth...
		boolean isSSK = key instanceof NodeSSK;
        boolean realTimeFlag = DMT.getRealTimeFlag(m);
		OfferReplyTag tag = new OfferReplyTag(isSSK, source, realTimeFlag, uid, node);
		node.lockUID(uid, isSSK, false, true, false, realTimeFlag, tag);
		boolean needPubKey;
		try {
		needPubKey = m.getBoolean(DMT.NEED_PUB_KEY);
		RejectReason reject = 
			nodeStats.shouldRejectRequest(true, false, isSSK, false, true, source, false, false, realTimeFlag);
		if(reject != null) {
			Logger.normal(this, "Rejecting FNPGetOfferedKey from "+source+" for "+key+" : "+reject);
			Message rejected = DMT.createFNPRejectedOverload(uid, true, true, realTimeFlag);
			if(reject.soft)
				rejected.addSubMessage(DMT.createFNPRejectIsSoft());
			try {
				source.sendAsync(rejected, null, node.failureTable.senderCounter);
			} catch (NotConnectedException e) {
				Logger.normal(this, "Rejecting (overload) data request from "+source.getPeer()+": "+e);
			}
			tag.unlockHandler(reject.soft);
			return true;
		}
		
		} catch (Error e) {
			tag.unlockHandler();
			throw e;
		} catch (RuntimeException e) {
			tag.unlockHandler();
			throw e;
		} // Otherwise, sendOfferedKey is responsible for unlocking. 
		
		// Accept it.
		
		try {
			node.failureTable.sendOfferedKey(key, isSSK, needPubKey, uid, source, tag,realTimeFlag);
		} catch (NotConnectedException e) {
			// Too bad.
		}
		return true;
	}

	private void handleDisconnect(final Message m, final PeerNode source) {
		// Must run ON the packet sender thread as it sends a packet directly
		node.getTicker().queueTimedJob(new FastRunnable() {
			public void run() {
				// Send the ack
					source.sendAnyUrgentNotifications(true);
				finishDisconnect(m, source);
			}
		}, 0);
	}
	
	private void finishDisconnect(final Message m, final PeerNode source) {
		source.disconnected(true, true);
		// If true, remove from active routing table, likely to be down for a while.
		// Otherwise just dump all current connection state and keep trying to connect.
		boolean remove = m.getBoolean(DMT.REMOVE);
		if(remove)
			node.peers.disconnect(source, false, false, false);
		// If true, purge all references to this node. Otherwise, we can keep the node
		// around in secondary tables etc in order to more easily reconnect later. 
		// (Mostly used on opennet)
		boolean purge = m.getBoolean(DMT.PURGE);
		if(purge) {
			OpennetManager om = node.getOpennet();
			if(om != null)
				om.purgeOldOpennetPeer(source);
		}
		// Process parting message
		int type = m.getInt(DMT.NODE_TO_NODE_MESSAGE_TYPE);
		ShortBuffer messageData = (ShortBuffer) m.getObject(DMT.NODE_TO_NODE_MESSAGE_DATA);
		if(messageData.getLength() == 0) return;
		node.receivedNodeToNodeMessage(source, type, messageData, true);
	}

	private boolean handleTime(Message m, PeerNode source) {
		long delta = m.getLong(DMT.TIME) - System.currentTimeMillis();
		source.setTimeDelta(delta);
		return true;
	}

	// We need to check the datastore before deciding whether to accept a request.
	// This can block - in bad cases, for a long time.
	// So we need to run it on a separate thread.
	
	private final PrioRunnable queueRunner = new PrioRunnable() {

		public void run() {
			while(true) {
				try {
					Message msg = requestQueue.take();
					boolean isSSK = msg.getSpec() == DMT.FNPSSKDataRequest;
					innerHandleDataRequest(msg, (PeerNode)msg.getSource(), isSSK);
				} catch (InterruptedException e) {
					// Ignore
				}
			}
		}

		public int getPriority() {
			// Slightly less than the actual requests themselves because accepting requests increases load.
			return NativeThread.HIGH_PRIORITY-1;
		}
		
	};
	
	private final ArrayBlockingQueue<Message> requestQueue = new ArrayBlockingQueue<Message>(100);
	
	private void handleDataRequest(Message m, PeerNode source, boolean isSSK) {
		// FIXME check probablyInStore and if not, we can handle it inline.
		// This and DatastoreChecker require that method be implemented...
		// For now just handle everything on the thread...
		if(!requestQueue.offer(m)) {
			rejectRequest(m, isSSK ? node.nodeStats.sskRequestCtr : node.nodeStats.chkRequestCtr);
		}
	}
	
	/**
	 * Handle an incoming FNPDataRequest.
	 */
	private void innerHandleDataRequest(Message m, PeerNode source, boolean isSSK) {
		if(!source.isConnected()) {
			if(logMINOR) Logger.minor(this, "Handling request off thread, source disconnected: "+source+" for "+m);
			return;
		}
		if(!source.isRoutable()) {
			if(logMINOR) Logger.minor(this, "Handling request off thread, source no longer routable: "+source+" for "+m);
			rejectRequest(m, isSSK ? node.nodeStats.sskRequestCtr : node.nodeStats.chkRequestCtr);
			return;
		}
		long id = m.getLong(DMT.UID);
		ByteCounter ctr = isSSK ? node.nodeStats.sskRequestCtr : node.nodeStats.chkRequestCtr;
		if(node.recentlyCompleted(id)) {
			Message rejected = DMT.createFNPRejectedLoop(id);
			try {
				source.sendAsync(rejected, null, ctr);
			} catch (NotConnectedException e) {
				Logger.normal(this, "Rejecting data request (loop, finished): "+e);
			}
			return;
		}
        short htl = m.getShort(DMT.HTL);
        Key key = (Key) m.getObject(DMT.FREENET_ROUTING_KEY);
        boolean realTimeFlag = DMT.getRealTimeFlag(m);
        final RequestTag tag = new RequestTag(isSSK, RequestTag.START.REMOTE, source, realTimeFlag, id, node);
		if(!node.lockUID(id, isSSK, false, false, false, realTimeFlag, tag)) {
			if(logMINOR) Logger.minor(this, "Could not lock ID "+id+" -> rejecting (already running)");
			Message rejected = DMT.createFNPRejectedLoop(id);
			try {
				source.sendAsync(rejected, null, ctr);
			} catch (NotConnectedException e) {
				Logger.normal(this, "Rejecting request from "+source.getPeer()+": "+e);
			}
			node.failureTable.onFinalFailure(key, null, htl, htl, -1, -1, source);
			return;
		} else {
			if(logMINOR) Logger.minor(this, "Locked "+id);
		}
		
		// There are at least 2 threads that call this function.
		// DO NOT reuse the meta object, unless on a per-thread basis.
		// Object allocation is pretty cheap in modern Java anyway...
		// If we do reuse it, call reset().
		BlockMetadata meta = new BlockMetadata();
		KeyBlock block = node.fetch(key, false, false, false, false, meta);
		if(block != null)
			tag.setNotRoutedOnwards();
		
		RejectReason rejectReason = nodeStats.shouldRejectRequest(!isSSK, false, isSSK, false, false, source, block != null, false, realTimeFlag);
		if(rejectReason != null) {
			// can accept 1 CHK request every so often, but not with SSKs because they aren't throttled so won't sort out bwlimitDelayTime, which was the whole reason for accepting them when overloaded...
			Logger.normal(this, "Rejecting "+(isSSK ? "SSK" : "CHK")+" request from "+source.getPeer()+" preemptively because "+rejectReason);
			Message rejected = DMT.createFNPRejectedOverload(id, true, true, realTimeFlag);
			if(rejectReason.soft)
				rejected.addSubMessage(DMT.createFNPRejectIsSoft());
			try {
				source.sendAsync(rejected, null, ctr);
			} catch (NotConnectedException e) {
				Logger.normal(this, "Rejecting (overload) data request from "+source.getPeer()+": "+e);
			}
			tag.setRejected();
			tag.unlockHandler(rejectReason.soft);
			// Do not tell failure table.
			// Otherwise an attacker can flood us with requests very cheaply and purge our
			// failure table even though we didn't accept any of them.
			return;
		}
		nodeStats.reportIncomingRequestLocation(key.toNormalizedDouble());
		//if(!node.lockUID(id)) return false;
		RequestHandler rh = new RequestHandler(m, source, id, node, htl, key, tag, block, realTimeFlag);
		node.executor.execute(rh, "RequestHandler for UID "+id+" on "+node.getDarknetPortNumber());
	}

	private void handleInsertRequest(Message m, PeerNode source, boolean isSSK) {
		ByteCounter ctr = isSSK ? node.nodeStats.sskInsertCtr : node.nodeStats.chkInsertCtr;
		long id = m.getLong(DMT.UID);
		if(node.recentlyCompleted(id)) {
			Message rejected = DMT.createFNPRejectedLoop(id);
			try {
				source.sendAsync(rejected, null, ctr);
			} catch (NotConnectedException e) {
				Logger.normal(this, "Rejecting insert request from "+source.getPeer()+": "+e);
			}
			return;
		}
        boolean realTimeFlag = DMT.getRealTimeFlag(m);
		InsertTag tag = new InsertTag(isSSK, InsertTag.START.REMOTE, source, realTimeFlag, id, node);
		if(!node.lockUID(id, isSSK, true, false, false, realTimeFlag, tag)) {
			if(logMINOR) Logger.minor(this, "Could not lock ID "+id+" -> rejecting (already running)");
			Message rejected = DMT.createFNPRejectedLoop(id);
			try {
				source.sendAsync(rejected, null, ctr);
			} catch (NotConnectedException e) {
				Logger.normal(this, "Rejecting insert request from "+source.getPeer()+": "+e);
			}
			return;
		}
		boolean preferInsert = Node.PREFER_INSERT_DEFAULT;
		boolean ignoreLowBackoff = Node.IGNORE_LOW_BACKOFF_DEFAULT;
		boolean forkOnCacheable = Node.FORK_ON_CACHEABLE_DEFAULT;
		Message forkControl = m.getSubMessage(DMT.FNPSubInsertForkControl);
		if(forkControl != null)
			forkOnCacheable = forkControl.getBoolean(DMT.ENABLE_INSERT_FORK_WHEN_CACHEABLE);
		Message lowBackoff = m.getSubMessage(DMT.FNPSubInsertIgnoreLowBackoff);
		if(lowBackoff != null)
			ignoreLowBackoff = lowBackoff.getBoolean(DMT.IGNORE_LOW_BACKOFF);
		Message preference = m.getSubMessage(DMT.FNPSubInsertPreferInsert);
		if(preference != null)
			preferInsert = preference.getBoolean(DMT.PREFER_INSERT);
		// SSKs don't fix bwlimitDelayTime so shouldn't be accepted when overloaded.
		RejectReason rejectReason = nodeStats.shouldRejectRequest(!isSSK, true, isSSK, false, false, source, false, preferInsert, realTimeFlag);
		if(rejectReason != null) {
			Logger.normal(this, "Rejecting insert from "+source.getPeer()+" preemptively because "+rejectReason);
			Message rejected = DMT.createFNPRejectedOverload(id, true, true, realTimeFlag);
			if(rejectReason.soft)
				rejected.addSubMessage(DMT.createFNPRejectIsSoft());
			try {
				source.sendAsync(rejected, null, ctr);
			} catch (NotConnectedException e) {
				Logger.normal(this, "Rejecting (overload) insert request from "+source.getPeer()+": "+e);
			}
			tag.unlockHandler(rejectReason.soft);
			return;
		}
		long now = System.currentTimeMillis();
		if(m.getSpec().equals(DMT.FNPSSKInsertRequest)) {
			NodeSSK key = (NodeSSK) m.getObject(DMT.FREENET_ROUTING_KEY);
	        byte[] data = ((ShortBuffer) m.getObject(DMT.DATA)).getData();
	        byte[] headers = ((ShortBuffer) m.getObject(DMT.BLOCK_HEADERS)).getData();
	        short htl = m.getShort(DMT.HTL);
			SSKInsertHandler rh = new SSKInsertHandler(key, data, headers, htl, source, id, node, now, tag, node.canWriteDatastoreInsert(htl), forkOnCacheable, preferInsert, ignoreLowBackoff, realTimeFlag);
	        rh.receivedBytes(m.receivedByteCount());
			node.executor.execute(rh, "SSKInsertHandler for "+id+" on "+node.getDarknetPortNumber());
		} else if(m.getSpec().equals(DMT.FNPSSKInsertRequestNew)) {
			NodeSSK key = (NodeSSK) m.getObject(DMT.FREENET_ROUTING_KEY);
			short htl = m.getShort(DMT.HTL);
			SSKInsertHandler rh = new SSKInsertHandler(key, null, null, htl, source, id, node, now, tag, node.canWriteDatastoreInsert(htl), forkOnCacheable, preferInsert, ignoreLowBackoff, realTimeFlag);
	        rh.receivedBytes(m.receivedByteCount());
			node.executor.execute(rh, "SSKInsertHandler for "+id+" on "+node.getDarknetPortNumber());
		} else {
			CHKInsertHandler rh = new CHKInsertHandler(m, source, id, node, now, tag, forkOnCacheable, preferInsert, ignoreLowBackoff, realTimeFlag);
			node.executor.execute(rh, "CHKInsertHandler for "+id+" on "+node.getDarknetPortNumber());
		}
		if(logMINOR) Logger.minor(this, "Started InsertHandler for "+id);
	}
	
	private boolean handleProbeRequest(Message m, PeerNode source) {
		long id = m.getLong(DMT.UID);
		if(node.recentlyCompleted(id)) {
			Message rejected = DMT.createFNPRejectedLoop(id);
			try {
				source.sendAsync(rejected, null, node.nodeStats.probeRequestCtr);
			} catch (NotConnectedException e) {
				Logger.normal(this, "Rejecting probe request from "+source.getPeer()+": "+e);
			}
			return true;
		}
		// Lets not bother with full lockUID, just add it to the recently completed list.
		node.completed(id);
		// SSKs don't fix bwlimitDelayTime so shouldn't be accepted when overloaded.
		if(source.shouldRejectProbeRequest()) {
			Logger.normal(this, "Rejecting probe request from "+source.getPeer());
			Message rejected = DMT.createFNPRejectedOverload(id, true, false, false);
			try {
				source.sendAsync(rejected, null, node.nodeStats.probeRequestCtr);
			} catch (NotConnectedException e) {
				Logger.normal(this, "Rejecting (overload) insert request from "+source.getPeer()+": "+e);
			}
			return true;
		}
		double target = m.getDouble(DMT.TARGET_LOCATION);
		if(target > 1.0 || target < 0.0) {
			Logger.normal(this, "Rejecting invalid (target="+target+") probe request from "+source.getPeer());
			Message rejected = DMT.createFNPRejectedOverload(id, true, false, false);
			try {
				source.sendAsync(rejected, null, node.nodeStats.probeRequestCtr);
			} catch (NotConnectedException e) {
				Logger.normal(this, "Rejecting (invalid) insert request from "+source.getPeer()+": "+e);
			}
			return true;
		}
		ProbeRequestHandler.start(m, source, node, target);
		return true;
	}

	private boolean handleAnnounceRequest(Message m, PeerNode source) {
		long uid = m.getLong(DMT.UID);
		OpennetManager om = node.getOpennet();
		if(om == null || !source.canAcceptAnnouncements()) {
			if(om != null && source instanceof SeedClientPeerNode)
				om.seedTracker.rejectedAnnounce((SeedClientPeerNode)source);
			Message msg = DMT.createFNPOpennetDisabled(uid);
			try {
				source.sendAsync(msg, null, node.nodeStats.announceByteCounter);
			} catch (NotConnectedException e) {
				// Ok
			}
			return true;
		}
		if(node.recentlyCompleted(uid)) {
			if(om != null && source instanceof SeedClientPeerNode)
				om.seedTracker.rejectedAnnounce((SeedClientPeerNode)source);
			Message msg = DMT.createFNPRejectedLoop(uid);
			try {
				source.sendAsync(msg, null, node.nodeStats.announceByteCounter);
			} catch (NotConnectedException e) {
				// Ok
			}
			return true;
		}
		boolean success = false;
		// No way to check whether it's actually running atm, so lets report it to the completed list immediately.
		// FIXME we should probably keep a list!
		node.completed(uid);
		try {
			if(!node.nodeStats.shouldAcceptAnnouncement(uid)) {
				if(om != null && source instanceof SeedClientPeerNode)
					om.seedTracker.rejectedAnnounce((SeedClientPeerNode)source);
				Message msg = DMT.createFNPRejectedOverload(uid, true, false, false);
				try {
					source.sendAsync(msg, null, node.nodeStats.announceByteCounter);
				} catch (NotConnectedException e) {
					// Ok
				}
				return true;
			}
			if(!source.shouldAcceptAnnounce(uid)) {
				if(om != null && source instanceof SeedClientPeerNode)
					om.seedTracker.rejectedAnnounce((SeedClientPeerNode)source);
				node.nodeStats.endAnnouncement(uid);
				Message msg = DMT.createFNPRejectedOverload(uid, true, false, false);
				try {
					source.sendAsync(msg, null, node.nodeStats.announceByteCounter);
				} catch (NotConnectedException e) {
					// Ok
				}
				return true;
			}
			if(om != null && source instanceof SeedClientPeerNode) {
				if(!om.seedTracker.acceptAnnounce((SeedClientPeerNode)source, node.fastWeakRandom)) {
					node.nodeStats.endAnnouncement(uid);
					Message msg = DMT.createFNPRejectedOverload(uid, true, false, false);
					try {
						source.sendAsync(msg, null, node.nodeStats.announceByteCounter);
					} catch (NotConnectedException e) {
						// Ok
					}
					return true;
				}
			}
			AnnounceSender sender = new AnnounceSender(m, uid, source, om, node);
			node.executor.execute(sender, "Announcement sender for "+uid);
			success = true;
			return true;
		} finally {
			if(!success)
				source.completedAnnounce(uid);
		}
	}

	final Hashtable<Long, RoutedContext> routedContexts = new Hashtable<Long, RoutedContext>();

	static class RoutedContext {
		long createdTime;
		long accessTime;
		PeerNode source;
		final HashSet<PeerNode> routedTo;
		Message msg;
		short lastHtl;
		final byte[] identity;

		RoutedContext(Message msg, PeerNode source, byte[] identity) {
			createdTime = accessTime = System.currentTimeMillis();
			this.source = source;
			routedTo = new HashSet<PeerNode>();
			this.msg = msg;
			lastHtl = msg.getShort(DMT.HTL);
			this.identity = identity;
		}

		void addSent(PeerNode n) {
			routedTo.add(n);
		}
	}
	
	/**
	 * Cleanup any old/stale routing contexts and reschedule execution.
	 */
	public void run() {
		long now=System.currentTimeMillis();
		synchronized (routedContexts) {
			Iterator<RoutedContext> i = routedContexts.values().iterator();
			while (i.hasNext()) {
				RoutedContext rc = i.next();
				if (now-rc.createdTime > STALE_CONTEXT) {
					i.remove();
				}
			}
		}
		node.getTicker().queueTimedJob(this, STALE_CONTEXT_CHECK);
	}

	/**
	 * Handle an FNPRoutedRejected message.
	 */
	private boolean handleRoutedRejected(Message m) {
		long id = m.getLong(DMT.UID);
		Long lid = Long.valueOf(id);
		RoutedContext rc = routedContexts.get(lid);
		if(rc == null) {
			// Gah
			Logger.error(this, "Unrecognized FNPRoutedRejected");
			return false; // locally originated??
		}
		short htl = rc.lastHtl;
		if(rc.source != null)
			htl = rc.source.decrementHTL(htl);
		short ohtl = m.getShort(DMT.HTL);
		if(ohtl < htl) htl = ohtl;
		if(htl == 0) {
			// Equivalent to DNF.
			// Relay.
			if(rc.source != null) {
				try {
					rc.source.sendAsync(DMT.createFNPRoutedRejected(id, (short)0), null, nodeStats.routedMessageCtr);
				} catch (NotConnectedException e) {
					// Ouch.
					Logger.error(this, "Unable to relay probe DNF: peer disconnected: "+rc.source);
				}
			}
		} else {
			// Try routing to the next node
			forward(rc.msg, id, rc.source, htl, rc.msg.getDouble(DMT.TARGET_LOCATION), rc, rc.identity);
		}
		return true;
	}

	/**
	 * Handle a routed-to-a-specific-node message.
	 * @param m
	 * @return False if we want the message put back on the queue.
	 */
	boolean handleRouted(Message m, PeerNode source) {
		if(logMINOR) Logger.minor(this, "handleRouted("+m+ ')');

		long id = m.getLong(DMT.UID);
		Long lid = Long.valueOf(id);
		short htl = m.getShort(DMT.HTL);
		byte[] identity = ((ShortBuffer) m.getObject(DMT.NODE_IDENTITY)).getData();
		if(source != null) htl = source.decrementHTL(htl);
		RoutedContext ctx;
		ctx = routedContexts.get(lid);
		if(ctx != null) {
			try {
				source.sendAsync(DMT.createFNPRoutedRejected(id, htl), null, nodeStats.routedMessageCtr);
			} catch (NotConnectedException e) {
				if(logMINOR) Logger.minor(this, "Lost connection rejecting "+m);
			}
			return true;
		}
		ctx = new RoutedContext(m, source, identity);
		synchronized (routedContexts) {
			routedContexts.put(lid, ctx);
		}
		// source == null => originated locally, keep full htl
		double target = m.getDouble(DMT.TARGET_LOCATION);
		if(logMINOR) Logger.minor(this, "id "+id+" from "+source+" htl "+htl+" target "+target);
		if(Math.abs(node.lm.getLocation() - target) <= Double.MIN_VALUE) {
			if(logMINOR) Logger.minor(this, "Dispatching "+m.getSpec()+" on "+node.getDarknetPortNumber());
			// Handle locally
			// Message type specific processing
			dispatchRoutedMessage(m, source, id);
			return true;
		} else if(htl == 0) {
			Message reject = DMT.createFNPRoutedRejected(id, (short)0);
			if(source != null) try {
				source.sendAsync(reject, null, nodeStats.routedMessageCtr);
			} catch (NotConnectedException e) {
				if(logMINOR) Logger.minor(this, "Lost connection rejecting "+m);
			}
			return true;
		} else {
			return forward(m, id, source, htl, target, ctx, identity);
		}
	}

	boolean handleRoutedReply(Message m) {
		long id = m.getLong(DMT.UID);
		if(logMINOR) Logger.minor(this, "Got reply: "+m);
		Long lid = Long.valueOf(id);
		RoutedContext ctx = routedContexts.get(lid);
		if(ctx == null) {
			Logger.error(this, "Unrecognized routed reply: "+m);
			return false;
		}
		PeerNode pn = ctx.source;
		if(pn == null) return false;
		try {
			pn.sendAsync(m, null, nodeStats.routedMessageCtr);
		} catch (NotConnectedException e) {
			if(logMINOR) Logger.minor(this, "Lost connection forwarding "+m+" to "+pn);
		}
		return true;
	}

	private boolean forward(Message m, long id, PeerNode pn, short htl, double target, RoutedContext ctx, byte[] targetIdentity) {
		if(logMINOR) Logger.minor(this, "Should forward");
		// Forward
		m = preForward(m, htl);
		while(true) {
			PeerNode next = node.peers.getByIdentity(targetIdentity);
			if(next != null && !next.isConnected()) {
				Logger.error(this, "Found target but disconnected!: "+next);
				next = null;
			}
			if(next == null)
			next = node.peers.closerPeer(pn, ctx.routedTo, target, true, node.isAdvancedModeEnabled(), -1, null,
				        null, htl, 0, pn == null, false);
			if(logMINOR) Logger.minor(this, "Next: "+next+" message: "+m);
			if(next != null) {
				// next is connected, or at least has been => next.getPeer() CANNOT be null.
				if(logMINOR) Logger.minor(this, "Forwarding "+m.getSpec()+" to "+next.getPeer().getPort());
				ctx.addSent(next);
				try {
					next.sendAsync(m, null, nodeStats.routedMessageCtr);
				} catch (NotConnectedException e) {
					continue;
				}
			} else {
				if(logMINOR) Logger.minor(this, "Reached dead end for "+m.getSpec()+" on "+node.getDarknetPortNumber());
				// Reached a dead end...
				Message reject = DMT.createFNPRoutedRejected(id, htl);
				if(pn != null) try {
					pn.sendAsync(reject, null, nodeStats.routedMessageCtr);
				} catch (NotConnectedException e) {
					Logger.error(this, "Cannot send reject message back to source "+pn);
					return true;
				}
			}
			return true;
		}
	}

	/**
	 * Prepare a routed-to-node message for forwarding.
	 */
	private Message preForward(Message m, short newHTL) {
		m.set(DMT.HTL, newHTL); // update htl
		if(m.getSpec() == DMT.FNPRoutedPing) {
			int x = m.getInt(DMT.COUNTER);
			x++;
			m.set(DMT.COUNTER, x);
		}
		return m;
	}

	/**
	 * Deal with a routed-to-node message that landed on this node.
	 * This is where message-type-specific code executes. 
	 * @param m
	 * @return
	 */
	private boolean dispatchRoutedMessage(Message m, PeerNode src, long id) {
		if(m.getSpec() == DMT.FNPRoutedPing) {
			if(logMINOR) Logger.minor(this, "RoutedPing reached other side! ("+id+")");
			int x = m.getInt(DMT.COUNTER);
			Message reply = DMT.createFNPRoutedPong(id, x);
			if(logMINOR) Logger.minor(this, "Replying - counter = "+x+" for "+id);
			try {
				src.sendAsync(reply, null, nodeStats.routedMessageCtr);
			} catch (NotConnectedException e) {
				if(logMINOR) Logger.minor(this, "Lost connection replying to "+m+" in dispatchRoutedMessage");
			}
			return true;
		}
		return false;
	}

	void start(NodeStats stats) {
		this.nodeStats = stats;
		node.executor.execute(queueRunner);
	}

	public static String peersUIDsToString(long[] peerUIDs, double[] peerLocs) {
		StringBuilder sb = new StringBuilder(peerUIDs.length*23+peerLocs.length*26);
		int min=Math.min(peerUIDs.length, peerLocs.length);
		for(int i=0;i<min;i++) {
			double loc = peerLocs[i];
			long uid = peerUIDs[i];
			sb.append(loc);
			sb.append('=');
			sb.append(uid);
			if(i != min-1)
				sb.append('|');
		}
		if(peerUIDs.length > min) {
			for(int i=min;i<peerUIDs.length;i++) {
				sb.append("|U:");
				sb.append(peerUIDs[i]);
			}
		} else if(peerLocs.length > min) {
			for(int i=min;i<peerLocs.length;i++) {
				sb.append("|L:");
				sb.append(peerLocs[i]);
			}
		}
		return sb.toString();
	}
	
	// Probe requests

	// FIXME
	public static final int PROBE_TYPE_RESETTING_HTL = 0;
	
	public void startProbe(final double target, final ProbeCallback cb) {
		final long uid = node.random.nextLong();
		
			ProbeRequestSender rs = new ProbeRequestSender(target, node.maxHTL(), uid, node, node.getLocation(), null, 2.0);
			rs.addListener(new ProbeRequestSender.Listener() {

				public void onCompletion(double nearest, double best, short counter, short uniqueCounter, short linearCounter) throws NotConnectedException {
					cb.onCompleted("completed", target, best, nearest, uid, counter, uniqueCounter, linearCounter);
				}

				public void onRNF(short htl, double nearest, double best, short counter, short uniqueCounter, short linearCounter) throws NotConnectedException {
					cb.onCompleted("rnf", target, best, nearest, uid, counter, uniqueCounter, linearCounter);					
				}

				public void onReceivedRejectOverload(double nearest, double best, short counter, short uniqueCounter, short linearCounter, String reason) throws NotConnectedException {
					cb.onRejectOverload();
				}

				public void onTimeout(double nearest, double best, short counter, short uniqueCounter, short linearCounter, String reason) throws NotConnectedException {
					cb.onCompleted("timeout", target, best, nearest, uid, counter, uniqueCounter, linearCounter);					
				}

				public void onTrace(long uid, double nearest, double best, short htl, short counter, short uniqueCounter, double location, long myUID, ShortBuffer peerLocs, ShortBuffer peerUIDs, short forkCount, short linearCounter, String reason, long prevUID) throws NotConnectedException {
					cb.onTrace(uid, target, nearest, best, htl, counter, location, myUID, Fields.bytesToDoubles(peerLocs.getData()), Fields.bytesToLongs(peerUIDs.getData()), new double[0], forkCount, linearCounter, reason, prevUID);
				}
				
			});
			rs.start();
	}

	public void setHook(NodeDispatcherCallback cb) {
		this.callback = cb;
	}
}
package freenet.node.simulator;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.io.PrintStream;
import java.text.DateFormat;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.Date;
import java.util.GregorianCalendar;
import java.util.HashMap;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.TimeZone;
import java.util.TreeMap;
import java.util.Map.Entry;

import com.db4o.ObjectContainer;

import freenet.client.ClientMetadata;
import freenet.client.FetchException;
import freenet.client.HighLevelSimpleClient;
import freenet.client.InsertBlock;
import freenet.client.InsertException;
import freenet.client.async.ClientContext;
import freenet.client.events.ClientEvent;
import freenet.client.events.ClientEventListener;
import freenet.crypt.RandomSource;
import freenet.keys.FreenetURI;
import freenet.node.Node;
import freenet.node.NodeStarter;
import freenet.node.Version;
import freenet.support.Fields;
import freenet.support.Logger;
import freenet.support.PooledExecutor;
import freenet.support.Logger.LogLevel;
import freenet.support.api.Bucket;
import freenet.support.io.FileUtil;

/**
 * Push / Pull test over long period of time
 * 
 * <p>
 * This class push a series of keys in the format of
 * <code>KSK@&lt;unique identifier&gt;-DATE-n</code>. It will then try to pull them after (2^n - 1)
 * days.
 * <p>
 * The result is recorded as a CSV file in the format of:
 * 
 * <pre>
 * 	DATE, VERSION, SEED-TIME-1, PUSH-TIME-#0, ... , PUSH-TIME-#N, SEED-TIME-2, PULL-TIME-#0, ... , PULL-TIME-#N
 * </pre>
 * 
 * @author sdiz
 */
public class LongTermPushPullTest {
	private static final int TEST_SIZE = 64 * 1024;

	private static final int EXIT_NO_SEEDNODES = 257;
	private static final int EXIT_FAILED_TARGET = 258;
	private static final int EXIT_THREW_SOMETHING = 261;

	private static final int DARKNET_PORT1 = 5010;
	private static final int OPENNET_PORT1 = 5011;
	private static final int DARKNET_PORT2 = 5012;
	private static final int OPENNET_PORT2 = 5013;

	private static final int MAX_N = 8;

	private static final DateFormat dateFormat = new SimpleDateFormat("yyyy.MM.dd", Locale.US);
	static {
		dateFormat.setTimeZone(TimeZone.getTimeZone("GMT"));
	}
	private static final Calendar today = Calendar.getInstance(TimeZone.getTimeZone("GMT"));

	public static void main(String[] args) {
		if (args.length < 0 || args.length > 2) {
			System.err.println("Usage: java freenet.node.simulator.LongTermPushPullTest <unique identifier>");
			System.exit(1);
		}
		String uid = args[0];
		
		if(args.length == 2 && (args[1].equalsIgnoreCase("--dump") || args[1].equalsIgnoreCase("-dump") || args[1].equalsIgnoreCase("dump"))) {
			try {
				dumpStats(uid);
			} catch (IOException e) {
				System.err.println("IO ERROR: "+e);
				e.printStackTrace();
				System.exit(1);
			} catch (ParseException e) {
				System.err.println("PARSE ERROR: "+e);
				e.printStackTrace();
				System.exit(2);
			}
			System.exit(0);
		}

		List<String> csvLine = new ArrayList<String>(3 + 2 * MAX_N);
		System.out.println("DATE:" + dateFormat.format(today.getTime()));
		csvLine.add(dateFormat.format(today.getTime()));

		System.out.println("Version:" + Version.buildNumber());
		csvLine.add(String.valueOf(Version.buildNumber()));

		int exitCode = 0;
		Node node = null;
		Node node2 = null;
		try {
			final File dir = new File("longterm-push-pull-test-" + uid);
			FileUtil.removeAll(dir);
			RandomSource random = NodeStarter.globalTestInit(dir.getPath(), false, LogLevel.ERROR, "", false);
			File seednodes = new File("seednodes.fref");
			if (!seednodes.exists() || seednodes.length() == 0 || !seednodes.canRead()) {
				System.err.println("Unable to read seednodes.fref, it doesn't exist, or is empty");
				System.exit(EXIT_NO_SEEDNODES);
			}

			final File innerDir = new File(dir, Integer.toString(DARKNET_PORT1));
			innerDir.mkdir();
			FileInputStream fis = new FileInputStream(seednodes);
			FileUtil.writeTo(fis, new File(innerDir, "seednodes.fref"));
			fis.close();

			// Create one node
			node = NodeStarter.createTestNode(DARKNET_PORT1, OPENNET_PORT1, dir.getPath(), false, Node.DEFAULT_MAX_HTL,
			        0, random, new PooledExecutor(), 1000, 4 * 1024 * 1024, true, true, true, true, true, true, true,
			        12 * 1024, true, true, false, false, null);
			Logger.getChain().setThreshold(LogLevel.ERROR);

			// Start it
			node.start(true);
			long t1 = System.currentTimeMillis();
			if (!TestUtil.waitForNodes(node)) {
				exitCode = EXIT_FAILED_TARGET;
				return;
			}
				
			long t2 = System.currentTimeMillis();
			System.out.println("SEED-TIME:" + (t2 - t1));
			csvLine.add(String.valueOf(t2 - t1));

			// PUSH N+1 BLOCKS
			for (int i = 0; i <= MAX_N; i++) {
				Bucket data = randomData(node);
				HighLevelSimpleClient client = node.clientCore.makeClient((short) 0);
				FreenetURI uri = new FreenetURI("KSK@" + uid + "-" + dateFormat.format(today.getTime()) + "-" + i);
				System.out.println("PUSHING " + uri);
				client.addEventHook(new ClientEventListener() {

					public void onRemoveEventProducer(ObjectContainer container) {
						// Ignore
					}

					public void receive(ClientEvent ce, ObjectContainer maybeContainer, ClientContext context) {
						System.out.println(ce.getDescription());
					}
					
				});

				try {
					InsertBlock block = new InsertBlock(data, new ClientMetadata(), uri);
					t1 = System.currentTimeMillis();
					client.insert(block, false, null);
					t2 = System.currentTimeMillis();

					System.out.println("PUSH-TIME-" + i + ":" + (t2 - t1));
					csvLine.add(String.valueOf(t2 - t1));
				} catch (InsertException e) {
					e.printStackTrace();
					csvLine.add("N/A");
				}

				data.free();
			}

			node.park();

			// Node 2
			File innerDir2 = new File(dir, Integer.toString(DARKNET_PORT2));
			innerDir2.mkdir();
			fis = new FileInputStream(seednodes);
			FileUtil.writeTo(fis, new File(innerDir2, "seednodes.fref"));
			fis.close();
			node2 = NodeStarter.createTestNode(DARKNET_PORT2, OPENNET_PORT2, dir.getPath(), false,
			        Node.DEFAULT_MAX_HTL, 0, random, new PooledExecutor(), 1000, 5 * 1024 * 1024, true, true, true,
			        true, true, true, true, 12 * 1024, false, true, false, false, null);
			node2.start(true);

			t1 = System.currentTimeMillis();
			if (!TestUtil.waitForNodes(node2)) {
				exitCode = EXIT_FAILED_TARGET;
				return;
			}
			t2 = System.currentTimeMillis();
			System.out.println("SEED-TIME:" + (t2 - t1));
			csvLine.add(String.valueOf(t2 - t1));

			// PULL N+1 BLOCKS
			for (int i = 0; i <= MAX_N; i++) {
				HighLevelSimpleClient client = node2.clientCore.makeClient((short) 0);
				Calendar targetDate = (Calendar) today.clone();
				targetDate.add(Calendar.DAY_OF_MONTH, -((1 << i) - 1));

				FreenetURI uri = new FreenetURI("KSK@" + uid + "-" + dateFormat.format(targetDate.getTime()) + "-" + i);
				System.out.println("PULLING " + uri);

				try {
					t1 = System.currentTimeMillis();
					client.fetch(uri);
					t2 = System.currentTimeMillis();

					System.out.println("PULL-TIME-" + i + ":" + (t2 - t1));
					csvLine.add(String.valueOf(t2 - t1));
				} catch (FetchException e) {
					if (e.getMode() != FetchException.ALL_DATA_NOT_FOUND
					        && e.getMode() != FetchException.DATA_NOT_FOUND)
						e.printStackTrace();
					csvLine.add(FetchException.getShortMessage(e.getMode()));
				}
			}
		} catch (Throwable t) {
			t.printStackTrace();
			exitCode = EXIT_THREW_SOMETHING;
		} finally {
			try {
				if (node != null)
					node.park();
			} catch (Throwable t1) {
			}
			try {
				if (node2 != null)
					node2.park();
			} catch (Throwable t1) {
			}

			try {
				File file = new File(uid + ".csv");
				FileOutputStream fos = new FileOutputStream(file, true);
				PrintStream ps = new PrintStream(fos);

				ps.println(Fields.commaList(csvLine.toArray()));

				ps.close();
				fos.close();
			} catch (IOException e) {
				e.printStackTrace();
				exitCode = EXIT_THREW_SOMETHING;
			}
			
			System.exit(exitCode);
		}
	}

	private static void dumpStats(String uid) throws IOException, ParseException {
		File file = new File(uid + ".csv");
		FileInputStream fis = new FileInputStream(file);
		BufferedReader br = new BufferedReader(new InputStreamReader(fis));
		String line = null;
		Calendar prevDate = null;
		TreeMap<GregorianCalendar,DumpElement> map = new TreeMap<GregorianCalendar,DumpElement>();
		while((line = br.readLine()) != null) {
			DumpElement element;
			//System.out.println("LINE: "+line);
			String[] split = line.split(",");
			Date date = dateFormat.parse(split[0]);
			GregorianCalendar calendar = new GregorianCalendar(TimeZone.getTimeZone("GMT"));
			calendar.setTime(date);
			System.out.println("Date: "+dateFormat.format(calendar.getTime()));
			if(prevDate != null) {
				long now = calendar.getTimeInMillis();
				long prev = prevDate.getTimeInMillis();
				long dist = (now - prev) / (24 * 60 * 60 * 1000);
				if(dist != 1) System.out.println(""+dist+" days since last report");
			}
			prevDate = calendar;
			int version = Integer.parseInt(split[1]);
			if(split.length > 2) {
				long seedTime = Long.parseLong(split[2]);
				int[] pushTimes = new int[MAX_N+1];
				String[] pushFailures = new String[MAX_N+1];
				for(int i=0;i<=MAX_N;i++) {
					String s = split[3+i];
					try {
						pushTimes[i] = Integer.parseInt(s);
					} catch (NumberFormatException e) {
						pushFailures[i] = s;
					}
				}
				if(split.length > 3 + MAX_N+1) {
					int[] pullTimes = new int[MAX_N+1];
					String[] pullFailures = new String[MAX_N+1];
					for(int i=0;i<=MAX_N;i++) {
						String s = split[3+MAX_N+2+i];
						try {
							pullTimes[i] = Integer.parseInt(s);
						} catch (NumberFormatException e) {
							pullFailures[i] = s;
						}
					}
					element = new DumpElement(calendar, version, pushTimes, pushFailures, pullTimes, pullFailures);
				} else {
					element = new DumpElement(calendar, version, pushTimes, pushFailures);
				}
			} else {
				element = new DumpElement(calendar, version);
			}
			calendar.set(Calendar.MILLISECOND, 0);
			calendar.set(Calendar.SECOND, 0);
			calendar.set(Calendar.MINUTE, 0);
			calendar.set(Calendar.HOUR_OF_DAY, 0);
			map.put(calendar, element);
		}
		fis.close();
		for(int i=0;i<=MAX_N;i++) {
			int delta = ((1<<i)-1);
			System.out.println("Checking delta: "+delta+" days");
			int failures = 0;
			int successes = 0;
			long successTime = 0;
			int noMatch = 0;
			int insertFailure = 0;
			Map<String,Integer> failureModes = new HashMap<String,Integer>();
			for(Entry<GregorianCalendar,DumpElement> entry : map.entrySet()) {
				GregorianCalendar date = entry.getKey();
				DumpElement element = entry.getValue();
				if(element.pullTimes != null) {
					date = (GregorianCalendar) date.clone();
					date.add(Calendar.DAY_OF_MONTH, -delta);
					System.out.println("Checking "+date.getTime()+" for "+element.date.getTime()+" delta "+delta);
					DumpElement inserted = map.get(date);
					if(inserted == null) {
						System.out.println("No match");
						noMatch++;
						continue;
					}
					if(inserted.pushTimes == null || inserted.pushTimes[i] == 0) {
						System.out.println("Insert failure");
						if(element.pullTimes[i] != 0) {
							System.err.println("Fetched it anyway??!?!?: time "+element.pullTimes[i]);
						}
						insertFailure++;
					}
					if(element.pullTimes[i] == 0) {
						String failureMode = element.pullFailures[i];
						Integer count = failureModes.get(failureMode);
						if(count == null)
							failureModes.put(failureMode, 1);
						else
							failureModes.put(failureMode, count+1);
						failures++;
					} else {
						successes++;
						successTime += element.pullTimes[i];
					}
				}
			}
			System.out.println("Successes: "+successes);
			if(successes != 0) System.out.println("Average success time "+(successTime / successes));
			System.out.println("Failures: "+failures);
			for(Map.Entry<String,Integer> entry : failureModes.entrySet())
				System.out.println(entry.getKey()+" : "+entry.getValue());
			System.out.println("No match: "+noMatch);
			System.out.println("Insert failure: "+insertFailure);
			double psuccess = (successes*1.0 / (1.0*(successes + failures)));
			System.out.println("Success rate for "+delta+" days: "+psuccess+" ("+(successes+failures)+" samples)");
			if(delta != 0) {
				double halfLifeEstimate = -1*Math.log(2)/(Math.log(psuccess)/delta);
				System.out.println("Half-life estimate: "+halfLifeEstimate+" days");
			}
			System.out.println();
		}
	}
	
	static class DumpElement {
		public DumpElement(GregorianCalendar date, int version) {
			this.date = date;
			this.version = version;
			this.seedTime = -1;
			this.pushTimes = null;
			this.pushFailures = null;
			this.pullTimes = null;
			this.pullFailures = null;
		}
		public DumpElement(GregorianCalendar date, int version, int[] pushTimes, String[] pushFailures) {
			this.date = date;
			this.version = version;
			this.seedTime = -1;
			this.pushTimes = pushTimes;
			this.pushFailures = pushFailures;
			this.pullTimes = null;
			this.pullFailures = null;
		}
		public DumpElement(GregorianCalendar date, int version, int[] pushTimes, String[] pushFailures, int[] pullTimes, String[] pullFailures) {
			this.date = date;
			this.version = version;
			this.seedTime = -1;
			this.pushTimes = pushTimes;
			this.pushFailures = pushFailures;
			this.pullTimes = pullTimes;
			this.pullFailures = pullFailures;
		}
		final GregorianCalendar date;
		final int version;
		final long seedTime;
		final int[] pushTimes; // 0 = failure, look up in pushFailures
		final String[] pushFailures;
		final int[] pullTimes;
		final String[] pullFailures;
	}
	

	private static Bucket randomData(Node node) throws IOException {
		Bucket data = node.clientCore.tempBucketFactory.makeBucket(TEST_SIZE);
		OutputStream os = data.getOutputStream();
		byte[] buf = new byte[4096];
		for (long written = 0; written < TEST_SIZE;) {
			node.fastWeakRandom.nextBytes(buf);
			int toWrite = (int) Math.min(TEST_SIZE - written, buf.length);
			os.write(buf, 0, toWrite);
			written += toWrite;
		}
		os.close();
		return data;
	}
}
package freenet.support;

public interface ExecutorIdleCallback {
	
	/** Called when the executor is idle for some period. On a single-thread executor,
	 * this will be called on the thread which runs the jobs, but after that the thread
	 * may change. */
	public void onIdle();

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support.codeshortification;


/**
 * Class for reducing the amount of code to type with regards to null pointers: <br>
 * <code>if(value == null) throw new NullPointerException(message);</code><br>
 * becomes:<br>
 * <code>IfNull.thenThrow(value, message);</code>
 * 
 * @author xor (xor@freenetproject.org)
 */
public final class IfNull {
	
	public static final void thenThrow(Object value) {
		if(value == null)
			throw new NullPointerException();
	}
	
	public static final void thenThrow(Object value, String message) {
		if(value == null)
			throw new NullPointerException(message);
	}
	
}
package freenet.pluginmanager;

public class PluginTooOldException extends PluginNotFoundException {

	final private static long serialVersionUID = -3104024342634046289L;

	public PluginTooOldException(String string) {
		super(string);
	}

}
/*
 * Copyright (c) 1997, 1998 Systemics Ltd on behalf of
 * the Cryptix Development Team. All rights reserved.
 */
package freenet.crypt.ciphers;

import java.io.PrintWriter;
import java.security.InvalidKeyException;

//...........................................................................
/**
 * Rijndael --pronounced Reindaal-- is a variable block-size (128-, 192- and
 * 256-bit), variable key-size (128-, 192- and 256-bit) symmetric cipher.<p>
 *
 * Rijndael was written by <a href="mailto:rijmen@esat.kuleuven.ac.be">Vincent
 * Rijmen</a> and <a href="mailto:Joan.Daemen@village.uunet.be">Joan Daemen</a>.<p>
 *
 * Portions of this code are <b>Copyright</b> &copy; 1997, 1998
 * <a href="http://www.systemics.com/">Systemics Ltd</a> on behalf of the
 * <a href="http://www.systemics.com/docs/cryptix/">Cryptix Development Team</a>.
 * <br>All rights reserved.<p>
 *
 * @author  Raif S. Naffah
 * @author  Paulo S. L. M. Barreto
 *
 * License is apparently available from http://www.cryptix.org/docs/license.html
 */
final class Rijndael_Algorithm // implicit no-argument constructor
{
//	Debugging methods and variables
//	...........................................................................

	private static final String NAME = "Rijndael_Algorithm";
	private static final boolean IN = true, OUT = false;

	private static final boolean RDEBUG = Rijndael_Properties.GLOBAL_DEBUG;
	private static final int debuglevel = RDEBUG ? Rijndael_Properties.getLevel(NAME) : 0;
	private static final PrintWriter err = RDEBUG ? Rijndael_Properties.getOutput() : null;

	private static final boolean TRACE = Rijndael_Properties.isTraceable(NAME);

	private static void debug(String s) {
		err.println(">>> " + NAME + ": " + s);
	}

	private static void trace(boolean in, String s) {
		if (TRACE) err.println((in?"==> ":"<== ")+NAME+ '.' +s);
	}
	
//	Constants and variables
//	...........................................................................

	private static final int BLOCK_SIZE = 16; // default block size in bytes

	private static final int[] alog = new int[256];
	private static final int[] log = new int[256];

	private static final byte[] S = new byte[256];
	private static final byte[] Si = new byte[256];
	private static final int[] T1 = new int[256];
	private static final int[] T2 = new int[256];
	private static final int[] T3 = new int[256];
	private static final int[] T4 = new int[256];
	private static final int[] T5 = new int[256];
	private static final int[] T6 = new int[256];
	private static final int[] T7 = new int[256];
	private static final int[] T8 = new int[256];
	private static final int[] U1 = new int[256];
	private static final int[] U2 = new int[256];
	private static final int[] U3 = new int[256];
	private static final int[] U4 = new int[256];
	private static final byte[] rcon = new byte[30];

	private static final int[][][] shifts = new int[][][] {
		{ {0, 0}, {1, 3}, {2, 2}, {3, 1} },
		{ {0, 0}, {1, 5}, {2, 4}, {3, 3} },
		{ {0, 0}, {1, 7}, {3, 5}, {4, 4} }
	};

	private static final char[] HEX_DIGITS = {
		'0','1','2','3','4','5','6','7','8','9','A','B','C','D','E','F'
	};


//	Static code - to intialise S-boxes and T-boxes
//	...........................................................................

	static {
		long time = System.currentTimeMillis();

		if (RDEBUG && (debuglevel > 6)) {
			System.out.println("Algorithm Name: "+Rijndael_Properties.FULL_NAME);
			System.out.println("Electronic Codebook (ECB) Mode");
			System.out.println();
		}
		int ROOT = 0x11B;
		int i, j = 0;

		//
		// produce log and alog tables, needed for multiplying in the
		// field GF(2^m) (generator = 3)
		//
		alog[0] = 1;
		for (i = 1; i < 256; i++) {
			j = (alog[i-1] << 1) ^ alog[i-1];
			if ((j & 0x100) != 0) j ^= ROOT;
			alog[i] = j;
		}
		for (i = 1; i < 255; i++) log[alog[i]] = i;
		byte[][] A = new byte[][] {
				{1, 1, 1, 1, 1, 0, 0, 0},
				{0, 1, 1, 1, 1, 1, 0, 0},
				{0, 0, 1, 1, 1, 1, 1, 0},
				{0, 0, 0, 1, 1, 1, 1, 1},
				{1, 0, 0, 0, 1, 1, 1, 1},
				{1, 1, 0, 0, 0, 1, 1, 1},
				{1, 1, 1, 0, 0, 0, 1, 1},
				{1, 1, 1, 1, 0, 0, 0, 1}
		};
		byte[] B = new byte[] { 0, 1, 1, 0, 0, 0, 1, 1};

		//
		// substitution box based on F^{-1}(x)
		//
		int t;
		byte[][] box = new byte[256][8];
		box[1][7] = 1;
		for (i = 2; i < 256; i++) {
			j = alog[255 - log[i]];
			for (t = 0; t < 8; t++)
				box[i][t] = (byte)((j >>> (7 - t)) & 0x01);
		}
		//
		// affine transform:  box[i] <- B + A*box[i]
		//
		byte[][] cox = new byte[256][8];
		for (i = 0; i < 256; i++)
			for (t = 0; t < 8; t++) {
				cox[i][t] = B[t];
				for (j = 0; j < 8; j++)
					cox[i][t] ^= A[t][j] * box[i][j];
			}
		//
		// S-boxes and inverse S-boxes
		//
		for (i = 0; i < 256; i++) {
			S[i] = (byte)(cox[i][0] << 7);
			for (t = 1; t < 8; t++)
				S[i] ^= cox[i][t] << (7-t);
			Si[S[i] & 0xFF] = (byte) i;
		}
		//
		// T-boxes
		//
		byte[][] G = new byte[][] {
				{2, 1, 1, 3},
				{3, 2, 1, 1},
				{1, 3, 2, 1},
				{1, 1, 3, 2}
		};
		byte[][] AA = new byte[4][8];
		for (i = 0; i < 4; i++) {
			for (j = 0; j < 4; j++) AA[i][j] = G[i][j];
			AA[i][i+4] = 1;
		}
		byte pivot, tmp;
		byte[][] iG = new byte[4][4];
		for (i = 0; i < 4; i++) {
			pivot = AA[i][i];
			if (pivot == 0) {
				t = i + 1;
				while ((AA[t][i] == 0) && (t < 4))
					t++;
				if (t == 4)
					throw new RuntimeException("G matrix is not invertible");
				else {
					for (j = 0; j < 8; j++) {
						tmp = AA[i][j];
						AA[i][j] = AA[t][j];
						AA[t][j] = tmp;
					}
					pivot = AA[i][i];
				}
			}
			for (j = 0; j < 8; j++)
				if (AA[i][j] != 0)
					AA[i][j] = (byte)
					alog[(255 + log[AA[i][j] & 0xFF] - log[pivot & 0xFF]) % 255];
			for (t = 0; t < 4; t++)
				if (i != t) {
					for (j = i+1; j < 8; j++)
						AA[t][j] ^= mul(AA[i][j], AA[t][i]);
					AA[t][i] = 0;
				}
		}
		for (i = 0; i < 4; i++)
			for (j = 0; j < 4; j++) iG[i][j] = AA[i][j + 4];

		int s;
		for (t = 0; t < 256; t++) {
			s = S[t];
			T1[t] = mul4(s, G[0]);
			T2[t] = mul4(s, G[1]);
			T3[t] = mul4(s, G[2]);
			T4[t] = mul4(s, G[3]);

			s = Si[t];
			T5[t] = mul4(s, iG[0]);
			T6[t] = mul4(s, iG[1]);
			T7[t] = mul4(s, iG[2]);
			T8[t] = mul4(s, iG[3]);

			U1[t] = mul4(t, iG[0]);
			U2[t] = mul4(t, iG[1]);
			U3[t] = mul4(t, iG[2]);
			U4[t] = mul4(t, iG[3]);
		}
		//
		// round constants
		//
		rcon[0] = 1;
		int r = 1;
		for (t = 1; t < 30; ) rcon[t++] = (byte)(r = mul(2, r));

		time = System.currentTimeMillis() - time;

		if (RDEBUG && (debuglevel > 8)) {
			System.out.println("==========");
			System.out.println();
			System.out.println("Static Data");
			System.out.println();
			System.out.println("S[]:"); for(i=0;i<16;i++) { for(j=0;j<16;j++) System.out.print("0x"+byteToString(S[i*16+j])+", "); System.out.println();}
			System.out.println();
			System.out.println("Si[]:"); for(i=0;i<16;i++) { for(j=0;j<16;j++) System.out.print("0x"+byteToString(Si[i*16+j])+", "); System.out.println();}

			System.out.println();
			System.out.println("iG[]:"); for(i=0;i<4;i++){for(j=0;j<4;j++) System.out.print("0x"+byteToString(iG[i][j])+", "); System.out.println();}

			System.out.println();
			System.out.println("T1[]:"); for(i=0;i<64;i++){for(j=0;j<4;j++) System.out.print("0x"+intToString(T1[i*4+j])+", "); System.out.println();}
			System.out.println();
			System.out.println("T2[]:"); for(i=0;i<64;i++){for(j=0;j<4;j++) System.out.print("0x"+intToString(T2[i*4+j])+", "); System.out.println();}
			System.out.println();
			System.out.println("T3[]:"); for(i=0;i<64;i++){for(j=0;j<4;j++) System.out.print("0x"+intToString(T3[i*4+j])+", "); System.out.println();}
			System.out.println();
			System.out.println("T4[]:"); for(i=0;i<64;i++){for(j=0;j<4;j++) System.out.print("0x"+intToString(T4[i*4+j])+", "); System.out.println();}
			System.out.println();
			System.out.println("T5[]:"); for(i=0;i<64;i++){for(j=0;j<4;j++) System.out.print("0x"+intToString(T5[i*4+j])+", "); System.out.println();}
			System.out.println();
			System.out.println("T6[]:"); for(i=0;i<64;i++){for(j=0;j<4;j++) System.out.print("0x"+intToString(T6[i*4+j])+", "); System.out.println();}
			System.out.println();
			System.out.println("T7[]:"); for(i=0;i<64;i++){for(j=0;j<4;j++) System.out.print("0x"+intToString(T7[i*4+j])+", "); System.out.println();}
			System.out.println();
			System.out.println("T8[]:"); for(i=0;i<64;i++){for(j=0;j<4;j++) System.out.print("0x"+intToString(T8[i*4+j])+", "); System.out.println();}

			System.out.println();
			System.out.println("U1[]:"); for(i=0;i<64;i++){for(j=0;j<4;j++) System.out.print("0x"+intToString(U1[i*4+j])+", "); System.out.println();}
			System.out.println();
			System.out.println("U2[]:"); for(i=0;i<64;i++){for(j=0;j<4;j++) System.out.print("0x"+intToString(U2[i*4+j])+", "); System.out.println();}
			System.out.println();
			System.out.println("U3[]:"); for(i=0;i<64;i++){for(j=0;j<4;j++) System.out.print("0x"+intToString(U3[i*4+j])+", "); System.out.println();}
			System.out.println();
			System.out.println("U4[]:"); for(i=0;i<64;i++){for(j=0;j<4;j++) System.out.print("0x"+intToString(U4[i*4+j])+", "); System.out.println();}

			System.out.println();
			System.out.println("rcon[]:"); for(i=0;i<5;i++){for(j=0;j<6;j++) System.out.print("0x"+byteToString(rcon[i*6+j])+", "); System.out.println();}

			System.out.println();
			System.out.println("Total initialization time: "+time+" ms.");
			System.out.println();
		}
	}

	// multiply two elements of GF(2^m)
	private static final int mul(int a, int b) {
		return ((a != 0) && (b != 0)) ?
				alog[(log[a & 0xFF] + log[b & 0xFF]) % 255] :
					0;
	}

	// convenience method used in generating Transposition boxes
	private static final int mul4(int a, byte[] b) {
		if (a == 0) return 0;
		a = log[a & 0xFF];
		int a0 = (b[0] != 0) ? alog[(a + log[b[0] & 0xFF]) % 255] & 0xFF : 0;
		int a1 = (b[1] != 0) ? alog[(a + log[b[1] & 0xFF]) % 255] & 0xFF : 0;
		int a2 = (b[2] != 0) ? alog[(a + log[b[2] & 0xFF]) % 255] & 0xFF : 0;
		int a3 = (b[3] != 0) ? alog[(a + log[b[3] & 0xFF]) % 255] & 0xFF : 0;
		return a0 << 24 | a1 << 16 | a2 << 8 | a3;
	}


//	Basic API methods
//	...........................................................................

	/**
	 * Convenience method to encrypt exactly one block of plaintext, assuming
	 * Rijndael's default block size (128-bit).
	 *
	 * @param  in         The plaintext.
	 * @param  result     The buffer into which to write the resulting ciphertext.
	 * @param  inOffset   Index of in from which to start considering data.
	 * @param  sessionKey The session key to use for encryption.
	 */
	private static final void
	blockEncrypt (byte[] in, byte[] result, int inOffset, Object sessionKey) {
		if (RDEBUG) trace(IN, "blockEncrypt("+in+", "+inOffset+", "+sessionKey+ ')');
		int[][] Ke = (int[][]) ((Object[]) sessionKey)[0]; // extract encryption round keys
		int ROUNDS = Ke.length - 1;
		int[] Ker = Ke[0];

		// plaintext to ints + key
		int t0   = ((in[inOffset++] & 0xFF) << 24 |
				(in[inOffset++] & 0xFF) << 16 |
				(in[inOffset++] & 0xFF) <<  8 |
				(in[inOffset++] & 0xFF)        ) ^ Ker[0];
		int t1   = ((in[inOffset++] & 0xFF) << 24 |
				(in[inOffset++] & 0xFF) << 16 |
				(in[inOffset++] & 0xFF) <<  8 |
				(in[inOffset++] & 0xFF)        ) ^ Ker[1];
		int t2   = ((in[inOffset++] & 0xFF) << 24 |
				(in[inOffset++] & 0xFF) << 16 |
				(in[inOffset++] & 0xFF) <<  8 |
				(in[inOffset++] & 0xFF)        ) ^ Ker[2];
		int t3   = ((in[inOffset++] & 0xFF) << 24 |
				(in[inOffset++] & 0xFF) << 16 |
				(in[inOffset++] & 0xFF) <<  8 |
				(in[inOffset++] & 0xFF)        ) ^ Ker[3];

		int a0, a1, a2, a3;
		for (int r = 1; r < ROUNDS; r++) {          // apply round transforms
			Ker = Ke[r];
		a0   = (T1[(t0 >>> 24) & 0xFF] ^
				T2[(t1 >>> 16) & 0xFF] ^
				T3[(t2 >>>  8) & 0xFF] ^
				T4[ t3         & 0xFF]  ) ^ Ker[0];
		a1   = (T1[(t1 >>> 24) & 0xFF] ^
				T2[(t2 >>> 16) & 0xFF] ^
				T3[(t3 >>>  8) & 0xFF] ^
				T4[ t0         & 0xFF]  ) ^ Ker[1];
		a2   = (T1[(t2 >>> 24) & 0xFF] ^
				T2[(t3 >>> 16) & 0xFF] ^
				T3[(t0 >>>  8) & 0xFF] ^
				T4[ t1         & 0xFF]  ) ^ Ker[2];
		a3   = (T1[(t3 >>> 24) & 0xFF] ^
				T2[(t0 >>> 16) & 0xFF] ^
				T3[(t1 >>>  8) & 0xFF] ^
				T4[ t2         & 0xFF]  ) ^ Ker[3];
		t0 = a0;
		t1 = a1;
		t2 = a2;
		t3 = a3;
		if (RDEBUG && (debuglevel > 6)) System.out.println("CT"+r+ '=' +intToString(t0)+intToString(t1)+intToString(t2)+intToString(t3));
		}

		// last round is special
		Ker = Ke[ROUNDS];
		int tt = Ker[0];
		result[ 0] = (byte)(S[(t0 >>> 24) & 0xFF] ^ (tt >>> 24));
		result[ 1] = (byte)(S[(t1 >>> 16) & 0xFF] ^ (tt >>> 16));
		result[ 2] = (byte)(S[(t2 >>>  8) & 0xFF] ^ (tt >>>  8));
		result[ 3] = (byte)(S[ t3         & 0xFF] ^  tt        );
		tt = Ker[1];
		result[ 4] = (byte)(S[(t1 >>> 24) & 0xFF] ^ (tt >>> 24));
		result[ 5] = (byte)(S[(t2 >>> 16) & 0xFF] ^ (tt >>> 16));
		result[ 6] = (byte)(S[(t3 >>>  8) & 0xFF] ^ (tt >>>  8));
		result[ 7] = (byte)(S[ t0         & 0xFF] ^  tt        );
		tt = Ker[2];
		result[ 8] = (byte)(S[(t2 >>> 24) & 0xFF] ^ (tt >>> 24));
		result[ 9] = (byte)(S[(t3 >>> 16) & 0xFF] ^ (tt >>> 16));
		result[10] = (byte)(S[(t0 >>>  8) & 0xFF] ^ (tt >>>  8));
		result[11] = (byte)(S[ t1         & 0xFF] ^  tt        );
		tt = Ker[3];
		result[12] = (byte)(S[(t3 >>> 24) & 0xFF] ^ (tt >>> 24));
		result[13] = (byte)(S[(t0 >>> 16) & 0xFF] ^ (tt >>> 16));
		result[14] = (byte)(S[(t1 >>>  8) & 0xFF] ^ (tt >>>  8));
		result[15] = (byte)(S[ t2         & 0xFF] ^  tt        );
		if (RDEBUG && (debuglevel > 6)) {
			System.out.println("CT="+toString(result));
			System.out.println();
		}
		if (RDEBUG) trace(OUT, "blockEncrypt()");
	}

	/**
	 * Convenience method to decrypt exactly one block of plaintext, assuming
	 * Rijndael's default block size (128-bit).
	 *
	 * @param  in         The ciphertext.
	 * @param  result the resulting ciphertext
	 * @param  inOffset   Index of in from which to start considering data.
	 * @param  sessionKey The session key to use for decryption.
	 */
	private static final void
	blockDecrypt (byte[] in, byte[] result, int inOffset, Object sessionKey) {
		if (RDEBUG) trace(IN, "blockDecrypt("+in+", "+inOffset+", "+sessionKey+ ')');
		int[][] Kd = (int[][]) ((Object[]) sessionKey)[1]; // extract decryption round keys
		int ROUNDS = Kd.length - 1;
		int[] Kdr = Kd[0];

		// ciphertext to ints + key
		int t0   = ((in[inOffset++] & 0xFF) << 24 |
				(in[inOffset++] & 0xFF) << 16 |
				(in[inOffset++] & 0xFF) <<  8 |
				(in[inOffset++] & 0xFF)        ) ^ Kdr[0];
		int t1   = ((in[inOffset++] & 0xFF) << 24 |
				(in[inOffset++] & 0xFF) << 16 |
				(in[inOffset++] & 0xFF) <<  8 |
				(in[inOffset++] & 0xFF)        ) ^ Kdr[1];
		int t2   = ((in[inOffset++] & 0xFF) << 24 |
				(in[inOffset++] & 0xFF) << 16 |
				(in[inOffset++] & 0xFF) <<  8 |
				(in[inOffset++] & 0xFF)        ) ^ Kdr[2];
		int t3   = ((in[inOffset++] & 0xFF) << 24 |
				(in[inOffset++] & 0xFF) << 16 |
				(in[inOffset++] & 0xFF) <<  8 |
				(in[inOffset++] & 0xFF)        ) ^ Kdr[3];

		int a0, a1, a2, a3;
		for (int r = 1; r < ROUNDS; r++) {          // apply round transforms
			Kdr = Kd[r];
		a0   = (T5[(t0 >>> 24) & 0xFF] ^
				T6[(t3 >>> 16) & 0xFF] ^
				T7[(t2 >>>  8) & 0xFF] ^
				T8[ t1         & 0xFF]  ) ^ Kdr[0];
		a1   = (T5[(t1 >>> 24) & 0xFF] ^
				T6[(t0 >>> 16) & 0xFF] ^
				T7[(t3 >>>  8) & 0xFF] ^
				T8[ t2         & 0xFF]  ) ^ Kdr[1];
		a2   = (T5[(t2 >>> 24) & 0xFF] ^
				T6[(t1 >>> 16) & 0xFF] ^
				T7[(t0 >>>  8) & 0xFF] ^
				T8[ t3         & 0xFF]  ) ^ Kdr[2];
		a3   = (T5[(t3 >>> 24) & 0xFF] ^
				T6[(t2 >>> 16) & 0xFF] ^
				T7[(t1 >>>  8) & 0xFF] ^
				T8[ t0         & 0xFF]  ) ^ Kdr[3];
		t0 = a0;
		t1 = a1;
		t2 = a2;
		t3 = a3;
		if (RDEBUG && (debuglevel > 6)) System.out.println("PT"+r+ '=' +intToString(t0)+intToString(t1)+intToString(t2)+intToString(t3));
		}

		// last round is special
		Kdr = Kd[ROUNDS];
		int tt = Kdr[0];
		result[ 0] = (byte)(Si[(t0 >>> 24) & 0xFF] ^ (tt >>> 24));
		result[ 1] = (byte)(Si[(t3 >>> 16) & 0xFF] ^ (tt >>> 16));
		result[ 2] = (byte)(Si[(t2 >>>  8) & 0xFF] ^ (tt >>>  8));
		result[ 3] = (byte)(Si[ t1         & 0xFF] ^  tt        );
		tt = Kdr[1];
		result[ 4] = (byte)(Si[(t1 >>> 24) & 0xFF] ^ (tt >>> 24));
		result[ 5] = (byte)(Si[(t0 >>> 16) & 0xFF] ^ (tt >>> 16));
		result[ 6] = (byte)(Si[(t3 >>>  8) & 0xFF] ^ (tt >>>  8));
		result[ 7] = (byte)(Si[ t2         & 0xFF] ^  tt        );
		tt = Kdr[2];
		result[ 8] = (byte)(Si[(t2 >>> 24) & 0xFF] ^ (tt >>> 24));
		result[ 9] = (byte)(Si[(t1 >>> 16) & 0xFF] ^ (tt >>> 16));
		result[10] = (byte)(Si[(t0 >>>  8) & 0xFF] ^ (tt >>>  8));
		result[11] = (byte)(Si[ t3         & 0xFF] ^  tt        );
		tt = Kdr[3];
		result[12] = (byte)(Si[(t3 >>> 24) & 0xFF] ^ (tt >>> 24));
		result[13] = (byte)(Si[(t2 >>> 16) & 0xFF] ^ (tt >>> 16));
		result[14] = (byte)(Si[(t1 >>>  8) & 0xFF] ^ (tt >>>  8));
		result[15] = (byte)(Si[ t0         & 0xFF] ^  tt        );
		if (RDEBUG && (debuglevel > 6)) {
			System.out.println("PT="+toString(result));
			System.out.println();
		}
		if (RDEBUG) trace(OUT, "blockDecrypt()");
	}

	/** A basic symmetric encryption/decryption test. */
	static boolean self_test() {
		return self_test(BLOCK_SIZE);
	}


//	Rijndael own methods
//	...........................................................................

	/** @return The default length in bytes of the Algorithm input block. */
	static final int blockSize() {
		return BLOCK_SIZE;
	}

	/**
	 * Expand a user-supplied key material into a session key.
	 *
	 * @param k        The 128/192/256-bit user-key to use.
	 * @param blockSize  The block size in bytes of this Rijndael.
	 * @exception  InvalidKeyException  If the key is invalid.
	 */
	//TODO: This method doesn't really need synchronization. The only reason
	//I can see for it to be synchronized is that it will consume 100% CPU (due to
	//heavy calculations) when called. Probably should be unsynchronized if we
	//want better support for dual+ CPU machines. /Iakin 2003-10-12
	//Concur:  the class has no fields which are not final, and does
	//not reference fields of any other classes.  Control over how
	//many simultaneous makeKey invocations should be allowed is
	//a problem the callers should resolve among themselves.
	//It is a fact that allowing no more than one makeKey on any given
	//CPU will result in fewer cache misses.  -- ejhuff 2003-10-12
	final static synchronized Object makeKey(byte[] k, int blockSize)
	throws InvalidKeyException {
		if (RDEBUG) trace(IN, "makeKey("+k+", "+blockSize+ ')');
		if (k == null)
			throw new InvalidKeyException("Empty key");
		if (!((k.length == 16) || (k.length == 24) || (k.length == 32)))
			throw new InvalidKeyException("Incorrect key length");
		int ROUNDS = getRounds(k.length, blockSize);
		int BC = blockSize / 4;
		int[][] Ke = new int[ROUNDS + 1][BC]; // encryption round keys
		int[][] Kd = new int[ROUNDS + 1][BC]; // decryption round keys
		int ROUND_KEY_COUNT = (ROUNDS + 1) * BC;
		int KC = k.length / 4;
		int[] tk = new int[KC];
		int i, j;

		// copy user material bytes into temporary ints
		for (i = 0, j = 0; i < KC; )
			tk[i++] = (k[j++] & 0xFF) << 24 |
			(k[j++] & 0xFF) << 16 |
			(k[j++] & 0xFF) <<  8 |
			(k[j++] & 0xFF);
		// copy values into round key arrays
		int t = 0;
		for (j = 0; (j < KC) && (t < ROUND_KEY_COUNT); j++, t++) {
			Ke[t / BC][t % BC] = tk[j];
			Kd[ROUNDS - (t / BC)][t % BC] = tk[j];
		}
		int tt, rconpointer = 0;
		while (t < ROUND_KEY_COUNT) {
			// extrapolate using phi (the round key evolution function)
			tt = tk[KC - 1];
			tk[0] ^= (S[(tt >>> 16) & 0xFF] & 0xFF) << 24 ^
			(S[(tt >>>  8) & 0xFF] & 0xFF) << 16 ^
			(S[ tt         & 0xFF] & 0xFF) <<  8 ^
			(S[(tt >>> 24) & 0xFF] & 0xFF)       ^
			(rcon[rconpointer++]   & 0xFF) << 24;
			if (KC != 8)
				for (i = 1, j = 0; i < KC; ) {
					//tk[i++] ^= tk[j++];
					// The above line replaced with the code below in order to work around
					// a bug in the kjc-1.4F java compiler (which has been reported).
					tk[i] ^= tk[j++];
					i++;
				}
			else {
				for (i = 1, j = 0; i < KC / 2; ) {
					//tk[i++] ^= tk[j++];
					// The above line replaced with the code below in order to work around
					// a bug in the kjc-1.4F java compiler (which has been reported).
					tk[i] ^= tk[j++];
					i++;
				}
				tt = tk[KC / 2 - 1];
				tk[KC / 2] ^= (S[ tt         & 0xFF] & 0xFF)       ^
				(S[(tt >>>  8) & 0xFF] & 0xFF) <<  8 ^
				(S[(tt >>> 16) & 0xFF] & 0xFF) << 16 ^
				(S[(tt >>> 24) & 0xFF] & 0xFF) << 24;
				for (j = KC / 2, i = j + 1; i < KC; ) {
					//tk[i++] ^= tk[j++];
					// The above line replaced with the code below in order to work around
					// a bug in the kjc-1.4F java compiler (which has been reported).
					tk[i] ^= tk[j++];
					i++;
				}
			}
			// copy values into round key arrays
			for (j = 0; (j < KC) && (t < ROUND_KEY_COUNT); j++, t++) {
				Ke[t / BC][t % BC] = tk[j];
				Kd[ROUNDS - (t / BC)][t % BC] = tk[j];
			}
		}
		for (int r = 1; r < ROUNDS; r++)    // inverse MixColumn where needed
			for (j = 0; j < BC; j++) {
				tt = Kd[r][j];
				Kd[r][j] = U1[(tt >>> 24) & 0xFF] ^
				U2[(tt >>> 16) & 0xFF] ^
				U3[(tt >>>  8) & 0xFF] ^
				U4[ tt         & 0xFF];
			}
		// assemble the encryption (Ke) and decryption (Kd) round keys into
		// one sessionKey object
		Object[] sessionKey = new Object[] {Ke, Kd};
		if (RDEBUG) trace(OUT, "makeKey()");
		return sessionKey;
	}

	/**
	 * Encrypt exactly one block of plaintext.
	 *
	 * @param  in         The plaintext.
	 * @param  result     The buffer into which to write the resulting ciphertext.
	 * @param  inOffset   Index of in from which to start considering data.
	 * @param  sessionKey The session key to use for encryption.
	 * @param  blockSize  The block size in bytes of this Rijndael.
	 */
	static final void
	blockEncrypt (byte[] in, byte[] result, int inOffset, Object sessionKey, int blockSize) {
		if (blockSize == BLOCK_SIZE) {
			blockEncrypt(in, result, inOffset, sessionKey);
			return;
		}

		int BC = blockSize / 4;

		int[] a = new int[BC];
		int[] t = new int[BC]; // temporary work array

		blockEncrypt(in, result, inOffset, sessionKey, blockSize, a, t);
	}

	/**
	 * Encrypt exactly one block of plaintext.
	 *
	 * @param  in         The plaintext.
	 * @param  result     The buffer into which to write the resulting ciphertext.
	 * @param  inOffset   Index of in from which to start considering data.
	 * @param  sessionKey The session key to use for encryption.
	 * @param  blockSize  The block size in bytes of this Rijndael.
	 */
	static final void
	blockEncrypt (byte[] in, byte[] result, int inOffset, Object sessionKey, int blockSize, int[] a, int[] t) {
		if (blockSize == BLOCK_SIZE) {
			blockEncrypt(in, result, inOffset, sessionKey);
			return;
		}
		if (RDEBUG) trace(IN, "blockEncrypt("+in+", "+inOffset+", "+sessionKey+", "+blockSize+ ')');
		Object[] sKey = (Object[]) sessionKey; // extract encryption round keys
		int[][] Ke = (int[][]) sKey[0];

		int BC = blockSize / 4;
		int ROUNDS = Ke.length - 1;
		int SC = BC == 4 ? 0 : (BC == 6 ? 1 : 2);
		int s1 = shifts[SC][1][0];
		int s2 = shifts[SC][2][0];
		int s3 = shifts[SC][3][0];
		int i;
		int j = 0, tt;

		for (i = 0; i < BC; i++)                   // plaintext to ints + key
		t[i] = ((in[inOffset++] & 0xFF) << 24 |
				(in[inOffset++] & 0xFF) << 16 |
				(in[inOffset++] & 0xFF) <<  8 |
				(in[inOffset++] & 0xFF)        ) ^ Ke[0][i];
		for (int r = 1; r < ROUNDS; r++) {          // apply round transforms
			for (i = 0; i < BC; i++)
				a[i] = (T1[(t[ i           ] >>> 24) & 0xFF] ^
						T2[(t[(i + s1) % BC] >>> 16) & 0xFF] ^
						T3[(t[(i + s2) % BC] >>>  8) & 0xFF] ^
						T4[ t[(i + s3) % BC]         & 0xFF]  ) ^ Ke[r][i];
			System.arraycopy(a, 0, t, 0, BC);
			if (RDEBUG && (debuglevel > 6)) System.out.println("CT"+r+ '=' +toString(t));
		}
		for (i = 0; i < BC; i++) {                   // last round is special
			tt = Ke[ROUNDS][i];
			result[j++] = (byte)(S[(t[ i           ] >>> 24) & 0xFF] ^ (tt >>> 24));
			result[j++] = (byte)(S[(t[(i + s1) % BC] >>> 16) & 0xFF] ^ (tt >>> 16));
			result[j++] = (byte)(S[(t[(i + s2) % BC] >>>  8) & 0xFF] ^ (tt >>>  8));
			result[j++] = (byte)(S[ t[(i + s3) % BC]         & 0xFF] ^ tt);
		}
		if (RDEBUG && (debuglevel > 6)) {
			System.out.println("CT="+toString(result));
			System.out.println();
		}
		if (RDEBUG) trace(OUT, "blockEncrypt()");
	}

	/**
	 * Decrypt exactly one block of ciphertext.
	 *
	 * @param  in         The ciphertext.
	 * @param  result     The resulting ciphertext.
	 * @param  inOffset   Index of in from which to start considering data.
	 * @param  sessionKey The session key to use for decryption.
	 * @param  blockSize  The block size in bytes of this Rijndael.
	 */
	static final void
	blockDecrypt (byte[] in, byte[] result, int inOffset, Object sessionKey, int blockSize) {
		if (blockSize == BLOCK_SIZE) {
			blockDecrypt(in, result, inOffset, sessionKey);
			return;
		}

		if (RDEBUG) trace(IN, "blockDecrypt("+in+", "+inOffset+", "+sessionKey+", "+blockSize+ ')');
		Object[] sKey = (Object[]) sessionKey; // extract decryption round keys
		int[][] Kd = (int[][]) sKey[1];

		int BC = blockSize / 4;
		int ROUNDS = Kd.length - 1;
		int SC = BC == 4 ? 0 : (BC == 6 ? 1 : 2);
		int s1 = shifts[SC][1][1];
		int s2 = shifts[SC][2][1];
		int s3 = shifts[SC][3][1];
		int[] a = new int[BC];
		int[] t = new int[BC]; // temporary work array
		int i;
		int j = 0, tt;

		for (i = 0; i < BC; i++)                   // ciphertext to ints + key
			t[i] = ((in[inOffset++] & 0xFF) << 24 |
					(in[inOffset++] & 0xFF) << 16 |
					(in[inOffset++] & 0xFF) <<  8 |
					(in[inOffset++] & 0xFF)        ) ^ Kd[0][i];
		for (int r = 1; r < ROUNDS; r++) {          // apply round transforms
			for (i = 0; i < BC; i++)
				a[i] = (T5[(t[ i           ] >>> 24) & 0xFF] ^
						T6[(t[(i + s1) % BC] >>> 16) & 0xFF] ^
						T7[(t[(i + s2) % BC] >>>  8) & 0xFF] ^
						T8[ t[(i + s3) % BC]         & 0xFF]  ) ^ Kd[r][i];
			System.arraycopy(a, 0, t, 0, BC);
			if (RDEBUG && (debuglevel > 6)) System.out.println("PT"+r+ '=' +toString(t));
		}
		for (i = 0; i < BC; i++) {                   // last round is special
			tt = Kd[ROUNDS][i];
			result[j++] = (byte)(Si[(t[ i           ] >>> 24) & 0xFF] ^ (tt >>> 24));
			result[j++] = (byte)(Si[(t[(i + s1) % BC] >>> 16) & 0xFF] ^ (tt >>> 16));
			result[j++] = (byte)(Si[(t[(i + s2) % BC] >>>  8) & 0xFF] ^ (tt >>>  8));
			result[j++] = (byte)(Si[ t[(i + s3) % BC]         & 0xFF] ^ tt);
		}
		if (RDEBUG && (debuglevel > 6)) {
			System.out.println("PT="+toString(result));
			System.out.println();
		}
		if (RDEBUG) trace(OUT, "blockDecrypt()");
	}

	/** A basic symmetric encryption/decryption test for a given key size. */
	private static boolean self_test (int keysize) {
		if (RDEBUG) trace(IN, "self_test("+keysize+ ')');
		boolean ok = false;
		try {
			byte[] kb = new byte[keysize];
			byte[] pt = new byte[BLOCK_SIZE];
			int i;

			for (i = 0; i < keysize; i++)
				kb[i] = (byte) i;
			for (i = 0; i < BLOCK_SIZE; i++)
				pt[i] = (byte) i;

			if (RDEBUG && (debuglevel > 6)) {
				System.out.println("==========");
				System.out.println();
				System.out.println("KEYSIZE="+(8*keysize));
				System.out.println("KEY="+toString(kb));
				System.out.println();
			}
			Object key = makeKey(kb, BLOCK_SIZE);

			if (RDEBUG && (debuglevel > 6)) {
				System.out.println("Intermediate Ciphertext Values (Encryption)");
				System.out.println();
				System.out.println("PT="+toString(pt));
			}
			byte[] ct = new byte[BLOCK_SIZE];
			blockEncrypt(pt, ct, 0, key, BLOCK_SIZE);

			if (RDEBUG && (debuglevel > 6)) {
				System.out.println("Intermediate Plaintext Values (Decryption)");
				System.out.println();
				System.out.println("CT="+toString(ct));
			}
			byte[] cpt = new byte[BLOCK_SIZE];
			blockDecrypt(ct, cpt, 0, key, BLOCK_SIZE);

			ok = areEqual(pt, cpt);
			if (!ok)
				throw new RuntimeException("Symmetric operation failed");
		}
		catch (Exception x) {
			if (RDEBUG && (debuglevel > 0)) {
				debug("Exception encountered during self-test: " + x.getMessage());
				x.printStackTrace();
			}
		}
		if (RDEBUG && (debuglevel > 0)) debug("Self-test OK? " + ok);
		if (RDEBUG) trace(OUT, "self_test()");
		return ok;
	}

	/**
	 * Return The number of rounds for a given Rijndael's key and block sizes.
	 *
	 * @param keySize    The size of the user key material in bytes.
	 * @param blockSize  The desired block size in bytes.
	 * @return The number of rounds for a given Rijndael's key and
	 *      block sizes.
	 */
	private static final int getRounds(int keySize, int blockSize) {
		switch (keySize) {
		case 16:
			return blockSize == 16 ? 10 : (blockSize == 24 ? 12 : 14);
		case 24:
			return blockSize != 32 ? 12 : 14;
		default: // 32 bytes = 256 bits
			return 14;
		}
	}


//	utility static methods (from cryptix.util.core ArrayUtil and Hex classes)
//	...........................................................................

	/**
	 * Compares two byte arrays for equality.
	 *
	 * @return true if the arrays have identical contents
	 */
	private static final boolean areEqual (byte[] a, byte[] b) {
		int aLength = a.length;
		if (aLength != b.length)
			return false;
		for (int i = 0; i < aLength; i++)
			if (a[i] != b[i])
				return false;
		return true;
	}

	/**
	 * Returns a string of 2 hexadecimal digits (most significant
	 * digit first) corresponding to the lowest 8 bits of <i>n</i>.
	 */
	private static final String byteToString (int n) {
		char[] buf = {
				HEX_DIGITS[(n >>> 4) & 0x0F],
				HEX_DIGITS[ n        & 0x0F]
		};
		return new String(buf);
	}

	/**
	 * Returns a string of 8 hexadecimal digits (most significant
	 * digit first) corresponding to the integer <i>n</i>, which is
	 * treated as unsigned.
	 */
	private static final String intToString (int n) {
		char[] buf = new char[8];
		for (int i = 7; i >= 0; i--) {
			buf[i] = HEX_DIGITS[n & 0x0F];
			n >>>= 4;
		}
		return new String(buf);
	}

	/**
	 * Returns a string of hexadecimal digits from a byte array. Each
	 * byte is converted to 2 hex symbols.
	 */
	private static final String toString (byte[] ba) {
		int length = ba.length;
		char[] buf = new char[length * 2];
		for (int i = 0, j = 0, k; i < length; ) {
			k = ba[i++];
			buf[j++] = HEX_DIGITS[(k >>> 4) & 0x0F];
			buf[j++] = HEX_DIGITS[ k        & 0x0F];
		}
		return new String(buf);
	}

	/**
	 * Returns a string of hexadecimal digits from an integer array. Each
	 * int is converted to 4 hex symbols.
	 */
	private static final String toString (int[] ia) {
		int length = ia.length;
		char[] buf = new char[length * 8];
		for (int i = 0, j = 0, k; i < length; i++) {
			k = ia[i];
			buf[j++] = HEX_DIGITS[(k >>> 28) & 0x0F];
			buf[j++] = HEX_DIGITS[(k >>> 24) & 0x0F];
			buf[j++] = HEX_DIGITS[(k >>> 20) & 0x0F];
			buf[j++] = HEX_DIGITS[(k >>> 16) & 0x0F];
			buf[j++] = HEX_DIGITS[(k >>> 12) & 0x0F];
			buf[j++] = HEX_DIGITS[(k >>>  8) & 0x0F];
			buf[j++] = HEX_DIGITS[(k >>>  4) & 0x0F];
			buf[j++] = HEX_DIGITS[ k         & 0x0F];
		}
		return new String(buf);
	}


//	main(): use to generate the Intermediate Values KAT
//	...........................................................................

	public static void main (String[] args) {
		self_test(16);
		self_test(24);
		self_test(32);
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.clients.http;

import java.io.File;
import java.io.IOException;
import java.io.PrintWriter;
import java.io.StringWriter;
import java.net.URI;

import freenet.client.HighLevelSimpleClient;
import freenet.client.filter.GenericReadFilterCallback;
import freenet.config.Config;
import freenet.config.ConfigException;
import freenet.config.Option;
import freenet.l10n.NodeL10n;
import freenet.node.MasterKeysFileSizeException;
import freenet.node.MasterKeysWrongPasswordException;
import freenet.node.Node;
import freenet.node.NodeClientCore;
import freenet.node.SecurityLevels;
import freenet.node.Node.AlreadySetPasswordException;
import freenet.node.SecurityLevels.FRIENDS_THREAT_LEVEL;
import freenet.node.SecurityLevels.NETWORK_THREAT_LEVEL;
import freenet.node.SecurityLevels.PHYSICAL_THREAT_LEVEL;
import freenet.pluginmanager.FredPluginBandwidthIndicator;
import freenet.support.Fields;
import freenet.support.HTMLNode;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.SizeUtil;
import freenet.support.Logger.LogLevel;
import freenet.support.api.HTTPRequest;
import freenet.support.io.FileUtil;

/**
 * A first time wizard aimed to ease the configuration of the node.
 *
 * TODO: a choose your CSS step?
 */
public class FirstTimeWizardToadlet extends Toadlet {
	private final NodeClientCore core;
	private final Config config;

        private static volatile boolean logMINOR;
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback(){
			@Override
			public void shouldUpdate(){
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}

	private enum WIZARD_STEP {
		WELCOME,
		// Before security levels, because once the network security level has been set, we won't redirect
		// the user to the wizard page.
		BROWSER_WARNING,
		// We have to set up UPnP before reaching the bandwidth stage, so we can autodetect bandwidth settings.
		MISC,
		OPENNET,
		SECURITY_NETWORK,
		SECURITY_PHYSICAL,
		NAME_SELECTION,
		BANDWIDTH,
		DATASTORE_SIZE,
		CONGRATZ,
		FINAL;
	}

	FirstTimeWizardToadlet(HighLevelSimpleClient client, Node node, NodeClientCore core) {
		super(client);
		this.core = core;
		this.config = node.config;
	}

	public static final String TOADLET_URL = "/wizard/";

	public void handleMethodGET(URI uri, HTTPRequest request, ToadletContext ctx) throws ToadletContextClosedException, IOException {
		if(!ctx.isAllowedFullAccess()) {
			super.sendErrorPage(ctx, 403, "Unauthorized", NodeL10n.getBase().getString("Toadlet.unauthorized"));
			return;
		}

		WIZARD_STEP currentStep = WIZARD_STEP.valueOf(request.getParam("step", WIZARD_STEP.WELCOME.toString()));

		if(currentStep == WIZARD_STEP.BROWSER_WARNING) {
			boolean incognito = request.isParameterSet("incognito");
			// Bug 3376: Opening Chrome in incognito mode from command line will open a new non-incognito window if the browser is already open.
			// See http://code.google.com/p/chromium/issues/detail?id=9636
			// When this is fixed, we should check the browser version and allow it for known good versions of Chrome.
			incognito = false;

			PageNode page = ctx.getPageMaker().getPageNode(incognito ? l10n("browserWarningIncognitoPageTitle") : l10n("browserWarningPageTitle"), false, ctx);
			HTMLNode pageNode = page.outer;
			HTMLNode contentNode = page.content;

			HTMLNode infobox = contentNode.addChild("div", "class", "infobox infobox-normal");
			HTMLNode infoboxHeader = infobox.addChild("div", "class", "infobox-header");
			HTMLNode infoboxContent = infobox.addChild("div", "class", "infobox-content");

			if(incognito)
				infoboxHeader.addChild("#", l10n("browserWarningIncognitoShort"));
			else
				infoboxHeader.addChild("#", l10n("browserWarningShort"));
			NodeL10n.getBase().addL10nSubstitution(infoboxContent, incognito ? "FirstTimeWizardToadlet.browserWarningIncognito" : "FirstTimeWizardToadlet.browserWarning", new String[] { "bold" }, new HTMLNode[] { HTMLNode.STRONG });

			if(incognito)
				infoboxContent.addChild("p", l10n("browserWarningIncognitoSuggestion"));
			else
				infoboxContent.addChild("p", l10n("browserWarningSuggestion"));

			infoboxContent.addChild("p").addChild("a", "href", "?step="+WIZARD_STEP.MISC, NodeL10n.getBase().getString("FirstTimeWizardToadlet.clickContinue"));

			this.writeHTMLReply(ctx, 200, "OK", pageNode.generate());
			return;
		} else if(currentStep == WIZARD_STEP.OPENNET){
			PageNode page = ctx.getPageMaker().getPageNode(l10n("opennetChoicePageTitle"), false, ctx);
			HTMLNode pageNode = page.outer;
			HTMLNode contentNode = page.content;

			HTMLNode infobox = contentNode.addChild("div", "class", "infobox infobox-normal");
			HTMLNode infoboxHeader = infobox.addChild("div", "class", "infobox-header");
			HTMLNode infoboxContent = infobox.addChild("div", "class", "infobox-content");

			infoboxHeader.addChild("#", l10n("opennetChoiceTitle"));
			
			infoboxContent.addChild("p", l10n("opennetChoiceIntroduction"));
			
			HTMLNode form = infoboxContent.addChild("form", new String[] { "action", "method", "id" }, new String[] { "GET", ".", "opennetChoiceForm" });
			form.addChild("input", new String[] { "type", "name", "value" }, new String[] { "hidden", "step", WIZARD_STEP.SECURITY_NETWORK.name() });
			
			HTMLNode p = form.addChild("p");
			HTMLNode input = p.addChild("input", new String[] { "type", "name", "value" }, new String[] { "radio", "opennet", "false" });
			input.addChild("b", l10n("opennetChoiceConnectFriends")+":");
			p.addChild("br");
			p.addChild("i", l10n("opennetChoicePro"));
			p.addChild("#", ": "+l10n("opennetChoiceConnectFriendsPRO") + "¹");
			p.addChild("br");
			p.addChild("i", l10n("opennetChoiceCon"));
			p.addChild("#", ": "+l10n("opennetChoiceConnectFriendsCON", "minfriends", "5"));
			
			p = form.addChild("p");
			input = p.addChild("input", new String[] { "type", "name", "value" }, new String[] { "radio", "opennet", "true" });
			input.addChild("b", l10n("opennetChoiceConnectStrangers")+":");
			p.addChild("br");
			p.addChild("i", l10n("opennetChoicePro"));
			p.addChild("#", ": "+l10n("opennetChoiceConnectStrangersPRO"));
			p.addChild("br");
			p.addChild("i", l10n("opennetChoiceCon"));
			p.addChild("#", ": "+l10n("opennetChoiceConnectStrangersCON"));
			
			form.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "opennetF", NodeL10n.getBase().getString("FirstTimeWizardToadlet.continue")});
			form.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "cancel", NodeL10n.getBase().getString("Toadlet.cancel")});
			HTMLNode foot = infoboxContent.addChild("div", "class", "toggleable");
			foot.addChild("i", "¹: " + l10n("opennetChoiceHowSafeIsFreenetToggle"));
			HTMLNode footHidden = foot.addChild("div", "class", "hidden");
			HTMLNode footList = footHidden.addChild("ol");
			footList.addChild("li", l10n("opennetChoiceHowSafeIsFreenetStupid"));
			footList.addChild("li", l10n("opennetChoiceHowSafeIsFreenetFriends") + "²");
			footList.addChild("li", l10n("opennetChoiceHowSafeIsFreenetTrustworthy"));
			footList.addChild("li", l10n("opennetChoiceHowSafeIsFreenetNoSuspect"));
			footList.addChild("li", l10n("opennetChoiceHowSafeIsFreenetChangeID"));
			footList.addChild("li", l10n("opennetChoiceHowSafeIsFreenetSSK"));
			footList.addChild("li", l10n("opennetChoiceHowSafeIsFreenetOS"));
			footList.addChild("li", l10n("opennetChoiceHowSafeIsFreenetBigPriv"));
			footList.addChild("li", l10n("opennetChoiceHowSafeIsFreenetDistant"));
			footList.addChild("li", l10n("opennetChoiceHowSafeIsFreenetBugs"));
			HTMLNode foot2 = footHidden.addChild("p");
			foot2.addChild("#", "²: " + l10n("opennetChoiceHowSafeIsFreenetFoot2"));
			this.writeHTMLReply(ctx, 200, "OK", pageNode.generate());
			return;
		} else if(currentStep == WIZARD_STEP.SECURITY_NETWORK) {
			PageNode page = ctx.getPageMaker().getPageNode(l10n("networkSecurityPageTitle"), false, ctx);
			HTMLNode pageNode = page.outer;
			HTMLNode contentNode = page.content;

			HTMLNode infobox = contentNode.addChild("div", "class", "infobox infobox-normal");
			HTMLNode infoboxHeader = infobox.addChild("div", "class", "infobox-header");
			HTMLNode infoboxContent = infobox.addChild("div", "class", "infobox-content");
			
			if(!request.isParameterSet("opennet")) {
				super.writeTemporaryRedirect(ctx, "step1", TOADLET_URL+"?step="+WIZARD_STEP.OPENNET);
				return;
			}
			
			boolean opennet = Fields.stringToBool(request.getParam("opennet", "false"));

			infoboxHeader.addChild("#", l10n(opennet ? "networkThreatLevelHeaderOpennet" : "networkThreatLevelHeaderDarknet"));
			infoboxContent.addChild("p", l10n(opennet ? "networkThreatLevelIntroOpennet" : "networkThreatLevelIntroDarknet"));
			HTMLNode form = ctx.addFormChild(infoboxContent, ".", "networkSecurityForm");
			String controlName = "security-levels.networkThreatLevel";
			if(opennet) {
				HTMLNode div = form.addChild("div", "class", "opennetDiv");
				for(NETWORK_THREAT_LEVEL level : NETWORK_THREAT_LEVEL.OPENNET_VALUES) {
					HTMLNode input;
					input = div.addChild("p").addChild("input", new String[] { "type", "name", "value" }, new String[] { "radio", controlName, level.name() });
					input.addChild("b", l10nSec("networkThreatLevel.name."+level));
					input.addChild("#", ": ");
					NodeL10n.getBase().addL10nSubstitution(input, "SecurityLevels.networkThreatLevel.choice."+level, new String[] { "bold" }, new HTMLNode[] { HTMLNode.STRONG });
					HTMLNode inner = input.addChild("p").addChild("i");
					NodeL10n.getBase().addL10nSubstitution(inner, "SecurityLevels.networkThreatLevel.desc."+level, new String[] { "bold" },
							new HTMLNode[] { HTMLNode.STRONG });
				}
			}
			if(!opennet) {
				HTMLNode div = form.addChild("div", "class", "darknetDiv");
				for(NETWORK_THREAT_LEVEL level : NETWORK_THREAT_LEVEL.DARKNET_VALUES) {
					HTMLNode input;
					input = div.addChild("p").addChild("input", new String[] { "type", "name", "value" }, new String[] { "radio", controlName, level.name() });
					input.addChild("b", l10nSec("networkThreatLevel.name."+level));
					input.addChild("#", ": ");
					NodeL10n.getBase().addL10nSubstitution(input, "SecurityLevels.networkThreatLevel.choice."+level, new String[] { "bold" }, new HTMLNode[] { HTMLNode.STRONG });
					HTMLNode inner = input.addChild("p").addChild("i");
					NodeL10n.getBase().addL10nSubstitution(inner, "SecurityLevels.networkThreatLevel.desc."+level, new String[] { "bold" },
							new HTMLNode[] { HTMLNode.STRONG });
				}
				form.addChild("p").addChild("b", l10nSec("networkThreatLevel.opennetFriendsWarning"));
			}
			form.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "networkSecurityF", NodeL10n.getBase().getString("FirstTimeWizardToadlet.continue")});
			form.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "cancel", NodeL10n.getBase().getString("Toadlet.cancel")});
			this.writeHTMLReply(ctx, 200, "OK", pageNode.generate());
			return;
		} else if(currentStep == WIZARD_STEP.SECURITY_PHYSICAL) {
			PageNode page = ctx.getPageMaker().getPageNode(l10n("physicalSecurityPageTitle"), false, ctx);
			HTMLNode pageNode = page.outer;
			HTMLNode contentNode = page.content;

			HTMLNode infobox = contentNode.addChild("div", "class", "infobox infobox-normal");
			HTMLNode infoboxHeader = infobox.addChild("div", "class", "infobox-header");
			HTMLNode infoboxContent = infobox.addChild("div", "class", "infobox-content");

			infoboxHeader.addChild("#", l10nSec("physicalThreatLevelShort"));
			infoboxContent.addChild("p", l10nSec("physicalThreatLevel"));
			HTMLNode form = ctx.addFormChild(infoboxContent, ".", "physicalSecurityForm");
			HTMLNode div = form.addChild("div", "class", "opennetDiv");
			String controlName = "security-levels.physicalThreatLevel";
			HTMLNode swapWarning = div.addChild("p").addChild("i");
			NodeL10n.getBase().addL10nSubstitution(swapWarning, "SecurityLevels.physicalThreatLevelSwapfile", new String[] { "bold", "truecrypt" }, new HTMLNode[] { HTMLNode.STRONG, HTMLNode.linkInNewWindow(GenericReadFilterCallback.escapeURL("http://www.truecrypt.org/")) });
			if(File.separatorChar == '\\')
				swapWarning.addChild("#", " " + l10nSec("physicalThreatLevelSwapfileWindows"));
			for(PHYSICAL_THREAT_LEVEL level : PHYSICAL_THREAT_LEVEL.values()) {
				HTMLNode input;
				input = div.addChild("p").addChild("input", new String[] { "type", "name", "value" }, new String[] { "radio", controlName, level.name() });
				input.addChild("b", l10nSec("physicalThreatLevel.name."+level));
				input.addChild("#", ": ");
				NodeL10n.getBase().addL10nSubstitution(input, "SecurityLevels.physicalThreatLevel.choice."+level, new String[] { "bold" }, new HTMLNode[] { HTMLNode.STRONG });
				if(level == PHYSICAL_THREAT_LEVEL.HIGH) {
					if(core.node.securityLevels.getPhysicalThreatLevel() != level) {
						// Add password form
						HTMLNode p = div.addChild("p");
						p.addChild("label", "for", "passwordBox", l10nSec("setPasswordLabel")+":");
						p.addChild("input", new String[] { "id", "type", "name" }, new String[] { "passwordBox", "password", "masterPassword" });
					}
				}
			}
			div.addChild("#", l10nSec("physicalThreatLevelEnd"));
			form.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "physicalSecurityF", NodeL10n.getBase().getString("FirstTimeWizardToadlet.continue")});
			form.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "cancel", NodeL10n.getBase().getString("Toadlet.cancel")});
			this.writeHTMLReply(ctx, 200, "OK", pageNode.generate());
			return;
		} else if(currentStep == WIZARD_STEP.NAME_SELECTION) {
			// Attempt to skip one step if possible: opennet nodes don't need a name
			if(Boolean.valueOf(request.getParam("opennet"))) {
				super.writeTemporaryRedirect(ctx, "step3", TOADLET_URL+"?step="+WIZARD_STEP.BANDWIDTH);
				return;
			}
			PageNode page = ctx.getPageMaker().getPageNode(l10n("step2Title"), false, ctx);
			HTMLNode pageNode = page.outer;
			HTMLNode contentNode = page.content;

			HTMLNode nnameInfobox = contentNode.addChild("div", "class", "infobox infobox-normal");
			HTMLNode nnameInfoboxHeader = nnameInfobox.addChild("div", "class", "infobox-header");
			HTMLNode nnameInfoboxContent = nnameInfobox.addChild("div", "class", "infobox-content");

			nnameInfoboxHeader.addChild("#", l10n("chooseNodeName"));
			nnameInfoboxContent.addChild("#", l10n("chooseNodeNameLong"));
			HTMLNode nnameForm = ctx.addFormChild(nnameInfoboxContent, ".", "nnameForm");
			nnameForm.addChild("input", "name", "nname");

			nnameForm.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "nnameF", NodeL10n.getBase().getString("FirstTimeWizardToadlet.continue")});
			nnameForm.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "cancel", NodeL10n.getBase().getString("Toadlet.cancel")});
			this.writeHTMLReply(ctx, 200, "OK", pageNode.generate());
			return;
		} else if(currentStep == WIZARD_STEP.BANDWIDTH) {
			// Attempt to skip one step if possible
			int autodetectedLimit = canAutoconfigureBandwidth();
			PageNode page = ctx.getPageMaker().getPageNode(l10n("step3Title"), false, ctx);
			HTMLNode pageNode = page.outer;
			HTMLNode contentNode = page.content;

			HTMLNode bandwidthInfobox = contentNode.addChild("div", "class", "infobox infobox-normal");
			HTMLNode bandwidthnfoboxHeader = bandwidthInfobox.addChild("div", "class", "infobox-header");
			HTMLNode bandwidthInfoboxContent = bandwidthInfobox.addChild("div", "class", "infobox-content");

			bandwidthnfoboxHeader.addChild("#", l10n("bandwidthLimit"));
			bandwidthInfoboxContent.addChild("#", l10n("bandwidthLimitLong"));
			HTMLNode bandwidthForm = ctx.addFormChild(bandwidthInfoboxContent, ".", "bwForm");
			HTMLNode result = bandwidthForm.addChild("select", "name", "bw");

			@SuppressWarnings("unchecked")
			Option<Integer> sizeOption = (Option<Integer>) config.get("node").getOption("outputBandwidthLimit");
			if(!sizeOption.isDefault()) {
				int current = sizeOption.getValue();
				result.addChild("option", new String[] { "value", "selected" }, new String[] { SizeUtil.formatSize(current), "on" }, l10n("currentSpeed")+" "+SizeUtil.formatSize(current)+"/s");
			} else if(autodetectedLimit != -1)
				result.addChild("option", new String[] { "value", "selected" }, new String[] { SizeUtil.formatSize(autodetectedLimit), "on" }, l10n("autodetectedSuggestedLimit")+" "+SizeUtil.formatSize(autodetectedLimit)+"/s");

			// don't forget to update handlePost too if you change that!
			if(autodetectedLimit != 8192)
				result.addChild("option", "value", "8K", l10n("bwlimitLowerSpeed"));
			// Special case for 128kbps to increase performance at the cost of some link degradation. Above that we use 50% of the limit.
			result.addChild("option", "value", "12K", "512+/128 kbps (12KB/s)");
			if(autodetectedLimit != -1 || !sizeOption.isDefault())
				result.addChild("option", "value", "16K", "1024+/256 kbps (16KB/s)");
			else
				result.addChild("option", new String[] { "value", "selected" }, new String[] { "16K", "selected" }, "1024+/256 kbps (16KB/s)");
			result.addChild("option", "value", "32K", "1024+/512 kbps (32K/s)");
			result.addChild("option", "value", "64K", "1024+/1024 kbps (64K/s)");
			result.addChild("option", "value", "1000K", l10n("bwlimitHigherSpeed"));

			bandwidthForm.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "bwF", NodeL10n.getBase().getString("FirstTimeWizardToadlet.continue")});
			bandwidthForm.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "cancel", NodeL10n.getBase().getString("Toadlet.cancel")});
			bandwidthInfoboxContent.addChild("#", l10n("bandwidthLimitAfter"));
			this.writeHTMLReply(ctx, 200, "OK", pageNode.generate());
			return;
		} else if(currentStep == WIZARD_STEP.DATASTORE_SIZE) {
			// Attempt to skip one step if possible
			PageNode page = ctx.getPageMaker().getPageNode(l10n("step4Title"), false, ctx);
			HTMLNode pageNode = page.outer;
			HTMLNode contentNode = page.content;

			HTMLNode bandwidthInfobox = contentNode.addChild("div", "class", "infobox infobox-normal");
			HTMLNode bandwidthnfoboxHeader = bandwidthInfobox.addChild("div", "class", "infobox-header");
			HTMLNode bandwidthInfoboxContent = bandwidthInfobox.addChild("div", "class", "infobox-content");

			bandwidthnfoboxHeader.addChild("#", l10n("datastoreSize"));
			bandwidthInfoboxContent.addChild("#", l10n("datastoreSizeLong"));
			HTMLNode bandwidthForm = ctx.addFormChild(bandwidthInfoboxContent, ".", "dsForm");
			HTMLNode result = bandwidthForm.addChild("select", "name", "ds");

			long autodetectedSize = canAutoconfigureDatastoreSize();

			@SuppressWarnings("unchecked")
			Option<Long> sizeOption = (Option<Long>) config.get("node").getOption("storeSize");
			if(!sizeOption.isDefault()) {
				long current = sizeOption.getValue();
				result.addChild("option", new String[] { "value", "selected" }, new String[] { SizeUtil.formatSize(current), "on" }, l10n("currentPrefix")+" "+SizeUtil.formatSize(current));
			} else if(autodetectedSize != -1)
				result.addChild("option", new String[] { "value", "selected" }, new String[] { SizeUtil.formatSize(autodetectedSize), "on" }, SizeUtil.formatSize(autodetectedSize));
			if(autodetectedSize != 512*1024*1024)
				result.addChild("option", "value", "512M", "512 MiB");
			result.addChild("option", "value", "1G", "1 GiB");
			if(autodetectedSize != -1 || !sizeOption.isDefault())
				result.addChild("option", "value", "2G", "2 GiB");
			else
				result.addChild("option", new String[] { "value", "selected" }, new String[] { "2G", "on" }, "2GiB");
			result.addChild("option", "value", "3G", "3 GiB");
			result.addChild("option", "value", "5G", "5 GiB");
			result.addChild("option", "value", "10G", "10 GiB");
			result.addChild("option", "value", "20G", "20 GiB");
			result.addChild("option", "value", "30G", "30 GiB");
			result.addChild("option", "value", "50G", "50 GiB");
			result.addChild("option", "value", "100G", "100 GiB");

			bandwidthForm.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "dsF", NodeL10n.getBase().getString("FirstTimeWizardToadlet.continue")});
			bandwidthForm.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "cancel", NodeL10n.getBase().getString("Toadlet.cancel")});
			this.writeHTMLReply(ctx, 200, "OK", pageNode.generate());
			return;
		} else if(currentStep == WIZARD_STEP.MISC) {
			PageNode page = ctx.getPageMaker().getPageNode(l10n("stepMiscTitle"), false, ctx);
			HTMLNode pageNode = page.outer;
			HTMLNode contentNode = page.content;

			HTMLNode form = ctx.addFormChild(contentNode, ".", "miscForm");

			HTMLNode miscInfobox = form.addChild("div", "class", "infobox infobox-normal");
			HTMLNode miscInfoboxHeader = miscInfobox.addChild("div", "class", "infobox-header");
			HTMLNode miscInfoboxContent = miscInfobox.addChild("div", "class", "infobox-content");

			miscInfoboxHeader.addChild("#", l10n("autoUpdate"));
			miscInfoboxContent.addChild("p", l10n("autoUpdateLong"));
			miscInfoboxContent.addChild("p").addChild("input", new String[] { "type", "checked", "name", "value" },
					new String[] { "radio", "on", "autodeploy", "true" }, l10n("autoUpdateAutodeploy"));
			miscInfoboxContent.addChild("p").addChild("input", new String[] { "type", "name", "value" },
					new String[] { "radio", "autodeploy", "false" }, l10n("autoUpdateNoAutodeploy"));

			miscInfobox = form.addChild("div", "class", "infobox infobox-normal");
			miscInfoboxHeader = miscInfobox.addChild("div", "class", "infobox-header");
			miscInfoboxContent = miscInfobox.addChild("div", "class", "infobox-content");

			miscInfoboxHeader.addChild("#", l10n("plugins"));
			miscInfoboxContent.addChild("p", l10n("pluginsLong"));
			miscInfoboxContent.addChild("p").addChild("input", new String[] { "type", "checked", "name", "value" },
					new String[] { "checkbox", "on", "upnp", "true" }, l10n("enableUPnP"));
			miscInfoboxContent.addChild("p").addChild("input", new String[] { "type", "checked", "name", "value" },
					new String[] { "checkbox", "on", "jstun", "true" }, l10n("enableJSTUN"));

			miscInfoboxContent.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "miscF", NodeL10n.getBase().getString("FirstTimeWizardToadlet.continue")});
			miscInfoboxContent.addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "cancel", NodeL10n.getBase().getString("Toadlet.cancel")});
			this.writeHTMLReply(ctx, 200, "OK", pageNode.generate());
			return;
		}else if(currentStep == WIZARD_STEP.CONGRATZ) {
			PageNode page = ctx.getPageMaker().getPageNode(l10n("step7Title"), true, ctx);
			HTMLNode pageNode = page.outer;
			HTMLNode contentNode = page.content;

			HTMLNode congratzInfobox = contentNode.addChild("div", "class", "infobox infobox-normal");
			HTMLNode congratzInfoboxHeader = congratzInfobox.addChild("div", "class", "infobox-header");
			HTMLNode congratzInfoboxContent = congratzInfobox.addChild("div", "class", "infobox-content");

			congratzInfoboxHeader.addChild("#", l10n("congratz"));
			congratzInfoboxContent.addChild("p", l10n("congratzLong"));

			congratzInfoboxContent.addChild("a", "href", "?step="+WIZARD_STEP.FINAL, NodeL10n.getBase().getString("FirstTimeWizardToadlet.continueEnd"));

			this.writeHTMLReply(ctx, 200, "OK", pageNode.generate());
			return;
		} else if(currentStep == WIZARD_STEP.FINAL) {
			try {
				config.get("fproxy").set("hasCompletedWizard", true);
                                config.store();
				this.writeTemporaryRedirect(ctx, "Return to home", "/");
			} catch (ConfigException e) {
				Logger.error(this, e.getMessage(), e);
			}
			return;
		}

		PageNode page = ctx.getPageMaker().getPageNode(l10n("homepageTitle"), false, ctx);
		HTMLNode pageNode = page.outer;
		HTMLNode contentNode = page.content;

		HTMLNode welcomeInfobox = contentNode.addChild("div", "class", "infobox infobox-normal");
		HTMLNode welcomeInfoboxHeader = welcomeInfobox.addChild("div", "class", "infobox-header");
		HTMLNode welcomeInfoboxContent = welcomeInfobox.addChild("div", "class", "infobox-content");
		welcomeInfoboxHeader.addChild("#", l10n("welcomeInfoboxTitle"));

		HTMLNode firstParagraph = welcomeInfoboxContent.addChild("p");
		firstParagraph.addChild("#", l10n("welcomeInfoboxContent1"));
		HTMLNode secondParagraph = welcomeInfoboxContent.addChild("p");
		boolean incognito = request.isParameterSet("incognito");
		String append = incognito ? "&incognito=true" : "";
		secondParagraph.addChild("a", "href", "?step="+WIZARD_STEP.BROWSER_WARNING+append).addChild("#", NodeL10n.getBase().getString("FirstTimeWizardToadlet.clickContinue"));

		this.writeHTMLReply(ctx, 200, "OK", pageNode.generate());
	}

	private String l10nSec(String key) {
		return NodeL10n.getBase().getString("SecurityLevels."+key);
	}

	private String l10nSec(String key, String pattern, String value) {
		return NodeL10n.getBase().getString("SecurityLevels."+key, pattern, value);
	}

	public void handleMethodPOST(URI uri, HTTPRequest request, ToadletContext ctx) throws ToadletContextClosedException, IOException {

		if(!ctx.isAllowedFullAccess()) {
			super.sendErrorPage(ctx, 403, "Unauthorized", NodeL10n.getBase().getString("Toadlet.unauthorized"));
			return;
		}

		String passwd = request.getPartAsString("formPassword", 32);
		boolean noPassword = (passwd == null) || !passwd.equals(core.formPassword);
		if(noPassword) {
			if(logMINOR) Logger.minor(this, "No password ("+passwd+" should be "+core.formPassword+ ')');
			super.writeTemporaryRedirect(ctx, "invalid/unhandled data", "/");
			return;
		}

		if(request.isPartSet("security-levels.networkThreatLevel")) {
			// We don't require a confirmation here, since it's one page at a time, so there's less information to
			// confuse the user, and we don't know whether the node has friends yet etc.
			// FIXME should we have confirmation here???
			String networkThreatLevel = request.getPartAsString("security-levels.networkThreatLevel", 128);
			NETWORK_THREAT_LEVEL newThreatLevel = SecurityLevels.parseNetworkThreatLevel(networkThreatLevel);
			if(newThreatLevel == null) {
				super.writeTemporaryRedirect(ctx, "step1", TOADLET_URL+"?step="+WIZARD_STEP.SECURITY_NETWORK);
				return;
			}
			if((newThreatLevel == NETWORK_THREAT_LEVEL.MAXIMUM || newThreatLevel == NETWORK_THREAT_LEVEL.HIGH)) {
				if((!request.isPartSet("security-levels.networkThreatLevel.confirm")) &&
					(!request.isPartSet("security-levels.networkThreatLevel.tryConfirm"))) {
					PageNode page = ctx.getPageMaker().getPageNode(l10n("networkSecurityPageTitle"), false, ctx);
					HTMLNode pageNode = page.outer;
					HTMLNode content = page.content;

					HTMLNode infobox = content.addChild("div", "class", "infobox infobox-information");
					infobox.addChild("div", "class", "infobox-header", l10n("networkThreatLevelConfirmTitle."+newThreatLevel));
					HTMLNode infoboxContent = infobox.addChild("div", "class", "infobox-content");
					HTMLNode formNode = ctx.addFormChild(infoboxContent, ".", "configFormSecLevels");
					formNode.addChild("input", new String[] { "type", "name", "value" }, new String[] { "hidden", "security-levels.networkThreatLevel", networkThreatLevel });
					if(newThreatLevel == NETWORK_THREAT_LEVEL.MAXIMUM) {
						HTMLNode p = formNode.addChild("p");
						NodeL10n.getBase().addL10nSubstitution(p, "SecurityLevels.maximumNetworkThreatLevelWarning", new String[] { "bold" }, new HTMLNode[] { HTMLNode.STRONG });
						p.addChild("#", " ");
						NodeL10n.getBase().addL10nSubstitution(p, "SecurityLevels.maxSecurityYouNeedFriends", new String[] { "bold" }, new HTMLNode[] { HTMLNode.STRONG });
						formNode.addChild("p").addChild("input", new String[] { "type", "name", "value" }, new String[] { "checkbox", "security-levels.networkThreatLevel.confirm", "off" }, l10nSec("maximumNetworkThreatLevelCheckbox"));
					} else /*if(newThreatLevel == NETWORK_THREAT_LEVEL.HIGH)*/ {
						HTMLNode p = formNode.addChild("p");
						NodeL10n.getBase().addL10nSubstitution(p, "FirstTimeWizardToadlet.highNetworkThreatLevelWarning", new String[] { "bold" }, new HTMLNode[] { HTMLNode.STRONG });
						formNode.addChild("p").addChild("input", new String[] { "type", "name", "value" }, new String[] { "checkbox", "security-levels.networkThreatLevel.confirm", "off" }, l10n("highNetworkThreatLevelCheckbox"));
					}
					formNode.addChild("input", new String[] { "type", "name", "value" }, new String[] { "hidden", "security-levels.networkThreatLevel.tryConfirm", "on" });
					formNode.addChild("input", new String[] { "type", "name", "value" }, new String[] { "hidden", "seclevels", "on" });
					formNode.addChild("input", new String[] { "type", "value" }, new String[] { "submit", l10n("continue")});
					writeHTMLReply(ctx, 200, "OK", pageNode.generate());
					return;
				} else if((!request.isPartSet("security-levels.networkThreatLevel.confirm")) &&
						request.isPartSet("security-levels.networkThreatLevel.tryConfirm")) {
					super.writeTemporaryRedirect(ctx, "step1", TOADLET_URL+"?step="+WIZARD_STEP.SECURITY_NETWORK);
					return;
				}
			}
			core.node.securityLevels.setThreatLevel(newThreatLevel);
			core.storeConfig();
			super.writeTemporaryRedirect(ctx, "step1", TOADLET_URL+"?step="+WIZARD_STEP.SECURITY_PHYSICAL);
			return;
		} else if(request.isPartSet("security-levels.physicalThreatLevel")) {
			// We don't require a confirmation here, since it's one page at a time, so there's less information to
			// confuse the user, and we don't know whether the node has friends yet etc.
			// FIXME should we have confirmation here???
			String physicalThreatLevel = request.getPartAsString("security-levels.physicalThreatLevel", 128);
			PHYSICAL_THREAT_LEVEL oldThreatLevel = core.node.securityLevels.getPhysicalThreatLevel();
			PHYSICAL_THREAT_LEVEL newThreatLevel = SecurityLevels.parsePhysicalThreatLevel(physicalThreatLevel);
			if(logMINOR) Logger.minor(this, "Old threat level: "+oldThreatLevel+" new threat level: "+newThreatLevel);
			if(newThreatLevel == null) {
				super.writeTemporaryRedirect(ctx, "step1", TOADLET_URL+"?step="+WIZARD_STEP.SECURITY_PHYSICAL);
				return;
			}
			if(newThreatLevel == PHYSICAL_THREAT_LEVEL.HIGH && oldThreatLevel != newThreatLevel) {
				// Check for password
				String pass = request.getPartAsString("masterPassword", SecurityLevelsToadlet.MAX_PASSWORD_LENGTH);
				if(pass != null && pass.length() > 0) {
					try {
						if(oldThreatLevel == PHYSICAL_THREAT_LEVEL.NORMAL || oldThreatLevel == PHYSICAL_THREAT_LEVEL.LOW)
							core.node.changeMasterPassword("", pass, true);
						else
							core.node.setMasterPassword(pass, true);
					} catch (AlreadySetPasswordException e) {
						// Do nothing, already set a password.
					} catch (MasterKeysWrongPasswordException e) {
						System.err.println("Wrong password!");
						PageNode page = ctx.getPageMaker().getPageNode(l10n("passwordPageTitle"), false, ctx);
						HTMLNode pageNode = page.outer;
						HTMLNode contentNode = page.content;

						HTMLNode content = ctx.getPageMaker().getInfobox("infobox-error",
								l10n("passwordWrongTitle"), contentNode, null, true).
								addChild("div", "class", "infobox-content");

						SecurityLevelsToadlet.generatePasswordFormPage(true, ctx.getContainer(), content, true, false, true, newThreatLevel.name(), null);

						addBackToPhysicalSeclevelsLink(content);

						writeHTMLReply(ctx, 200, "OK", pageNode.generate());
						return;
					} catch (MasterKeysFileSizeException e) {
						sendPasswordFileCorruptedPage(e.isTooBig(), ctx, false, true);
						return;
					}
				} else {
					// Must set a password!
					PageNode page = ctx.getPageMaker().getPageNode(l10n("passwordPageTitle"), false, ctx);
					HTMLNode pageNode = page.outer;
					HTMLNode contentNode = page.content;

					HTMLNode content = ctx.getPageMaker().getInfobox("infobox-error",
							l10nSec("enterPasswordTitle"), contentNode, null, true).
							addChild("div", "class", "infobox-content");

					if(pass != null && pass.length() == 0) {
						content.addChild("p", l10nSec("passwordNotZeroLength"));
					}

					SecurityLevelsToadlet.generatePasswordFormPage(false, ctx.getContainer(), content, true, false, true, newThreatLevel.name(), null);

					addBackToPhysicalSeclevelsLink(content);

					writeHTMLReply(ctx, 200, "OK", pageNode.generate());
					return;
				}
			}
			if((newThreatLevel == PHYSICAL_THREAT_LEVEL.LOW || newThreatLevel == PHYSICAL_THREAT_LEVEL.NORMAL) &&
					oldThreatLevel == PHYSICAL_THREAT_LEVEL.HIGH) {
				// Check for password
				String pass = request.getPartAsString("masterPassword", SecurityLevelsToadlet.MAX_PASSWORD_LENGTH);
				if(pass != null && pass.length() > 0) {
					// This is actually the OLD password ...
					try {
						core.node.changeMasterPassword(pass, "", true);
					} catch (IOException e) {
						if(!core.node.getMasterPasswordFile().exists()) {
							// Ok.
							System.out.println("Master password file no longer exists, assuming this is deliberate");
						} else {
							System.err.println("Cannot change password as cannot write new passwords file: "+e);
							e.printStackTrace();
							String msg = "<html><head><title>"+l10n("cantWriteNewMasterKeysFileTitle")+
									"</title></head><body><h1>"+l10n("cantWriteNewMasterKeysFileTitle")+"</h1><pre>";
							StringWriter sw = new StringWriter();
							PrintWriter pw = new PrintWriter(sw);
							e.printStackTrace(pw);
							pw.flush();
							msg = msg + sw.toString() + "</pre></body></html>";
							writeHTMLReply(ctx, 500, "Internal Error", msg);
							return;
						}
					} catch (MasterKeysWrongPasswordException e) {
						System.err.println("Wrong password!");
						PageNode page = ctx.getPageMaker().getPageNode(l10n("passwordForDecryptTitle"), false, ctx);
						HTMLNode pageNode = page.outer;
						HTMLNode contentNode = page.content;

						HTMLNode content = ctx.getPageMaker().getInfobox("infobox-error",
								l10n("passwordWrongTitle"), contentNode, null, true).
								addChild("div", "class", "infobox-content");

						SecurityLevelsToadlet.generatePasswordFormPage(true, ctx.getContainer(), content, true, false, false, newThreatLevel.name(), null);

						addBackToPhysicalSeclevelsLink(content);

						writeHTMLReply(ctx, 200, "OK", pageNode.generate());
						return;
					} catch (MasterKeysFileSizeException e) {
						sendPasswordFileCorruptedPage(e.isTooBig(), ctx, false, true);
						return;
					} catch (AlreadySetPasswordException e) {
						System.err.println("Already set a password when changing it - maybe master.keys copied in at the wrong moment???");
					}
				} else if(core.node.getMasterPasswordFile().exists()) {
					// We need the old password
					PageNode page = ctx.getPageMaker().getPageNode(l10n("passwordForDecryptTitle"), false, ctx);
					HTMLNode pageNode = page.outer;
					HTMLNode contentNode = page.content;

					HTMLNode content = ctx.getPageMaker().getInfobox("infobox-error",
							l10nSec("passwordForDecryptTitle"), contentNode, null, true).
							addChild("div", "class", "infobox-content");

					if(pass != null && pass.length() == 0) {
						content.addChild("p", l10nSec("passwordNotZeroLength"));
					}

					SecurityLevelsToadlet.generatePasswordFormPage(false, ctx.getContainer(), content, true, true, false, newThreatLevel.name(), null);

					addBackToPhysicalSeclevelsLink(content);

					writeHTMLReply(ctx, 200, "OK", pageNode.generate());
					return;

				}

			}
			if(newThreatLevel == PHYSICAL_THREAT_LEVEL.MAXIMUM) {
				try {
					core.node.killMasterKeysFile();
				} catch (IOException e) {
					sendCantDeleteMasterKeysFile(ctx, newThreatLevel.name());
					return;
				}
			}
			core.node.securityLevels.setThreatLevel(newThreatLevel);
			core.storeConfig();
			try {
				core.node.lateSetupDatabase(null);
			} catch (MasterKeysWrongPasswordException e) {
				// Ignore, impossible???
				System.err.println("Failed starting up database while switching physical security level to "+newThreatLevel+" from "+oldThreatLevel+" : wrong password - this is impossible, it should have been handled by the other cases, suggest you remove master.keys");
			} catch (MasterKeysFileSizeException e) {
				System.err.println("Failed starting up database while switching physical security level to "+newThreatLevel+" from "+oldThreatLevel+" : "+core.node.getMasterPasswordFile()+" is too " + e.sizeToString());
			}
			super.writeTemporaryRedirect(ctx, "step1", TOADLET_URL+"?step="+WIZARD_STEP.NAME_SELECTION+"&opennet="+core.node.isOpennetEnabled());
			return;
		} else if(request.isPartSet("nnameF")) {
			String selectedNName = request.getPartAsString("nname", 128);
			try {
				config.get("node").set("name", selectedNName);
				Logger.normal(this, "The node name has been set to "+ selectedNName);
			} catch (ConfigException e) {
				Logger.error(this, "Should not happen, please report!" + e, e);
			}
			super.writeTemporaryRedirect(ctx, "step3", TOADLET_URL+"?step="+WIZARD_STEP.BANDWIDTH);
			return;
		} else if(request.isPartSet("bwF")) {
			_setUpstreamBandwidthLimit(request.getPartAsString("bw", 20)); // drop down options may be 6 chars or less, but formatted ones e.g. old value if re-running can be more
			super.writeTemporaryRedirect(ctx, "step4", TOADLET_URL+"?step="+WIZARD_STEP.DATASTORE_SIZE);
			return;
		} else if(request.isPartSet("dsF")) {
			_setDatastoreSize(request.getPartAsString("ds", 20)); // drop down options may be 6 chars or less, but formatted ones e.g. old value if re-running can be more
			super.writeTemporaryRedirect(ctx, "step5", TOADLET_URL+"?step="+WIZARD_STEP.CONGRATZ);
			return;
		} else if(request.isPartSet("miscF")) {
			try {
				config.get("node.updater").set("autoupdate", Boolean.parseBoolean(request.getPartAsString("autodeploy", 10)));
			} catch (ConfigException e) {
				Logger.error(this, "Should not happen, please report!" + e, e);
			}
			final boolean enableUPnP = request.isPartSet("upnp");
			final boolean enableJSTUN = request.isPartSet("jstun");
			if(enableUPnP != core.node.pluginManager.isPluginLoaded("plugins.UPnP.UPnP")) {
					// We can probably get connected without it, so don't force HTTPS.
					// We'd have to ask the user anyway...
					core.node.executor.execute(new Runnable() {

						private final boolean enable = enableUPnP;

						public void run() {
							if(enable)
								core.node.pluginManager.startPluginOfficial("UPnP", true, false, false);
							else
								core.node.pluginManager.killPluginByClass("plugins.UPnP.UPnP", 5000);
						}

					});
			}
			if(enableJSTUN != core.node.pluginManager.isPluginLoaded("plugins.JSTUN.JSTUN")) {
					core.node.executor.execute(new Runnable() {

						private final boolean enable = enableJSTUN;

						public void run() {
							// We can probably get connected without it, so don't force HTTPS.
							// We'd have to ask the user anyway...
							if(enable)
							core.node.pluginManager.startPluginOfficial("JSTUN", true, false, false);
							else
								core.node.pluginManager.killPluginByClass("plugins.JSTUN.JSTUN", 5000);
						}
					});
			}
			super.writeTemporaryRedirect(ctx, "step7", TOADLET_URL+"?step="+WIZARD_STEP.OPENNET);
			return;

		}

		super.writeTemporaryRedirect(ctx, "invalid/unhandled data", TOADLET_URL);
	}

	private void sendPasswordFileCorruptedPage(boolean tooBig, ToadletContext ctx, boolean forSecLevels, boolean forFirstTimeWizard) throws ToadletContextClosedException, IOException {
		writeHTMLReply(ctx, 500, "OK", SecurityLevelsToadlet.sendPasswordFileCorruptedPageInner(tooBig, ctx, forSecLevels, forFirstTimeWizard, core.node.getMasterPasswordFile().getPath(), core.node).generate());
	}

	private void addBackToPhysicalSeclevelsLink(HTMLNode content) {
		content.addChild("a", "href", TOADLET_URL+"?step="+WIZARD_STEP.SECURITY_PHYSICAL, l10n("backToSecurityLevels"));
	}

	private String l10n(String key) {
		return NodeL10n.getBase().getString("FirstTimeWizardToadlet."+key);
	}
	
	private String l10n(String key, String pattern, String value) {
		return NodeL10n.getBase().getString("FirstTimeWizardToadlet."+key, pattern, value);
	}
	
	private void _setDatastoreSize(String selectedStoreSize) {
		try {
			long size = Fields.parseLong(selectedStoreSize);
			// client cache: 10% up to 200MB
			long clientCacheSize = Math.min(size / 10, 200*1024*1024);
			// recent requests cache / slashdot cache / ULPR cache
			int upstreamLimit = config.get("node").getInt("outputBandwidthLimit");
			int downstreamLimit = config.get("node").getInt("inputBandwidthLimit");
			// is used for remote stuff, so go by the minimum of the two
			int limit;
			if(downstreamLimit <= 0) limit = upstreamLimit;
			else limit = Math.min(downstreamLimit, upstreamLimit);
			// 35KB/sec limit has been seen to have 0.5 store writes per second.
			// So saying we want to have space to cache everything is only doubling that ...
			// OTOH most stuff is at low enough HTL to go to the datastore and thus not to
			// the slashdot cache, so we could probably cut this significantly...
			long lifetime = config.get("node").getLong("slashdotCacheLifetime");
			long maxSlashdotCacheSize = (lifetime / 1000) * limit;
			long slashdotCacheSize = Math.min(size / 10, maxSlashdotCacheSize);

			long storeSize = size - (clientCacheSize + slashdotCacheSize);

			System.out.println("Setting datastore size to "+Fields.longToString(storeSize, true));
			config.get("node").set("storeSize", Fields.longToString(storeSize, true));
			if(config.get("node").getString("storeType").equals("ram"))
				config.get("node").set("storeType", "salt-hash");
			System.out.println("Setting client cache size to "+Fields.longToString(clientCacheSize, true));
			config.get("node").set("clientCacheSize", Fields.longToString(clientCacheSize, true));
			if(config.get("node").getString("clientCacheType").equals("ram"))
				config.get("node").set("clientCacheType", "salt-hash");
			System.out.println("Setting slashdot/ULPR/recent requests cache size to "+Fields.longToString(slashdotCacheSize, true));
			config.get("node").set("slashdotCacheSize", Fields.longToString(slashdotCacheSize, true));


			Logger.normal(this, "The storeSize has been set to " + selectedStoreSize);
		} catch(ConfigException e) {
			Logger.error(this, "Should not happen, please report!" + e, e);
		}
	}

	private void _setUpstreamBandwidthLimit(String selectedUploadSpeed) {
		try {
			config.get("node").set("outputBandwidthLimit", selectedUploadSpeed);
			Logger.normal(this, "The outputBandwidthLimit has been set to " + selectedUploadSpeed);
		} catch (ConfigException e) {
			Logger.error(this, "Should not happen, please report!" + e, e);
		}
	}

	private void _setDownstreamBandwidthLimit(String selectedDownloadSpeed) {
		try {
			config.get("node").set("inputBandwidthLimit", selectedDownloadSpeed);
			Logger.normal(this, "The inputBandwidthLimit has been set to " + selectedDownloadSpeed);
		} catch(ConfigException e) {
			Logger.error(this, "Should not happen, please report!" + e, e);
		}
	}

	private int canAutoconfigureBandwidth() {
		if(!config.get("node").getOption("outputBandwidthLimit").isDefault())
			return -1;
		FredPluginBandwidthIndicator bwIndicator = core.node.ipDetector.getBandwidthIndicator();
		if(bwIndicator == null)
			return -1;

		int downstreamBWLimit = bwIndicator.getDownstreamMaxBitRate();
		int upstreamBWLimit = bwIndicator.getUpstramMaxBitRate();
		if((downstreamBWLimit > 0 && downstreamBWLimit < 65536) || (upstreamBWLimit > 0 && upstreamBWLimit < 8192)) {
			// These are kilobits, not bits, per second, right?
			// Assume the router is buggy and don't autoconfigure.
			// Nothing that implements UPnP would be that slow.
			System.err.println("Buggy router? downstream: "+downstreamBWLimit+" upstream: "+upstreamBWLimit+" - these are supposed to be in bits per second!");
			return -1;
		}
		if(downstreamBWLimit > 0) {
			int bytes = (downstreamBWLimit / 8) - 1;
			String downstreamBWLimitString = SizeUtil.formatSize(bytes * 2 / 3);
			// Set the downstream limit anyway, it is usually so high as to be irrelevant.
			// The user can choose the upstream limit.
			_setDownstreamBandwidthLimit(downstreamBWLimitString);
			Logger.normal(this, "The node has a bandwidthIndicator: it has reported downstream=" + downstreamBWLimit + "bits/sec... we will use " + downstreamBWLimitString + " and skip the bandwidth selection step of the wizard.");
		}

		// We don't mind if the downstreamBWLimit couldn't be set, but upstreamBWLimit is important
		if(upstreamBWLimit > 0) {
			int bytes = (upstreamBWLimit / 8) - 1;
			if(bytes < 16384) return 8192;
			return bytes / 2;
		}else
			return -1;
	}

	private long canAutoconfigureDatastoreSize() {
		if(!config.get("node").getOption("storeSize").isDefault())
			return -1;

		long freeSpace = FileUtil.getFreeSpace(core.node.getStoreDir());

		if(freeSpace <= 0)
			return -1;
		else {
			long shortSize = -1;
			if(freeSpace / 20 > 1024 * 1024 * 1024) { // 20GB+ => 5%, limit 256GB
				// If 20GB+ free, 5% of available disk space.
				// Maximum of 256GB. That's a 128MB bloom filter.
				shortSize = Math.min(freeSpace / 20, 256*1024*1024*1024L);
			}else if(freeSpace / 10 > 1024 * 1024 * 1024) { // 10GB+ => 10%
				// If 10GB+ free, 10% of available disk space.
				shortSize = freeSpace / 10;
			}else if(freeSpace / 5 > 1024 * 1024 * 1024) { // 5GB+ => 512MB
				// If 5GB+ free, default to 512MB
				shortSize = 512*1024*1024;
			}else // <5GB => 256MB
				shortSize = 256*1024*1024;

			return shortSize;
		}
	}

	private void sendCantDeleteMasterKeysFile(ToadletContext ctx, String physicalSecurityLevel) throws ToadletContextClosedException, IOException {
		HTMLNode pageNode = SecurityLevelsToadlet.sendCantDeleteMasterKeysFileInner(ctx, core.node.getMasterPasswordFile().getPath(), false, physicalSecurityLevel, this.core.node);
		writeHTMLReply(ctx, 200, "OK", pageNode.generate());
	}

	@Override
	public String path() {
		return TOADLET_URL;
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.keys;

import java.io.IOException;

import freenet.crypt.PCFBMode;
import freenet.crypt.UnsupportedCipherException;
import freenet.crypt.ciphers.Rijndael;
import freenet.support.Logger;
import freenet.support.api.Bucket;
import freenet.support.api.BucketFactory;
import freenet.support.io.ArrayBucket;
import freenet.support.io.ArrayBucketFactory;
import freenet.support.io.BucketTools;

public class ClientSSKBlock extends SSKBlock implements ClientKeyBlock {
	
	static final int DATA_DECRYPT_KEY_LENGTH = 32;
	
	static public final int MAX_DECOMPRESSED_DATA_LENGTH = 32768;
	
	/** Is metadata. Set on decode. */
	private boolean isMetadata;
	/** Has decoded? */
	private boolean decoded;
	/** Client-key. This contains the decryption key etc. */
	private final ClientSSK key;

	/** Compression algorithm from last time tried to decompress. */
	private short compressionAlgorithm = -1;
	
	public ClientSSKBlock(byte[] data, byte[] headers, ClientSSK key, boolean dontVerify) throws SSKVerifyException {
		super(data, headers, (NodeSSK) key.getNodeKey(true), dontVerify);
		this.key = key;
	}
	
	public static ClientSSKBlock construct(SSKBlock block, ClientSSK key) throws SSKVerifyException {
		// Constructor expects clientkey to have the pubkey.
		// In the case of binary blobs, the block may have it instead.
		if(key.getPubKey() == null)
			key.setPublicKey(block.getPubKey());
		return new ClientSSKBlock(block.data, block.headers, key, false);
	}
	
	/**
	 * Decode the data.
	 */
	public Bucket decode(BucketFactory factory, int maxLength, boolean dontDecompress) throws KeyDecodeException, IOException {
		/* We know the signature is valid because it is checked in the constructor. */
		/* We also know e(h(docname)) is valid */
		byte[] decryptedHeaders = new byte[ENCRYPTED_HEADERS_LENGTH];
		System.arraycopy(headers, headersOffset, decryptedHeaders, 0, ENCRYPTED_HEADERS_LENGTH);
		Rijndael aes;
		try {
			Logger.minor(this, "cryptoAlgorithm="+key.cryptoAlgorithm+" for "+getClientKey().getURI());
			aes = new Rijndael(256,256);
		} catch (UnsupportedCipherException e) {
			throw new Error(e);
		}
		aes.initialize(key.cryptoKey);
		// ECB-encrypted E(H(docname)) serves as IV.
		PCFBMode pcfb = PCFBMode.create(aes, key.ehDocname);
		pcfb.blockDecipher(decryptedHeaders, 0, decryptedHeaders.length);
		// First 32 bytes are the key
		byte[] dataDecryptKey = new byte[DATA_DECRYPT_KEY_LENGTH];
		System.arraycopy(decryptedHeaders, 0, dataDecryptKey, 0, DATA_DECRYPT_KEY_LENGTH);
		aes.initialize(dataDecryptKey);
		byte[] dataOutput = new byte[data.length];
		System.arraycopy(data, 0, dataOutput, 0, data.length);
		// Data decrypt key should be unique, so use it as IV
		pcfb.reset(dataDecryptKey);
		pcfb.blockDecipher(dataOutput, 0, dataOutput.length);
		// 2 bytes - data length
		int dataLength = ((decryptedHeaders[DATA_DECRYPT_KEY_LENGTH] & 0xff) << 8) +
			(decryptedHeaders[DATA_DECRYPT_KEY_LENGTH+1] & 0xff);
		// Metadata flag is top bit
		if((dataLength & 32768) != 0) {
			dataLength = dataLength & ~32768;
			isMetadata = true;
		}
		if(dataLength > data.length) {
			throw new SSKDecodeException("Data length: "+dataLength+" but data.length="+data.length);
		}
		
        compressionAlgorithm = (short)(((decryptedHeaders[DATA_DECRYPT_KEY_LENGTH+2] & 0xff) << 8) + (decryptedHeaders[DATA_DECRYPT_KEY_LENGTH+3] & 0xff));
        decoded = true;
        
        if(dontDecompress) {
        	if(compressionAlgorithm == (short)-1)
        		return BucketTools.makeImmutableBucket(factory, dataOutput, dataLength);
        	else if(dataLength < 2)
        		throw new SSKDecodeException("Data length is less than 2 yet compressed!");
        	else
        		return BucketTools.makeImmutableBucket(factory, dataOutput, 2, dataLength - 2);
        }

        Bucket b = Key.decompress(compressionAlgorithm >= 0, dataOutput, dataLength, factory, Math.min(MAX_DECOMPRESSED_DATA_LENGTH, maxLength), compressionAlgorithm, true);
        return b;
	}

	public boolean isMetadata() {
		if(!decoded)
			throw new IllegalStateException("Cannot read isMetadata before decoded");
		return isMetadata;
	}

	public ClientSSK getClientKey() {
		return key;
	}

	public short getCompressionCodec() {
		return compressionAlgorithm;
	}
	
	public byte[] memoryDecode() throws KeyDecodeException {
		return memoryDecode(false);
	}
	
    /**
     * Decode into RAM, if short.
     * @throws KeyDecodeException 
     */
	public byte[] memoryDecode(boolean dontDecompress) throws KeyDecodeException {
		try {
			ArrayBucket a = (ArrayBucket) decode(new ArrayBucketFactory(), 32*1024, dontDecompress);
			return BucketTools.toByteArray(a); // FIXME
		} catch (IOException e) {
			throw new Error(e);
		}
	}

	@Override
	public int hashCode() {
		return super.hashCode() ^ key.hashCode();
	}
	
	@Override
	public boolean equals(Object o) {
		if(!(o instanceof ClientSSKBlock)) return false;
		ClientSSKBlock block = (ClientSSKBlock) o;
		if(!key.equals(block.key)) return false;
		return super.equals(o);
	}
	
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.client.filter;

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.util.HashMap;

/**
 * Data filter for a specific MIME type.
 */
public interface ContentDataFilter {
	
	public void readFilter(InputStream input, OutputStream output, String charset, HashMap<String, String> otherParams,
		FilterCallback cb) throws DataFilterException, IOException;

	public void writeFilter(InputStream input, OutputStream output, String charset, HashMap<String, String> otherParams,
		FilterCallback cb) throws DataFilterException, IOException;
}
package freenet.support.io;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;

import com.db4o.ObjectContainer;

import freenet.support.api.Bucket;

/**
 * A bucket that stores data in the memory.
 * 
 * FIXME: No synchronization, should there be?
 * 
 * @author oskar
 */
public class ArrayBucket implements Bucket {
	private volatile byte[] data;
	private String name;
	private boolean readOnly;

	public ArrayBucket() {
		this("ArrayBucket");
	}

	public ArrayBucket(byte[] initdata) {
		this("ArrayBucket");
		data = initdata;
	}

	public ArrayBucket(String name) {
		data = new byte[0];
		this.name = name;
	}

	public OutputStream getOutputStream() throws IOException {
		if(readOnly) throw new IOException("Read only");
		return new ArrayBucketOutputStream();
	}

	public InputStream getInputStream() {
		return new ByteArrayInputStream(data);
	}

	@Override
	public String toString() {
		return new String(data);
	}

	public long size() {
		return data.length;
	}

	public String getName() {
		return name;
	}

	private class ArrayBucketOutputStream extends ByteArrayOutputStream {
		private boolean hasBeenClosed = false;
		
		public ArrayBucketOutputStream() {
			super();
		}

		@Override
		public synchronized void close() throws IOException {
			if(hasBeenClosed) return;
			data = super.toByteArray();
			if(readOnly) throw new IOException("Read only");
			// FIXME maybe we should throw on write instead? :)
			hasBeenClosed = true;
		}
	}

	public boolean isReadOnly() {
		return readOnly;
	}

	public void setReadOnly() {
		readOnly = true;
	}

	public void free() {
		data = new byte[0];
		// Not much else we can do.
	}

	public byte[] toByteArray() {
		long sz = size();
		int size = (int)sz;
		byte[] buf = new byte[size];
		System.arraycopy(data, 0, buf, 0, size);
		return buf;
	}

	public void storeTo(ObjectContainer container) {
		container.store(data);
		container.store(this);
	}

	public void removeFrom(ObjectContainer container) {
		container.delete(data);
		container.delete(this);
	}

	public Bucket createShadow() {
		return null;
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.clients.http;

import java.net.URI;
import java.net.URISyntaxException;

/**
 * If thrown, the ToadletContainer re-runs the request with the new URI.
 * Note that it DOES NOT change the method to "GET"! So you can redirect to another toadlet
 * and expect the other toadlet to deal with a POST. However if you want to dump the contents
 * of the POST, you need to actually write a redirect.
 * @author Matthew Toseland <toad@amphibian.dyndns.org> (0xE43DA450)
 */
public class RedirectException extends Exception {
	private static final long serialVersionUID = -1;
	URI newuri;
	
	public RedirectException() {
		super();
	}

	public RedirectException(String newURI) throws URISyntaxException {
		this.newuri = new URI(newURI);
	}
	
	public RedirectException(URI newURI) {
		this.newuri = newURI;
	}
}
package freenet.support;

import freenet.support.Logger.LogLevel;

/**
 * Token bucket. Can be used for e.g. bandwidth limiting.
 * Tokens are added once per tick.
 */
public class TokenBucket {

	private static boolean logMINOR;
	protected long current;
	protected long max;
	protected long timeLastTick;
	protected long nanosPerTick;
	
	/**
	 * Create a token bucket.
	 * @param max The maximum size of the bucket, in tokens.
	 * @param nanosPerTick The number of nanoseconds between ticks.
	 */
	public TokenBucket(long max, long nanosPerTick, long initialValue) {
		this.max = max;
		this.current = initialValue;
		if(current > max) {
			Logger.error(this, "initial value ("+current+") > max ("+max+") in "+this, new Exception("error"));
			current = max;
		}
		this.nanosPerTick = nanosPerTick;
		long now = System.currentTimeMillis();
		this.timeLastTick = now * (1000 * 1000);
		logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
	}
	
	/**
	 * Either grab a bunch of tokens, or don't. Never block.
	 * @param tokens The number of tokens to grab.
	 * @return True if we could acquire the tokens.
	 */
	public synchronized boolean instantGrab(long tokens) {
		if(tokens < 0) throw new IllegalArgumentException("Can't grab negative tokens: "+tokens);
		if(logMINOR)
			Logger.minor(this, "instant grab: "+tokens+" current="+current+" max="+max);
		addTokens();
		if(logMINOR)
			Logger.minor(this, "instant grab: "+tokens+" current="+current+" max="+max);
		if(current >= tokens) {
			current -= tokens;
			return true;
		} else {
			return false;
		}
	}
	
	/**
	 * Try to grab some tokens; if there aren't enough, grab all of them. Never block.
	 * @param tokens The number of tokens to grab.
	 * @return The number of tokens grabbed.
	 */
	public synchronized long partialInstantGrab(long tokens) {
		if(tokens < 0) throw new IllegalArgumentException("Can't grab negative tokens: "+tokens);
		if(logMINOR)
			Logger.minor(this, "instant grab: "+tokens+" current="+current+" max="+max);
		addTokens();
		if(logMINOR)
			Logger.minor(this, "instant grab: "+tokens+" current="+current+" max="+max);
		if(current >= tokens) {
			current -= tokens;
			return tokens;
		} else {
			tokens = current;
			current = 0;
			return tokens;
		}
	}
	
	/**
	 * Remove tokens, without blocking, even if it causes the balance to go negative.
	 * @param tokens The number of tokens to remove.
	 */
	public synchronized void forceGrab(long tokens) {
		if(tokens < 0) throw new IllegalArgumentException("Can't grab negative tokens: "+tokens);
		if(logMINOR) Logger.minor(this, "forceGrab("+tokens+")");
		addTokens();
		current -= tokens;
		if(logMINOR) Logger.minor(this, "Removed tokens, balance now "+current);
	}
	
	public synchronized long count() {
		return current;
	}
	
	/**
	 * Get the current number of available tokens.
	 */
	public synchronized long getCount() {
		addTokens();
		return current;
	}

	protected long offset() {
		return 0;
	}
	
	public synchronized void blockingGrab(long tokens) {
		if(tokens < 0) throw new IllegalArgumentException("Can't grab negative tokens: "+tokens);
		logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
		if(logMINOR) Logger.minor(this, "Blocking grab: "+tokens);
		if(tokens < max)
			innerBlockingGrab(tokens);
		else {
			for(int i=0;i<tokens;i+=max) {
				innerBlockingGrab(Math.min(tokens, max));
			}
		}
	}
	
	/**
	 * Grab a bunch of tokens. Block if necessary.
	 * @param tokens The number of tokens to grab.
	 */
	public synchronized void innerBlockingGrab(long tokens) {
		if(tokens < 0) throw new IllegalArgumentException("Can't grab negative tokens: "+tokens);
		if(logMINOR) Logger.minor(this, "Inner blocking grab: "+tokens);
		addTokens();
		if(logMINOR) Logger.minor(this, "current="+current);
		
		current -= tokens;
		
		if(current >= 0) {
			if(logMINOR) Logger.minor(this, "Got tokens instantly, current="+current);
			return;
		} else {
			if(logMINOR) Logger.minor(this, "Blocking grab removed tokens, current="+current+" - will have to wait because negative...");
		}
		
		long minDelayNS = nanosPerTick * (-current);
		long minDelayMS = minDelayNS / (1000*1000) + (minDelayNS % (1000*1000) == 0 ? 0 : 1);
		long now = System.currentTimeMillis();
		long wakeAt = now + minDelayMS;
		
		if(logMINOR) Logger.minor(this, "Waking in "+minDelayMS+" millis");
		
		while(true) {
			now = System.currentTimeMillis();
			int delay = (int) Math.min(Integer.MAX_VALUE, wakeAt - now);
			if(delay <= 0) break;
			if(logMINOR) Logger.minor(this, "Waiting "+delay+"ms");
			try {
				wait(delay);
			} catch (InterruptedException e) {
				// Go around the loop again.
			}
		}
		if(logMINOR) Logger.minor(this, "Blocking grab finished: current="+current);
	}

	public synchronized void recycle(long tokens) {
		if(tokens < 0) throw new IllegalArgumentException("Can't recycle negative tokens: "+tokens);
		current += tokens;
		if(current > max) current = max;
	}
	
	/**
	 * Change the number of nanos per tick.
	 * @param nanosPerTick The new number of nanos per tick.
	 */
	public synchronized void changeNanosPerTick(long nanosPerTick) {
		if(nanosPerTick <= 0) throw new IllegalArgumentException();
		// Synchronize up first, using the old nanosPerTick.
		addTokens();
		this.nanosPerTick = nanosPerTick;
		if(nanosPerTick < this.nanosPerTick)
			notifyAll();
	}

	public synchronized void changeBucketSize(long newMax) {
		if(newMax <= 0) throw new IllegalArgumentException();
		max = newMax;
		addTokens();
	}
	
	public synchronized void changeNanosAndBucketSize(long nanosPerTick, long newMax) {
		if(nanosPerTick <= 0) throw new IllegalArgumentException();
		if(newMax <= 0) throw new IllegalArgumentException();
		// Synchronize up first, using the old nanosPerTick.
		addTokensNoClip();
		if(nanosPerTick < this.nanosPerTick)
			notifyAll();
		this.nanosPerTick = nanosPerTick;
		this.max = newMax;
		if(current > max) current = max;
	}
	
	public synchronized void addTokens() {
		addTokensNoClip();
		if(current > max) current = max;
		if(logMINOR)
			Logger.minor(this, "addTokens: Clipped, current="+current);
	}
	
	/**
	 * Update the number of tokens according to elapsed time.
	 */
	public synchronized void addTokensNoClip() {
		long add = tokensToAdd();
		current += add;
		timeLastTick += add * nanosPerTick;
		if(logMINOR)
			Logger.minor(this, "addTokensNoClip: Added "+add+" tokens, current="+current);
		// Deliberately do not clip to size at this point; caller must do this, but it is usually beneficial for the caller to do so.
	}
	
	synchronized long tokensToAdd() {
		long nowNS = System.currentTimeMillis() * (1000 * 1000);
		if(timeLastTick > nowNS) {
			System.err.println("CLOCK SKEW DETECTED! CLOCK WENT BACKWARDS BY AT LEAST "+TimeUtil.formatTime((timeLastTick - nowNS)/(1000*1000), 2, true));
			System.err.println("FREENET WILL BREAK SEVERELY IF THIS KEEPS HAPPENING!");
			Logger.error(this, "CLOCK SKEW DETECTED! CLOCK WENT BACKWARDS BY AT LEAST "+TimeUtil.formatTime((timeLastTick - nowNS)/(1000*1000), 2, true));
			timeLastTick = nowNS;
			return 0;
		}
		long nextTick = timeLastTick + nanosPerTick;
		if(nextTick > nowNS) {
			return 0;
		}
		if(nextTick + nanosPerTick > nowNS) {
			return 1;
		}
		return (nowNS - nextTick) / nanosPerTick;
	}
	
	public synchronized long getNanosPerTick() {
		return nanosPerTick;
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.io.comm;

public class IncomingPacketFilterException extends Exception {
	private static final long serialVersionUID = -1;

	public IncomingPacketFilterException(String string) {
		super(string);
	}

	public IncomingPacketFilterException() {
		super();
	}

}
package freenet.io.comm;

public class NullAsyncMessageFilterCallback implements
		AsyncMessageFilterCallback {

	public void onMatched(Message m) {
		// Do nothing
	}

	public boolean shouldTimeout() {
		// Not until matched.
		return false;
	}

	public void onTimeout() {
		// Do nothing
	}

	public void onDisconnect(PeerContext ctx) {
		// Do nothing
	}

	public void onRestarted(PeerContext ctx) {
		// Do nothing
	}

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.client.async;

import java.util.List;

import com.db4o.ObjectContainer;

import freenet.client.ClientMetadata;
import freenet.client.FetchException;
import freenet.support.compress.Compressor;
import freenet.client.InsertContext.CompatibilityMode;
import freenet.crypt.HashResult;

/**
 * Callback called when part of a get request completes - either with a 
 * Bucket full of data, or with an error.
 */
public interface GetCompletionCallback {

	public void onSuccess(StreamGenerator streamGenerator, ClientMetadata clientMetadata, List<? extends Compressor> decompressors, ClientGetState state, ObjectContainer container, ClientContext context);
	
	public void onFailure(FetchException e, ClientGetState state, ObjectContainer container, ClientContext context);
	
	/** Called when the ClientGetState knows that it knows about
	 * all the blocks it will need to fetch.
	 */
	public void onBlockSetFinished(ClientGetState state, ObjectContainer container, ClientContext context);

	public void onTransition(ClientGetState oldState, ClientGetState newState, ObjectContainer container);

	public void onExpectedSize(long size, ObjectContainer container, ClientContext context);
	
	public void onExpectedMIME(String mime, ObjectContainer container, ClientContext context) throws FetchException;
	
	public void onFinalizedMetadata(ObjectContainer container);
	
	public void onExpectedTopSize(long size, long compressed, int blocksReq, int blocksTotal, ObjectContainer container, ClientContext context);
	
	public void onSplitfileCompatibilityMode(CompatibilityMode min, CompatibilityMode max, byte[] customSplitfileKey, boolean compressed, boolean bottomLayer, boolean definitiveAnyway, ObjectContainer container, ClientContext context);

	public void onHashes(HashResult[] hashes, ObjectContainer container, ClientContext context);
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.fcp;

import com.db4o.ObjectContainer;

import freenet.keys.FreenetURI;
import freenet.node.Node;
import freenet.support.SimpleFieldSet;

public class PutSuccessfulMessage extends FCPMessage {

	public final String identifier;
	public final boolean global;
	public final FreenetURI uri;
	public final long startupTime, completionTime;
	
	public PutSuccessfulMessage(String identifier, boolean global, FreenetURI uri, long startupTime, long completionTime) {
		this.identifier = identifier;
		this.global = global;
		this.uri = uri;
		this.startupTime = startupTime;
		this.completionTime = completionTime;
	}

	@Override
	public SimpleFieldSet getFieldSet() {
		SimpleFieldSet fs = new SimpleFieldSet(true);
		fs.putSingle("Identifier", identifier);
		if(global) fs.putSingle("Global", "true");
		// This is useful for simple clients.
		if(uri != null)
			fs.putSingle("URI", uri.toString(false, false));
		fs.put("StartupTime", startupTime);
		fs.put("CompletionTime", completionTime);
		return fs;
	}

	@Override
	public String getName() {
		return "PutSuccessful";
	}

	@Override
	public void run(FCPConnectionHandler handler, Node node)
			throws MessageInvalidException {
		throw new MessageInvalidException(ProtocolErrorMessage.INVALID_MESSAGE, "InsertSuccessful goes from server to client not the other way around", identifier, global);
	}

	@Override
	public void removeFrom(ObjectContainer container) {
		container.activate(uri, 5);
		uri.removeFrom(container);
		container.delete(this);
	}

}
package freenet.support.io;

// WARNING: THIS CLASS IS STORED IN DB4O -- THINK TWICE BEFORE ADD/REMOVE/RENAME FIELDS
public class PersistentBlobTempBucketTag {
	
	final PersistentBlobTempBucketFactory factory;
	final long index;
	PersistentBlobTempBucket bucket;
	/** db4o bug: http://tracker.db4o.com/browse/COR-1446
	 * We cannot just query for bucket == null, because it will instantiate every
	 * object with bucket != null during the query even though we have an index. */
	boolean isFree;

	PersistentBlobTempBucketTag(PersistentBlobTempBucketFactory f, long idx) {
		factory = f;
		index = idx;
		isFree = true;
	}
	
}
package freenet.support;

import com.db4o.ObjectContainer;

public interface RemoveRandomWithObject extends RemoveRandom {

	public Object getObject();

	public boolean isEmpty(ObjectContainer container);

	public void removeFrom(ObjectContainer container);

	public void setObject(Object client, ObjectContainer container);

}
package freenet.clients.http;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.HashMap;
import java.util.Map;

import freenet.client.HighLevelSimpleClient;
import freenet.config.InvalidConfigValueException;
import freenet.config.SubConfig;
import freenet.l10n.NodeL10n;
import freenet.node.Node;
import freenet.support.Logger;
import freenet.support.api.HTTPRequest;
import freenet.support.api.StringArrCallback;

/**
 * Symlinker Toadlet
 * 
 * Provide alias to other toadlet URLs by throwing {@link RedirectException}.
 */
public class SymlinkerToadlet extends Toadlet {	
	private final HashMap<String, String> linkMap = new HashMap<String, String>();
	private final Node node;
	SubConfig tslconfig;
	
	public SymlinkerToadlet(HighLevelSimpleClient client,final Node node) {
		super(client);
		this.node = node;
		tslconfig = new SubConfig("toadletsymlinker", node.config);
		tslconfig.register("symlinks", null, 9, true, false, "SymlinkerToadlet.symlinks", "SymlinkerToadlet.symlinksLong", 
        		new StringArrCallback() {
			@Override
			public String[] get() {
				return getConfigLoadString();
			}
			@Override
			public void set(String[] val) throws InvalidConfigValueException {
				//if(storeDir.equals(new File(val))) return;
				// FIXME
				throw new InvalidConfigValueException("Cannot set the plugins that's loaded.");
			}

			        @Override
					public boolean isReadOnly() {
				        return true;
			        }
		});
		
		String fns[] = tslconfig.getStringArr("symlinks");
		if (fns != null) {
			for (String fn : fns) {
				String tuple[] = fn.split("#");
				if (tuple.length == 2)
					addLink(tuple[0], tuple[1], false);
			}
		}
		
		tslconfig.finishedInitialization();
		
		addLink("/sl/search/", "/plugins/plugins.Librarian/", false);
		addLink("/sl/gallery/", "/plugins/plugins.TestGallery/", false);
	}
	
	public boolean addLink(String alias, String target, boolean store) {
		boolean ret;
		synchronized (linkMap) {
			if (alias.equals(linkMap.put(alias, target))) {
				ret = true;
			} else  {
				ret = false;
			}
			Logger.normal(this, "Adding link: " + alias + " => " + target);
		}
		if(store) node.clientCore.storeConfig();
		return ret;
	}
	
	public boolean removeLink(String alias, boolean store) {
		boolean ret;
		synchronized (linkMap) {
			Object o;
			if ((o = linkMap.remove(alias))!= null)
				ret = true;
			else 
				ret = false;
			
			Logger.normal(this, "Removing link: " + alias + " => " + o);
		}
		if(store) node.clientCore.storeConfig();
		return ret;
	}
	
	private String[] getConfigLoadString() {
		String retarr[] = new String[linkMap.size()];
		synchronized (linkMap) {
			int i = 0;
			for (Map.Entry<String,String> entry : linkMap.entrySet()) {
				retarr[i++] = entry.getKey() + '#' + entry.getValue();
			}
		}
		return retarr;
	}

	public void handleMethodGET(URI uri, HTTPRequest request, ToadletContext ctx)
	throws ToadletContextClosedException, IOException, RedirectException {
		String path = uri.getPath();
		String foundkey = null;
		String foundtarget = null;
		synchronized (linkMap) {
			for (Map.Entry<String,String> entry : linkMap.entrySet()) {
				String key = entry.getKey();
				if (path.startsWith(key)) {
					foundkey = key;
					foundtarget = entry.getValue();
				}
			}
		}
		
		// TODO redirect to errorpage
		if ((foundtarget == null) || (foundkey == null)) {
			writeTextReply(ctx, 404, "Not found", 
					NodeL10n.getBase().getString("StaticToadlet.pathNotFound"));
			return;
		}
		
		path = foundtarget + path.substring(foundkey.length());
		URI outuri = null;
		try {
			outuri = new URI(null, null,
			         path, uri.getQuery(), uri.getFragment());
		} catch (URISyntaxException e) {
			// TODO Handle error somehow
			writeHTMLReply(ctx, 200, "OK", e.getMessage());
			return;
		}
		
		uri.getRawQuery();
	    
		throw new RedirectException(outuri);
	}

	@Override
	public String path() {
		return "/sl/";
	}
	
}
package freenet.config;

import freenet.l10n.NodeL10n;
import freenet.support.Fields;
import freenet.support.api.ShortCallback;

public class ShortOption extends Option<Short> {
	protected final boolean isSize;
	
	public ShortOption(SubConfig conf, String optionName, short defaultValue, int sortOrder, 
			boolean expert, boolean forceWrite, String shortDesc, String longDesc, ShortCallback cb, boolean isSize) {
		super(conf, optionName, cb, sortOrder, expert, forceWrite, shortDesc, longDesc, Option.DataType.NUMBER);
		this.defaultValue = defaultValue;
		this.currentValue = defaultValue;
		this.isSize = isSize;
	}

	private String l10n(String key, String pattern, String value) {
		return NodeL10n.getBase().getString("ShortOption."+key, pattern, value);
	}
	
	@Override
	protected Short parseString(String val) throws InvalidConfigValueException {
		short x;
		try {
			x = Fields.parseShort(val);
		} catch (NumberFormatException e) {
			throw new InvalidConfigValueException(l10n("unrecognisedShort", "val", val));
		}
		return x;
	}

	@Override
	protected String toString(Short val) {
		return Fields.shortToString(val, isSize);
	}	
}
package freenet.node.fcp;

import java.io.UnsupportedEncodingException;

import freenet.support.api.Bucket;
import freenet.support.io.ArrayBucket;
import freenet.support.io.NullBucket;

public class TextFeedMessage extends N2NFeedMessage {

	public static final String NAME = "TextFeed";
	private final Bucket messageTextBucket;

	public TextFeedMessage(String header, String shortText, String text, short priorityClass, long updatedTime,
			String sourceNodeName, long composed, long sent, long received,
			String messageText) {
		super(header, shortText, text, priorityClass, updatedTime, sourceNodeName, composed, sent, received);
		try {
			if(messageText != null)
				messageTextBucket = new ArrayBucket(messageText.getBytes("UTF-8"));
			else
				messageTextBucket = new NullBucket();
		} catch (UnsupportedEncodingException e) {
			throw new Error("Impossible: JVM doesn't support UTF-8: " + e, e);
		}
		buckets.put("MessageText", messageTextBucket);
	}

	@Override
	public String getName() {
		return NAME;
	}

}
package freenet.client.async;

import com.db4o.ObjectContainer;

import freenet.keys.Key;
import freenet.keys.KeyBlock;
import freenet.node.SendableGet;

/**
 * Transient object created on startup for persistent requests (or at creation
 * time for non-persistent requests), to monitor the stream of successfully
 * fetched keys. If a key appears interesting, we schedule a job on the database
 * thread to double-check and process the data if we still want it.
 * @author Matthew Toseland <toad@amphibian.dyndns.org> (0xE43DA450)
 * 
 * saltedKey is the routing key from the key, salted globally (concat a global
 * salt value and then SHA) in order to save some cycles. Implementations that
 * use two internal bloom filters may need to have an additional local salt, as
 * in SplitFileFetcherKeyListener.
 */
public interface KeyListener {
	
	/**
	 * Fast guess at whether we want a key or not. Usually implemented by a 
	 * bloom filter.
	 * LOCKING: Should avoid external locking if possible. Will be called
	 * within the CRSBase lock.
	 * @return True if we probably want the key. False if we definitely don't
	 * want it.
	 */
	public boolean probablyWantKey(Key key, byte[] saltedKey);
	
	/**
	 * Do we want the key? This is called by the ULPR code, because fetching the
	 * key will involve significant work. tripPendingKey() on the other hand
	 * will go straight to handleBlock().
	 * @return -1 if we don't want the key, otherwise the priority of the request
	 * interested in the key.
	 */
	public short definitelyWantKey(Key key, byte[] saltedKey, ObjectContainer container, ClientContext context);

	/**
	 * Find the requests related to a specific key, used in retrying after cooldown.
	 * Caller should call probablyWantKey() first.
	 */
	public SendableGet[] getRequestsForKey(Key key, byte[] saltedKey, ObjectContainer container, ClientContext context);
	
	/**
	 * Handle the found data, if we really want it.
	 */
	public boolean handleBlock(Key key, byte[] saltedKey, KeyBlock found, ObjectContainer container, ClientContext context);
	
	/**
	 * Is this related to a persistent request?
	 */
	boolean persistent();

	/**
	 * Priority of the associated request.
	 * LOCKING: Should avoid external locking if possible. Will be called
	 * within the CRSBase lock.
	 * @param container Database handle.
	 */
	short getPriorityClass(ObjectContainer container);

	public long countKeys();

	/**
	 * @return The parent HasKeyListener. This does mean it will be pinned in
	 * RAM, but it can be deactivated so it's not a big deal.
	 * LOCKING: Should avoid external locking if possible. Will be called
	 * within the CRSBase lock.
	 */
	public HasKeyListener getHasKeyListener();

	/**
	 * Deactivate the request once it has been removed.
	 */
	public void onRemove();
	
	/**
	 * Has the request finished? If every key has been found, or enough keys have
	 * been found, return true so that the caller can remove it from the list.
	 */
	public boolean isEmpty();

	public boolean isSSK();

	/**
	 * Should this be on the bulk or the real-time scheduler? The actual listener itself
	 * of course is neither, but it is usually associated with a request...
	 */
	public boolean isRealTime();

}
package freenet.node;

import freenet.io.comm.AsyncMessageCallback;

/** Waits for multiple asynchronous message sends, then calls finish(). */
public abstract class MultiMessageCallback {
	
	private int waiting;
	
	private boolean armed;
	
	private boolean someFailed;
	
	abstract void finish(boolean success);

	public AsyncMessageCallback make() {
		synchronized(this) {
			AsyncMessageCallback cb = new AsyncMessageCallback() {

				private boolean finished;
				
				public void sent() {
					// Ignore
				}

				public void acknowledged() {
					complete(true);
				}

				public void disconnected() {
					complete(false);
				}

				public void fatalError() {
					complete(false);
				}
				
				private void complete(boolean success) {
					synchronized(MultiMessageCallback.this) {
						if(finished) return;
						if(!success) someFailed = true;
						finished = true;
						waiting--;
						if(!finished()) return;
						if(someFailed) success = false;
					}
					finish(success);
				}
				
			};
			waiting++;
			return cb;
		}
	}

	public void arm() {
		boolean success;
		synchronized(this) {
			armed = true;
			if(!finished()) return;
			success = !someFailed;
		}
		finish(success);
	}
	
	protected final synchronized boolean finished() {
		return armed && waiting == 0;
	}

}
package freenet.support;

/**
 * An object with a number (as an int).
 * @see NumberedItem
 */
public interface IntNumberedItem {

    int getNumber();
    
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.crypt;

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;

import freenet.crypt.ciphers.Rijndael;

/**
 * Control mechanism for the Periodic Cipher Feed Back mode.  This is
 * a CFB variant used apparently by a number of programs, including PGP. 
 * Thanks to Hal for suggesting it.
 * 
 * http://www.streamsec.com/pcfb1.pdf
 * 
 * NOTE: This is identical to CFB if block size = key size. As of Freenet 0.7, 
 * we use it with block size = key size.
 *
 * @author Scott
 */
public class PCFBMode {
    
	/** The underlying block cipher. */
    protected BlockCipher c;
    /** The register, with which data is XOR'ed */
    protected byte[] feedback_register;
    /** When this reaches the end of the register, we refillBuffer() i.e. re-encrypt the
     * register. */
    protected int registerPointer;
    
    /** Create the PCFB with no IV. The caller must either:
     * a) Call reset() with a proper IV, or 
     * b) Accept the initial IV of all zero's. (We will still encrypt this before using it).
     * If the key is random and never reused, for instance if it is a one time key or 
     * derived from a hash, b) may be acceptable. It is used in some parts of Freenet. 
     * However, it is very bad practice cryptographically, and we should get rid of it.
     * NOTE THAT IV:KEY PAIRS *MUST* BE UNIQUE! If two instances use the same key with the
     * same empty IV, the bad guys will be able to XOR the two ciphertexts to get the XOR
     * of the plaintext. If they can deduce the register's value they may even be able to
     * decrypt the next 32 bytes, however after that they should hopefully be stumped -
     * but it will certainly make more sophisticated attacks easier.
     * FIXME CRYPTO !!!
     * @param c The underlying block cipher
     * @deprecated
     */
    public static PCFBMode create(BlockCipher c) {
    	if(c instanceof Rijndael)
    		return new RijndaelPCFBMode((Rijndael)c);
    	return new PCFBMode(c);
    }
    
    public static PCFBMode create(BlockCipher c, byte[] iv) {
    	return create(c, iv, 0);
    }
    
    /** Create the PCFB with an IV. The register pointer will be set to the end of the IV,
     * so refillBuffer() will be called prior to any encryption. IV's *must* be unique for
     * a given key. IT IS STRONGLY RECOMMENDED TO USE THIS CONSTRUCTOR, THE OTHER ONE WILL 
     * BE REMOVED EVENTUALLY. 
     * @param offset */
    public static PCFBMode create(BlockCipher c, byte[] iv, int offset) {
    	if(c instanceof Rijndael)
    		return new RijndaelPCFBMode((Rijndael)c, iv, offset);
    	return new PCFBMode(c, iv, offset);
    }
    
    protected PCFBMode(BlockCipher c) {
        this.c = c;
        feedback_register = new byte[c.getBlockSize() >> 3];
        registerPointer = feedback_register.length;
    }

    protected PCFBMode(BlockCipher c, byte[] iv, int offset) {
        this(c);
        System.arraycopy(iv, offset, feedback_register, 0, feedback_register.length);
        // registerPointer is already set to the end by this(c), so we will refillBuffer() immediately.
    }

    /**
     * Resets the PCFBMode to an initial IV
     */
    public final void reset(byte[] iv) {
        System.arraycopy(iv, 0, feedback_register, 0, feedback_register.length);
        registerPointer = feedback_register.length;
    }
    
    /**
     * Resets the PCFBMode to an initial IV
     * @param iv The buffer containing the IV.
     * @param offset The offset to start reading the IV at.
     */
    public final void reset(byte[] iv, int offset) {
        System.arraycopy(iv, offset, feedback_register, 0, feedback_register.length);
        registerPointer = feedback_register.length;
    }

    /**
     * Writes the initialization vector to the stream.  Though the IV
     * is transmitted in the clear, this gives the attacker no additional 
     * information because the registerPointer is set so that the encrypted
     * buffer is empty.  This causes an immediate encryption of the IV,
     * thus invalidating any information that the attacker had.
     */
    public void writeIV(RandomSource rs, OutputStream out) throws IOException {
        rs.nextBytes(feedback_register);
        out.write(feedback_register);
    }
    
    /**
     * Reads the initialization vector from the given stream.  
     */
    public void readIV(InputStream in) throws IOException {
        //for (int i=0; i<feedback_register.length; i++) {
        //    feedback_register[i]=(byte)in.read();
        //}
        Util.readFully(in, feedback_register);
    }

    /**
     * returns the length of the IV
     */
    public int lengthIV() {
        return feedback_register.length;
    }

    /**
     * returns the length of the IV for a PCFB created with a specific cipher.
     */
	public static int lengthIV(BlockCipher c) {
		return c.getBlockSize() >> 3;
	}

    /**
     * Deciphers one byte of data, by XOR'ing the ciphertext byte with
     * one byte from the encrypted buffer.  Then places the received
     * byte in the feedback register.  If no bytes are available in 
     * the encrypted buffer, the feedback register is encrypted, providing
     * block_size/8 new bytes for decryption
     */
    //public synchronized int decipher(int b) {
    public int decipher(int b) {
        if (registerPointer == feedback_register.length) refillBuffer();
        int rv = (feedback_register[registerPointer] ^ (byte) b) & 0xff;
        feedback_register[registerPointer++] = (byte) b;
        return rv;
    }

    /**
     * NOTE: As a side effect, this will decrypt the data in the array.
     */
    //public synchronized byte[] blockDecipher(byte[] buf, int off, int len) {
    public byte[] blockDecipher(byte[] buf, int off, int len) {
        while (len > 0) {
            if (registerPointer == feedback_register.length) refillBuffer();
            int n = Math.min(len, feedback_register.length - registerPointer);
            for (int i=off; i<off+n; ++i) {
                byte b = buf[i];
                buf[i] ^= feedback_register[registerPointer];
                feedback_register[registerPointer++] = b;
            }
            off += n;
            len -= n;
        }
        return buf;
    }

    /**
     * Enciphers one byte of data, by XOR'ing the plaintext byte with
     * one byte from the encrypted buffer.  Then places the enciphered 
     * byte in the feedback register.  If no bytes are available in 
     * the encrypted buffer, the feedback register is encrypted, providing
     * block_size/8 new bytes for encryption
     */
    //public synchronized int encipher(int b) {
    public int encipher(int b) {
        if (registerPointer == feedback_register.length) refillBuffer();
        feedback_register[registerPointer] ^= (byte) b;
        return feedback_register[registerPointer++] & 0xff;
    }

    /**
     * NOTE: As a sideeffect, this will encrypt the data in the array.
     */
    //public synchronized byte[] blockEncipher(byte[] buf, int off, int len) {
    public byte[] blockEncipher(byte[] buf, int off, int len) {
        while (len > 0) {
            if (registerPointer == feedback_register.length) refillBuffer();
            int n = Math.min(len, feedback_register.length - registerPointer);
            for (int i=off; i<off+n; ++i)
                buf[i] = (feedback_register[registerPointer++] ^= buf[i]);
            off += n;
            len -= n;
        }
        return buf;
    }
        
    // Refills the encrypted buffer with data.
    //private synchronized void refillBuffer() {
    protected void refillBuffer() {
        // Encrypt feedback into result
        c.encipher(feedback_register, feedback_register);

        registerPointer=0;
    }

}
package freenet.support;
import java.lang.reflect.Constructor;
import java.lang.reflect.InvocationTargetException;
import java.util.Hashtable;

/**
 * @author <a href=mailto:blanu@uts.cc.utexas.edu>Brandon Wiley</a>
 * @author oskar (I made this a generic loader, not just for messages). 
 **/

public class Loader {

    static final private Hashtable<String, Class<?>> classes = new Hashtable<String, Class<?>>();
    //  static final public String prefix="freenet.message.";

    /**
     * This is a caching Class loader.
     * @param name The name of the class to load.
     **/
    static public Class<?> load(String name) throws ClassNotFoundException {
		Class<?> c = classes.get(name);
	if(c==null) {
	    c=Class.forName(name);
	    classes.put(name, c);
	}
	return c;
    }

    /**
     * Creates a new object of given classname.
     * @param classname  Name of class to instantiate.
     **/
    public static Object getInstance(String classname) 
	throws InvocationTargetException, NoSuchMethodException, 
	       InstantiationException, IllegalAccessException,
	       ClassNotFoundException {
	return getInstance(classname,new Class[] {}, new Object[] {});

    }

    /**
     * Creates a new object of given classname.
     * @param classname  Name of class to instantiate.
     * @param argtypes   The classes of the arguments.
     * @param args       The arguments. Since this uses the reflect methods
     *                   it's ok to wrap primitives.
     **/
    public static Object getInstance(String classname, Class<?>[] argtypes, 
				     Object[] args) 
	throws InvocationTargetException, NoSuchMethodException, 
	       InstantiationException, IllegalAccessException,
	       ClassNotFoundException {
	return getInstance(load(classname),argtypes,args);
    }

    /**
     * Creats a new object of a given class.
     * @param c          The class to instantiate.
     * @param argtypes   The classes of the arguments.
     * @param args       The arguments. Since this uses the reflect methods
     *                   it's ok to wrap primitives.
     **/
    public static Object getInstance(Class<?> c, Class<?>[] argtypes, 
				     Object[] args) 
	throws InvocationTargetException, NoSuchMethodException, 
	       InstantiationException, IllegalAccessException {
	Constructor<?> con = c.getConstructor(argtypes);
	return con.newInstance(args);
    }
}


/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.support;

/**
 * @author sdiz
 */
public class NullBloomFilter extends BloomFilter {
	protected NullBloomFilter(int length, int k) {
		super(length, k);
	}

	@Override
	public boolean checkFilter(byte[] key) {
		return true;
	}

	@Override
	public void addKey(byte[] key) {
		// ignore
	}

	@Override
	public void removeKey(byte[] key) {
		// ignore
	}

	@Override
	protected boolean getBit(int offset) {
		// ignore
		return true;
	}

	@Override
	protected void setBit(int offset) {
		// ignore
	}

	@Override
	protected void unsetBit(int offset) {
		// ignore
	}

	@Override
	public void fork(int k) {
		return;
	}

	@Override
	public void discard() {
		return;
	}

	@Override
	public void merge() {
		return;
	}
}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.useralerts;

import freenet.l10n.NodeL10n;
import freenet.node.updater.NodeUpdateManager;
import freenet.node.updater.RevocationChecker;
import freenet.support.HTMLNode;
import freenet.support.TimeUtil;

public class UpdatedVersionAvailableUserAlert extends AbstractUserAlert {
	private final NodeUpdateManager updater;

	public UpdatedVersionAvailableUserAlert(NodeUpdateManager updater){
		super(false, null, null, null, null, (short) 0, false, NodeL10n.getBase().getString("UserAlert.hide"), false, null);
		this.updater = updater;
	}
	
	@Override
	public String getTitle() {
		return l10n("title");
	}

	private String l10n(String key) {
		return NodeL10n.getBase().getString("UpdatedVersionAvailableUserAlert."+key);
	}

	private String l10n(String key, String pattern, String value) {
		return NodeL10n.getBase().getString("UpdatedVersionAvailableUserAlert."+key, pattern, value);
	}

	private String l10n(String key, String[] patterns, String[] values) {
		return NodeL10n.getBase().getString("UpdatedVersionAvailableUserAlert."+key, patterns, values);
	}

	@Override
	public String getText() {
		
		UpdateThingy ut = createUpdateThingy();

		StringBuilder sb = new StringBuilder();
		
		sb.append(ut.firstBit);
		
		if(ut.formText != null) {
			sb.append(" <form action=\"/\" method=\"post\"><input type=\"submit\" name=\"update\" value=\"");
			sb.append(ut.formText);
			sb.append("\" /></form>");
		}
		
		return sb.toString();
	}
	
	@Override
	public String getShortText() {
		if(!updater.isArmed()) {
			if(updater.canUpdateNow()) {
				return l10n("shortReadyNotArmed");
			} else {
				return l10n("shortNotReadyNotArmed");
			}
		} else {
			return l10n("shortArmed");
		}
	}

	private static class UpdateThingy {
		public UpdateThingy(String first, String form) {
			this.firstBit = first;
			this.formText = form;
		}
		String firstBit;
		String formText;
	}
	
	@Override
	public HTMLNode getHTMLText() {
		
		UpdateThingy ut = createUpdateThingy();
		
		HTMLNode alertNode = new HTMLNode("div");
		
		alertNode.addChild("#", ut.firstBit);
		
		if(ut.formText != null) {
			alertNode.addChild("form", new String[] { "action", "method" }, new String[] { "/", "post" }).addChild("input", new String[] { "type", "name", "value" }, new String[] { "submit", "update", ut.formText });
		}
		
		return alertNode;
	}
	
	private UpdateThingy createUpdateThingy() {
		StringBuilder sb = new StringBuilder();
		sb.append(l10n("notLatest"));
		sb.append(' ');
		
		if(updater.isArmed() && updater.inFinalCheck()) {
			sb.append(l10n("finalCheck", new String[] { "count", "max", "time" }, 
					new String[] { Integer.toString(updater.getRevocationDNFCounter()), 
						Integer.toString(RevocationChecker.REVOCATION_DNF_MIN), TimeUtil.formatTime(updater.timeRemainingOnCheck()) }));
			sb.append(' ');
		} else if(updater.isArmed()) {
			sb.append(l10n("armed"));
		} else {
			String formText;
			if(updater.canUpdateNow()) {
				boolean b = false;
				if(updater.hasNewMainJar()) {
					sb.append(l10n("downloadedNewJar", "version", Integer.toString(updater.newMainJarVersion())));
					sb.append(' ');
					b = true;
				}
				if(updater.hasNewExtJar()) {
					sb.append(l10n(b ? "alsoDownloadedNewExtJar" : "downloadedNewExtJar", "version", Integer.toString(updater.newExtJarVersion())));
					sb.append(' ');
				}
				if(updater.canUpdateImmediately()) {
					sb.append(l10n("clickToUpdateNow"));
					formText = l10n("updateNowButton");
				} else {
					sb.append(l10n("clickToUpdateASAP"));
					formText = l10n("updateASAPButton");
				}
			} else {
				boolean fetchingNew = updater.fetchingNewMainJar();
				boolean fetchingNewExt = updater.fetchingNewExtJar();
				if(fetchingNew) {
					if(fetchingNewExt)
						sb.append(l10n("fetchingNewBoth", new String[] { "nodeVersion", "extVersion" },
								new String[] { Integer.toString(updater.fetchingNewMainJarVersion()), Integer.toString(updater.fetchingNewExtJarVersion()) }));
					else
						sb.append(l10n("fetchingNewNode", "nodeVersion", Integer.toString(updater.fetchingNewMainJarVersion())));
				} else {
					if(fetchingNewExt)
						sb.append(l10n("fetchingNewExt", "extVersion", Integer.toString(updater.fetchingNewExtJarVersion())));
				}
				sb.append(l10n("updateASAPQuestion"));
				formText = l10n("updateASAPButton");
			}
			
			return new UpdateThingy(sb.toString(), formText);
		}
		
		return new UpdateThingy(sb.toString(), null);
	}

	@Override
	public short getPriorityClass() {
		if(updater.inFinalCheck() || updater.canUpdateNow() || !updater.isArmed())
			return UserAlert.ERROR;
		else
			return UserAlert.MINOR;
	}
	
	@Override
	public boolean isValid() {
		return updater.isEnabled() && (!updater.isBlown()) && 
			(updater.fetchingNewExtJar() || updater.fetchingNewMainJar() || updater.hasNewExtJar() || updater.hasNewMainJar());
	}
	
	@Override
	public void isValid(boolean b){
		// Ignore
	}
	
}
package freenet.node.stats;

public abstract class StoreAccessStats {
	
	public abstract long hits();
	
	public abstract long misses();
	
	public abstract long falsePos();
	
	public abstract long writes();
	
	public long readRequests() {
		return hits() + misses();
	}

	public long successfulReads() {
		if (readRequests() > 0)
			return hits();
		else
			return 0;
	}

	public double successRate() throws StatsNotAvailableException {
		if (readRequests() > 0)
			return (100.0 * hits() / readRequests());
		else
			throw new StatsNotAvailableException();
	}

	public double accessRate(long nodeUptimeSeconds) {
		return (1.0 * readRequests() / nodeUptimeSeconds);
	}

	public double writeRate(long nodeUptimeSeconds) {
		return (1.0 * writes() / nodeUptimeSeconds);
	}




}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node;

import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;

public class LowLevelGetException extends Exception {
    private static volatile boolean logDEBUG;

    static {
        Logger.registerLogThresholdCallback(new LogThresholdCallback() {

            @Override
            public void shouldUpdate() {
                logDEBUG = Logger.shouldLog(LogLevel.DEBUG, this);
            }
        });
    }

	private static final long serialVersionUID = 1L;
	/** Decode of data failed, probably was bogus at source */
	public static final int DECODE_FAILED = 1;
	/** Data was not in store and request was local-only */
	public static final int DATA_NOT_FOUND_IN_STORE = 2;
	/** An internal error occurred */
	public static final int INTERNAL_ERROR = 3;
	/** The request went to many hops, but could not find the data. Maybe
	 * it doesn't exist. */
	public static final int DATA_NOT_FOUND = 4;
	/** The request could not find enough nodes to visit while looking for
	 * the datum. */
	public static final int ROUTE_NOT_FOUND = 5;
	/** A downstream node is overloaded, and rejected the request. We should
	 * reduce our rate of sending requests.
	 */
	public static final int REJECTED_OVERLOAD = 6;
	/** Transfer of data started, but then failed. */
	public static final int TRANSFER_FAILED = 7;
	/** Data successfully transferred, but was not valid (at the node key level
	 * i.e. before decode) */
	public static final int VERIFY_FAILED = 8;
	/** Request cancelled by user */
	public static final int CANCELLED = 9;
	/** Ran into a failure table */
	public static final int RECENTLY_FAILED = 10;
	
	public static final String getMessage(int reason) {
		switch(reason) {
		case DECODE_FAILED:
			return "Decode of data failed, probably was bogus at source";
		case DATA_NOT_FOUND_IN_STORE:
			return "Data was not in store and request was local-only";
		case INTERNAL_ERROR:
			return "Internal error - probably a bug";
		case DATA_NOT_FOUND:
			return "Could not find the data";
		case ROUTE_NOT_FOUND:
			return "Could not find enough nodes to be sure that the data is not out there somewhere";
		case REJECTED_OVERLOAD:
			return "A node downstream either timed out or was overloaded (retry)";
		case TRANSFER_FAILED:
			return "Started to transfer data, then failed (should be rare)";
		case VERIFY_FAILED:
			return "Node sent us invalid data";
		case CANCELLED:
			return "Request cancelled";
		case RECENTLY_FAILED:
			return "Request killed by failure table due to recently DNFing on a downstream node";
		default:
			return "Unknown error code: "+reason;
		}
	}
	
	/** Failure code */
	public final int code;
	
	public LowLevelGetException(int code, String message, Throwable t) {
		super(message, t);
		this.code = code;
	}

	public LowLevelGetException(int reason) {
		super(getMessage(reason));
		this.code = reason;
	}
	
	@Override
	public String toString() {
		return super.toString()+':'+getMessage(code);
	}

    @Override
    public final synchronized Throwable fillInStackTrace() {
        if(logDEBUG || code == INTERNAL_ERROR || code == DECODE_FAILED || code == VERIFY_FAILED)
            return super.fillInStackTrace();
        return null;
    }

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.pluginmanager;

import freenet.l10n.BaseL10n.LANGUAGE;

/**
 * Interface that has to be implemented for plugins that wants to use
 * the node's localization system (recommended).
 *
 * Those methods are called by the node when plugin l10n data are needed,
 * ex. to automate things in the translation page.
 *
 * @author Artefact2
 */
public interface FredPluginBaseL10n {

	/**
	 * Called when the plugin should change its language.
	 * @param newLanguage New language to use.
	 */
	public void setLanguage(LANGUAGE newLanguage);

	public String getL10nFilesBasePath();

	public String getL10nFilesMask();

	public String getL10nOverrideFilesMask();

	public ClassLoader getPluginClassLoader();
}
package freenet.client.async;

import freenet.client.FetchException;

/**
 * Thrown when creating a KeyListener fails.
 * @author Matthew Toseland <toad@amphibian.dyndns.org> (0xE43DA450)
 *
 */
public class KeyListenerConstructionException extends Exception {

   final private static long serialVersionUID = 8246734637696483122L;

	KeyListenerConstructionException(FetchException e) {
		super(e);
	}

	public FetchException getFetchException() {
		return (FetchException) getCause();
	}

}
package freenet.keys;

import com.db4o.ObjectContainer;

/**
 * Base class for client keys.
 * Client keys are decodable. Node keys are not. When data has been fetched
 * to a node-level KeyBlock, it can only be decoded after a ClientKeyBlock
 * has been constructed from the node-level block and the client key. The
 * client key generally contains the encryption keys which the node level
 * does not know about, but which are in the URI - usually the second part,
 * after the comma.
 */
public abstract class ClientKey extends BaseClientKey {

	/**
	 * @return a NodeCHK corresponding to this key. Basically keep the 
	 * routingKey and lose everything else.
	 */
	public abstract Key getNodeKey(boolean cloneKey);
	
	public Key getNodeKey() {
		return getNodeKey(true);
	}

	public abstract ClientKey cloneKey();

	public abstract void removeFrom(ObjectContainer container);

}
/* This code is part of Freenet. It is distributed under the GNU General
 * Public License, version 2 (or at your option any later version). See
 * http://www.gnu.org/ for further details of the GPL. */
package freenet.node.useralerts;

import freenet.l10n.NodeL10n;
import freenet.support.HTMLNode;

public class RevocationKeyFoundUserAlert extends AbstractUserAlert {
	public RevocationKeyFoundUserAlert(String msg, boolean disabledNotBlown){
		super(false,
				getTitle(disabledNotBlown),
				getText(disabledNotBlown, msg),
				getText(disabledNotBlown, msg), 
				new HTMLNode("#", getText(disabledNotBlown, msg)), 
				UserAlert.CRITICAL_ERROR, true, null, false, null);
	}
	
	private static String getText(boolean disabledNotBlown, String msg) {
		if(disabledNotBlown)
			return NodeL10n.getBase().getString("RevocationKeyFoundUserAlert.textDisabled", "message", msg);
		else
			return NodeL10n.getBase().getString("RevocationKeyFoundUserAlert.text", "message", msg);
	}

	private static String getTitle(boolean disabledNotBlown) {
		if(disabledNotBlown)
			return NodeL10n.getBase().getString("RevocationKeyFoundUserAlert.titleDisabled");
		else
			return NodeL10n.getBase().getString("RevocationKeyFoundUserAlert.title");
	}

	@Override
	public void isValid(boolean b){
		// We ignore it : it's ALWAYS valid !
	}
	
}
package freenet.support.compress;

import java.io.IOException;

/**
 * The output was too big for the buffer.
 */
public class CompressionOutputSizeException extends IOException {

	private static final long serialVersionUID = -1;
	public final long estimatedSize;

	CompressionOutputSizeException() {
		this(-1);
	}

	CompressionOutputSizeException(long sz) {
		super("The output was too big for the buffer; estimated size: " + sz);
		estimatedSize = sz;
	}
}
package freenet.node.updater;

import java.io.File;
import java.io.IOException;

import com.db4o.ObjectContainer;

import freenet.client.FetchContext;
import freenet.client.FetchException;
import freenet.client.FetchResult;
import freenet.client.async.ClientGetCallback;
import freenet.client.async.ClientGetter;
import freenet.client.async.DatabaseDisabledException;
import freenet.node.NodeClientCore;
import freenet.node.RequestClient;
import freenet.node.RequestStarter;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.io.FileBucket;
import freenet.support.io.FileUtil;

/**
 * Fetches the revocation key. Each time it starts, it will try to fetch it until it has 3 DNFs. If it ever finds it, it will
 * be immediately fed to the NodeUpdateManager.
 */
public class RevocationChecker implements ClientGetCallback, RequestClient {

	public final static int REVOCATION_DNF_MIN = 3;
	
	private boolean logMINOR;

	private NodeUpdateManager manager;
	private NodeClientCore core;
	private int revocationDNFCounter;
	private FetchContext ctxRevocation;
	private ClientGetter revocationGetter;
	private boolean wasAggressive;
	/** Last time at which we got 3 DNFs on the revocation key */
	private long lastSucceeded;
	// Kept separately from NodeUpdateManager.hasBeenBlown because there are local problems that can blow the key.
	private volatile boolean blown;
	
	private File blobFile;
	private File tmpBlobFile;

	public RevocationChecker(NodeUpdateManager manager, File blobFile) {
		this.manager = manager;
		core = manager.node.clientCore;
		this.revocationDNFCounter = 0;
		this.blobFile = blobFile;
		this.logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
		ctxRevocation = core.makeClient((short)0, true).getFetchContext();
		ctxRevocation.allowSplitfiles = false;
		ctxRevocation.maxArchiveLevels = 1;
		// big enough ?
		ctxRevocation.maxOutputLength = NodeUpdateManager.MAX_REVOCATION_KEY_LENGTH;
		ctxRevocation.maxTempLength = NodeUpdateManager.MAX_REVOCATION_KEY_TEMP_LENGTH;
		ctxRevocation.maxSplitfileBlockRetries = -1; // if we find content, try forever to get it; not used because of the above size limits.
		ctxRevocation.maxNonSplitfileRetries = 0; // but return quickly normally
	}
	
	public int getRevocationDNFCounter() {
		return revocationDNFCounter;
	}

	public void start(boolean aggressive) {
		start(aggressive, true);
	}
	
	/** Start a fetch.
	 * @param aggressive If set to true, then we have just fetched an update, and therefore can increase the priority of the
	 * fetch to maximum.
	 * @return True if the checker was already running and the counter was not reset.
	 * */
	public boolean start(boolean aggressive, boolean reset) {
		
		if(manager.isBlown()) {
			Logger.error(this, "Not starting revocation checker: key already blown!");
			return false;
		}
		boolean wasRunning = false;
		logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
		try {
			ClientGetter cg = null;
			ClientGetter toCancel = null;
			synchronized(this) {
				if(aggressive && !wasAggressive) {
					// Ignore old one.
					toCancel = revocationGetter;
					if(logMINOR) Logger.minor(this, "Ignoring old request, because was low priority");
					revocationGetter = null;
					if(toCancel != null) wasRunning = true;
				}
				wasAggressive = aggressive;
				if(revocationGetter != null && 
						!(revocationGetter.isCancelled() || revocationGetter.isFinished()))  {
					if(logMINOR) Logger.minor(this, "Not queueing another revocation fetcher yet, old one still running");
					reset = false;
					wasRunning = false;
				} else {
					if(reset) {
						if(logMINOR) Logger.minor(this, "Resetting DNF count from "+revocationDNFCounter, new Exception("debug"));
						revocationDNFCounter = 0;
					} else {
						if(logMINOR) Logger.minor(this, "Revocation count "+revocationDNFCounter);
					}
					if(logMINOR) Logger.minor(this, "fetcher="+revocationGetter);
					if(revocationGetter != null && logMINOR) Logger.minor(this, "revocation fetcher: cancelled="+revocationGetter.isCancelled()+", finished="+revocationGetter.isFinished());
					try {
						// Client startup may not have completed yet.
						manager.node.clientCore.getPersistentTempDir().mkdirs();
						tmpBlobFile = File.createTempFile("revocation-", ".fblob.tmp", manager.node.clientCore.getPersistentTempDir());
					} catch (IOException e) {
						Logger.error(this, "Cannot record revocation fetch (therefore cannot pass it on to peers)!: "+e+" for "+tmpBlobFile+" dir "+manager.node.clientCore.getPersistentTempDir()+" exists = "+manager.node.clientCore.getPersistentTempDir().exists(), e);
					}
					cg = revocationGetter = new ClientGetter(this, 
							manager.revocationURI, ctxRevocation, 
							aggressive ? RequestStarter.MAXIMUM_PRIORITY_CLASS : RequestStarter.IMMEDIATE_SPLITFILE_PRIORITY_CLASS, 
							this, null, tmpBlobFile == null ? null : new FileBucket(tmpBlobFile, false, false, false, false, false));
					if(logMINOR) Logger.minor(this, "Queued another revocation fetcher (count="+revocationDNFCounter+")");
				}
			}
			if(toCancel != null)
				toCancel.cancel(null, core.clientContext);
			if(cg != null) {
				core.clientContext.start(cg);
				if(logMINOR) Logger.minor(this, "Started revocation fetcher");
			}
			return wasRunning;
		} catch (FetchException e) {
			Logger.error(this, "Not able to start the revocation fetcher.");
			manager.blow("Cannot start fetch for the auto-update revocation key", true);
			return false;
		} catch (DatabaseDisabledException e) {
			// Impossible
			return false;
		}
	}

	long lastSucceeded() {
		return lastSucceeded;
	}
	
	long lastSucceededDelta() {
		if(lastSucceeded <= 0) return -1;
		return System.currentTimeMillis() - lastSucceeded;
	}
	
	/** Called when the revocation URI changes. */
	public void onChangeRevocationURI() {
		kill();
		start(wasAggressive);
	}

	public void onSuccess(FetchResult result, ClientGetter state, ObjectContainer container) {
		onSuccess(result, state, tmpBlobFile);
	}
	
	void onSuccess(FetchResult result, ClientGetter state, File blob) {
		// The key has been blown !
		// FIXME: maybe we need a bigger warning message.
		blown = true;
		moveBlob(blob);
		String msg = null;
		try {
			byte[] buf = result.asByteArray();
			msg = new String(buf);
		} catch (Throwable t) {
			try {
				msg = "Failed to extract result when key blown: "+t;
				Logger.error(this, msg, t);
				System.err.println(msg);
				t.printStackTrace();
			} catch (Throwable t1) {
				msg = "Internal error after retreiving revocation key";
			}
		}
		manager.blow(msg, false); // Real one, even if we can't extract the message.
	}
	
	public boolean hasBlown() {
		return blown;
	}

	private void moveBlob(File tmpBlobFile) {
		if(tmpBlobFile == null) {
			Logger.error(this, "No temporary binary blob file moving it: may not be able to propagate revocation, bug???");
			return;
		}
                FileUtil.renameTo(tmpBlobFile, blobFile);
	}

	public void onFailure(FetchException e, ClientGetter state, ObjectContainer container) {
		onFailure(e, state, tmpBlobFile);
	}
	
	void onFailure(FetchException e, ClientGetter state, File tmpBlobFile) {
		logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
		if(logMINOR) Logger.minor(this, "Revocation fetch failed: "+e);
		int errorCode = e.getMode();
		boolean completed = false;
		long now = System.currentTimeMillis();
		if(errorCode == FetchException.CANCELLED) {
			if(tmpBlobFile != null) tmpBlobFile.delete();
			return; // cancelled by us above, or killed; either way irrelevant and doesn't need to be restarted
		}
		if(e.isFatal()) {
			manager.blow("Permanent error fetching revocation (error inserting the revocation key?): "+e.toString(), true);
			moveBlob(tmpBlobFile); // other peers need to know,
			return;
		}
		if(tmpBlobFile != null) tmpBlobFile.delete();
		if(e.newURI != null) {
			manager.blow("Revocation URI redirecting to "+e.newURI+" - maybe you set the revocation URI to the update URI?", false);
		}
		synchronized(this) {
			if(errorCode == FetchException.DATA_NOT_FOUND){
				revocationDNFCounter++;
				if(logMINOR) Logger.minor(this, "Incremented DNF counter to "+revocationDNFCounter);
			}
			if(revocationDNFCounter >= 3) {
				lastSucceeded = now;
				completed = true;
				revocationDNFCounter = 0;
			}
			revocationGetter = null;
		}
		if(completed)
			manager.noRevocationFound();
		else
			start(wasAggressive, false);
	}
	
	public void onMajorProgress(ObjectContainer container) {
		// TODO Auto-generated method stub
		
	}

	public void kill() {
		if(revocationGetter != null)
			revocationGetter.cancel(null, core.clientContext);
	}

	public long getBlobSize() {
		return blobFile.length();
	}

	/** Get the binary blob, if we have fetched it. */
	public File getBlobFile() {
		if(!manager.isBlown()) return null;
		if(blobFile.exists()) return blobFile;
		return null;
	}

	public boolean persistent() {
		return false;
	}

	public void removeFrom(ObjectContainer container) {
		throw new UnsupportedOperationException();
	}

	public boolean realTimeFlag() {
		return false;
	}

}
package freenet.client.async;

import java.lang.ref.WeakReference;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;

import com.db4o.ObjectContainer;

import freenet.client.FetchContext;
import freenet.crypt.RandomSource;
import freenet.keys.ClientKey;
import freenet.keys.Key;
import freenet.node.BaseSendableGet;
import freenet.node.KeysFetchingLocally;
import freenet.node.Node;
import freenet.node.RequestStarter;
import freenet.node.SendableGet;
import freenet.node.SendableInsert;
import freenet.node.SendableRequest;
import freenet.node.SendableRequestItem;
import freenet.support.LogThresholdCallback;
import freenet.support.Logger;
import freenet.support.Logger.LogLevel;
import freenet.support.RandomGrabArray;
import freenet.support.RemoveRandom.RemoveRandomReturn;
import freenet.support.SectoredRandomGrabArray;
import freenet.support.SectoredRandomGrabArrayWithObject;
import freenet.support.TimeUtil;

/** Chooses requests from both CRSCore and CRSNP */
class ClientRequestSelector implements KeysFetchingLocally {
	
	final boolean isInsertScheduler;
	
	final ClientRequestScheduler sched;
	
	ClientRequestSelector(boolean isInsertScheduler, ClientRequestScheduler sched) {
		this.sched = sched;
		this.isInsertScheduler = isInsertScheduler;
		if(!isInsertScheduler) {
			keysFetching = new HashSet<Key>();
			persistentRequestsWaitingForKeysFetching = new HashMap<Key, Long[]>();
			transientRequestsWaitingForKeysFetching = new HashMap<Key, WeakReference<BaseSendableGet>[]>();
			runningTransientInserts = null;
			this.recentSuccesses = new ArrayList<RandomGrabArray>();
		} else {
			keysFetching = null;
			runningTransientInserts = new HashSet<RunningTransientInsert>();
		}
	}
	
	private static volatile boolean logMINOR;
	
	static {
		Logger.registerLogThresholdCallback(new LogThresholdCallback() {
			
			@Override
			public void shouldUpdate() {
				logMINOR = Logger.shouldLog(LogLevel.MINOR, this);
			}
		});
	}
	
	/**
	 * All Key's we are currently fetching. 
	 * Locally originated requests only, avoids some complications with HTL, 
	 * and also has the benefit that we can see stuff that's been scheduled on a SenderThread
	 * but that thread hasn't started yet. FIXME: Both issues can be avoided: first we'd get 
	 * rid of the SenderThread and start the requests directly and asynchronously, secondly
	 * we'd move this to node but only track keys we are fetching at max HTL.
	 * LOCKING: Always lock this LAST.
	 */
	private transient HashSet<Key> keysFetching;
	
	private transient HashMap<Key, Long[]> persistentRequestsWaitingForKeysFetching;
	private transient HashMap<Key, WeakReference<BaseSendableGet>[]> transientRequestsWaitingForKeysFetching;
	
	private static class RunningTransientInsert {
		
		final SendableInsert insert;
		final Object token;
		
		RunningTransientInsert(SendableInsert i, Object t) {
			insert = i;
			token = t;
		}
		
		@Override
		public int hashCode() {
			return insert.hashCode() ^ token.hashCode();
		}
		
		@Override
		public boolean equals(Object o) {
			if(!(o instanceof RunningTransientInsert)) return false;
			RunningTransientInsert r = (RunningTransientInsert) o;
			return r.insert == insert && (r.token == token || r.token.equals(token));
		}
		
	}
	
	private transient HashSet<RunningTransientInsert> runningTransientInserts;
	
	private transient List<RandomGrabArray> recentSuccesses;
	
	// We pass in the schedTransient to the next two methods so that we can select between either of them.
	
	private long removeFirstAccordingToPriorities(int fuzz, RandomSource random, ClientRequestSchedulerCore schedCore, ClientRequestSchedulerNonPersistent schedTransient, boolean transientOnly, short maxPrio, ObjectContainer container, ClientContext context, long now){
		SectoredRandomGrabArray result = null;
		
		long wakeupTime = Long.MAX_VALUE;
		
		short iteration = 0, priority;
		// we loop to ensure we try every possibilities ( n + 1)
		//
		// PRIO will do 0,1,2,3,4,5,6,0
		// TWEAKED will do rand%6,0,1,2,3,4,5,6
		while(iteration++ < RequestStarter.NUMBER_OF_PRIORITY_CLASSES + 1){
			boolean persistent = false;
			priority = fuzz<0 ? tweakedPrioritySelector[random.nextInt(tweakedPrioritySelector.length)] : prioritySelector[Math.abs(fuzz % prioritySelector.length)];
			if(transientOnly || schedCore == null)
				result = null;
			else {
				result = schedCore.newPriorities[priority];
				if(result != null) {
					long cooldownTime = context.cooldownTracker.getCachedWakeup(result, true, container, now);
					if(cooldownTime > 0) {
						if(cooldownTime < wakeupTime) wakeupTime = cooldownTime;
						Logger.normal(this, "Priority "+priority+" (persistent) is in cooldown for another "+(cooldownTime - now)+" "+TimeUtil.formatTime(cooldownTime - now));
						result = null;
					} else {
						container.activate(result, 1);
						persistent = true;
					}
				}
			}
			if(result == null) {
				result = schedTransient.newPriorities[priority];
				if(result != null) {
					long cooldownTime = context.cooldownTracker.getCachedWakeup(result, false, container, now);
					if(cooldownTime > 0) {
						if(cooldownTime < wakeupTime) wakeupTime = cooldownTime;
						Logger.normal(this, "Priority "+priority+" (transient) is in cooldown for another "+(cooldownTime - now)+" "+TimeUtil.formatTime(cooldownTime - now)+" : "+result);
						result = null;
					}
				}
			}
			if(priority > maxPrio) {
				fuzz++;
				continue; // Don't return because first round may be higher with soft scheduling
			}
			if(((result != null) && (!result.isEmpty(persistent ? container : null)))) {
				if(logMINOR) Logger.minor(this, "using priority : "+priority);
				return priority;
			}
			
			if(logMINOR) Logger.minor(this, "Priority "+priority+" is null (fuzz = "+fuzz+ ')');
			fuzz++;
		}
		
		//FIXME: implement NONE
		return wakeupTime;
	}
	
	// LOCKING: ClientRequestScheduler locks on (this) before calling. 
	// We prevent a number of race conditions (e.g. adding a retry count and then another 
	// thread removes it cos its empty) ... and in addToGrabArray etc we already sync on this.
	// The worry is ... is there any nested locking outside of the hierarchy?
	ChosenBlock removeFirstTransient(int fuzz, RandomSource random, OfferedKeysList offeredKeys, RequestStarter starter, ClientRequestSchedulerNonPersistent schedTransient, short maxPrio, boolean realTime, ClientContext context, ObjectContainer container) {
		// If a block is already running it will return null. Try to find a valid block in that case.
		long now = System.currentTimeMillis();
		for(int i=0;i<5;i++) {
			// Must synchronize on scheduler to avoid problems with cooldown queue. See notes on CooldownTracker.clearCachedWakeup, which also applies to other cooldown operations.
			SelectorReturn r;
			synchronized(sched) {
				r = removeFirstInner(fuzz, random, offeredKeys, starter, null, schedTransient, true, false, maxPrio, realTime, context, container, now);
			}
			SendableRequest req = null;
			if(r != null && r.req != null) req = r.req;
			if(req == null) continue;
			if(isInsertScheduler && req instanceof SendableGet) {
				IllegalStateException e = new IllegalStateException("removeFirstInner returned a SendableGet on an insert scheduler!!");
				req.internalError(e, sched, container, context, req.persistent());
				throw e;
			}
			ChosenBlock block = maybeMakeChosenRequest(req, container, context, now);
			if(block != null) return block;
		}
		return null;
	}
	
	public ChosenBlock maybeMakeChosenRequest(SendableRequest req, ObjectContainer container, ClientContext context, long now) {
		if(req == null) return null;
		if(req.isCancelled(container)) {
			if(logMINOR) Logger.minor(this, "Request is cancelled: "+req);
			return null;
		}
		if(req.getCooldownTime(container, context, now) != 0) {
			if(logMINOR) Logger.minor(this, "Request is in cooldown: "+req);
			return null;
		}
		SendableRequestItem token = req.chooseKey(this, req.persistent() ? container : null, context);
		if(token == null) {
			if(logMINOR) Logger.minor(this, "Choose key returned null: "+req);
			return null;
		} else {
			Key key;
			ClientKey ckey;
			if(isInsertScheduler) {
				key = null;
				ckey = null;
			} else {
				key = ((BaseSendableGet)req).getNodeKey(token, null);
				if(req instanceof SendableGet)
					ckey = ((SendableGet)req).getKey(token, null);
				else
					ckey = null;
			}
			ChosenBlock ret;
			assert(!req.persistent());
			if(key != null && key.getRoutingKey() == null)
				throw new NullPointerException();
			boolean localRequestOnly;
			boolean ignoreStore;
			boolean canWriteClientCache;
			boolean forkOnCacheable;
			boolean realTimeFlag;
			if(req instanceof SendableGet) {
				SendableGet sg = (SendableGet) req;
				FetchContext ctx = sg.getContext(container);
				localRequestOnly = ctx.localRequestOnly;
				ignoreStore = ctx.ignoreStore;
				canWriteClientCache = ctx.canWriteClientCache;
				realTimeFlag = sg.realTimeFlag();
				forkOnCacheable = false;
			} else {
				localRequestOnly = false;
				if(req instanceof SendableInsert) {
					canWriteClientCache = ((SendableInsert)req).canWriteClientCache(null);
					forkOnCacheable = ((SendableInsert)req).forkOnCacheable(null);
					localRequestOnly = ((SendableInsert)req).localRequestOnly(null);
					realTimeFlag = ((SendableInsert)req).realTimeFlag();
				} else {
					canWriteClientCache = false;
					forkOnCacheable = Node.FORK_ON_CACHEABLE_DEFAULT;
					localRequestOnly = false;
					realTimeFlag = false;
				}
				ignoreStore = false;
			}
			ret = new TransientChosenBlock(req, token, key, ckey, localRequestOnly, ignoreStore, canWriteClientCache, forkOnCacheable, realTimeFlag, sched);
			if(logMINOR) Logger.minor(this, "Created "+ret+" for "+req);
			return ret;
		}
	}

	public class SelectorReturn {
		public final SendableRequest req;
		public final long wakeupTime;
		SelectorReturn(SendableRequest req) {
			this.req = req;
			this.wakeupTime = -1;
		}
		SelectorReturn(long wakeupTime) {
			this.wakeupTime = wakeupTime;
			this.req = null;
		}
	}
	
	SelectorReturn removeFirstInner(int fuzz, RandomSource random, OfferedKeysList offeredKeys, RequestStarter starter, ClientRequestSchedulerCore schedCore, ClientRequestSchedulerNonPersistent schedTransient, boolean transientOnly, boolean notTransient, short maxPrio, boolean realTime, ClientContext context, ObjectContainer container, long now) {
		// Priorities start at 0
		if(logMINOR) Logger.minor(this, "removeFirst()");
		if(schedCore == null) transientOnly = true;
		if(transientOnly && notTransient) {
			Logger.error(this, "Not transient but no core");
			return null;
		}
		boolean tryOfferedKeys = offeredKeys != null && (!notTransient) && random.nextBoolean();
		if(tryOfferedKeys) {
			if(offeredKeys.getCooldownTime(container, context, now) == 0)
				return new SelectorReturn(offeredKeys);
		}
		long l = removeFirstAccordingToPriorities(fuzz, random, schedCore, schedTransient, transientOnly, maxPrio, container, context, now);
		if(l > Integer.MAX_VALUE) {
			if(logMINOR) Logger.minor(this, "No priority available for the next "+TimeUtil.formatTime(l - now));
			return null;
		}
		int choosenPriorityClass = (int)l;
		if(choosenPriorityClass == -1) {
			if((!notTransient) && !tryOfferedKeys) {
				if(offeredKeys != null && offeredKeys.getCooldownTime(container, context, now) == 0)
					return new SelectorReturn(offeredKeys);
			}
			if(logMINOR)
				Logger.minor(this, "Nothing to do");
			return null;
		}
		long wakeupTime = Long.MAX_VALUE;
		if(maxPrio >= RequestStarter.MINIMUM_PRIORITY_CLASS)
			maxPrio = RequestStarter.MINIMUM_PRIORITY_CLASS;
outer:	for(;choosenPriorityClass <= maxPrio;choosenPriorityClass++) {
			if(logMINOR) Logger.minor(this, "Using priority "+choosenPriorityClass);
			SectoredRandomGrabArray perm = null;
			if(!transientOnly)
				perm = schedCore.newPriorities[choosenPriorityClass];
			SectoredRandomGrabArray trans = null;
			if(!notTransient)
				trans = schedTransient.newPriorities[choosenPriorityClass];
			if(perm == null && trans == null) {
				if(logMINOR) Logger.minor(this, "No requests to run: chosen priority empty");
				continue; // Try next priority
			}
			boolean triedPerm = false;
			boolean triedTrans = false;
			while(true) {
				boolean persistent;
				SectoredRandomGrabArray chosenTracker = null;
				// If we can't find anything on perm (on the previous loop), try trans, and vice versa
				if(triedTrans) trans = null;
				if(triedPerm) perm = null;
				if(perm == null && trans == null) continue outer;
				else if(perm == null && trans != null) {
					chosenTracker = trans;
					triedTrans = true;
					long cooldownTime = context.cooldownTracker.getCachedWakeup(trans, false, container, now);
					if(cooldownTime > 0) {
						if(cooldownTime < wakeupTime) wakeupTime = cooldownTime;
						Logger.normal(this, "Priority "+choosenPriorityClass+" (transient) is in cooldown for another "+(cooldownTime - now)+" "+TimeUtil.formatTime(cooldownTime - now));
						continue outer;
					}
					persistent = false;
				} else if(perm != null && trans == null) {
					chosenTracker = perm;
					triedPerm = true;
					long cooldownTime = context.cooldownTracker.getCachedWakeup(perm, true, container, now);
					if(cooldownTime > 0) {
						if(cooldownTime < wakeupTime) wakeupTime = cooldownTime;
						Logger.normal(this, "Priority "+choosenPriorityClass+" (persistent) is in cooldown for another "+(cooldownTime - now)+" "+TimeUtil.formatTime(cooldownTime - now));
						continue outer;
					}
					container.activate(perm, 1);
					persistent = true;
				} else {
					container.activate(perm, 1);
					int permSize = perm.size();
					int transSize = trans.size();
					boolean choosePerm = random.nextInt(permSize + transSize) < permSize;
					if(choosePerm) {
						chosenTracker = perm;
						triedPerm = true;
						persistent = true;
					} else {
						chosenTracker = trans;
						triedTrans = true;
						persistent = false;
					}
					long cooldownTime = context.cooldownTracker.getCachedWakeup(trans, choosePerm, container, now);
					if(cooldownTime > 0) {
						if(cooldownTime < wakeupTime) wakeupTime = cooldownTime;
						Logger.normal(this, "Priority "+choosenPriorityClass+" (perm="+choosePerm+") is in cooldown for another "+(cooldownTime - now)+" "+TimeUtil.formatTime(cooldownTime - now));
						continue outer;
					}
				}
				
				if(logMINOR)
					Logger.minor(this, "Got priority tracker "+chosenTracker);
				RemoveRandomReturn val = chosenTracker.removeRandom(starter, persistent ? container : null, context, now);
				SendableRequest req;
				if(val == null) {
					Logger.normal(this, "Priority "+choosenPriorityClass+" returned null - nothing to schedule, should remove priority");
					continue;
				} else if(val.item == null) {
					if(val.wakeupTime == -1)
						Logger.normal(this, "Priority "+choosenPriorityClass+" returned cooldown time of -1 - nothing to schedule, should remove priority");
					else {
						Logger.normal(this, "Priority "+choosenPriorityClass+" returned cooldown time of "+(val.wakeupTime - now)+" = "+TimeUtil.formatTime(val.wakeupTime - now));
						if(val.wakeupTime > 0 && val.wakeupTime < wakeupTime)
							wakeupTime = val.wakeupTime;
					}
					continue;
				} else {
					req = (SendableRequest) val.item;
				}
				if(persistent)
					container.activate(req, 1); // FIXME
				if(chosenTracker.persistent() != persistent) {
					Logger.error(this, "Tracker.persistent()="+chosenTracker.persistent()+" but is in the queue for persistent="+persistent+" for "+chosenTracker);
					// FIXME fix it
				}
				if(req.persistent() != persistent) {
					Logger.error(this, "Request.persistent()="+req.persistent()+" but is in the queue for persistent="+chosenTracker.persistent()+" for "+req);
					// FIXME fix it
				}
				if(req.getPriorityClass(container) != choosenPriorityClass) {
					// Reinsert it : shouldn't happen if we are calling reregisterAll,
					// maybe we should ask people to report that error if seen
					Logger.normal(this, "In wrong priority class: "+req+" (req.prio="+req.getPriorityClass(container)+" but chosen="+choosenPriorityClass+ ')');
					// Remove it.
					SectoredRandomGrabArrayWithObject clientGrabber = (SectoredRandomGrabArrayWithObject) chosenTracker.getGrabber(req.getClient(container));
					if(clientGrabber != null) {
						if(chosenTracker.persistent())
							container.activate(clientGrabber, 1);
						RandomGrabArray baseRGA = (RandomGrabArray) clientGrabber.getGrabber(req.getClientRequest());
						if(baseRGA != null) {
							if(chosenTracker.persistent())
								container.activate(baseRGA, 1);
							// Must synchronize on scheduler to avoid nasty race conditions with cooldown.
							synchronized(sched) {
								baseRGA.remove(req, container, context);
							}
						} else {
							// Okay, it's been removed already. Cool.
						}
					} else {
						Logger.error(this, "Could not find client grabber for client "+req.getClient(container)+" from "+chosenTracker);
					}
					if(req.persistent())
						schedCore.innerRegister(req, random, container, context, null);
					else
						schedTransient.innerRegister(req, random, container, context, null);
					continue;
				}
				
				// Check recentSuccesses
				/** Choose a recently succeeded request.
				 * 50% chance of using a recently succeeded request, if there is one.
				 * For transient requests, we keep a list of recently succeeded BaseSendableGet's,
				 * because transient requests are chosen individually.
				 * But for persistent requests, we keep a list of RandomGrabArray's, because
				 * persistent requests are chosen a whole SendableRequest at a time.
				 * 
				 * FIXME: Only replaces persistent requests with persistent requests (of similar priority and retry count), or transient with transient.
				 * Probably this is acceptable.
				 */
				if(!req.persistent() && !isInsertScheduler) {
					List<BaseSendableGet> recent = schedTransient.recentSuccesses;
					BaseSendableGet altReq = null;
					synchronized(recent) {
						if(!recent.isEmpty()) {
							if(random.nextBoolean()) {
								altReq = recent.remove(recent.size()-1);
							}
						}
					}
					if(altReq != null && (altReq.isCancelled(container))) {
						if(logMINOR)
							Logger.minor(this, "Ignoring cancelled recently succeeded item "+altReq);
						altReq = null;
					}
					if(altReq != null && (l = altReq.getCooldownTime(container, context, now)) != 0) {
						if(logMINOR) {
							Logger.minor(this, "Ignoring recently succeeded item, cooldown time = "+l+((l > 0) ? " ("+TimeUtil.formatTime(l - now)+")" : ""));
							altReq = null;
						}
					}
					if (altReq != null && altReq != req) {
						int prio = altReq.getPriorityClass(container);
						if(prio <= choosenPriorityClass) {
							// Use the recent one instead
							if(logMINOR)
								Logger.minor(this, "Recently succeeded (transient) req "+altReq+" (prio="+altReq.getPriorityClass(container)+") is better than "+req+" (prio="+req.getPriorityClass(container)+"), using that");
							// Don't need to reregister, because removeRandom doesn't actually remove!
							req = altReq;
						} else {
							// Don't use the recent one
							if(logMINOR)
								Logger.minor(this, "Chosen req "+req+" is better, reregistering recently succeeded "+altReq);
							synchronized(recent) {
								recent.add(altReq);
							}
						}
					}
				} else if(!isInsertScheduler) {
					RandomGrabArray altRGA = null;
					synchronized(recentSuccesses) {
						if(!(recentSuccesses.isEmpty() || random.nextBoolean())) {
							altRGA = recentSuccesses.remove(recentSuccesses.size()-1);
						}
					}
					if(altRGA != null) {
						container.activate(altRGA, 1);
						SendableRequest altReq = null;
						if(container.ext().isStored(altRGA) && !altRGA.isEmpty(container)) {
							if(logMINOR)
								Logger.minor(this, "Maybe using recently succeeded item from "+altRGA);
							val = altRGA.removeRandom(starter, container, context, now);
							if(val != null) {
								if(val.item == null) {
									if(logMINOR) Logger.minor(this, "Ignoring recently succeeded item, removeRandom returned cooldown time "+val.wakeupTime+((val.wakeupTime > 0) ? " ("+TimeUtil.formatTime(val.wakeupTime - now)+")" : ""));
								} else {
									altReq = (SendableRequest) val.item;
								}
							}
							if(altReq != null && altReq != req) {
								container.activate(altReq, 1);
								int prio = altReq.getPriorityClass(container);
								boolean useRecent = false;
								if(prio <= choosenPriorityClass) {
									if(altReq.getCooldownTime(container, context, now) != 0)
										useRecent = true;
								}
								if(useRecent) {
									// Use the recent one instead
									if(logMINOR)
										Logger.minor(this, "Recently succeeded (persistent) req "+altReq+" (prio="+altReq.getPriorityClass(container)+") is better than "+req+" (prio="+req.getPriorityClass(container)+"), using that");
									// Don't need to reregister, because removeRandom doesn't actually remove!
									req = altReq;
								} else {
									if(logMINOR)
										Logger.minor(this, "Chosen (persistent) req "+req+" is better, reregistering recently succeeded "+altRGA+" for "+altReq);
									synchronized(recentSuccesses) {
										recentSuccesses.add(altRGA);
									}
								}
							}
						} else {
							container.deactivate(altRGA, 1);
						}
					}
				}
				
				// Now we have chosen a request.
				if(logMINOR) Logger.minor(this, "removeFirst() returning "+req+" (prio "+
						req.getPriorityClass(container)+", client "+req.getClient(container)+", client-req "+req.getClientRequest()+ ')');
				if(logMINOR) Logger.minor(this, "removeFirst() returning "+req+" of "+req.getClientRequest());
				assert(req.realTimeFlag() == realTime);
				return new SelectorReturn(req);
				
			}
		}
		if(logMINOR) Logger.minor(this, "No requests to run");
		return null;
	}
	
	private static final short[] tweakedPrioritySelector = { 
		RequestStarter.MAXIMUM_PRIORITY_CLASS,
		RequestStarter.MAXIMUM_PRIORITY_CLASS,
		RequestStarter.MAXIMUM_PRIORITY_CLASS,
		RequestStarter.MAXIMUM_PRIORITY_CLASS,
		RequestStarter.MAXIMUM_PRIORITY_CLASS,
		RequestStarter.MAXIMUM_PRIORITY_CLASS,
		RequestStarter.MAXIMUM_PRIORITY_CLASS,
		
		RequestStarter.INTERACTIVE_PRIORITY_CLASS,
		RequestStarter.INTERACTIVE_PRIORITY_CLASS,
		RequestStarter.INTERACTIVE_PRIORITY_CLASS,
		RequestStarter.INTERACTIVE_PRIORITY_CLASS,
		RequestStarter.INTERACTIVE_PRIORITY_CLASS,
		RequestStarter.INTERACTIVE_PRIORITY_CLASS,
		
		RequestStarter.IMMEDIATE_SPLITFILE_PRIORITY_CLASS,
		RequestStarter.IMMEDIATE_SPLITFILE_PRIORITY_CLASS,
		RequestStarter.IMMEDIATE_SPLITFILE_PRIORITY_CLASS, 
		RequestStarter.IMMEDIATE_SPLITFILE_PRIORITY_CLASS, 
		RequestStarter.IMMEDIATE_SPLITFILE_PRIORITY_CLASS,
		
		RequestStarter.UPDATE_PRIORITY_CLASS,
		RequestStarter.UPDATE_PRIORITY_CLASS, 
		RequestStarter.UPDATE_PRIORITY_CLASS, 
		RequestStarter.UPDATE_PRIORITY_CLASS,
		
		RequestStarter.BULK_SPLITFILE_PRIORITY_CLASS, 
		RequestStarter.BULK_SPLITFILE_PRIORITY_CLASS, 
		RequestStarter.BULK_SPLITFILE_PRIORITY_CLASS,
		
		RequestStarter.PREFETCH_PRIORITY_CLASS, 
		RequestStarter.PREFETCH_PRIORITY_CLASS,
		
		RequestStarter.MINIMUM_PRIORITY_CLASS
	};
	private static final short[] prioritySelector = {
		RequestStarter.MAXIMUM_PRIORITY_CLASS,
		RequestStarter.INTERACTIVE_PRIORITY_CLASS,
		RequestStarter.IMMEDIATE_SPLITFILE_PRIORITY_CLASS, 
		RequestStarter.UPDATE_PRIORITY_CLASS,
		RequestStarter.BULK_SPLITFILE_PRIORITY_CLASS,
		RequestStarter.PREFETCH_PRIORITY_CLASS,
		RequestStarter.MINIMUM_PRIORITY_CLASS
	};

	/**
	 * @return True unless the key was already present.
	 */
	public boolean addToFetching(Key key) {
		synchronized(keysFetching) {
			boolean retval = keysFetching.add(key);
			if(!retval) {
				Logger.normal(this, "Already in keysFetching: "+key);
			} else {
				if(logMINOR)
					Logger.minor(this, "Added to keysFetching: "+key);
			}
			return retval;
		}
	}
	
	public boolean hasKey(Key key, BaseSendableGet getterWaiting, boolean persistent, ObjectContainer container) {
		if(keysFetching == null) {
			throw new NullPointerException();
		}
		long pid = -1;
		if(getterWaiting != null && persistent) {
			pid = container.ext().getID(getterWaiting);
		}
		synchronized(keysFetching) {
			boolean ret = keysFetching.contains(key);
			if(!ret) return ret;
			// It is being fetched. Add the BaseSendableGet to the wait list so it gets woken up when the request finishes.
			if(getterWaiting != null) {
				if(persistent) {
					Long[] waiting = persistentRequestsWaitingForKeysFetching.get(key);
					if(waiting == null) {
						persistentRequestsWaitingForKeysFetching.put(key, new Long[] { pid });
					} else {
						for(long l : waiting) {
							if(l == pid) return true;
						}
						Long[] newWaiting = new Long[waiting.length+1];
						System.arraycopy(waiting, 0, newWaiting, 0, waiting.length);
						newWaiting[waiting.length] = pid;
						persistentRequestsWaitingForKeysFetching.put(key, newWaiting);
					}
				} else {
					WeakReference<BaseSendableGet>[] waiting = transientRequestsWaitingForKeysFetching.get(key);
					if(waiting == null) {
						transientRequestsWaitingForKeysFetching.put(key, new WeakReference[] { new WeakReference<BaseSendableGet>(getterWaiting) });
					} else {
						for(WeakReference<BaseSendableGet> ref : waiting) {
							if(ref.get() == getterWaiting) return true;
						}
						WeakReference<BaseSendableGet>[] newWaiting = new WeakReference[waiting.length+1];
						System.arraycopy(waiting, 0, newWaiting, 0, waiting.length);
						newWaiting[waiting.length] = new WeakReference<BaseSendableGet>(getterWaiting);
						transientRequestsWaitingForKeysFetching.put(key, newWaiting);
					}
				}
			}
			return true;
		}
	}

	/** LOCKING: Caller should hold as few locks as possible */ 
	public void removeFetchingKey(final Key key) {
		Long[] persistentWaiting;
		WeakReference<BaseSendableGet>[] transientWaiting;
		if(logMINOR)
			Logger.minor(this, "Removing from keysFetching: "+key);
		if(key != null) {
			synchronized(keysFetching) {
				keysFetching.remove(key);
				persistentWaiting = this.persistentRequestsWaitingForKeysFetching.remove(key);
				transientWaiting = this.transientRequestsWaitingForKeysFetching.remove(key);
			}
			if(persistentWaiting != null || transientWaiting != null) {
				CooldownTracker tracker = sched.clientContext.cooldownTracker;
				if(persistentWaiting != null) {
					for(Long l : persistentWaiting)
						tracker.clearCachedWakeupPersistent(l);
				}
				if(transientWaiting != null) {
					for(WeakReference<BaseSendableGet> ref : transientWaiting) {
						BaseSendableGet get = ref.get();
						if(get == null) continue;
						synchronized(sched) {
							tracker.clearCachedWakeup(get, false, null);
						}
					}
				}
			}
		}
	}

	public boolean hasTransientInsert(SendableInsert insert, Object token) {
		RunningTransientInsert tmp = new RunningTransientInsert(insert, token);
		synchronized(runningTransientInserts) {
			return runningTransientInserts.contains(tmp);
		}
	}

	public boolean addTransientInsertFetching(SendableInsert insert, Object token) {
		RunningTransientInsert tmp = new RunningTransientInsert(insert, token);
		synchronized(runningTransientInserts) {
			boolean retval = runningTransientInserts.add(tmp);
			if(!retval) {
				Logger.normal(this, "Already in runningTransientInserts: "+insert+" : "+token);
			} else {
				if(logMINOR)
					Logger.minor(this, "Added to runningTransientInserts: "+insert+" : "+token);
			}
			return retval;
		}
	}
	
	public void removeTransientInsertFetching(SendableInsert insert, Object token) {
		RunningTransientInsert tmp = new RunningTransientInsert(insert, token);
		if(logMINOR)
			Logger.minor(this, "Removing from runningTransientInserts: "+insert+" : "+token);
		synchronized(runningTransientInserts) {
			runningTransientInserts.remove(tmp);
		}
	}

	public void succeeded(BaseSendableGet succeeded, ObjectContainer container) {
		RandomGrabArray array = succeeded.getParentGrabArray();
		container.activate(array, 1);
		if(array == null) return; // Unregistered already?
		synchronized(recentSuccesses) {
			if(recentSuccesses.contains(array)) return;
			recentSuccesses.add(array);
			while(recentSuccesses.size() > 8)
				recentSuccesses.remove(0);
		}
	}

	public long checkRecentlyFailed(Key key, boolean realTime) {
		Node node = sched.getNode();
		return node.clientCore.checkRecentlyFailed(key, realTime);
	}
	
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.exceptions;

import java.awt.Component;

import wordfinder.utils.ResourceManager;

public class WordFinderException
		extends Exception {

	private static final long serialVersionUID = 1151386943739058551L;

	/** ResourceManager f�r Beschriftungen */
	private static final ResourceManager resourceManager = ResourceManager.getInstance();

	/** Gibt den error Code an der in der errorMessages.properties auf einen Fehlertext zeigt */
	private int errorCode;

	/**
	 * Gibt die Fehlerdetails an die in der errorMessgaes.properties auf eine detaillierte Beschreibung des Fehlers
	 * zeigt
	 */
	private String detail;

	/** GUI Element, dass nach der Fehlermeldung den Fokus erhalten soll */
	private Component focusComponent;

	public WordFinderException(int errorCode) {
		this(errorCode, "", null);
	}

	public WordFinderException(int errorCode, Component focusComponent) {
		this(errorCode, "", focusComponent);
	}

	public WordFinderException(int errorCode, int detail) {
		this(errorCode, resourceManager.getErrorMessages(detail), null);
	}

	public WordFinderException(int errorCode, int detail, Component focusComponent) {
		this(errorCode, resourceManager.getErrorMessages(detail), focusComponent);
	}

	public WordFinderException(int errorCode, String detail) {
		this(errorCode, detail, null);
	}

	public WordFinderException(int errorCode, String detail, Component focusComponent) {
		this.errorCode = errorCode;
		this.detail = detail;
		this.focusComponent = focusComponent;
	}

	public String getDetail() {
		return this.detail;
	}

	public int getErrorCode() {
		return this.errorCode;
	}

	public Component getFocusComponent() {
		return this.focusComponent;
	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.gui;

import java.awt.Component;

import javax.swing.JOptionPane;
import javax.swing.JRootPane;

import wordfinder.utils.ResourceManager;

/**
 * Diese Klasse stellt Methoden bereit um Fehler- oder Hinweismeldungen anzuzeigen.
 * 
 * @author tgattinger
 * @since 28.02.2011
 */
public class ErrorMessageDisplayer {

	private static final int MAX_STACKTRACEELEMENTS = 10;

	/** ResourceManager f�r Beschriftungen */
	private ResourceManager resourceManager = ResourceManager.getInstance();

	/** Konstante f�r einen Hinweistext */
	private static final String HWT = "(HWT)";

	/** Einzige Instanz dieser Klasse */
	private static ErrorMessageDisplayer instance;

	/** Rootpane des Hauptfensters */
	private static JRootPane rootPane;

	/** Singleton-Pattern */
	public static ErrorMessageDisplayer getInstance() {
		if (instance == null) {
			instance = new ErrorMessageDisplayer();
		}
		return instance;
	}

	public static void setRootPane(JRootPane pane) {
		rootPane = pane;
	}

	/** Privater Konstruktor des Singleton-Patterns */
	private ErrorMessageDisplayer() {
		super();
	}

	/** {@link ErrorMessageDisplayer#showErrorMessage(int, int, Component)} */
	public void showErrorMessage(int errorCode) {
		showErrorMessage(errorCode, "", null);
	}

	/** {@link ErrorMessageDisplayer#showErrorMessage(int, int, Component)} */
	public void showErrorMessage(int errorCode, Component focusComponent) {
		showErrorMessage(errorCode, "", focusComponent);
	}

	/** {@link ErrorMessageDisplayer#showErrorMessage(int, int, Component)} */
	public void showErrorMessage(int errorCode, String detailMsg) {
		showErrorMessage(errorCode, detailMsg, null);
	}

	/**
	 * Diese Methode zeigt eine Fehler- / Hinweismeldung an. Die Fehlertexte werden anhand von keys aus der
	 * errorMessage.properties geladen.
	 * 
	 * @param errorCode Prim�rer Fehlerhinweiskey
	 * @param detailMsg Detailierte Fehlerbeschreibung
	 * @param focusComponent GUI Element, dass nach der Best�tigung der Meldung den Fokus erhalten soll
	 */
	public void showErrorMessage(int errorCode, String detailMsg, Component focusComponent) {
		String errorMsg = this.resourceManager.getErrorMessages(errorCode);
		int messageType = JOptionPane.ERROR_MESSAGE;
		if ((errorMsg != null) && errorMsg.startsWith(HWT)) {
			messageType = JOptionPane.INFORMATION_MESSAGE;
			errorMsg = errorMsg.substring(HWT.length());
		}
		StringBuilder message = new StringBuilder();
		message.append("<html><b>");
		message.append(errorMsg);
		message.append("</b><br>");
		message.append(detailMsg);
		message.append("</html>");
		String title = this.resourceManager.getString("label_3");
		JOptionPane.showMessageDialog(rootPane, message, title, messageType);
		if (focusComponent != null) {
			focusComponent.requestFocus();
		}
	}

	public void showErrorMessage(int errorCode, Throwable e) {
		StringBuilder details = new StringBuilder();
		StackTraceElement[] elements = e.getStackTrace();
		details.append("(");
		details.append(e.getClass());
		details.append("): ");
		details.append("<span style=\"color:0000ff;\"><b>");
		details.append(e.getMessage());
		details.append("</b></span>");
		details.append("<span style=\"color:aa0000;\">");
		details.append("<br>");
		for (int i = 0; i < elements.length; i++) {
			if (i > MAX_STACKTRACEELEMENTS) {
				details.append("...");
				break;
			}
			details.append("&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;at ");
			details.append(elements[i]);
			details.append("<br>");
		}
		details.append("</span>");
		showErrorMessage(errorCode, details.toString(), null);
	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.gui;

import java.awt.event.WindowAdapter;
import java.awt.event.WindowEvent;
import java.awt.event.WindowListener;

import wordfinder.gui.models.WordFinderModel;
import wordfinder.utils.FileUtil;

public class WindowCloseListener
		extends WindowAdapter
		implements WindowListener {

	private WordFinderModel model;

	public WindowCloseListener(WordFinderModel model) {
		super();

		this.model = model;
	}

	@Override
	public void windowClosing(WindowEvent e) {
		FileUtil.saveConfig(this.model);
		FileUtil.saveUserDefinedFilter(this.model.getAllUserDefinedFilter());
		System.exit(0);
	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.misc;

import java.io.File;

import javax.swing.filechooser.FileFilter;

import wordfinder.utils.ResourceManager;

public class HTMLFileFilter
		extends FileFilter {

	/** ResourceManager für Beschriftungen */
	private static final ResourceManager resourceManager = ResourceManager.getInstance();

	@Override
	public boolean accept(File f) {
		return f.getName().toLowerCase().endsWith(".html") || f.getName().toLowerCase().endsWith(".htm")
				|| f.isDirectory();
	}

	@Override
	public String getDescription() {
		return resourceManager.getString("htmlfileFilterDescription");
	}

}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.misc;

import java.io.File;

import javax.swing.filechooser.FileFilter;

import wordfinder.Constants;
import wordfinder.utils.ResourceManager;

public class PathFileFilter
		extends FileFilter {

	/** ResourceManager für Beschriftungen */
	private static final ResourceManager resourceManager = ResourceManager.getInstance();

	@Override
	public boolean accept(File f) {
		return f.getName().toLowerCase().endsWith(Constants.PATH_FILE_ENDING) || f.isDirectory();
	}

	@Override
	public String getDescription() {
		return resourceManager.getString("pathFileFilterDescription");
	}

}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.misc;

import java.io.File;

import javax.swing.filechooser.FileFilter;

import wordfinder.utils.ResourceManager;

public class TextFileFilter
		extends FileFilter {

	/** ResourceManager für Beschriftungen */
	private static final ResourceManager resourceManager = ResourceManager.getInstance();

	@Override
	public boolean accept(File f) {
		return f.getName().toLowerCase().endsWith(".txt") || f.isDirectory();
	}

	@Override
	public String getDescription() {
		return resourceManager.getString("textfileFilterDescription");
	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.misc;

import java.io.File;

import javax.swing.filechooser.FileFilter;

import wordfinder.utils.ResourceManager;

public class TextHTMLFileFilter
		extends FileFilter {

	/** ResourceManager für Beschriftungen */
	private static final ResourceManager resourceManager = ResourceManager.getInstance();

	@Override
	public boolean accept(File f) {
		return f.getName().toLowerCase().endsWith(".txt") || f.getName().toLowerCase().endsWith(".html")
				|| f.getName().toLowerCase().endsWith(".htm") || f.isDirectory();
	}

	@Override
	public String getDescription() {
		return resourceManager.getString("textHtmlfileFilterDescription");
	}

}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.observer;

/**
 * Dieses Interface muss von Observern implementiert werden. Es ist der "Custom-Made"-Ersatz des Standard Java Interface
 * java.util.Observer. Es wurde selbst gestrickt, da java.util.Observable eine Klasse ist und Java keine mehrfache
 * Vererbung erlaubt.
 * 
 * @author tgattinger
 */
public interface IObserver {

	/**
	 * Callback, der vom IObservable aufgerufen wird. (Zeichnet z.B. die GUI neu)
	 * 
	 * @param obs Handle zu dem Observable
	 * @param updateObject zus�tzliche Information (z.B. welche Aktion wird jetzt durchgef�hrt)
	 */
	void update(IObserverable obs, Object updateObject);

}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.observer;

public interface IObserverable {

	void addObserver(IObserver obs);

	void deleteObservers();

	void notifyObservers(IObserverable obs);

	void notifyObservers(IObserverable obs, Object obj);

	void removeObserver(IObserver obs);
}/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.observer;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;

public class ObserverableImpl
		implements IObserverable {

	private final List<IObserver> observers = new ArrayList<IObserver>();
	private boolean hasChanged;

	public void addObserver(final IObserver obs) {
		this.observers.add(obs);
	}

	public final void clearChanged() {
		this.hasChanged = false;
	}

	public final void deleteObservers() {
		this.observers.clear();

	}

	public final boolean hasChanged() {
		return this.hasChanged;
	}

	public final void notifyObservers(final IObserverable obs) {
		notifyObservers(obs, null);
	}

	public final void notifyObservers(final IObserverable obs, final Object obj) {
		if (this.hasChanged) {
			final Iterator<IObserver> iter = this.observers.iterator();
			while (iter.hasNext()) {
				final IObserver observer = iter.next();
				observer.update(obs, obj);
			}
		}
	}

	public final void removeObserver(final IObserver obs) {
		this.observers.remove(obs);

	}

	public final void setChanged() {
		this.hasChanged = true;
	}

}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

import java.io.File;
import java.io.FileFilter;

import org.apache.log4j.Logger;

public class FileFilterImpl
		implements FileFilter {

	/** Logger für Debugausgaben */
	private static final Logger LOG = Logger.getLogger(FileFilterImpl.class);
	private static final Character EXTENSION_SEPARATOR = '.';

	private String[] extensions;

	public FileFilterImpl(String... extensions) {
		super();

		this.extensions = extensions;
	}

	public boolean accept(File pathname) {
		try {
			String ext = getExtension(pathname.getName());
			System.out.println("filename = " + pathname + " ext = " + ext);
			for (int i = 0; i < this.extensions.length; i++) {
				if (ext.equals(this.extensions[i])) {
					System.out.println("Filter ok");
					return true;
				}
			}
		} catch (IllegalArgumentException e) {
			System.err.println(e);
		}
		return false;
	}

	private String getExtension(String filename) {
		int separatorIndex = filename.lastIndexOf(EXTENSION_SEPARATOR);
		if (separatorIndex > -1) {
			return filename.substring(separatorIndex + 1).toLowerCase();
		}
		return "";
	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

import java.io.File;

import org.apache.log4j.Logger;

public class FileResult {
	/** Logger für Debugausgaben */
	private static final Logger LOG = Logger.getLogger(FileResult.class);
	private String content;
	private File file;

	public FileResult(String content, File file) {
		this.content = content;
		this.file = file;
	}

	public String getContent() {
		return this.content;
	}

	public File getFile() {
		return this.file;
	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.io.OutputStreamWriter;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.Stack;

import org.apache.log4j.Logger;

import wordfinder.Constants;
import wordfinder.exceptions.WordFinderException;
import wordfinder.gui.ErrorMessageDisplayer;
import wordfinder.gui.models.ImportPanelModel;
import wordfinder.gui.models.WordFinderModel;
import wordfinder.gui.models.entries.PathEntry;
import wordfinder.gui.models.entries.UserDefinedFilter;

/**
 * FileUtil stellt verschiedene Methoden zum arbeiten mit Dateien und Ordnern zur Verfügung.
 * 
 * @author mgattinger, tgatinger
 * @since xx.xx.2011
 */
public class FileUtil {

	/** Logger für Debugausgaben */
	private static final Logger LOG = Logger.getLogger(FileUtil.class);

	/** Einzige Instanz dieser Klasse */
	private static FileUtil instance;

	/**
	 * Überprüft ob ein Ordner existiert und erstellt ihn falls nicht vorhanden rekursiv. Das heißt mit dieser Methode
	 * einen ganzen Verzeichnisstrang komplett erstellen lassen. Zum Beispiel kann man den Pfad
	 * 'C:\root\sub1\sub2\sub\', also 4 Verzeichnisse in einem Rutsch erstellen lassen ohne für jedes Verzeichnis diese
	 * Methode einzeln aufrufen zu müssen.
	 * 
	 * @param pfad zu überprüfender oder zu erstellender Pfad zu einem Ordner
	 * @return true wenn Ordner vorhanden oder Ordner erfolgreich erstellt. false wenn Ordner nicht vorhanden und beim
	 *         erstellen des Ordners ein Fehler auftrat
	 */
	public static boolean checkdir(String pfad) {
		File f = new File(pfad);
		if (f.isDirectory()) {
			LOG.trace("Ordner " + pfad + " vorhanden.");
			return true;
		} else {
			if (f.mkdirs()) {
				LOG.trace("Ordner " + pfad + " erstellt.");
				return true;
			} else {
				LOG.warn("Beim erstellen des Ordners " + pfad + " ist ein Fehler aufgetreten.");
				return false;
			}
		}
	}

	/** Gibt die einzige Instanz der Klasse FileUtil zurück. */
	public static FileUtil getInstance() {
		if (instance == null) {
			instance = new FileUtil();
		}
		return instance;
	}

	/**
	 * Gibt alle Unterverzeichnisse in einem Ordner nicht rekursiv zurück.
	 * 
	 * @param path Der Pfad der nicht rekursiv auf Unterverzeichnisse untersucht werden soll.
	 * @return Eine Liste von File-Objekten, welche die Unterverzeichnisse darstellen.
	 */
	public static List<File> giveDirectorysInDirectory(String path) {
		File[] temp = giveObjectsInDirectory(path);
		List<File> dirs = new ArrayList<File>();
		for (File file : temp) {
			if (file.isDirectory()) {
				dirs.add(file);
			}
		}
		// System.out.println("dirs = " + dirs);
		return dirs;
	}

	/**
	 * Gibt alle Dateien in einem Ordner nicht rekursiv zurück.
	 * 
	 * @param path Der Pfad, der nicht rekursiv auf Dateien untersucht werden soll.
	 * @return Eine Liste von File-Objekten, welche die Dateien darstellen.
	 */
	public static List<File> giveFilesInDirectory(String path) {
		File[] temp = giveObjectsInDirectory(path);
		List<File> files = new ArrayList<File>();
		for (File file : temp) {
			if (file.isFile()) {
				files.add(file);
			}
		}
		// System.out.println("files = " + files);
		return files;
	}

	/**
	 * Gibt alle Objekte (Dateien und Ordner) in einem Ordner nicht rekursiv zurück.
	 * 
	 * @param path Der Pfad der nicht rekursiv auf Objekte untersucht werden soll.
	 * @return Ein File-Array mit den Dateien und Ordnern als File-Objekte.
	 */
	public static File[] giveObjectsInDirectory(String path) {
		return new File(path).listFiles();
	}

	/**
	 * Gibt alle Unterverzeichnisse in einem Ordner rekursiv zurück.
	 * 
	 * @param dir Der PFad der rekursiv auf Unterverzeichnisse untersucht werden soll.
	 * @return Eine Liste von Strings, welche die Pfade zu den Unterverzeichnissen beinhalten.
	 */
	static public List<String> giveRekursivDirectorys(String dir) {
		List<File> rekdirs = giveDirectorysInDirectory(dir);
		List<String> temp = new ArrayList<String>();
		temp.add(dir);
		for (File aktdir : rekdirs) {
			temp.addAll(giveRekursivDirectorys(aktdir.getAbsolutePath()));
		}
		// LOG.trace(temp);
		return temp;
	}

	// TODO Doku erstellen;
	public static List<List<PathEntry>> loadPathLists(File file) {
		List<List<PathEntry>> result = new ArrayList<List<PathEntry>>();
		try {
			FileInputStream fileIn = new FileInputStream(file);
			ObjectInputStream objIn = new ObjectInputStream(fileIn);
			result = (ArrayList<List<PathEntry>>) objIn.readObject();
			objIn.close();
		} catch (IOException e) {
			ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
		} catch (ClassNotFoundException e) {
			ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
		}
		return result;
	}

	// TODO Doku erstellen;
	public static List<UserDefinedFilter> loadUserDefinedFilter() {
		List<UserDefinedFilter> result = new ArrayList<UserDefinedFilter>();
		try {
			File file = new File(Constants.USER_DEFINED_FILTER_FILENAME);
			if (file.exists()) {
				FileInputStream fileIn = new FileInputStream(file);
				ObjectInputStream objIn = new ObjectInputStream(fileIn);
				result = (ArrayList<UserDefinedFilter>) objIn.readObject();
				objIn.close();
			}
		} catch (IOException e) {
			ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
		} catch (ClassNotFoundException e) {
			ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
		}
		return result;
	}

	/**
	 * Diese Methode liest alle Dateien, die eine bestimmte Dateiendung besitzen ein und gibt das Ergebnis als
	 * StringBuffer zurück.
	 * 
	 * @return Liefert eine Menge von Wörtern
	 * @throws WordFinderException Falls es beim Lesen der Dateien zu einem Fehler kam
	 */
	public static StringBuilder readTextFromFilesInDirectory(WordFinderModel model) throws WordFinderException {
		ImportPanelModel importModel = model.getImportPanelSubModel();
		File directory = new File(importModel.getPath() + "\\");
		if (LOG.isTraceEnabled()) {
			LOG.trace("filter = " + importModel.getFileFilter());
		}
		FileFilterImpl fileFilter = new FileFilterImpl(importModel.getFileFilter());
		File[] files = directory.listFiles();
		if (files == null) {
			return new StringBuilder();
		}

		StringBuilder textBuffer = new StringBuilder();
		List<String> tempLines = new ArrayList<String>();
		BufferedReader in = null;

		double progressFactor = 10.0 / files.length;
		model.setAdditionalInfoKey("loadFiles");

		if (LOG.isTraceEnabled()) {
			LOG.trace("dir = " + directory);
			LOG.trace("files = " + Arrays.toString(files));
		}

		for (File file : files) {
			System.out.println("file = " + file);
			double progress = model.getProgress();
			progress += progressFactor;
			model.setProgress(progress);

			try {
				tempLines.clear();
				in = new BufferedReader(new FileReader(file));
				while (in.ready()) {
					String line = in.readLine();
					tempLines.add(line);
				}
				int fromIndex = model.getIgnoreFirstLines();
				if (fromIndex > tempLines.size()) {
					fromIndex = tempLines.size();
				}
				int toIndex = tempLines.size() - model.getIgnoreLastLines();
				if (toIndex < 0) {
					toIndex = 0;
				}
				if (fromIndex <= toIndex) {
					textBuffer.append(tempLines.subList(fromIndex, toIndex));
				}
				if (LOG.isTraceEnabled()) {
					LOG.trace("Datei '" + file.getName() + "' wurde geladen.");
				}
			} catch (Exception e) {
				throw new WordFinderException(1, e.getMessage());
			} finally {
				try {
					in.close();
				} catch (Exception e) {
					throw new WordFinderException(1, e.getMessage());
				}
			}
		}

		model.setAdditionalInfoKey(null);

		return textBuffer;
	}

	/**
	 * Ließt eine Datei als UTF-Text ein. <br />
	 * Wenn die Datei nicht UTF-codiert ist, wird eine Warnung ausgegeben und die Datei als ISO-8859-15 eingelesen.
	 * 
	 * @param file Datei, die eingelesen werden soll.
	 * @return Ein String, der den UTF-codierten Text der Datei beinhaltet.
	 */
	public static String readUTFTextFromFile(File file) {
		if ((file == null)) {
			return "";
		}

		String defaultencoding = null; // ISO8859_1 // defaultencoding wird in UnicodeReader nur dann genutzt, wenn 1)
		// UnicodeReader.init() nicht ausgeführt wurde oder 2) wenn
		// UnicodeReader.init() keinen UTF (8,16,32) Zeichensatz feststellen konnte.
		// Es ist also wichtig UnicodeReader.init() auszuführen, damit ein UTF
		// Zeichensatz ausgewählt wird. Die Angabe von ISO-8859-15 erfolgt hier 1.
		// zur nachkontrolle um den Benutzer vor einer falschkodierten Datei zu
		// warnen und 2. zur Prävention: Wenn keine UTF-codierte Datei vorliegt wird
		// angenommen, dass der Nutzer die Datei als ANSI gespeichert hat und die
		// Datei wird als ISO-8859-15 gelesen, welches ANSI am nächsten kommt. Siehe
		// http://www.alanwood.net/demos/charsetdiffs.html ;
		// http://de.wikipedia.org/wiki/ISO_8859-1 ;
		// http://de.wikipedia.org/wiki/ISO_8859-15
		String linesep = Constants.LINESEPARATOR;
		if (file.exists()) {
			try {
				StringBuilder result = new StringBuilder();
				InputStream input = new FileInputStream(file);
				UnicodeReader uniread = new UnicodeReader(input, defaultencoding);

				uniread.init(); // stellt fest ob Datei UTF-codiert. Wenn nicht wird defaultencoding benutzt.
				if (!defaultencoding.contains("UTF") && defaultencoding.equals(uniread.getEncoding())) {
					LOG.warn("Achtung: Bei der zu lesenden Datei " + linesep + file.getAbsolutePath() + linesep
							+ " wurde kein UFT-codierter Text festgestellt." + linesep + "Die Datei wurde als "
							+ defaultencoding + " gelesen. Dadurch können Zeichenfehler auftreten!");
				}

				BufferedReader buffReader = new BufferedReader(uniread);
				String curline = buffReader.readLine();
				if (curline != null) {
					result.append(curline);
				}
				while ((curline = buffReader.readLine()) != null) {
					;
					result.append(linesep);
					result.append(curline);
				}
				buffReader.close();
				// LOG.trace("Datei '" + file.getName() + "' wurde geladen.");

				return result.toString();
			} catch (IOException e) {
				LOG.error(e.getMessage(), e);
			}
		}
		return "";
	}

	/**
	 * Diese Methode speichert die aktuellen Einstellungen in einer Datei, damit diese beim nächsten Start wieder
	 * geladen werden können.
	 */
	public static void saveConfig(WordFinderModel model) {
		ImportPanelModel importModel = model.getImportPanelSubModel();
		StringBuilder content = new StringBuilder();
		content.append(Constants.CONFIG_DIRECTORY_PATH);
		content.append(Constants.CONFIG_SEPARATOR);
		content.append(importModel.getPath().replaceAll("\\\\", "/"));
		content.append("\n");
		content.append(Constants.CONFIG_FILE_TYPES);
		content.append(Constants.CONFIG_SEPARATOR);
		content.append(importModel.getFileFilter());
		content.append("\n");
		content.append(Constants.CONFIG_LETTERS);
		content.append(Constants.CONFIG_SEPARATOR);
		content.append(model.getListOfCharsForGui());
		content.append("\n");
		content.append(Constants.CONFIG_CASE_SENSITIV);
		content.append(Constants.CONFIG_SEPARATOR);
		content.append(model.isCaseSensitive());
		content.append("\n");
		content.append(Constants.CONFIG_SORT_RESULT);
		content.append(Constants.CONFIG_SEPARATOR);
		content.append(model.isSortResult());
		content.append("\n");
		content.append(Constants.CONFIG_SHOW_RESULT);
		content.append(Constants.CONFIG_SEPARATOR);
		content.append(model.isShowResult());
		content.append("\n");
		content.append(Constants.CONFIG_KEYBOARD_LAYOUT);
		content.append(Constants.CONFIG_SEPARATOR);
		content.append(model.getSelectedKeyboardLayout());
		content.append("\n");
		content.append(Constants.CONFIG_LANGUAGE);
		content.append(Constants.CONFIG_SEPARATOR);
		content.append(model.getSelectedLanguage());
		content.append("\n");
		content.append(Constants.CONFIG_IGNORE_FIRST_LINES);
		content.append(Constants.CONFIG_SEPARATOR);
		content.append(model.getIgnoreFirstLines());
		content.append("\n");
		content.append(Constants.CONFIG_IGNORE_LAST_LINES);
		content.append(Constants.CONFIG_SEPARATOR);
		content.append(model.getIgnoreLastLines());
		content.append("\n");
		content.append(Constants.CONFIG_SPECIAL_CONFIG);
		content.append(Constants.CONFIG_SEPARATOR);
		content.append(model.getSpecialConfig());

		FileUtil.writeUTF8TextToFile(content.toString(), Constants.SETTINGS_PATH, true);
	}

	/**
	 * Speichert alle Listen mit Pfaden in der angegebenen Datei ab.
	 * 
	 * @param file Das Fileobjekt, in dem die Pfade gespeichert werden
	 * @param pathLists Die Liste der Pfade, die gespeichert werden
	 */
	public static void savePathLists(File file, List<List<PathEntry>> pathLists) {
		try {
			FileOutputStream fileOut = new FileOutputStream(file);
			ObjectOutputStream objOut = new ObjectOutputStream(fileOut);
			objOut.writeObject(pathLists);
			objOut.close();
		} catch (IOException e) {
			ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
		}
	}

	// TODO doku erstellen
	public static void saveUserDefinedFilter(List<UserDefinedFilter> allUserDefinedFilter) {
		try {
			File file = new File(Constants.USER_DEFINED_FILTER_FILENAME);
			FileOutputStream fileOut = new FileOutputStream(file);
			ObjectOutputStream objOut = new ObjectOutputStream(fileOut);
			objOut.writeObject(allUserDefinedFilter);
			objOut.close();
		} catch (IOException e) {
			ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
		}
	}

	/**
	 * Schreibt eine Datei in UTF-8-Codierung.
	 * 
	 * @param ptext UTF-8-codierter Text, der geschrieben werden soll.
	 * @param pdName Dateipfad wo die Datei geschrieben werden soll.
	 */
	public static void writeUTF8TextToFile(String ptext, String pdName) {
		writeUTF8TextToFile(ptext, pdName, false);
	}

	/**
	 * Schreibt eine Datei in UTF-8-Codierung.
	 * 
	 * @param ptext UTF-8-codierter Text, der geschrieben werden soll.
	 * @param pdName Dateipfad wo die Datei geschrieben werden soll.
	 * @param overwrite legt fest ob die Datei überschrieben werden soll oder am Ende der Datei weitergeschrieben wird.
	 */
	public static void writeUTF8TextToFile(String ptext, String pdName, Boolean overwrite) {
		if ((pdName == null) || (ptext == null)) {
			return;
		}
		checkdir(new File(pdName).getParent());
		try {
			BufferedWriter schreibeStrom = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(pdName,
					!overwrite), "UTF8"));
			schreibeStrom.write(ptext);
			/*
			 * for (int i = 0; i < text.length(); i++) { schreibeStrom.write(text.charAt(i)); } schreibeStrom.write(31);
			 * schreibeStrom.write(13); schreibeStrom.write(10);
			 */
			schreibeStrom.close();
			/*
			 * } catch (UnsupportedEncodingException e1) { e1.printStackTrace(); } catch (FileNotFoundException e1) {
			 * e1.printStackTrace();
			 */
		} catch (IOException e) {
			LOG.error(e.getMessage(), e);
		}
	}

	/** TODO Doku erstellen */
	private Map<PathEntry, Stack<String>> fileMap;

	private Map<PathEntry, Stack<String>> orginalFileMap;

	/** Anzahl der Dateien, die sich insgesamt in der HashMap befinden */
	private int fileCount;

	/**
	 * TODO Doku erstellen
	 * 
	 * FIXME pivate Konstruktor für Singleton!
	 * 
	 */
	public FileUtil() {
		super();
	}

	/**
	 * TODO Doku erstellen
	 * 
	 * @param paths
	 */
	public void createFileMap(List<PathEntry> pathList) {
		this.fileCount = 0;
		this.fileMap = new HashMap<PathEntry, Stack<String>>();
		this.orginalFileMap = new HashMap<PathEntry, Stack<String>>();
		for (PathEntry pathEntry : pathList) {
			Stack<String> fileList = new Stack<String>();
			createFileMap(fileList, pathEntry);
			this.fileMap.put(pathEntry, fileList);
			this.orginalFileMap.put(pathEntry, (Stack<String>) fileList.clone());
		}
	}

	/** {@link FileUtil#fileCount} */
	public int getFileCount() {
		return this.fileCount;
	}

	/**
	 * TODO Doku erstellen
	 * 
	 * @param path
	 * @return
	 */
	public FileResult getNextFile(PathEntry path) {
		Stack<String> fileList = this.fileMap.get(path);
		if (!fileList.isEmpty()) {
			File file = new File(fileList.pop());
			return new FileResult(readUTFTextFromFile(file), file);
		} else {
			return null;
		}
	}

	public void resetToOriginalFileMap() {
		this.fileMap.clear();
		Set<PathEntry> keys = this.orginalFileMap.keySet();
		Iterator<PathEntry> it = keys.iterator();
		while (it.hasNext()) {
			PathEntry key = it.next();
			Stack<String> stack = this.orginalFileMap.get(key);
			this.fileMap.put(key, (Stack<String>) stack.clone());
		}
	}

	/**
	 * TODO Doku erstellen
	 * 
	 * @param fileList
	 */
	private void createFileMap(Stack<String> fileList, PathEntry pathEntry) {
		File dirOrFile = new File(pathEntry.getPath());
		File[] files = new File[1];
		if (dirOrFile.isDirectory()) {
			files = (dirOrFile).listFiles();
		} else {
			files[0] = dirOrFile;
		}
		for (File file : files) {

			String absPath = file.getAbsolutePath();
			// file ist ein Verzeichnis, also gehe eine Ebene tiefer
			if (file.isDirectory() && pathEntry.isSubDirectories()) {
				createFileMap(fileList, new PathEntry(true, file.getAbsolutePath(), true));
			}
			// file ist eine Datei, also füge zum Stack hinzu
			else {
				fileList.push(absPath);
				this.fileCount++;
			}
		}
	}
}package wordfinder.utils;

import java.io.File;
import java.io.IOException;

import org.apache.log4j.Logger;

import wordfinder.Constants;
import wordfinder.gui.models.WordFinderModelTest;

public class FileUtilTest {

	/** Logger für Debugausgaben */
	private static final Logger LOG = Logger.getLogger(FileUtilTest.class);

	/**
	 * @param args
	 * @throws IOException
	 */
	public static void main(String[] args) throws IOException {
		// TODO Auto-generated method stub
		WordFinderModelTest test = new WordFinderModelTest();
		readUTFTextFromFileTest();
		// gibUTF8Zeichen();
	}

	private static void gibUTF8Zeichen() {
		StringBuilder result = new StringBuilder();
		int j = 0;
		for (int i = 0; i < Integer.MAX_VALUE; i++) {
			if ((char) i == (char) 0) {
				j++;
				if (j > 1) {
					System.out.println(i);
					break;
				}
			}
			result.append(i + "=" + (char) i + "	");
			if (i % 10 == 0) {
				result.append(Constants.LINESEPARATOR);
			}
		}
		FileUtil.writeUTF8TextToFile(result.toString(), "C:\\unicode\\unicodezeichen.txt");
	}

	private static void readUTFTextFromFileTest() {
		String testdatei1 = "E:\\neo\\quellen\\muster\\Zwölf Boxkämpfer jagen Viktor quer über den großen Sylter Deich.txt";
		readUTFTextFromFileTestAllgemein(new File(testdatei1));
		String testdatei2 = "E:\\neo\\quellen\\muster\\UFT-8-test.txt";
		readUTFTextFromFileTestAllgemein(new File(testdatei2));
		String testdatei3 = "E:\\neo\\quellen\\muster\\ANSI-Zwölf Boxkämpfer jagen Viktor quer über den großen Sylter Deich.txt";
		readUTFTextFromFileTestAllgemein(new File(testdatei3));
	}

	private static void readUTFTextFromFileTestAllgemein(File file) {
		LOG
				.trace(("Bevor die Datei eingelesen wird empfiehlt sich zu kontrollieren ob der Ausgabebereich UTF-8-codierte Zeichen anzeigen kann:")
						+ (Constants.LINESEPARATOR + "Copyright, Ohm, Plus, Euro, Pfund: ")
						+ ((char) 169 + ", " + (char) 937 + ", " + (char) 5161 + ", " + (char) 8364 + ", " + (char) 65505)
						+ Constants.LINESEPARATOR
						+ "Wenn sie die genannten Zeichen nicht sehen, liegt ein Problem mit der Codierung ihres Ausgabebereiches vor.");
		LOG.trace(FileUtil.readUTFTextFromFile(file));
		LOG
				.trace(("Bevor die Datei eingelesen wird empfiehlt sich zu kontrollieren ob der Ausgabebereich UTF-8-codierte Zeichen anzeigen kann:")
						+ (Constants.LINESEPARATOR + "Copyright, Ohm, Plus, Euro, Pfund: ")
						+ ((char) 169 + ", " + (char) 937 + ", " + (char) 5161 + ", " + (char) 8364 + ", " + (char) 65505)
						+ Constants.LINESEPARATOR
						+ "Wenn sie die genannten Zeichen nicht sehen, liegt ein Problem mit der Codierung ihres Ausgabebereiches vor.");
	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

import org.apache.log4j.Logger;

import wordfinder.Constants;

/**
 * TODO Doku erstellen
 * 
 * @author Thomas
 * @since 10.04.2011
 */
public class HTMLFilter {

	/** Logger für Debugausgaben */
	private static final Logger LOG = Logger.getLogger(HTMLFilter.class);
	/** TODO Doku erstellen */
	private static final Character START_SYMBOL = '<';
	/** TODO Doku erstellen */
	private static final Character END_SYMBOL = '>';
	/** TODO Doku erstellen */
	private static final Character[] TAG_ENDING_SYMBOL = { ' ', '>', };

	/** TODO Doku erstellen */
	/**
	 * TODO Doku erstellen
	 * 
	 */
	public HTMLFilter() {
		super();
	}

	/**
	 * TODO Doku erstellen
	 * 
	 * @param source
	 * @return
	 */
	public String filterPage(String source) {
		StringBuilder result = new StringBuilder();

		int beginIndex = 0;
		int endIndex = 0;
		int tmpIndex = 0;
		boolean newLine = false;
		boolean spanClosed = false;

		// Suche nächsten möglichen Tag Beginn
		endIndex = source.indexOf(START_SYMBOL);
		while (endIndex != -1) {
			beginIndex = 0;

			// Übernehme Inhalt bis zum möglichen Tag Beginn
			String content = source.substring(beginIndex, endIndex);
			if (content.length() > 0) {
				result.append(content);
				newLine = false;
			}
			source = source.substring(endIndex);
			// LOG.trace("result = " + result.toString());

			// Ermittle das Schlüsselwort des Tags
			beginIndex = 0;
			endIndex = source.indexOf(TAG_ENDING_SYMBOL[0]);
			for (int i = 1; i < TAG_ENDING_SYMBOL.length; i++) {
				tmpIndex = source.indexOf(TAG_ENDING_SYMBOL[i]);
				if (tmpIndex < endIndex) {
					endIndex = tmpIndex;
				}
			}
			if (endIndex == -1) {
				continue;
			}

			// LOG.trace("beginIndex = " + beginIndex + " endIndex = " + endIndex);
			String tag = source.substring(beginIndex, endIndex);
			// LOG.trace("tag = " + tag);
			if (spanClosed && !tag.equals("<span")) {
				spanClosed = false;
			}
			if (tag.equals("</span")) {
				spanClosed = true;
			}

			if (tag.contains("<!--")) {
				// Entferne kompletten Kommentar
				endIndex = source.indexOf("-->") + "-->".length();
				source = source.substring(endIndex);
			} else if (tag.contains("<![CDATA")) {
				// Entferne kompletten CDATA
				endIndex = source.indexOf(END_SYMBOL) + 1;
				source = source.substring(endIndex);
			} else if (tag.contains("<style")) {
				// Entferne komplette STYLE Anweisung
				endIndex = source.indexOf("style>") + "style>".length();
				source = source.substring(endIndex);
			} else if (tag.contains("<script")) {
				// Entferne komplette STYLE Anweisung
				endIndex = source.indexOf("script>") + "script>".length();
				source = source.substring(endIndex);
			} else {

				// Ermittle das Ende des Tags
				endIndex = source.indexOf(END_SYMBOL) + 1;
				// LOG.trace("beginIndex = " + beginIndex + " endIndex = " + endIndex);
				// String html = source.substring(beginIndex, endIndex);
				// LOG.trace("html = " + html);

				source = source.substring(endIndex);
			}

			// Entferne Zeilenumbrüche direkt nach Tag Ende
			boolean newLineRemoved = false;
			while ((source.charAt(0) == 13)) {
				source = source.substring(2);
				newLineRemoved = true;
			}

			if ((source.charAt(0) != START_SYMBOL) && (source.charAt(0) != ' ') && (source.charAt(0) != '\t')) {
				if (newLineRemoved) {
					source = Constants.LINESEPARATOR + source; // TODO check it: vormals "\n", geändert in
																// Constants.LINESEPARATOR
				}
			}

			// Entferne Leerzeichen direkt nach Tag Ende
			boolean whiteSpaceRemoved = false;
			while ((source.charAt(0) == ' ') || (source.charAt(0) == '\t')) {
				source = source.substring(1);
				whiteSpaceRemoved = true;
			}

			if (source.charAt(0) != START_SYMBOL) {
				if (whiteSpaceRemoved || (spanClosed && tag.contains("<span"))) {
					source = " " + source;
				}
			}

			// Falls Tags benutzt wurden, die einen Zeilenumbruch zur Folge haben, so füge diesen ein
			if (tag.contains("<title") // 
					|| tag.contains("<h1") // 
					|| tag.contains("<h2") // 
					|| tag.contains("<h3") // 
					|| tag.contains("<h4") // 
					|| tag.contains("<h5") // 
					|| tag.contains("<h6") // 
					|| tag.contains("<br") // 
					|| tag.contains("<p") // 
					|| tag.contains("<li") // 
					|| tag.contains("<tr") // 
					|| tag.contains("<table") // 
					|| tag.contains("</table") // 
			) {
				result.append(Constants.LINESEPARATOR); // TODO check it: vormals "\n", geändert in
														// Constants.LINESEPARATOR
				newLine = true;
			}

			// Füge Leereichen ein, falls nötig
			if (tag.contains("<td") // 
			) {
				if (!newLine) {
					result.append(" ");
				}
			}

			// Suche nächsten möglichen Tag Beginn
			endIndex = source.indexOf(START_SYMBOL);
			// LOG.trace("endIndex = " + endIndex);

		}

		return result.toString();
	}
}
package wordfinder.utils;

import java.io.File;

import org.apache.log4j.Logger;

import wordfinder.gui.models.WordFinderModelTest;

public class HTMLFilterTest {

	/** Logger für Debugausgaben */
	private static final Logger LOG = Logger.getLogger(FileUtilTest.class);

	public static String filterpageTestAllgemein(String string) {
		LOG.trace("Eingabe = " + string);
		String result = new HTMLFilter().filterPage(string);
		LOG.trace("HTMLFilterausgabe = " + result);
		return result;
	}

	/**
	 * @param args
	 */
	public static void main(String[] args) {
		// TODO Auto-generated method stub
		WordFinderModelTest test = new WordFinderModelTest();
		filterpageTestAllgemein("<p>hallo<br> du <br /> da...</p>");
		// filterpageTestAllgemein("<div class=\"problem\"><p><strong>(2)</strong></p> <p>Das CSV-Format ist normalerweise noch etwas\nraffinierter als hier dargestellt; namentlich werden die einzelnen Felder noch in Anführungszeichen gesetzt, etwa\nso:<pre>\n\"Chomsky\",&#x00a0;\"Noam\",&#x00a0;\"617-555-4543\"\n</pre>\n</p>\n<p>Die Idee ist, dass die Abgrenzung der Felder besser wird. Kommata innerhalb der Anführungszeichen werden natürlich\nignoriert, und Whitespace sollte nicht relevant sein. Wie würde ein regulärer Ausdruck aussehen, der damit fertig wird?\nNoch gemeiner wird es, wenn man auch Anführungszeichen innerhalb der Werte zulassen möchte. Üblicherweise werden\ndiese &bdquo;escaped&rdquo;, also mit einem Backslash geschützt: <tt>[\"</tt>. Wenn man das hinkriegen möchte, werden die regulären\nAusdrücke schon ziemlich furchtbar (Tipp: man muss im Wesentlichen backslashes vor dem schließenden\nAnführungszeichen verbieten). </p>\n</div>");
		File datei = new File("E:\\neo\\quellen\\html\\Stille_Reserve_(Arbeitsmarkt).htm");
		String text = FileUtil.readUTFTextFromFile(datei);
		String filtered = filterpageTestAllgemein(text);
	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

import java.awt.Image;
import java.util.HashMap;
import java.util.Map;

import javax.swing.Icon;
import javax.swing.ImageIcon;

import org.apache.log4j.Logger;

public class IconFactory {

	/** Logger für Debugausgaben */
	private static final Logger LOG = Logger.getLogger(IconFactory.class);

	public static final String ICON_RED_CROSS = "redCross";
	public static final String ICON_DOCUMENT = "document";
	public static final String ICON_FOLDER = "folder";
	public static final String ICON_UP = "up";
	public static final String ICON_DOWN = "down";
	public static final String ICON_ADD_ALL = "addAll";
	public static final String ICON_ADD = "add";
	public static final String ICON_REMOVE = "remove";
	public static final String ICON_REMOVE_ALL = "removeAll";

	public static final int ID_DOCUMENT = 1;
	public static final int ID_FOLDER = 2;
	public static final int ID_DELETE = 3;

	private static IconFactory instance = new IconFactory();

	public static IconFactory getInstance() {
		if (instance == null) {
			instance = new IconFactory();
		}
		return instance;
	}

	private Map<String, Icon> icons;
	private Map<String, Image> images;

	private IconFactory() {
		loadIcons();
		loadImages();
	}

	public Icon getIcon(String key) {
		return this.icons.get(key);
	}

	public Icon getIconById(Integer id) {
		if (id == null) {
			return null;
		}

		switch (id) {
		case ID_DOCUMENT:
			return getIcon(IconFactory.ICON_DOCUMENT);
		case ID_FOLDER:
			return getIcon(IconFactory.ICON_FOLDER);
		case ID_DELETE:
			return getIcon(IconFactory.ICON_RED_CROSS);

		default:
			return null;
		}
	}

	public Image getImage(String key) {
		return this.images.get(key);
	}

	private void loadIcons() {
		this.icons = new HashMap<String, Icon>();
		this.icons.put(ICON_RED_CROSS, new ImageIcon(getClass().getResource("/images/redCross.png")));
		this.icons.put(ICON_DOCUMENT, new ImageIcon(getClass().getResource("/images/document.png")));
		this.icons.put(ICON_FOLDER, new ImageIcon(getClass().getResource("/images/folder.png")));
		this.icons.put(ICON_UP, new ImageIcon(getClass().getResource("/images/up.png")));
		this.icons.put(ICON_DOWN, new ImageIcon(getClass().getResource("/images/down.png")));
		this.icons.put(ICON_ADD_ALL, new ImageIcon(getClass().getResource("/images/addAll.png")));
		this.icons.put(ICON_ADD, new ImageIcon(getClass().getResource("/images/add.png")));
		this.icons.put(ICON_REMOVE, new ImageIcon(getClass().getResource("/images/remove.png")));
		this.icons.put(ICON_REMOVE_ALL, new ImageIcon(getClass().getResource("/images/removeAll.png")));
	}

	private void loadImages() {
		this.images = new HashMap<String, Image>();
		// try {
		// this.images.put(IMAGE_LOGO, ImageIO.read(getClass().getResourceAsStream("/graphics/logo.png")));
		// } catch (IOException e) {
		// LOG.error(e.getMessage(), e);
		// }
	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

import java.awt.Color;
import java.awt.GridBagConstraints;
import java.awt.Insets;

import javax.swing.JComponent;
import javax.swing.UIManager;

/**
 * In diesem Util befinden sich nützliche Methoden, die in keines der anderen Utils reinpasst und Anwendungsweit benutzt
 * werden.
 * 
 * @author Morrigan
 * @since 20.04.2011
 */
public class MiscUtil {

	/** Einzige Instanz dieser Klasse */
	private static MiscUtil instance = null;

	public static MiscUtil getInstance() {
		if (instance == null) {
			instance = new MiscUtil();
		}
		return instance;
	}

	/** Privater Singleton Konstruktor */
	private MiscUtil() {
		super();
	}

	public void disableComponent(JComponent component) {
		component.setEnabled(false);
		component.setBackground(Color.LIGHT_GRAY);
	}

	public void enableComponent(JComponent component) {
		component.setEnabled(true);
		component.setBackground(UIManager.getColor("TextField.background"));
	}

	/**
	 * Diese Methode konfiguriert ein GridbagConstraints Objekt.
	 * 
	 * @param gbc Das zu konfigurierende GridbagConstraint
	 * @param gridx {@link GridBagConstraints#gridx}
	 * @param gridy {@link GridBagConstraints#gridy}
	 * @param anchor {@link GridBagConstraints#anchor}
	 * @param fill {@link GridBagConstraints#fill}
	 * @param weightx {@link GridBagConstraints#weightx}
	 * @param weighty {@link GridBagConstraints#weighty}
	 * @param gridwidth {@link GridBagConstraints#gridwidth}
	 * @param gridheight {@link GridBagConstraints#gridheight}
	 * @param insets {@link GridBagConstraints#insets}
	 */
	public void setGBC(GridBagConstraints gbc, int gridx, int gridy, int anchor, int fill, double weightx,
			double weighty, int gridwidth, int gridheight, Insets insets) {
		gbc.gridx = gridx;
		gbc.gridy = gridy;
		gbc.anchor = anchor;
		gbc.fill = fill;
		gbc.weightx = weightx;
		gbc.weighty = weighty;
		gbc.gridwidth = gridwidth;
		gbc.gridheight = gridheight;
		gbc.insets = insets;
	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.log4j.Logger;

/**
 * TODO Doku erstellen
 * 
 * @author Michael
 * @since xx.xx.2011
 */
public class NeoUtil {

	/** Logger für Debugausgaben */
	private static final Logger LOG = Logger.getLogger(NeoUtil.class);
	/** TODO Doku erstellen */
	private static final String NEO_HTML_AUSGABEDATEI = "C:/woerterzaehlung.txt";

	/**
	 * TODO Dokumentation erstellen!
	 * 
	 * @param wordList
	 * @param LISTOFneo
	 * @param minlettercount
	 * @param newchars
	 * @param schreibeanzahl
	 * @param ausgabetext
	 * @param minwordlength
	 * @param maxwordlength
	 * @param minwordcount
	 * @throws IOException
	 */
	private void buchstabenAuswertungFuerNeo(List<WordCount> wordList, List<Character> LISTOFneo, int minlettercount,
			char[] newchars, boolean schreibeanzahl, String ausgabetext, int minwordlength, int maxwordlength,
			int minwordcount) throws IOException {
		boolean isneoword;
		int hasnewletters;
		int aktwordlaenge;
		// System.out.println(ausgabetext);
		// SchreibeInDatei.dateischreiben(ausgabetext,"C:/woerterzaehlung.txt");
		for (WordCount wordCount : wordList) {
			aktwordlaenge = wordCount.getWord().length();
			if ((aktwordlaenge >= minwordlength) && (aktwordlaenge <= maxwordlength)
					&& (wordCount.getCount() >= minwordcount)) {
				isneoword = true;
				hasnewletters = 0;
				for (int i = 0; i < aktwordlaenge; i++) {
					if (!LISTOFneo.contains(wordCount.getWord().charAt(i))) {
						isneoword = false;
						break;
					} else {
						for (int j = 0; j < newchars.length; j++) {
							if (wordCount.getWord().charAt(i) == newchars[j]) {
								hasnewletters++;
								break;
							}
						}
					}
				}
				if (isneoword && (hasnewletters >= minlettercount)) {
					if (schreibeanzahl) {
						// System.out.println(wordCount.getWord() + " (" + wordCount.getCount() + ")");
						FileUtil.writeUTF8TextToFile(wordCount.getWord() + " (" + wordCount.getCount() + ")" /**/,
								NEO_HTML_AUSGABEDATEI);
					} else {
						// System.out.println(wordCount.getWord()/* + " (" + wordCount.getCount() + ")"*/);
						FileUtil.writeUTF8TextToFile(wordCount.getWord()/* + " (" + wordCount.getCount() + ")" / */,
								NEO_HTML_AUSGABEDATEI);
					}
				}
			}
		}
		FileUtil.writeUTF8TextToFile("</pre></td>", NEO_HTML_AUSGABEDATEI);
		FileUtil.writeUTF8TextToFile("<td><pre>", NEO_HTML_AUSGABEDATEI);
	}

	/**
	 * TODO Dokumentation erstellen!
	 * 
	 * @param wordList
	 * @param LISTOFneo
	 * @param givenchars
	 * @param newchars
	 * @param minlettercount
	 * @param maxlettercount
	 * @param minwordlength
	 * @param maxwordlength
	 * @param minwordcount
	 * @throws IOException
	 */
	private String neoCharsAddenUndAuswertungStarten(List<WordCount> wordList, List<Character> LISTOFneo,
			String givenchars, char[] newchars, int minlettercount, int maxlettercount, int minwordlength,
			int maxwordlength, int minwordcount) throws IOException {
		String ausgabetext;
		String chars = "";
		for (int i = 0; i <= newchars.length; i++) {
			LISTOFneo.add(newchars[i]);
			chars += newchars[i] + ",";
		}
		chars = chars.substring(0, chars.length() - 1 - 1); // letztes komma l�schen
		FileUtil.writeUTF8TextToFile("<td bgcolor='lime'><pre>", NEO_HTML_AUSGABEDATEI);
		for (int min = minlettercount; min <= maxlettercount; min++) {
			ausgabetext = givenchars + "   |" + chars + " mit minimum " + min + " vorkommen der neuen Buchstaben ("
					+ chars + ") mit Anzahlausgabe:";
			buchstabenAuswertungFuerNeo(wordList, LISTOFneo, min, newchars, true, ausgabetext, minwordlength,
					maxwordlength, minwordcount);
			ausgabetext = givenchars + "   |" + chars + " mit minimum " + min + " vorkommen der neuen Buchstaben ("
					+ chars + ") ohne Anzahlausgabe:";
			buchstabenAuswertungFuerNeo(wordList, LISTOFneo, min, newchars, false, ausgabetext, minwordlength,
					maxwordlength, minwordcount);
		}
		FileUtil.writeUTF8TextToFile("</pre></td>", NEO_HTML_AUSGABEDATEI);
		System.out.println(chars + " Fertig");
		return "," + chars;
	}

	/**
	 * TODO Dokumentation erstellen!
	 * 
	 * @param wordList
	 * @throws IOException
	 */
	private void neoLayoutCheckerByMichael(List<WordCount> wordList) throws IOException {
		List<Character> LISTOFneo = new ArrayList<Character>();
		LISTOFneo.clear();
		String givenchars = "";
		int minlettercount = 1;
		int maxlettercount = 5;
		int minwordlength = 1;
		int maxwordlength = 100;
		int minwordcount = 1;

		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { 'e', 'n', 'i',
				't' }, minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { 'a', 'r' },
				minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { 'u', 'd' },
				minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { 'o', 's' },
				minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { 'c', 'h' },
				minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { 'l', 'g' },
				minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { 'E', 'N', 'I',
				'T', 'A', 'R', 'U', 'D', 'O', 'S', 'C', 'H', 'L', 'G' }, 1, 1, minwordlength, maxwordlength,
				minwordcount);

		givenchars = "e,E,n,N,i,I,t,T,a,A,r,R,u,U,d,D,o,O,s,S,c,C,h,H,l,L,g,G";

		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { 'p', 'm', 'P',
				'M' }, minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { 'z', 'b', 'Z',
				'B' }, minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { 'w', 'b', 'W',
				'B' }, minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { 'v', 'f', 'V',
				'F' }, minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { '�', 'j', '�',
				'J' }, minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { '�', '�', '�',
				'�' }, minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { '�', 'y', '�',
				'Y' }, minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { 'x', 'q', 'X',
				'Q' }, minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { '1', '2' },
				minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { '3', '0' },
				minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { '4', '9' },
				minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { '5', '8' },
				minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
		givenchars += neoCharsAddenUndAuswertungStarten(wordList, LISTOFneo, givenchars, new char[] { '6', '7' },
				minlettercount, maxlettercount, minwordlength, maxwordlength, minwordcount);
	}
}/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

import org.apache.log4j.Logger;

/**
 * TODO Doku erstellen
 * 
 * @author Michael
 * @since 11.04.2011
 */
public class ParseUtil {

	/** Logger für Debugausgaben */
	private static final Logger LOG = Logger.getLogger(ParseUtil.class);

}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

import java.io.IOException;
import java.io.Serializable;
import java.util.MissingResourceException;
import java.util.PropertyResourceBundle;
import java.util.ResourceBundle;

import org.apache.log4j.Logger;

import wordfinder.gui.ErrorMessageDisplayer;

public class ResourceManager
		implements Serializable {

	public static enum LANGUAGE {
		GERMAN, ENGLISH
	}

	/** Logger für Debugausgaben */
	private static final Logger LOG = Logger.getLogger(ResourceManager.class);

	public static final String SEPERATOR = ":";

	private static final long serialVersionUID = 6265347322096512469L;

	/** Einzige Instanz dieser Klasse mit Singleton-Design-Pattern. */
	private static final ResourceManager INSTANZ = new ResourceManager();
	private static final String BUNDLE_DE_PROPERTIES = "/properties/wordFinder_de.properties";
	private static final String BUNDLE_EN_PROPERTIES = "/properties/wordFinder_en.properties";
	private static final String BUNDLE_MNEMONIC = "/properties/mnemonics.properties";
	private static final String BUNDLE_ERROR_MESSAGES = "/properties/errorMessages.properties";
	private static final String BUNDLE_TOOLTIP = "/properties/tooltips.properties";

	/**
	 * Diese Methode implementiert das Singleton Design-Pattern. Sie sorgt daf�r, dass Instanzen dieser Klasse nur �ber
	 * sie bezogen werden k�nnen. Mit Hilfe der statischen Variable "instanz" gibt sie immer die gleiche Instanz zur�ck
	 * 
	 * @return gibt die einzige Instanz des RouletteResourceManager zur�ck.
	 */
	public static ResourceManager getInstance() {
		return INSTANZ;
	}

	/** Resourcebundle "wordFinder_xx.properties" */
	private ResourceBundle bundle;
	/** Resourcebundle "mnemonics.properties" */
	private ResourceBundle mnemonics;
	/** Resourcebundle "errorMessages.properties" */
	private ResourceBundle errorMessages;
	/** Resourcebundle "tooltips.properties" */
	private ResourceBundle tooltips;

	/**
	 * Dieser private Konstruktor des RouletteResourceManager ist Teil des Singleton Design-Patterns. Er wird einmalig
	 * aus der Methode getInstance() aufgerufen und die Instanz, die der Konstruktor erzeugt, anschlie�end in der
	 * statischen Variable "instanz" gespeichert.
	 */
	private ResourceManager() {
		try {
			this.bundle = new PropertyResourceBundle(getClass().getResourceAsStream(BUNDLE_DE_PROPERTIES));
		} catch (IOException e) {
			ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
			e.printStackTrace();
		}
		try {
			this.mnemonics = new PropertyResourceBundle(getClass().getResourceAsStream(BUNDLE_MNEMONIC));
		} catch (IOException e) {
			ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
			e.printStackTrace();
		}
		try {
			this.errorMessages = new PropertyResourceBundle(getClass().getResourceAsStream(BUNDLE_ERROR_MESSAGES));
		} catch (IOException e) {
			ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
			e.printStackTrace();
		}
		try {
			this.tooltips = new PropertyResourceBundle(getClass().getResourceAsStream(BUNDLE_TOOLTIP));
		} catch (IOException e) {
			ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
			e.printStackTrace();
		}
	}

	public ResourceBundle getBundle() {
		return this.bundle;
	}

	public String getErrorMessages(int key) {
		try {
			return this.errorMessages.getString(Integer.toString(key));
		} catch (final MissingResourceException e) {
			System.err.println("Fehlender Property-Key: " + key);
			return " ";
		}
	}

	public ResourceBundle getMnemonics() {
		return this.mnemonics;
	}

	public String getString(final String key) {
		if ((key == null) || "".equals(key.trim())) {
			return "";
		}
		try {
			return this.bundle.getString(key);
		} catch (final MissingResourceException e) {
			System.err.println("Fehlender Property-Key: " + key);
			return " ";
		}
	}

	public String getStringWithSeperator(final String key) {
		return getString(key) + SEPERATOR;
	}

	public ResourceBundle getTooltips() {
		return this.tooltips;
	}

	public void switchLanguage(LANGUAGE lang) {
		switch (lang) {
		case GERMAN:
			try {
				this.bundle = new PropertyResourceBundle(getClass().getResourceAsStream(BUNDLE_DE_PROPERTIES));
			} catch (IOException e) {
				ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
				e.printStackTrace();
			}
			break;
		case ENGLISH:
			try {
				this.bundle = new PropertyResourceBundle(getClass().getResourceAsStream(BUNDLE_EN_PROPERTIES));
			} catch (IOException e) {
				ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
				e.printStackTrace();
			}
			break;
		default:
			try {
				this.bundle = new PropertyResourceBundle(getClass().getResourceAsStream(BUNDLE_DE_PROPERTIES));
			} catch (IOException e) {
				ErrorMessageDisplayer.getInstance().showErrorMessage(1, e);
				e.printStackTrace();
			}
			break;
		}
	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

/**
Original pseudocode   : Thomas Weidenfeller
Implementation tweaked: Aki Nieminen

http://www.unicode.org/unicode/faq/utf_bom.html
BOMs:
00 00 FE FF    = UTF-32, big-endian
FF FE 00 00    = UTF-32, little-endian
FE FF          = UTF-16, big-endian
FF FE          = UTF-16, little-endian
EF BB BF       = UTF-8

Win2k Notepad:
Unicode format = UTF-16LE
***/

import java.io.*;

/**
* Generic unicode textreader, which will use BOM mark
* to identify the encoding to be used. If BOM is not found
* then use a given default encoding.
* System default is used if:
*    BOM mark is not found and defaultEnc is NULL
*
* Usage pattern:
String defaultEnc = "ISO-8859-1"; // or NULL to use system default
FileInputStream fis = new FileInputStream(file);
Reader in = new UnicodeReader(fis, defaultEnc);
*/
public class UnicodeReader extends Reader {
   PushbackInputStream internalIn;

   InputStreamReader internalIn2 = null;

   String defaultEnc;

   private static final int BOM_SIZE = 4;

   UnicodeReader(InputStream in, String defaultEnc) {
       internalIn = new PushbackInputStream(in, BOM_SIZE);
       this.defaultEnc = defaultEnc;
   }

   public String getDefaultEncoding() {
       return defaultEnc;
   }

   public String getEncoding() {
       if (internalIn2 == null)
           return null;
       return internalIn2.getEncoding();
   }

   /**
    * Read-ahead four bytes and check for BOM marks. Extra bytes are
    * unread back to the stream, only BOM bytes are skipped.
    */
   protected void init() throws IOException {
       if (internalIn2 != null)
           return;

       String encoding;
       byte bom[] = new byte[BOM_SIZE];
       int n, unread;
       n = internalIn.read(bom, 0, bom.length);

       if ((bom[0] == (byte) 0xEF) && (bom[1] == (byte) 0xBB)
               && (bom[2] == (byte) 0xBF)) {
           encoding = "UTF-8";
           unread = n - 3;
       } else if ((bom[0] == (byte) 0xFE) && (bom[1] == (byte) 0xFF)) {
           encoding = "UTF-16BE";
           unread = n - 2;
       } else if ((bom[0] == (byte) 0xFF) && (bom[1] == (byte) 0xFE)) {
           encoding = "UTF-16LE";
           unread = n - 2;
       } else if ((bom[0] == (byte) 0x00) && (bom[1] == (byte) 0x00)
               && (bom[2] == (byte) 0xFE) && (bom[3] == (byte) 0xFF)) {
           encoding = "UTF-32BE";
           unread = n - 4;
       } else if ((bom[0] == (byte) 0xFF) && (bom[1] == (byte) 0xFE)
               && (bom[2] == (byte) 0x00) && (bom[3] == (byte) 0x00)) {
           encoding = "UTF-32LE";
           unread = n - 4;
       } else {
           // Unicode BOM mark not found, unread all bytes
           encoding = defaultEnc;
           unread = n;
       }
       //      System.out.println("read=" + n + ", unread=" + unread);

       if (unread > 0)
           internalIn.unread(bom, (n - unread), unread);

       // Use given encoding
       if (encoding == null) {
           internalIn2 = new InputStreamReader(internalIn);
       } else {
           internalIn2 = new InputStreamReader(internalIn, encoding);
       }
   }

   public void close() throws IOException {
       init();
       internalIn2.close();
   }

   public int read(char[] cbuf, int off, int len) throws IOException {
       init();
       return internalIn2.read(cbuf, off, len);
   }
} 
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

import org.apache.log4j.Logger;

import wordfinder.gui.models.WordFinderModel;

/**
 * TODO Doku erstellen
 * 
 * @author Thomas
 * @since xx.xx.2011
 */
public class WordCount
		implements Comparable<WordCount> {

	/** Logger für Debugausgaben */
	private static final Logger LOG = Logger.getLogger(WordCount.class);

	/** Legt die länge des Symbols auf -1 wenn Wörter gezählt werden, da bei Wörtern die Länge nicht benutzt wird. */
	public static final int NO_LENGTH = -1;
	/** TODO Doku erstellen */
	private final String word;
	/** TODO Doku erstellen */
	private int count;
	/** legt fest zu welcher n-Gramm länge dieses WordCount-Objekt gehört */
	private int symbollength;
	/** TODO Doku erstellen */
	private WordFinderModel model;

	/**
	 * TODO Doku erstellen
	 * 
	 * @param word
	 * @param model
	 * @param symbolFlag
	 */

	public WordCount(String word, WordFinderModel model) {
		this.word = word;
		this.count = 1;
		this.symbollength = NO_LENGTH;
		this.model = model;
	}

	public WordCount(String word, WordFinderModel model, int symbollength) {
		if (symbollength < 1) {
			throw new IllegalArgumentException("symbollaenge muss größer gleich 1 sein.");
		}
		this.word = word;
		this.count = 1;
		this.symbollength = symbollength;
		this.model = model;
	}

	/** Absteigende Sortierung nach der H�ufigkeit eines Wortes */
	/**
	 * TODO Doku erstellen
	 * 
	 */
	public int compareTo(WordCount o) {
		if (this.count < o.count) {
			return 1;
		} else if (this.count > o.count) {
			return -1;
		}
		return 0;
	}

	@Override
	public boolean equals(Object obj) {
		if (this == obj) {
			return true;
		}
		if (obj == null) {
			return false;
		}
		if (getClass() != obj.getClass()) {
			return false;
		}
		WordCount other = (WordCount) obj;
		if (this.symbollength != other.symbollength) {
			return false;
		}
		if (this.word == null) {
			if (other.word != null) {
				return false;
			}
		} else if (!this.word.equals(other.word)) {
			return false;
		}
		return true;
	}

	public int getCount() {
		return this.count;
	}

	public int getSymbollength() {
		return this.symbollength;
	}

	public String getWord() {
		return this.word;
	}

	public void incCount() {
		this.count++;
	}

}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

import java.util.Comparator;

public class WordCountCountComparator
		implements Comparator<WordCount> {

	public int compare(WordCount o1, WordCount o2) {
		if (o1.getCount() < o2.getCount()) {
			return 1;
		} else if (o1.getCount() > o2.getCount()) {
			return -1;
		}
		return 0;
	}

}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder.utils;

import java.util.Comparator;

public class WordCountNameComparator
		implements Comparator<WordCount> {

	public int compare(WordCount o1, WordCount o2) {
		return o1.getWord().compareTo(o2.getWord());
	}

}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package bastelstube;

import java.awt.BorderLayout;
import java.awt.event.ActionEvent;
import java.awt.event.ActionListener;
import java.util.ArrayList;
import java.util.List;

import javax.swing.JButton;
import javax.swing.JFrame;
import javax.swing.JScrollPane;
import javax.swing.JTable;
import javax.swing.table.AbstractTableModel;

import wordfinder.gui.models.entries.PathEntry;

public class SparseTest {
	public static void main(String args[]) {
		JFrame frame = new JFrame("Sparse Test");
		final List<PathEntry> list = new ArrayList<PathEntry>();
		list.add(new PathEntry(false, "C:\\test", true));
		final TestModel model = new TestModel();
		JTable table = new JTable(model);

		// model.setValueAt("one", 0, 0);
		// model.setValueAt("ten", 9, 0);
		// model.setValueAt("roku - \u516D", 5, 1);
		// model.setValueAt("hachi - \u516B", 8, 1);

		JButton btAdd = new JButton("add");
		btAdd.addActionListener(new ActionListener() {

			public void actionPerformed(ActionEvent e) {
				list.add(new PathEntry(true, "scheiße", false));
				model.setData(list);
			}
		});

		JScrollPane scrollPane = new JScrollPane(table);
		frame.getContentPane().add(scrollPane, BorderLayout.CENTER);
		frame.getContentPane().add(btAdd, BorderLayout.SOUTH);
		frame.setSize(300, 150);
		frame.setVisible(true);
	}
}

abstract class SparseTableModel<T>
		extends AbstractTableModel {

	protected List<T> lookup;

	private final String headers[];

	public SparseTableModel(String columnHeaders[]) {
		this.headers = columnHeaders;
		this.lookup = new ArrayList<T>();
	}

	public int getColumnCount() {
		return this.headers.length;
	}

	@Override
	public String getColumnName(int column) {
		return this.headers[column];
	}

	public int getRowCount() {
		return this.lookup.size();
	}

	public void setData(List<T> list) {
		this.lookup = list;
		fireTableDataChanged();
	}

	// @Override
	// public void setValueAt(Object value, int row, int column) {
	// if ((this.rows < 0) || (this.columns < 0)) {
	// throw new IllegalArgumentException("Invalid row/column setting");
	// }
	// if ((row < this.rows) && (column < this.columns)) {
	// this.lookup.put(new Point(row, column), value);
	// }
	// }
}

class TestModel
		extends SparseTableModel<PathEntry> {

	public TestModel() {
		super(new String[] { "Icon", "Unterverzeichnisse", "Pfad" });
	}

	public Object getValueAt(int row, int column) {
		switch (column) {
		case 0:
			return this.lookup.get(row).isDirectoryFlag();
		case 1:
			return this.lookup.get(row).isSubDirectories();
		case 2:
			return this.lookup.get(row).getPath();

		default:
			return null;
		}
	}
}/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package bastelstube;

import htmlparser.HTMLEntities;

import java.io.File;
import java.util.List;
import java.util.Map;

import org.apache.log4j.Logger;

import wordfinder.exceptions.WordFinderException;
import wordfinder.gui.models.WordFinderModel;
import wordfinder.gui.models.entries.AnalysisRule;
import wordfinder.gui.models.entries.PathEntry;
import wordfinder.utils.FileResult;
import wordfinder.utils.FileUtil;
import wordfinder.utils.WordCount;

/**
 * TODO Doku erstellen
 * 
 * @author Michael
 * @since 11.04.2011
 */
public class entitiparser {

	/** Logger f�r Debugausgaben */
	private static final Logger LOG = Logger.getLogger(entitiparser.class);

	/**
	 * TODO Doku erstellen
	 * 
	 * @param args
	 * @throws Exception
	 */
	public static void main(String[] args) throws Exception {
		new entitiparser();
	}

	/**
	 * TODO Doku erstellen
	 * 
	 * @throws Exception
	 * 
	 */
	public entitiparser() throws Exception {
		super();

		// TODO Auto-generated method stub
		//
		// File[] files = FileUtil.giveFilesInDirectory("./src/main/java/bastelstube/arbeitsbereich/");
		// // File[] files = hallo.giveFiles(getClass().getResource("/bastelstube/arbeitsbereich/").getPath());
		// String[] texte = new String[10];
		// int i = 0;
		// for (File file : files) {
		// texte[i] = FileUtil.readUTF8TextFromFile(file);
		// // String Filepath = file.getPath().substring(0, file.getPath().length() - file.getName().length())
		// String Filepath = "./src/main/java/bastelstube/ergebnisbereich/" + "_entitiefree_" + file.getName();
		// System.out.println(Filepath);
		// // File newfile = new File(Filepath);
		// texte[i] = HTMLEntities.unhtmlentities(texte[i]);
		// texte[i] = HTMLEntities.unhtmlAmpersand(texte[i]);
		// texte[i] = HTMLEntities.unhtmlAngleBrackets(texte[i]);
		// texte[i] = HTMLEntities.unhtmlQuotes(texte[i]);
		// FileUtil.writeToFile("", new File(Filepath), true);
		// FileUtil.writeUTF8TextToFile(texte[i], Filepath);
		// i++;
		// }
		// // HTMLEntities hallo1 = new HTMLEntities();
		// // System.out.println(texte[0]);
		// StringBuilder stringbuild = new StringBuilder();
		// stringbuild.append(HTMLEntities.unhtmlentities(texte[0])); //
		// System.out.println(HTMLEntities.unhtmlentities(texte[0]));

		FileUtil vFileUtil = new FileUtil();
		FileResult fileandtext;
		File file;
		String text;
		String Resultpath;
		StringBuilder holeTextOfAllFiles = new StringBuilder();
		fileandtext = vFileUtil.getNextFile(new PathEntry(true, "./src/main/java/bastelstube/arbeitsbereich/", false));
		while (fileandtext != null) {
			file = fileandtext.getFile();
			text = fileandtext.getContent();
			text = HTMLEntities.unhtmlentities(text);
			text = HTMLEntities.unhtmlAmpersand(text);
			text = HTMLEntities.unhtmlAngleBrackets(text);
			text = HTMLEntities.unhtmlQuotes(text);
			Resultpath = "./src/main/java/bastelstube/ergebnisbereich/" + "entitiefree_" + file.getName();
			FileUtil.writeUTF8TextToFile(text, Resultpath);
			holeTextOfAllFiles.append(text);

			fileandtext = vFileUtil.getNextFile(new PathEntry(true, "./src/main/java/bastelstube/arbeitsbereich/",
					false));
		}

		WordFinderModel wordmodel;
		try {
			wordmodel = new WordFinderModel();

			Map<String, WordCount> words = wordmodel.countSymbols(holeTextOfAllFiles, 1);

			StringBuilder result = new StringBuilder();

			List<WordCount> wordList = wordmodel.getWordList(AnalysisRule.SORT.COUNT);
			for (WordCount wordCount : wordList) {
				if ((wordCount.getWord().length() > -1) && (wordCount.getWord().length() < 40)
						&& (wordCount.getCount() > -1)) {
					result.append(wordCount.getWord());
					result.append(" (");
					result.append(wordCount.getCount());
					result.append(")");
					// result.append("\n");

					// System.out.println(wordCount.getWord() + " (" + wordCount.getCount() + ")");
					FileUtil.writeUTF8TextToFile(wordCount.getWord()/**/+ " (" + wordCount.getCount() + ")" /**/,
							"E:/wordcountresult.txt");
				}
			}
			System.out.println(result);
			String res = result.toString();
			FileUtil.writeUTF8TextToFile(res, "E:/wordcount.txt");

		} catch (WordFinderException e) {
			LOG.error(e.getMessage(), e);
		}

	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package bastelstube;

import htmlparser.HTMLInputFilter;

import java.io.File;
import java.util.List;

import org.apache.log4j.Logger;

import wordfinder.Constants;
import wordfinder.gui.models.entries.PathEntry;
import wordfinder.utils.FileResult;
import wordfinder.utils.FileUtil;

public class htmlparser {

	/** Logger für Debugausgaben */
	private static final Logger LOG = Logger.getLogger(htmlparser.class);
	public static final String SRC_MAIN_JAVA_BASTELSTUBE_ARBEITSBEREICH = "./src/main/java/bastelstube/arbeitsbereich/";

	/**
	 * @param args
	 */
	public static void main(String[] args) {
		// TODO Auto-generated method stub
		new htmlparser();
	}

	public htmlparser() {
		super();
		HTMLInputFilter filt = new HTMLInputFilter();

		FileUtil vFileUtil = new FileUtil();
		FileResult fileandtext;
		File file;
		String text;
		String Resultpath;
		StringBuilder holeTextOfAllFiles = new StringBuilder();
		String line = Constants.LINESEPARATOR;
		String filteredtext;
		boolean[] methods = new boolean[] { true, true, true, true, true };
		fileandtext = vFileUtil.getNextFile(new PathEntry(true, SRC_MAIN_JAVA_BASTELSTUBE_ARBEITSBEREICH, true));
		while (fileandtext != null) {
			// file = (File) fileandtext[1];
			// text = (String) fileandtext[0];
			// // for (int i = 1; i < 33; i++) {
			// methods[4] = !methods[4];
			// if (i % 2 == 0) {
			// methods[3] = !methods[3];
			// }
			// if (i % 4 == 0) {
			// methods[2] = !methods[2];
			// }
			// if (i % 8 == 0) {
			// methods[1] = !methods[1];
			// }
			// if (i % 16 == 0) {
			// methods[0] = !methods[0];
			// }
			//
			// System.out.println(Arrays.toString(methods));
			// filteredtext = filt.filter(text, methods);
			// Resultpath = "./src/main/java/bastelstube/ergebnisbereich/" + "entitiefree_" + file.getName();
			// // FileUtil.writeToFile("", new File(Resultpath), true);
			//
			// FileUtil.writeUTF8TextToFile(line + "#############################", Resultpath);
			// FileUtil.writeUTF8TextToFile(line + Arrays.toString(methods), Resultpath);
			// FileUtil.writeUTF8TextToFile(line + "#############################", Resultpath);
			// FileUtil.writeUTF8TextToFile(line + filteredtext, Resultpath);
			// FileUtil.writeUTF8TextToFile(line + "#############################", Resultpath);

			// filteredtext = stripHtmlTags(text);
			// Resultpath = "./src/main/java/bastelstube/ergebnisbereich/" + "entitiefree_" + file.getName();
			// FileUtil.writeUTF8TextToFile(filteredtext, Resultpath);

			fileandtext = vFileUtil.getNextFile(new PathEntry(true, SRC_MAIN_JAVA_BASTELSTUBE_ARBEITSBEREICH, true));
		}

		// holeTextOfAllFiles.append(text);
	}

	public htmlparser(String path) {
		List<String> dirs = FileUtil.giveRekursivDirectorys(path);
		FileResult fileandtext;
		FileUtil vFileUtil = new FileUtil();
		for (String aktpath : dirs) {
			fileandtext = vFileUtil.getNextFile(new PathEntry(true, aktpath, true));
			while (fileandtext != null) {
				fileandtext = vFileUtil.getNextFile(new PathEntry(true, aktpath, true));
			}
		}

	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package htmlparser;

import java.util.Hashtable;

/**
 * Collection of static methods to convert special and extended characters into HTML entitities and vice
 * versa.<br/><br/> Copyright (c) 2004-2005 Tecnick.com S.r.l (www.tecnick.com) Via Ugo Foscolo n.19 - 09045 Quartu
 * Sant'Elena (CA) - ITALY - www.tecnick.com - info@tecnick.com<br/> Project homepage: <a
 * href="http://htmlentities.sourceforge.net" target="_blank">http://htmlentities.sourceforge.net</a><br/> License:
 * http://www.gnu.org/copyleft/lesser.html LGPL
 * 
 * @author Nicola Asuni [www.tecnick.com].
 * @version 1.0.004
 */
public class HTMLEntities {

	/**
	 * Translation table for HTML entities.<br/> reference: W3C - Character entity references in HTML 4 [<a
	 * href="http://www.w3.org/TR/html401/sgml/entities.html"
	 * target="_blank">http://www.w3.org/TR/html401/sgml/entities.html</a>].
	 */
	private static final Object[][] html_entities_table = { { new String("&Aacute;"), new Integer(193) },
			{ new String("&aacute;"), new Integer(225) }, { new String("&Acirc;"), new Integer(194) },
			{ new String("&acirc;"), new Integer(226) }, { new String("&acute;"), new Integer(180) },
			{ new String("&AElig;"), new Integer(198) }, { new String("&aelig;"), new Integer(230) },
			{ new String("&Agrave;"), new Integer(192) }, { new String("&agrave;"), new Integer(224) },
			{ new String("&alefsym;"), new Integer(8501) }, { new String("&Alpha;"), new Integer(913) },
			{ new String("&alpha;"), new Integer(945) }, { new String("&amp;"), new Integer(38) },
			{ new String("&and;"), new Integer(8743) }, { new String("&ang;"), new Integer(8736) },
			{ new String("&Aring;"), new Integer(197) }, { new String("&aring;"), new Integer(229) },
			{ new String("&asymp;"), new Integer(8776) }, { new String("&Atilde;"), new Integer(195) },
			{ new String("&atilde;"), new Integer(227) }, { new String("&Auml;"), new Integer(196) },
			{ new String("&auml;"), new Integer(228) }, { new String("&bdquo;"), new Integer(8222) },
			{ new String("&Beta;"), new Integer(914) }, { new String("&beta;"), new Integer(946) },
			{ new String("&brvbar;"), new Integer(166) }, { new String("&bull;"), new Integer(8226) },
			{ new String("&cap;"), new Integer(8745) }, { new String("&Ccedil;"), new Integer(199) },
			{ new String("&ccedil;"), new Integer(231) }, { new String("&cedil;"), new Integer(184) },
			{ new String("&cent;"), new Integer(162) }, { new String("&Chi;"), new Integer(935) },
			{ new String("&chi;"), new Integer(967) }, { new String("&circ;"), new Integer(710) },
			{ new String("&clubs;"), new Integer(9827) }, { new String("&cong;"), new Integer(8773) },
			{ new String("&copy;"), new Integer(169) }, { new String("&crarr;"), new Integer(8629) },
			{ new String("&cup;"), new Integer(8746) }, { new String("&curren;"), new Integer(164) },
			{ new String("&dagger;"), new Integer(8224) }, { new String("&Dagger;"), new Integer(8225) },
			{ new String("&darr;"), new Integer(8595) }, { new String("&dArr;"), new Integer(8659) },
			{ new String("&deg;"), new Integer(176) }, { new String("&Delta;"), new Integer(916) },
			{ new String("&delta;"), new Integer(948) }, { new String("&diams;"), new Integer(9830) },
			{ new String("&divide;"), new Integer(247) }, { new String("&Eacute;"), new Integer(201) },
			{ new String("&eacute;"), new Integer(233) }, { new String("&Ecirc;"), new Integer(202) },
			{ new String("&ecirc;"), new Integer(234) }, { new String("&Egrave;"), new Integer(200) },
			{ new String("&egrave;"), new Integer(232) }, { new String("&empty;"), new Integer(8709) },
			{ new String("&emsp;"), new Integer(8195) }, { new String("&ensp;"), new Integer(8194) },
			{ new String("&Epsilon;"), new Integer(917) }, { new String("&epsilon;"), new Integer(949) },
			{ new String("&equiv;"), new Integer(8801) }, { new String("&Eta;"), new Integer(919) },
			{ new String("&eta;"), new Integer(951) }, { new String("&ETH;"), new Integer(208) },
			{ new String("&eth;"), new Integer(240) }, { new String("&Euml;"), new Integer(203) },
			{ new String("&euml;"), new Integer(235) }, { new String("&euro;"), new Integer(8364) },
			{ new String("&exist;"), new Integer(8707) }, { new String("&fnof;"), new Integer(402) },
			{ new String("&forall;"), new Integer(8704) }, { new String("&frac12;"), new Integer(189) },
			{ new String("&frac14;"), new Integer(188) }, { new String("&frac34;"), new Integer(190) },
			{ new String("&frasl;"), new Integer(8260) }, { new String("&Gamma;"), new Integer(915) },
			{ new String("&gamma;"), new Integer(947) }, { new String("&ge;"), new Integer(8805) },
			{ new String("&harr;"), new Integer(8596) }, { new String("&hArr;"), new Integer(8660) },
			{ new String("&hearts;"), new Integer(9829) }, { new String("&hellip;"), new Integer(8230) },
			{ new String("&Iacute;"), new Integer(205) }, { new String("&iacute;"), new Integer(237) },
			{ new String("&Icirc;"), new Integer(206) }, { new String("&icirc;"), new Integer(238) },
			{ new String("&iexcl;"), new Integer(161) }, { new String("&Igrave;"), new Integer(204) },
			{ new String("&igrave;"), new Integer(236) }, { new String("&image;"), new Integer(8465) },
			{ new String("&infin;"), new Integer(8734) }, { new String("&int;"), new Integer(8747) },
			{ new String("&Iota;"), new Integer(921) }, { new String("&iota;"), new Integer(953) },
			{ new String("&iquest;"), new Integer(191) }, { new String("&isin;"), new Integer(8712) },
			{ new String("&Iuml;"), new Integer(207) }, { new String("&iuml;"), new Integer(239) },
			{ new String("&Kappa;"), new Integer(922) }, { new String("&kappa;"), new Integer(954) },
			{ new String("&Lambda;"), new Integer(923) }, { new String("&lambda;"), new Integer(955) },
			{ new String("&lang;"), new Integer(9001) }, { new String("&laquo;"), new Integer(171) },
			{ new String("&larr;"), new Integer(8592) }, { new String("&lArr;"), new Integer(8656) },
			{ new String("&lceil;"), new Integer(8968) }, { new String("&ldquo;"), new Integer(8220) },
			{ new String("&le;"), new Integer(8804) }, { new String("&lfloor;"), new Integer(8970) },
			{ new String("&lowast;"), new Integer(8727) }, { new String("&loz;"), new Integer(9674) },
			{ new String("&lrm;"), new Integer(8206) }, { new String("&lsaquo;"), new Integer(8249) },
			{ new String("&lsquo;"), new Integer(8216) }, { new String("&macr;"), new Integer(175) },
			{ new String("&mdash;"), new Integer(8212) }, { new String("&micro;"), new Integer(181) },
			{ new String("&middot;"), new Integer(183) }, { new String("&minus;"), new Integer(8722) },
			{ new String("&Mu;"), new Integer(924) }, { new String("&mu;"), new Integer(956) },
			{ new String("&nabla;"), new Integer(8711) }, { new String("&nbsp;"), new Integer(160) },
			{ new String("&ndash;"), new Integer(8211) }, { new String("&ne;"), new Integer(8800) },
			{ new String("&ni;"), new Integer(8715) }, { new String("&not;"), new Integer(172) },
			{ new String("&notin;"), new Integer(8713) }, { new String("&nsub;"), new Integer(8836) },
			{ new String("&Ntilde;"), new Integer(209) }, { new String("&ntilde;"), new Integer(241) },
			{ new String("&Nu;"), new Integer(925) }, { new String("&nu;"), new Integer(957) },
			{ new String("&Oacute;"), new Integer(211) }, { new String("&oacute;"), new Integer(243) },
			{ new String("&Ocirc;"), new Integer(212) }, { new String("&ocirc;"), new Integer(244) },
			{ new String("&OElig;"), new Integer(338) }, { new String("&oelig;"), new Integer(339) },
			{ new String("&Ograve;"), new Integer(210) }, { new String("&ograve;"), new Integer(242) },
			{ new String("&oline;"), new Integer(8254) }, { new String("&Omega;"), new Integer(937) },
			{ new String("&omega;"), new Integer(969) }, { new String("&Omicron;"), new Integer(927) },
			{ new String("&omicron;"), new Integer(959) }, { new String("&oplus;"), new Integer(8853) },
			{ new String("&or;"), new Integer(8744) }, { new String("&ordf;"), new Integer(170) },
			{ new String("&ordm;"), new Integer(186) }, { new String("&Oslash;"), new Integer(216) },
			{ new String("&oslash;"), new Integer(248) }, { new String("&Otilde;"), new Integer(213) },
			{ new String("&otilde;"), new Integer(245) }, { new String("&otimes;"), new Integer(8855) },
			{ new String("&Ouml;"), new Integer(214) }, { new String("&ouml;"), new Integer(246) },
			{ new String("&para;"), new Integer(182) }, { new String("&part;"), new Integer(8706) },
			{ new String("&permil;"), new Integer(8240) }, { new String("&perp;"), new Integer(8869) },
			{ new String("&Phi;"), new Integer(934) }, { new String("&phi;"), new Integer(966) },
			{ new String("&Pi;"), new Integer(928) }, { new String("&pi;"), new Integer(960) },
			{ new String("&piv;"), new Integer(982) }, { new String("&plusmn;"), new Integer(177) },
			{ new String("&pound;"), new Integer(163) }, { new String("&prime;"), new Integer(8242) },
			{ new String("&Prime;"), new Integer(8243) }, { new String("&prod;"), new Integer(8719) },
			{ new String("&prop;"), new Integer(8733) }, { new String("&Psi;"), new Integer(936) },
			{ new String("&psi;"), new Integer(968) }, { new String("&radic;"), new Integer(8730) },
			{ new String("&rang;"), new Integer(9002) }, { new String("&raquo;"), new Integer(187) },
			{ new String("&rarr;"), new Integer(8594) }, { new String("&rArr;"), new Integer(8658) },
			{ new String("&rceil;"), new Integer(8969) }, { new String("&rdquo;"), new Integer(8221) },
			{ new String("&real;"), new Integer(8476) }, { new String("&reg;"), new Integer(174) },
			{ new String("&rfloor;"), new Integer(8971) }, { new String("&Rho;"), new Integer(929) },
			{ new String("&rho;"), new Integer(961) }, { new String("&rlm;"), new Integer(8207) },
			{ new String("&rsaquo;"), new Integer(8250) }, { new String("&rsquo;"), new Integer(8217) },
			{ new String("&sbquo;"), new Integer(8218) }, { new String("&Scaron;"), new Integer(352) },
			{ new String("&scaron;"), new Integer(353) }, { new String("&sdot;"), new Integer(8901) },
			{ new String("&sect;"), new Integer(167) }, { new String("&shy;"), new Integer(173) },
			{ new String("&Sigma;"), new Integer(931) }, { new String("&sigma;"), new Integer(963) },
			{ new String("&sigmaf;"), new Integer(962) }, { new String("&sim;"), new Integer(8764) },
			{ new String("&spades;"), new Integer(9824) }, { new String("&sub;"), new Integer(8834) },
			{ new String("&sube;"), new Integer(8838) }, { new String("&sum;"), new Integer(8721) },
			{ new String("&sup1;"), new Integer(185) }, { new String("&sup2;"), new Integer(178) },
			{ new String("&sup3;"), new Integer(179) }, { new String("&sup;"), new Integer(8835) },
			{ new String("&supe;"), new Integer(8839) }, { new String("&szlig;"), new Integer(223) },
			{ new String("&Tau;"), new Integer(932) }, { new String("&tau;"), new Integer(964) },
			{ new String("&there4;"), new Integer(8756) }, { new String("&Theta;"), new Integer(920) },
			{ new String("&theta;"), new Integer(952) }, { new String("&thetasym;"), new Integer(977) },
			{ new String("&thinsp;"), new Integer(8201) }, { new String("&THORN;"), new Integer(222) },
			{ new String("&thorn;"), new Integer(254) }, { new String("&tilde;"), new Integer(732) },
			{ new String("&times;"), new Integer(215) }, { new String("&trade;"), new Integer(8482) },
			{ new String("&Uacute;"), new Integer(218) }, { new String("&uacute;"), new Integer(250) },
			{ new String("&uarr;"), new Integer(8593) }, { new String("&uArr;"), new Integer(8657) },
			{ new String("&Ucirc;"), new Integer(219) }, { new String("&ucirc;"), new Integer(251) },
			{ new String("&Ugrave;"), new Integer(217) }, { new String("&ugrave;"), new Integer(249) },
			{ new String("&uml;"), new Integer(168) }, { new String("&upsih;"), new Integer(978) },
			{ new String("&Upsilon;"), new Integer(933) }, { new String("&upsilon;"), new Integer(965) },
			{ new String("&Uuml;"), new Integer(220) }, { new String("&uuml;"), new Integer(252) },
			{ new String("&weierp;"), new Integer(8472) }, { new String("&Xi;"), new Integer(926) },
			{ new String("&xi;"), new Integer(958) }, { new String("&Yacute;"), new Integer(221) },
			{ new String("&yacute;"), new Integer(253) }, { new String("&yen;"), new Integer(165) },
			{ new String("&yuml;"), new Integer(255) }, { new String("&Yuml;"), new Integer(376) },
			{ new String("&Zeta;"), new Integer(918) }, { new String("&zeta;"), new Integer(950) },
			{ new String("&zwj;"), new Integer(8205) }, { new String("&zwnj;"), new Integer(8204) } };

	/**
	 * Map to convert extended characters in html entities.
	 */
	private static final Hashtable htmlentities_map = new Hashtable();

	/**
	 * Map to convert html entities in exteden characters.
	 */
	private static final Hashtable unhtmlentities_map = new Hashtable();

	// ==============================================================================
	// METHODS
	// ==============================================================================

	/**
	 * Get the html entities translation table.
	 * 
	 * @return translation table
	 */
	public static Object[][] getEntitiesTable() {
		return html_entities_table;
	}

	/**
	 * Replace &amp; characters with &amp;amp; HTML entities.
	 * 
	 * @param str the input string
	 * @return string with replaced characters
	 */
	public static String htmlAmpersand(String str) {
		return str.replaceAll("&", "&amp;");
	}

	/**
	 * Replace &lt; &gt; characters with &amp;lt; &amp;gt; entities.
	 * 
	 * @param str the input string
	 * @return string with replaced characters
	 */
	public static String htmlAngleBrackets(String str) {
		str = str.replaceAll("<", "&lt;");
		str = str.replaceAll(">", "&gt;");
		return str;
	}

	/**
	 * Replace double quotes characters with HTML entities.
	 * 
	 * @param str the input string
	 * @return string with replaced double quotes
	 */
	public static String htmlDoubleQuotes(String str) {
		str = str.replaceAll("[\"]", "&quot;");
		str = str.replaceAll("&#147;", "&quot;");
		str = str.replaceAll("&#148;", "&quot;");
		return str;
	}

	/**
	 * Convert special and extended characters into HTML entitities.
	 * 
	 * @param str input string
	 * @return formatted string
	 * @see #unhtmlentities(String)
	 */
	public static String htmlentities(String str) {

		if (str == null) {
			return "";
		}
		// initialize html translation maps table the first time is called
		if (htmlentities_map.isEmpty()) {
			initializeEntitiesTables();
		}

		StringBuffer buf = new StringBuffer(); // the otput string buffer

		for (int i = 0; i < str.length(); ++i) {
			char ch = str.charAt(i);
			String entity = (String) htmlentities_map.get(new Integer((int) ch)); // get equivalent html entity
			if (entity == null) { // if entity has not been found
				if (((int) ch) > 128) { // check if is an extended character
					buf.append("&#" + ((int) ch) + ";"); // convert extended character
				} else {
					buf.append(ch); // append the character as is
				}
			} else {
				buf.append(entity); // append the html entity
			}
		}
		return buf.toString();
	}

	// methods to convert special characters

	/**
	 * Replace single and double quotes characters with HTML entities.
	 * 
	 * @param str the input string
	 * @return string with replaced quotes
	 */
	public static String htmlQuotes(String str) {
		str = htmlDoubleQuotes(str); // convert double quotes
		str = htmlSingleQuotes(str); // convert single quotes
		return str;
	}

	/**
	 * Replace single quotes characters with HTML entities.
	 * 
	 * @param str the input string
	 * @return string with replaced single quotes
	 */
	public static String htmlSingleQuotes(String str) {
		str = str.replaceAll("[\']", "&rsquo;");
		str = str.replaceAll("&#039;", "&rsquo;");
		str = str.replaceAll("&#145;", "&rsquo;");
		str = str.replaceAll("&#146;", "&rsquo;");
		return str;
	}

	/**
	 * Replace &amp;amp; HTML entities with &amp; characters.
	 * 
	 * @param str the input string
	 * @return string with replaced entities
	 */
	public static String unhtmlAmpersand(String str) {
		return str.replaceAll("&amp;", "&");
	}

	/**
	 * Replace &amp;lt; &amp;gt; entities with &lt; &gt; characters.
	 * 
	 * @param str the input string
	 * @return string with replaced entities
	 */
	public static String unhtmlAngleBrackets(String str) {
		str = str.replaceAll("&lt;", "<");
		str = str.replaceAll("&gt;", ">");
		return str;
	}

	/**
	 * Replace single quotes HTML entities with equivalent character.
	 * 
	 * @param str the input string
	 * @return string with replaced single quotes
	 */
	public static String unhtmlDoubleQuotes(String str) {
		return str.replaceAll("&quot;", "\"");
	}

	/**
	 * Convert HTML entities to special and extended unicode characters equivalents.
	 * 
	 * @param str input string
	 * @return formatted string
	 * @see #htmlentities(String)
	 */
	public static String unhtmlentities(String str) {

		// initialize html translation maps table the first time is called
		if (htmlentities_map.isEmpty()) {
			initializeEntitiesTables();
		}

		StringBuffer buf = new StringBuffer();

		for (int i = 0; i < str.length(); ++i) {
			char ch = str.charAt(i);
			if (ch == '&') {
				int semi = str.indexOf(';', i + 1);
				if ((semi == -1) || ((semi - i) > 7)) {
					buf.append(ch);
					continue;
				}
				String entity = str.substring(i, semi + 1);
				Integer iso;
				if (entity.charAt(1) == ' ') {
					buf.append(ch);
					continue;
				}
				if (entity.charAt(1) == '#') {
					if (entity.charAt(2) == 'x') {
						iso = new Integer(Integer.parseInt(entity.substring(3, entity.length() - 1), 16));
					} else {
						iso = new Integer(entity.substring(2, entity.length() - 1));
					}
				} else {
					iso = (Integer) unhtmlentities_map.get(entity);
				}
				if (iso == null) {
					buf.append(entity);
				} else {
					buf.append((char) (iso.intValue()));
				}
				i = semi;
			} else {
				buf.append(ch);
			}
		}
		return buf.toString();
	}

	/**
	 * Replace single and double quotes HTML entities with equivalent characters.
	 * 
	 * @param str the input string
	 * @return string with replaced quotes
	 */
	public static String unhtmlQuotes(String str) {
		str = unhtmlDoubleQuotes(str); // convert double quotes
		str = unhtmlSingleQuotes(str); // convert single quotes
		return str;
	}

	/**
	 * Replace single quotes HTML entities with equivalent character.
	 * 
	 * @param str the input string
	 * @return string with replaced single quotes
	 */
	public static String unhtmlSingleQuotes(String str) {
		return str.replaceAll("&rsquo;", "\'");
	}

	/**
	 * Initialize HTML entities table.
	 */
	private static void initializeEntitiesTables() {
		// initialize html translation maps
		for (int i = 0; i < html_entities_table.length; ++i) {
			htmlentities_map.put(html_entities_table[i][1], html_entities_table[i][0]);
			unhtmlentities_map.put(html_entities_table[i][0], html_entities_table[i][1]);
		}
	}

	/**
	 * Initialize HTML translation maps.
	 */
	public HTMLEntities() {
		initializeEntitiesTables();
	}

}/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package htmlparser;
public class HTMLFilter
{    

    public String filter(StringBuffer input)
    {
        return new String(privateHelpMethod(new String(input)));    
    }
    
    public String filter(String input)
    {
        return new String(privateHelpMethod(input));    
    }
    
    
    private  String privateHelpMethod(String input)
    {
        
        StringBuilder clean = new StringBuilder();
        boolean add = true;
        
        for(int i = 0 ; i < input.length() ; i++)
        {
            
            if(input.charAt(i) == '<')
                add = false;
            
            else if(input.charAt(i) == '>')
                add = true;    
                
            else if(add == true)
            {
                clean.append(input.charAt(i));    
            }
                                    
        }
            
        return clean.toString();
    }
    

}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package htmlparser;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import junit.framework.TestCase;

import org.junit.Assert;

/**
 * 
 * HTML filtering utility for protecting against XSS (Cross Site Scripting).
 * 
 * This code is licensed under a Creative Commons Attribution-ShareAlike 2.5 License
 * http://creativecommons.org/licenses/by-sa/2.5/
 * 
 * This code is a Java port of the original work in PHP by Cal Hendersen. http://code.iamcal.com/php/lib_filter/
 * 
 * The trickiest part of the translation was handling the differences in regex handling between PHP and Java. These
 * resources were helpful in the process:
 * 
 * http://java.sun.com/j2se/1.4.2/docs/api/java/util/regex/Pattern.html
 * http://us2.php.net/manual/en/reference.pcre.pattern.modifiers.php http://www.regular-expressions.info/modifiers.html
 * 
 * A note on naming conventions: instance variables are prefixed with a "v"; global constants are in all caps.
 * 
 * Sample use: String input = ... String clean = new HTMLInputFilter().filter( input );
 * 
 * If you find bugs or have suggestions on improvement (especially regarding perfomance), please contact me at the email
 * below. The latest version of this source can be found at
 * 
 * http://josephoconnell.com/java/xss-html-filter/
 * 
 * @author Joseph O'Connell <joe.oconnell at gmail dot com>
 * @version 1.0
 */
public class HTMLInputFilter {
	// ============================================ START-UNIT-TEST =========================================
	public static class Test
			extends TestCase {
		protected HTMLInputFilter vFilter;

		public void test_attributes() {
			t("<img src=foo>", "<img src=\"foo\" />");
			t("<img asrc=foo>", "<img />");
			t("<img src=test test>", "<img src=\"test\" />");
		}

		public void test_balancing_angle_brackets() {
			if (ALWAYS_MAKE_TAGS) {
				t("<img src=\"foo\"", "<img src=\"foo\" />");
				t("i>", "");
				t("<img src=\"foo\"/", "<img src=\"foo\" />");
				t(">", "");
				t("foo<b", "foo");
				t("b>foo", "<b>foo</b>");
				t("><b", "");
				t("b><", "");
				t("><b>", "");
			} else {
				t("<img src=\"foo\"", "&lt;img src=\"foo\"");
				t("b>", "b&gt;");
				t("<img src=\"foo\"/", "&lt;img src=\"foo\"/");
				t(">", "&gt;");
				t("foo<b", "foo&lt;b");
				t("b>foo", "b&gt;foo");
				t("><b", "&gt;&lt;b");
				t("b><", "b&gt;&lt;");
				t("><b>", "&gt;");
			}
		}

		public void test_balancing_tags() {
			t("<b>hello", "<b>hello</b>");
			t("<b>hello", "<b>hello</b>");
			t("hello<b>", "hello");
			t("hello</b>", "hello");
			t("hello<b/>", "hello");
			t("<b><b><b>hello", "<b><b><b>hello</b></b></b>");
			t("</b><b>", "");
		}

		public void test_basics() {
			t("", "");
			t("hello", "hello");
		}

		public void test_comments() {
			if (STRIP_COMMENTS) {
				t("<!-- a<b --->", "");
			} else {
				t("<!-- a<b --->", "<!-- a&lt;b --->");
			}
		}

		public void test_disallow_script_tags() {
			t("<script>", "");
			if (ALWAYS_MAKE_TAGS) {
				t("<script", "");
			} else {
				t("<script", "&lt;script");
			}
			t("<script/>", "");
			t("</script>", "");
			t("<script woo=yay>", "");
			t("<script woo=\"yay\">", "");
			t("<script woo=\"yay>", "");
			t("<script woo=\"yay<b>", "");
			t("<script<script>>", "");
			t("<<script>script<script>>", "script");
			t("<<script><script>>", "");
			t("<<script>script>>", "");
			t("<<script<script>>", "");
		}

		public void test_end_slashes() {
			t("<img>", "<img />");
			t("<img/>", "<img />");
			t("<b/></b>", "");
		}

		public void test_protocols() {
			t("<a href=\"http://foo\">bar</a>", "<a href=\"http://foo\">bar</a>");
			// we don't allow ftp. t("<a href=\"ftp://foo\">bar</a>", "<a href=\"ftp://foo\">bar</a>");
			t("<a href=\"mailto:foo\">bar</a>", "<a href=\"mailto:foo\">bar</a>");
			t("<a href=\"javascript:foo\">bar</a>", "<a href=\"#foo\">bar</a>");
			t("<a href=\"java script:foo\">bar</a>", "<a href=\"#foo\">bar</a>");
			t("<a href=\"java\tscript:foo\">bar</a>", "<a href=\"#foo\">bar</a>");
			t("<a href=\"java\nscript:foo\">bar</a>", "<a href=\"#foo\">bar</a>");
			t("<a href=\"java" + chr(1) + "script:foo\">bar</a>", "<a href=\"#foo\">bar</a>");
			t("<a href=\"jscript:foo\">bar</a>", "<a href=\"#foo\">bar</a>");
			t("<a href=\"vbscript:foo\">bar</a>", "<a href=\"#foo\">bar</a>");
			t("<a href=\"view-source:foo\">bar</a>", "<a href=\"#foo\">bar</a>");
		}

		public void test_self_closing_tags() {
			t("<img src=\"a\">", "<img src=\"a\" />");
			t("<img src=\"a\">foo</img>", "<img src=\"a\" />foo");
			t("</img>", "");
		}

		private void t(String input, String result) {
			Assert.assertEquals(result, this.vFilter.filter(input, new boolean[] { true, true, true, true, true, }));
		}

		protected void setUp() {
			this.vFilter = new HTMLInputFilter(true);
		}

		protected void tearDown() {
			this.vFilter = null;
		}

	}

	// ============================================ END-UNIT-TEST ===========================================

	/**
	 * flag determining whether to try to make tags when presented with "unbalanced" angle brackets (e.g. "<b text </b>"
	 * becomes "<b> text </b>"). If set to false, unbalanced angle brackets will be html escaped.
	 */
	protected static final boolean ALWAYS_MAKE_TAGS = true;

	/**
	 * flag determing whether comments are allowed in input String.
	 */
	protected static final boolean STRIP_COMMENTS = true;

	/** regex flag union representing /si modifiers in php **/
	protected static final int REGEX_FLAGS_SI = Pattern.CASE_INSENSITIVE | Pattern.DOTALL;

	public static String chr(int decimal) {
		return String.valueOf((char) decimal);
	}

	public static String htmlSpecialChars(String s) {
		s = s.replaceAll("&", "&amp;");
		s = s.replaceAll("\"", "&quot;");
		s = s.replaceAll("<", "&lt;");
		s = s.replaceAll(">", "&gt;");
		return s;
	}

	/** set of allowed html elements, along with allowed attributes for each element **/
	protected Map<String, List<String>> vAllowed;

	/** counts of open tags for each (allowable) html element **/
	protected Map<String, Integer> vTagCounts;

	/** html elements which must always be self-closing (e.g. "<img />") **/
	protected String[] vSelfClosingTags;

	/** html elements which must always have separate opening and closing tags (e.g. "<b></b>") **/
	protected String[] vNeedClosingTags;

	/** attributes which should be checked for valid protocols **/
	protected String[] vProtocolAtts;

	/** allowed protocols **/
	protected String[] vAllowedProtocols;

	/** tags which should be removed if they contain no content (e.g. "<b></b>" or "<b />") **/
	protected String[] vRemoveBlanks;

	/** entities allowed within html markup **/
	protected String[] vAllowedEntities;

	protected boolean vDebug;

	public HTMLInputFilter() {
		this(false);
	}

	// ---------------------------------------------------------------
	// my versions of some PHP library functions

	public HTMLInputFilter(boolean debug) {
		this.vDebug = debug;

		this.vAllowed = new HashMap<String, List<String>>();
		this.vTagCounts = new HashMap<String, Integer>();

		ArrayList<String> a_atts = new ArrayList<String>();
		a_atts.add("href");
		a_atts.add("target");
		this.vAllowed.put("a", a_atts);

		ArrayList<String> img_atts = new ArrayList<String>();
		img_atts.add("src");
		img_atts.add("width");
		img_atts.add("height");
		img_atts.add("alt");
		this.vAllowed.put("img", img_atts);

		ArrayList<String> no_atts = new ArrayList<String>();
		this.vAllowed.put("b", no_atts);
		this.vAllowed.put("strong", no_atts);
		this.vAllowed.put("i", no_atts);
		this.vAllowed.put("em", no_atts);

		this.vSelfClosingTags = new String[] { "img" };
		this.vNeedClosingTags = new String[] { "a", "b", "strong", "i", "em" };
		this.vAllowedProtocols = new String[] { "http", "mailto" }; // no ftp.
		this.vProtocolAtts = new String[] { "src", "href" };
		this.vRemoveBlanks = new String[] { "a", "b", "strong", "i", "em" };
		this.vAllowedEntities = new String[] { "amp", "gt", "lt", "quot" };
	}

	/**
	 * given a user submitted input String, filter out any invalid or restricted html.
	 * 
	 * @param input text (i.e. submitted by a user) than may contain html
	 * @return "clean" version of input, with only valid, whitelisted html elements allowed
	 */
	public synchronized String filter(String input, boolean[] methods) {
		reset();
		String s = input;

		debug("************************************************");
		debug("              INPUT: " + input);

		if (methods[0]) {
			s = escapeComments(s);
			debug("     escapeComments: " + s);
		}
		if (methods[1]) {
			s = balanceHTML(s);
			debug("        balanceHTML: " + s);
		}
		if (methods[2]) {
			s = checkTags(s);
			debug("          checkTags: " + s);
		}
		if (methods[3]) {
			s = processRemoveBlanks(s);
			debug("processRemoveBlanks: " + s);
		}
		if (methods[4]) {
			s = validateEntities(s);
			debug("    validateEntites: " + s);
		}

		debug("************************************************\n\n");
		return s;
	}

	// ---------------------------------------------------------------

	private boolean inArray(String s, String[] array) {
		for (String item : array) {
			if ((item != null) && item.equals(s)) {
				return true;
			}
		}

		return false;
	}

	protected String balanceHTML(String s) {
		if (ALWAYS_MAKE_TAGS) {
			//
			// try and form html
			//
			s = regexReplace("^>", "", s);
			s = regexReplace("<([^>]*?)(?=<|$)", "<$1>", s);
			s = regexReplace("(^|>)([^<]*?)(?=>)", "$1<$2", s);

		} else {
			//
			// escape stray brackets
			//
			s = regexReplace("<([^>]*?)(?=<|$)", "&lt;$1", s);
			s = regexReplace("(^|>)([^<]*?)(?=>)", "$1$2&gt;<", s);

			//
			// the last regexp causes '<>' entities to appear
			// (we need to do a lookahead assertion so that the last bracket can
			// be used in the next pass of the regexp)
			//
			s = s.replaceAll("<>", "");
		}

		return s;
	}

	protected String checkEntity(String preamble, String term) {
		if (!term.equals(";")) {
			return "&amp;" + preamble;
		}

		if (isValidEntity(preamble)) {
			return "&" + preamble;
		}

		return "&amp;" + preamble;
	}

	protected String checkTags(String s) {
		Pattern p = Pattern.compile("<(.*?)>", Pattern.DOTALL);
		Matcher m = p.matcher(s);

		StringBuffer buf = new StringBuffer();
		while (m.find()) {
			String replaceStr = m.group(1);
			replaceStr = processTag(replaceStr);
			m.appendReplacement(buf, replaceStr);
		}
		m.appendTail(buf);

		s = buf.toString();

		// these get tallied in processTag
		// (remember to reset before subsequent calls to filter method)
		for (String key : this.vTagCounts.keySet()) {
			for (int ii = 0; ii < this.vTagCounts.get(key); ii++) {
				s += "</" + key + ">";
			}
		}

		return s;
	}

	protected void debug(String msg) {
		if (this.vDebug) {
			System.out.println(msg);
		}
	}

	protected String decodeEntities(String s) {
		StringBuffer buf = new StringBuffer();

		Pattern p = Pattern.compile("&#(\\d+);?");
		Matcher m = p.matcher(s);
		while (m.find()) {
			String match = m.group(1);
			int decimal = Integer.decode(match).intValue();
			m.appendReplacement(buf, chr(decimal));
		}
		m.appendTail(buf);
		s = buf.toString();

		buf = new StringBuffer();
		p = Pattern.compile("&#x([0-9a-f]+);?");
		m = p.matcher(s);
		while (m.find()) {
			String match = m.group(1);
			int decimal = Integer.decode(match).intValue();
			m.appendReplacement(buf, chr(decimal));
		}
		m.appendTail(buf);
		s = buf.toString();

		buf = new StringBuffer();
		p = Pattern.compile("%([0-9a-f]{2});?");
		m = p.matcher(s);
		while (m.find()) {
			String match = m.group(1);
			int decimal = Integer.decode(match).intValue();
			m.appendReplacement(buf, chr(decimal));
		}
		m.appendTail(buf);
		s = buf.toString();

		s = validateEntities(s);
		return s;
	}

	protected String escapeComments(String s) {
		Pattern p = Pattern.compile("<!--(.*?)-->", Pattern.DOTALL);
		Matcher m = p.matcher(s);
		StringBuffer buf = new StringBuffer();
		if (m.find()) {
			String match = m.group(1); // (.*?)
			m.appendReplacement(buf, "<!--" + htmlSpecialChars(match) + "-->");
		}
		m.appendTail(buf);

		return buf.toString();
	}

	protected boolean isValidEntity(String entity) {
		return inArray(entity, this.vAllowedEntities);
	}

	protected String processParamProtocol(String s) {
		s = decodeEntities(s);
		Pattern p = Pattern.compile("^([^:]+):", REGEX_FLAGS_SI);
		Matcher m = p.matcher(s);
		if (m.find()) {
			String protocol = m.group(1);
			if (!inArray(protocol, this.vAllowedProtocols)) {
				// bad protocol, turn into local anchor link instead
				s = "#" + s.substring(protocol.length() + 1, s.length());
				if (s.startsWith("#//")) {
					s = "#" + s.substring(3, s.length());
				}
			}
		}

		return s;
	}

	protected String processRemoveBlanks(String s) {
		for (String tag : this.vRemoveBlanks) {
			s = regexReplace("<" + tag + "(\\s[^>]*)?></" + tag + ">", "", s);
			s = regexReplace("<" + tag + "(\\s[^>]*)?/>", "", s);
		}

		return s;
	}

	protected String processTag(String s) {
		// ending tags
		Pattern p = Pattern.compile("^/([a-z0-9]+)", REGEX_FLAGS_SI);
		Matcher m = p.matcher(s);
		if (m.find()) {
			String name = m.group(1).toLowerCase();
			if (this.vAllowed.containsKey(name)) {
				if (!inArray(name, this.vSelfClosingTags)) {
					if (this.vTagCounts.containsKey(name)) {
						this.vTagCounts.put(name, this.vTagCounts.get(name) - 1);
						return "</" + name + ">";
					}
				}
			}
		}

		// starting tags
		p = Pattern.compile("^([a-z0-9]+)(.*?)(/?)$", REGEX_FLAGS_SI);
		m = p.matcher(s);
		if (m.find()) {
			String name = m.group(1).toLowerCase();
			String body = m.group(2);
			String ending = m.group(3);

			// debug( "in a starting tag, name='" + name + "'; body='" + body + "'; ending='" + ending + "'" );
			if (this.vAllowed.containsKey(name)) {
				String params = "";

				Pattern p2 = Pattern.compile("([a-z0-9]+)=([\"'])(.*?)\\2", REGEX_FLAGS_SI);
				Pattern p3 = Pattern.compile("([a-z0-9]+)(=)([^\"\\s']+)", REGEX_FLAGS_SI);
				Matcher m2 = p2.matcher(body);
				Matcher m3 = p3.matcher(body);
				List<String> paramNames = new ArrayList<String>();
				List<String> paramValues = new ArrayList<String>();
				while (m2.find()) {
					paramNames.add(m2.group(1)); // ([a-z0-9]+)
					paramValues.add(m2.group(3)); // (.*?)
				}
				while (m3.find()) {
					paramNames.add(m3.group(1)); // ([a-z0-9]+)
					paramValues.add(m3.group(3)); // ([^\"\\s']+)
				}

				String paramName, paramValue;
				for (int ii = 0; ii < paramNames.size(); ii++) {
					paramName = paramNames.get(ii).toLowerCase();
					paramValue = paramValues.get(ii);

					// debug( "paramName='" + paramName + "'" );
					// debug( "paramValue='" + paramValue + "'" );
					// debug( "allowed? " + vAllowed.get( name ).contains( paramName ) );

					if (this.vAllowed.get(name).contains(paramName)) {
						if (inArray(paramName, this.vProtocolAtts)) {
							paramValue = processParamProtocol(paramValue);
						}
						params += " " + paramName + "=\"" + paramValue + "\"";
					}
				}

				if (inArray(name, this.vSelfClosingTags)) {
					ending = " /";
				}

				if (inArray(name, this.vNeedClosingTags)) {
					ending = "";
				}

				if ((ending == null) || (ending.length() < 1)) {
					if (this.vTagCounts.containsKey(name)) {
						this.vTagCounts.put(name, this.vTagCounts.get(name) + 1);
					} else {
						this.vTagCounts.put(name, 1);
					}
				} else {
					ending = " /";
				}
				return "<" + name + params + ending + ">";
			} else {
				return "";
			}
		}

		// comments
		p = Pattern.compile("^!--(.*)--$", REGEX_FLAGS_SI);
		m = p.matcher(s);
		if (m.find()) {
			String comment = m.group();
			if (STRIP_COMMENTS) {
				return "";
			} else {
				return "<" + comment + ">";
			}
		}

		return "";
	}

	protected String regexReplace(String regex_pattern, String replacement, String s) {
		Pattern p = Pattern.compile(regex_pattern);
		Matcher m = p.matcher(s);
		return m.replaceAll(replacement);
	}

	protected void reset() {
		this.vTagCounts = new HashMap<String, Integer>();
	}

	protected String validateEntities(String s) {
		// validate entities throughout the string
		Pattern p = Pattern.compile("&([^&;]*)(?=(;|&|$))");
		Matcher m = p.matcher(s);
		if (m.find()) {
			String one = m.group(1); // ([^&;]*)
			String two = m.group(2); // (?=(;|&|$))
			s = checkEntity(one, two);
		}

		// validate quotes outside of tags
		p = Pattern.compile("(>|^)([^<]+?)(<|$)", Pattern.DOTALL);
		m = p.matcher(s);
		StringBuffer buf = new StringBuffer();
		if (m.find()) {
			String one = m.group(1); // (>|^)
			String two = m.group(2); // ([^<]+?)
			String three = m.group(3); // (<|$)
			m.appendReplacement(buf, one + two.replaceAll("\"", "&quot;") + three);
		}
		m.appendTail(buf);

		return s;
	}
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder;

import java.awt.Color;
import java.io.File;

import javax.swing.border.EmptyBorder;

public class Constants {

	public static final String CONFIG_SPECIAL_CONFIG = "specialConfig";
	public static final String CONFIG_IGNORE_LAST_LINES = "ignoreLastLines";
	public static final String CONFIG_IGNORE_FIRST_LINES = "ignoreFirstLines";
	public static final String CONFIG_LANGUAGE = "language";
	public static final String CONFIG_KEYBOARD_LAYOUT = "keyboardLayout";
	public static final String CONFIG_SHOW_RESULT = "showResult";
	public static final String CONFIG_SORT_RESULT = "sortResult";
	public static final String CONFIG_CASE_SENSITIV = "caseSensitiv";
	public static final String CONFIG_LETTERS = "letters";
	public static final String CONFIG_FILE_TYPES = "fileTypes";
	public static final String CONFIG_DIRECTORY_PATH = "directoryPath";
	public static final String CONFIG_SEPARATOR = " = ";
	public static final String SETTINGS_PATH = System.getProperty("user.dir") + File.separatorChar + "settings.ini";
	public static final String PATH_FILE_ENDING = ".path";
	public static final String USER_DEFINED_FILTER_FILENAME = "userDefinedFilter.udf";

	public static final String LABEL_SEPARATOR = ":";
	public static final String LEERZEICHEN = " ";
	public static final String LINESEPARATOR = System.getProperty("line.separator");

	public static final EmptyBorder EMPTY_BORDER = new EmptyBorder(0, 2, 0, 2);
	public static final Color DISABLE_COMPONENT_COLOR = Color.LIGHT_GRAY;
	public static final Color CELL_SELECTION_COLOR = new Color(211, 236, 249);

	/** Diese Konstante beinhaltet alle Buchstaben, die ein Wort erzeugen */
	public static final Character[] CHARS_OF_WORD = { 'Ä', 'Ö', 'Ü', 'ä', 'ö', 'ü', 'ß', //
			'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', //
			'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', //
			'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', //
			'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', //
			'1', '2', '3', '4', '5', '6', '7', '8', '9', '0', };
}
/**
 * This file is part of WordFinder.
 *
 *     WordFinder is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     WordFinder is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with WordFinder.  If not, see <http://www.gnu.org/licenses/>.
 *
 *     Author:
 *       - Thomas Gattinger  (programmer)
 *       - Michael Gattinger (designer & testing)
 */

package wordfinder;

public class Insets {

	/** Normaler Abstand zum nächsten Element in Pixel */
	public static int DISTANCE = 5;

	public static java.awt.Insets ALL = new java.awt.Insets(DISTANCE, DISTANCE, DISTANCE, DISTANCE);
	public static java.awt.Insets NONE = new java.awt.Insets(0, 0, 0, 0);

	public static java.awt.Insets TOP = new java.awt.Insets(DISTANCE, 0, 0, 0);
	public static java.awt.Insets TOP_LEFT = new java.awt.Insets(DISTANCE, DISTANCE, 0, 0);
	public static java.awt.Insets TOP_LEFT_BOTTOM = new java.awt.Insets(DISTANCE, DISTANCE, DISTANCE, 0);
	public static java.awt.Insets TOP_LEFT_RIGHT = new java.awt.Insets(DISTANCE, DISTANCE, 0, DISTANCE);
	public static java.awt.Insets TOP_BOTTOM = new java.awt.Insets(DISTANCE, 0, DISTANCE, 0);
	public static java.awt.Insets TOP_BOTTOM_RIGHT = new java.awt.Insets(DISTANCE, 0, DISTANCE, DISTANCE);
	public static java.awt.Insets TOP_RIGHT = new java.awt.Insets(DISTANCE, 0, 0, DISTANCE);
	public static java.awt.Insets LEFT = new java.awt.Insets(0, DISTANCE, 0, 0);
	public static java.awt.Insets LEFT_BOTTOM = new java.awt.Insets(0, DISTANCE, DISTANCE, 0);
	public static java.awt.Insets LEFT_BOTTOM_RIGHT = new java.awt.Insets(0, DISTANCE, DISTANCE, DISTANCE);
	public static java.awt.Insets BOTTOM = new java.awt.Insets(0, 0, DISTANCE, 0);
	public static java.awt.Insets BOTTOM_RIGHT = new java.awt.Insets(0, 0, DISTANCE, DISTANCE);
	public static java.awt.Insets RIGHT = new java.awt.Insets(0, 0, 0, DISTANCE);

}
